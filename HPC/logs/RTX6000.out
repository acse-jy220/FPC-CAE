{'vtu_dir': '/rds/general/user/jy220/home/slugflow/', 'data_dir': '/rds/general/user/jy220/home/tensors/', 'data_type': 'tensors', 'vtu_fields': 'Component1::ComponentMassFractionPhase1, phase1::Velocity', 'variational': 'False', 'structured': 'False', 'changing_lr': 'False', 'self_concat': '2', 'dimension': '3', 'sfc_nums': '3', 'components': '4', 'nearest_neighbouring': 'True', 'dims_latent': '128', 'activation': 'Tanh', 'sfc_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/sfcs.pt', 'inv_sfc_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/inv_sfcs.pt', 'coords_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/coords.pt', 'cells_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/cells.npy', 'state_load': '/rds/general/user/jy220/home/results/Variational_False_Changelr_False_Latent_128_Nearest_neighbouring_True_SFC_nums_3_startlr_0.0005_n_epoches_50_dict.pth', 'batch_size': '16', 'lr': '0.0005', 'n_epoches': '150', 'seed': '15', 'visualize': 'False', 'reconstructed_path': 'None', 'reconstruct_start_index': '0', 'reconstruct_end_index': '50', 'save_path': '/rds/general/user/jy220/home/results/'}
[[ 2.50825269e-01 -1.67256990e-02 -1.22268730e-03]
 [ 2.38611959e-01 -5.50627777e-03 -5.06451913e-03]
 [ 2.39454874e-01 -1.59563257e-02  3.96778323e-03]
 ...
 [ 9.11361096e+00 -2.21908381e-02  2.22384902e-03]
 [ 9.11847310e+00 -2.84346931e-02 -2.34839590e-04]
 [ 9.11039661e+00 -2.51498573e-02 -8.70661044e-03]]
{'tetra': array([[      0,       1,       2,       3],
       [      4,       5,       6,       7],
       [      8,       9,      10,      11],
       ...,
       [1342744, 1342745, 1342746, 1342747],
       [1342748, 1342749, 1342750, 1342751],
       [1342752, 1342753, 1342754, 1342755]])}
structured  False 
 activation  Tanh() 
 self concat  2 
 sfc_nums  3 

dims_latent  128 
 components  4 
 nearest_neighbouring  True 

visualize  False 
 sample number  1706 

reading sfc nums......
[array([  41684,   41685,   41674, ..., 1235560, 1235564, 1235558]), array([954289, 954290, 954291, ..., 420685, 420687, 420688]), array([  40156,   40155,   40158, ..., 1123478, 1123475, 1123467])]
reading inverse sfc nums......
[array([   8146,    8145,    8144, ..., 1274444, 1274445, 1273166]), array([547280, 547283, 593037, ..., 933933, 933932, 864488]), array([ 66437,  66439, 122512, ..., 459416, 415130, 459418])]
Computing min and max......

tk:  tensor([0.7581, 0.0418, 0.1840, 0.1464], dtype=torch.float64) 

tb:  tensor([-0.3816, -0.3618, -0.0308,  0.1446], dtype=torch.float64) 

length of train set: 1505 

length of valid set: 100 

length of test set: 101 

torch device num: 4 

Let's use 4 GPUs!
epoch 50 starting......
Epoch:  50 | train loss: 1.164008e-02 | valid loss: 9.872334e-03 
      	| train loss (relative): 2.843600e-01 | valid loss (relative): 2.411056e-01 
Epoch 50 use: 545.95 second.

epoch 51 starting......
Epoch:  51 | train loss: 1.160241e-02 | valid loss: 9.905815e-03 
      	| train loss (relative): 2.835111e-01 | valid loss (relative): 2.434397e-01 
Epoch 51 use: 507.30 second.

epoch 52 starting......
Epoch:  52 | train loss: 1.160069e-02 | valid loss: 9.879783e-03 
      	| train loss (relative): 2.843736e-01 | valid loss (relative): 2.420516e-01 
Epoch 52 use: 496.82 second.

epoch 53 starting......
Epoch:  53 | train loss: 1.160500e-02 | valid loss: 9.913016e-03 
      	| train loss (relative): 2.836837e-01 | valid loss (relative): 2.432368e-01 
Epoch 53 use: 484.80 second.

epoch 54 starting......
Epoch:  54 | train loss: 1.160748e-02 | valid loss: 9.882590e-03 
      	| train loss (relative): 2.841245e-01 | valid loss (relative): 2.446340e-01 
Epoch 54 use: 512.45 second.

epoch 55 starting......
Epoch:  55 | train loss: 1.160502e-02 | valid loss: 9.911245e-03 
      	| train loss (relative): 2.828428e-01 | valid loss (relative): 2.429838e-01 
Epoch 55 use: 489.82 second.

epoch 56 starting......
Epoch:  56 | train loss: 1.160850e-02 | valid loss: 9.905271e-03 
      	| train loss (relative): 2.847849e-01 | valid loss (relative): 2.428499e-01 
Epoch 56 use: 482.30 second.

epoch 57 starting......
Epoch:  57 | train loss: 1.160378e-02 | valid loss: 9.889974e-03 
      	| train loss (relative): 2.840075e-01 | valid loss (relative): 2.434020e-01 
Epoch 57 use: 482.78 second.

epoch 58 starting......
Epoch:  58 | train loss: 1.160556e-02 | valid loss: 9.904007e-03 
      	| train loss (relative): 2.846777e-01 | valid loss (relative): 2.435427e-01 
Epoch 58 use: 477.56 second.

epoch 59 starting......
Epoch:  59 | train loss: 1.160848e-02 | valid loss: 9.892579e-03 
      	| train loss (relative): 2.836697e-01 | valid loss (relative): 2.435354e-01 
Epoch 59 use: 476.42 second.

epoch 60 starting......
Epoch:  60 | train loss: 1.160037e-02 | valid loss: 9.929671e-03 
      	| train loss (relative): 2.830017e-01 | valid loss (relative): 2.462121e-01 
Epoch 60 use: 518.38 second.

epoch 61 starting......
Epoch:  61 | train loss: 1.160933e-02 | valid loss: 9.927348e-03 
      	| train loss (relative): 2.860846e-01 | valid loss (relative): 2.449650e-01 
Epoch 61 use: 504.78 second.

epoch 62 starting......
Epoch:  62 | train loss: 1.160208e-02 | valid loss: 9.890649e-03 
      	| train loss (relative): 2.842200e-01 | valid loss (relative): 2.425349e-01 
Epoch 62 use: 506.97 second.

epoch 63 starting......
Epoch:  63 | train loss: 1.160489e-02 | valid loss: 9.897384e-03 
      	| train loss (relative): 2.853467e-01 | valid loss (relative): 2.439850e-01 
Epoch 63 use: 488.19 second.

epoch 64 starting......
Epoch:  64 | train loss: 1.160231e-02 | valid loss: 9.918285e-03 
      	| train loss (relative): 2.855560e-01 | valid loss (relative): 2.452202e-01 
Epoch 64 use: 491.51 second.

epoch 65 starting......
Epoch:  65 | train loss: 1.160764e-02 | valid loss: 9.909170e-03 
      	| train loss (relative): 2.860315e-01 | valid loss (relative): 2.436105e-01 
Epoch 65 use: 504.26 second.

epoch 66 starting......
Epoch:  66 | train loss: 1.160328e-02 | valid loss: 9.903897e-03 
      	| train loss (relative): 2.847628e-01 | valid loss (relative): 2.440945e-01 
Epoch 66 use: 521.88 second.

epoch 67 starting......
Epoch:  67 | train loss: 1.160547e-02 | valid loss: 9.877431e-03 
      	| train loss (relative): 2.862321e-01 | valid loss (relative): 2.404681e-01 
Epoch 67 use: 494.97 second.

epoch 68 starting......
Epoch:  68 | train loss: 1.160153e-02 | valid loss: 9.891091e-03 
      	| train loss (relative): 2.829952e-01 | valid loss (relative): 2.426296e-01 
Epoch 68 use: 506.62 second.

epoch 69 starting......
Epoch:  69 | train loss: 1.160002e-02 | valid loss: 9.885763e-03 
      	| train loss (relative): 2.838429e-01 | valid loss (relative): 2.432730e-01 
Epoch 69 use: 477.06 second.

epoch 70 starting......
Epoch:  70 | train loss: 1.160329e-02 | valid loss: 9.875518e-03 
      	| train loss (relative): 2.853018e-01 | valid loss (relative): 2.423444e-01 
Epoch 70 use: 510.83 second.

epoch 71 starting......
Epoch:  71 | train loss: 1.160536e-02 | valid loss: 9.885962e-03 
      	| train loss (relative): 2.858835e-01 | valid loss (relative): 2.419334e-01 
Epoch 71 use: 504.07 second.

epoch 72 starting......
Epoch:  72 | train loss: 1.160072e-02 | valid loss: 9.903855e-03 
      	| train loss (relative): 2.836067e-01 | valid loss (relative): 2.444352e-01 
Epoch 72 use: 516.92 second.

epoch 73 starting......
Epoch:  73 | train loss: 1.160308e-02 | valid loss: 9.920893e-03 
      	| train loss (relative): 2.850076e-01 | valid loss (relative): 2.453970e-01 
Epoch 73 use: 503.86 second.

epoch 74 starting......
Epoch:  74 | train loss: 1.160921e-02 | valid loss: 9.893176e-03 
      	| train loss (relative): 2.861611e-01 | valid loss (relative): 2.429294e-01 
Epoch 74 use: 507.39 second.

epoch 75 starting......
Epoch:  75 | train loss: 1.160182e-02 | valid loss: 9.906872e-03 
      	| train loss (relative): 2.853041e-01 | valid loss (relative): 2.430179e-01 
Epoch 75 use: 509.29 second.

epoch 76 starting......
Epoch:  76 | train loss: 1.160097e-02 | valid loss: 9.888707e-03 
      	| train loss (relative): 2.837707e-01 | valid loss (relative): 2.425497e-01 
Epoch 76 use: 506.85 second.

epoch 77 starting......
Epoch:  77 | train loss: 1.160653e-02 | valid loss: 9.879716e-03 
      	| train loss (relative): 2.855901e-01 | valid loss (relative): 2.415403e-01 
Epoch 77 use: 503.89 second.

epoch 78 starting......
Epoch:  78 | train loss: 1.160166e-02 | valid loss: 9.914804e-03 
      	| train loss (relative): 2.839071e-01 | valid loss (relative): 2.440637e-01 
Epoch 78 use: 508.00 second.

epoch 79 starting......
Epoch:  79 | train loss: 1.160338e-02 | valid loss: 9.901345e-03 
      	| train loss (relative): 2.859636e-01 | valid loss (relative): 2.419115e-01 
Epoch 79 use: 505.66 second.

epoch 80 starting......
Epoch:  80 | train loss: 1.160340e-02 | valid loss: 9.881831e-03 
      	| train loss (relative): 2.846848e-01 | valid loss (relative): 2.404787e-01 
Epoch 80 use: 499.31 second.

epoch 81 starting......
Epoch:  81 | train loss: 1.160233e-02 | valid loss: 9.885026e-03 
      	| train loss (relative): 2.839660e-01 | valid loss (relative): 2.441382e-01 
Epoch 81 use: 515.68 second.

epoch 82 starting......
Epoch:  82 | train loss: 1.160288e-02 | valid loss: 9.894097e-03 
      	| train loss (relative): 2.852849e-01 | valid loss (relative): 2.433301e-01 
Epoch 82 use: 503.46 second.

epoch 83 starting......
Epoch:  83 | train loss: 1.160076e-02 | valid loss: 9.867487e-03 
      	| train loss (relative): 2.849738e-01 | valid loss (relative): 2.406821e-01 
Epoch 83 use: 507.81 second.

epoch 84 starting......
Epoch:  84 | train loss: 1.160627e-02 | valid loss: 9.892468e-03 
      	| train loss (relative): 2.832861e-01 | valid loss (relative): 2.421836e-01 
Epoch 84 use: 529.98 second.

epoch 85 starting......
Epoch:  85 | train loss: 1.160489e-02 | valid loss: 9.877737e-03 
      	| train loss (relative): 2.842206e-01 | valid loss (relative): 2.410124e-01 
Epoch 85 use: 509.07 second.

epoch 86 starting......
Epoch:  86 | train loss: 1.160424e-02 | valid loss: 9.900080e-03 
      	| train loss (relative): 2.842715e-01 | valid loss (relative): 2.424016e-01 
Epoch 86 use: 505.60 second.

epoch 87 starting......
Epoch:  87 | train loss: 1.160166e-02 | valid loss: 9.885164e-03 
      	| train loss (relative): 2.835057e-01 | valid loss (relative): 2.409748e-01 
Epoch 87 use: 514.38 second.

epoch 88 starting......
Epoch:  88 | train loss: 1.160337e-02 | valid loss: 9.906628e-03 
      	| train loss (relative): 2.839051e-01 | valid loss (relative): 2.438658e-01 
Epoch 88 use: 508.25 second.

epoch 89 starting......
Epoch:  89 | train loss: 1.160355e-02 | valid loss: 9.899010e-03 
      	| train loss (relative): 2.841719e-01 | valid loss (relative): 2.427598e-01 
Epoch 89 use: 520.60 second.

epoch 90 starting......
Epoch:  90 | train loss: 1.159993e-02 | valid loss: 9.885364e-03 
      	| train loss (relative): 2.840075e-01 | valid loss (relative): 2.430612e-01 
Epoch 90 use: 520.23 second.

epoch 91 starting......
Epoch:  91 | train loss: 1.160419e-02 | valid loss: 9.908232e-03 
      	| train loss (relative): 2.846727e-01 | valid loss (relative): 2.444093e-01 
Epoch 91 use: 525.30 second.

epoch 92 starting......
Epoch:  92 | train loss: 1.160502e-02 | valid loss: 9.900973e-03 
      	| train loss (relative): 2.848430e-01 | valid loss (relative): 2.438416e-01 
Epoch 92 use: 517.85 second.

epoch 93 starting......
Epoch:  93 | train loss: 1.160125e-02 | valid loss: 9.887371e-03 
      	| train loss (relative): 2.841662e-01 | valid loss (relative): 2.433310e-01 
Epoch 93 use: 537.32 second.

epoch 94 starting......
Epoch:  94 | train loss: 1.160267e-02 | valid loss: 9.894135e-03 
      	| train loss (relative): 2.834029e-01 | valid loss (relative): 2.427154e-01 
Epoch 94 use: 521.90 second.

epoch 95 starting......
Epoch:  95 | train loss: 1.160062e-02 | valid loss: 9.903610e-03 
      	| train loss (relative): 2.840608e-01 | valid loss (relative): 2.432869e-01 
Epoch 95 use: 523.53 second.

epoch 96 starting......
Epoch:  96 | train loss: 1.160617e-02 | valid loss: 9.877498e-03 
      	| train loss (relative): 2.844799e-01 | valid loss (relative): 2.400642e-01 
Epoch 96 use: 505.44 second.

epoch 97 starting......
Epoch:  97 | train loss: 1.164207e-02 | valid loss: 9.880714e-03 
      	| train loss (relative): 2.853509e-01 | valid loss (relative): 2.420909e-01 
Epoch 97 use: 527.49 second.

epoch 98 starting......
Epoch:  98 | train loss: 1.160482e-02 | valid loss: 9.896025e-03 
      	| train loss (relative): 2.840091e-01 | valid loss (relative): 2.423526e-01 
Epoch 98 use: 519.06 second.

epoch 99 starting......
Epoch:  99 | train loss: 1.160606e-02 | valid loss: 9.885207e-03 
      	| train loss (relative): 2.848458e-01 | valid loss (relative): 2.416981e-01 
Epoch 99 use: 520.69 second.

epoch 100 starting......
Epoch:  100 | train loss: 1.160279e-02 | valid loss: 9.884781e-03 
      	| train loss (relative): 2.841634e-01 | valid loss (relative): 2.421249e-01 
Epoch 100 use: 524.40 second.

epoch 101 starting......
Epoch:  101 | train loss: 1.160268e-02 | valid loss: 9.866081e-03 
      	| train loss (relative): 2.844609e-01 | valid loss (relative): 2.404272e-01 
Epoch 101 use: 526.14 second.

epoch 102 starting......
Epoch:  102 | train loss: 1.160546e-02 | valid loss: 9.858480e-03 
      	| train loss (relative): 2.836747e-01 | valid loss (relative): 2.404104e-01 
Epoch 102 use: 525.91 second.

epoch 103 starting......
Epoch:  103 | train loss: 1.160438e-02 | valid loss: 9.882285e-03 
      	| train loss (relative): 2.838763e-01 | valid loss (relative): 2.416858e-01 
Epoch 103 use: 526.31 second.

epoch 104 starting......
Epoch:  104 | train loss: 1.160342e-02 | valid loss: 9.882572e-03 
      	| train loss (relative): 2.844438e-01 | valid loss (relative): 2.417419e-01 
Epoch 104 use: 519.84 second.

epoch 105 starting......
Epoch:  105 | train loss: 1.160634e-02 | valid loss: 9.878569e-03 
      	| train loss (relative): 2.838057e-01 | valid loss (relative): 2.422967e-01 
Epoch 105 use: 533.59 second.

epoch 106 starting......
Epoch:  106 | train loss: 1.160597e-02 | valid loss: 9.904663e-03 
      	| train loss (relative): 2.837712e-01 | valid loss (relative): 2.428723e-01 
Epoch 106 use: 513.10 second.

epoch 107 starting......
Epoch:  107 | train loss: 1.160015e-02 | valid loss: 9.909554e-03 
      	| train loss (relative): 2.836525e-01 | valid loss (relative): 2.435887e-01 
Epoch 107 use: 519.80 second.

epoch 108 starting......
Epoch:  108 | train loss: 1.160726e-02 | valid loss: 9.873948e-03 
      	| train loss (relative): 2.851312e-01 | valid loss (relative): 2.417690e-01 
Epoch 108 use: 518.55 second.

epoch 109 starting......
Epoch:  109 | train loss: 1.160294e-02 | valid loss: 9.885926e-03 
      	| train loss (relative): 2.840076e-01 | valid loss (relative): 2.421549e-01 
Epoch 109 use: 514.52 second.

epoch 110 starting......
Epoch:  110 | train loss: 1.160198e-02 | valid loss: 9.888878e-03 
      	| train loss (relative): 2.839484e-01 | valid loss (relative): 2.419080e-01 
Epoch 110 use: 522.01 second.

epoch 111 starting......
Epoch:  111 | train loss: 1.160024e-02 | valid loss: 9.890116e-03 
      	| train loss (relative): 2.836354e-01 | valid loss (relative): 2.426522e-01 
Epoch 111 use: 515.78 second.

epoch 112 starting......
Epoch:  112 | train loss: 1.160271e-02 | valid loss: 9.905506e-03 
      	| train loss (relative): 2.839387e-01 | valid loss (relative): 2.432987e-01 
Epoch 112 use: 516.21 second.

epoch 113 starting......
Epoch:  113 | train loss: 1.160046e-02 | valid loss: 9.865374e-03 
      	| train loss (relative): 2.854489e-01 | valid loss (relative): 2.405065e-01 
Epoch 113 use: 511.55 second.

epoch 114 starting......
Epoch:  114 | train loss: 1.160255e-02 | valid loss: 9.879789e-03 
      	| train loss (relative): 2.830439e-01 | valid loss (relative): 2.417840e-01 
Epoch 114 use: 520.03 second.

epoch 115 starting......
Epoch:  115 | train loss: 1.160329e-02 | valid loss: 9.887204e-03 
      	| train loss (relative): 2.847177e-01 | valid loss (relative): 2.422054e-01 
Epoch 115 use: 502.22 second.

epoch 116 starting......
Epoch:  116 | train loss: 1.160032e-02 | valid loss: 9.873722e-03 
      	| train loss (relative): 2.837835e-01 | valid loss (relative): 2.411356e-01 
Epoch 116 use: 518.88 second.

epoch 117 starting......
Epoch:  117 | train loss: 1.160549e-02 | valid loss: 9.879716e-03 
      	| train loss (relative): 2.836945e-01 | valid loss (relative): 2.423498e-01 
Epoch 117 use: 469.49 second.

epoch 118 starting......
Epoch:  118 | train loss: 1.160350e-02 | valid loss: 9.894024e-03 
      	| train loss (relative): 2.851071e-01 | valid loss (relative): 2.432361e-01 
Epoch 118 use: 454.50 second.

epoch 119 starting......
Epoch:  119 | train loss: 1.160407e-02 | valid loss: 9.886157e-03 
      	| train loss (relative): 2.867104e-01 | valid loss (relative): 2.426688e-01 
Epoch 119 use: 452.83 second.

epoch 120 starting......
Epoch:  120 | train loss: 1.160332e-02 | valid loss: 9.913227e-03 
      	| train loss (relative): 2.833706e-01 | valid loss (relative): 2.439165e-01 
Epoch 120 use: 432.21 second.

epoch 121 starting......
Epoch:  121 | train loss: 1.160433e-02 | valid loss: 9.909736e-03 
      	| train loss (relative): 2.849588e-01 | valid loss (relative): 2.433603e-01 
Epoch 121 use: 430.55 second.

epoch 122 starting......
Epoch:  122 | train loss: 1.160870e-02 | valid loss: 9.887470e-03 
      	| train loss (relative): 2.844040e-01 | valid loss (relative): 2.460196e-01 
Epoch 122 use: 436.44 second.

epoch 123 starting......
Epoch:  123 | train loss: 1.160552e-02 | valid loss: 9.905620e-03 
      	| train loss (relative): 2.840962e-01 | valid loss (relative): 2.445474e-01 
Epoch 123 use: 420.22 second.

epoch 124 starting......
Epoch:  124 | train loss: 1.160310e-02 | valid loss: 9.919587e-03 
      	| train loss (relative): 2.847882e-01 | valid loss (relative): 2.445972e-01 
Epoch 124 use: 441.26 second.

epoch 125 starting......
Epoch:  125 | train loss: 1.160101e-02 | valid loss: 9.884352e-03 
      	| train loss (relative): 2.850236e-01 | valid loss (relative): 2.426682e-01 
Epoch 125 use: 433.84 second.

epoch 126 starting......
Epoch:  126 | train loss: 1.160036e-02 | valid loss: 9.897204e-03 
      	| train loss (relative): 2.834312e-01 | valid loss (relative): 2.424567e-01 
Epoch 126 use: 437.67 second.

epoch 127 starting......
Epoch:  127 | train loss: 1.160088e-02 | valid loss: 9.886220e-03 
      	| train loss (relative): 2.840941e-01 | valid loss (relative): 2.419724e-01 
Epoch 127 use: 438.10 second.

epoch 128 starting......
Epoch:  128 | train loss: 1.160273e-02 | valid loss: 9.878127e-03 
      	| train loss (relative): 2.843399e-01 | valid loss (relative): 2.417512e-01 
Epoch 128 use: 419.68 second.

epoch 129 starting......
Epoch:  129 | train loss: 1.160393e-02 | valid loss: 9.875470e-03 
      	| train loss (relative): 2.840011e-01 | valid loss (relative): 2.416013e-01 
Epoch 129 use: 431.71 second.

epoch 130 starting......
Epoch:  130 | train loss: 1.160359e-02 | valid loss: 9.881633e-03 
      	| train loss (relative): 2.838392e-01 | valid loss (relative): 2.422542e-01 
Epoch 130 use: 431.97 second.

epoch 131 starting......
Epoch:  131 | train loss: 1.160488e-02 | valid loss: 9.898913e-03 
      	| train loss (relative): 2.833730e-01 | valid loss (relative): 2.422291e-01 
Epoch 131 use: 412.23 second.

epoch 132 starting......
Epoch:  132 | train loss: 1.160091e-02 | valid loss: 9.887099e-03 
      	| train loss (relative): 2.847089e-01 | valid loss (relative): 2.420907e-01 
Epoch 132 use: 424.95 second.

epoch 133 starting......
Epoch:  133 | train loss: 1.160594e-02 | valid loss: 9.870995e-03 
      	| train loss (relative): 2.834683e-01 | valid loss (relative): 2.409814e-01 
Epoch 133 use: 429.51 second.

epoch 134 starting......
Epoch:  134 | train loss: 1.160281e-02 | valid loss: 9.874414e-03 
      	| train loss (relative): 2.843823e-01 | valid loss (relative): 2.411139e-01 
Epoch 134 use: 419.99 second.

epoch 135 starting......
Epoch:  135 | train loss: 1.160359e-02 | valid loss: 9.860162e-03 
      	| train loss (relative): 2.846054e-01 | valid loss (relative): 2.403924e-01 
Epoch 135 use: 427.93 second.

epoch 136 starting......
Epoch:  136 | train loss: 1.160370e-02 | valid loss: 9.881690e-03 
      	| train loss (relative): 2.831715e-01 | valid loss (relative): 2.417175e-01 
Epoch 136 use: 421.32 second.

epoch 137 starting......
Epoch:  137 | train loss: 1.160073e-02 | valid loss: 9.912240e-03 
      	| train loss (relative): 2.837098e-01 | valid loss (relative): 2.436457e-01 
Epoch 137 use: 436.41 second.

epoch 138 starting......
Epoch:  138 | train loss: 1.159684e-02 | valid loss: 9.864802e-03 
      	| train loss (relative): 2.849153e-01 | valid loss (relative): 2.409958e-01 
Epoch 138 use: 424.40 second.

epoch 139 starting......
Epoch:  139 | train loss: 1.160171e-02 | valid loss: 9.891429e-03 
      	| train loss (relative): 2.831268e-01 | valid loss (relative): 2.426842e-01 
Epoch 139 use: 424.40 second.

epoch 140 starting......
Epoch:  140 | train loss: 1.160182e-02 | valid loss: 9.902231e-03 
      	| train loss (relative): 2.839166e-01 | valid loss (relative): 2.426958e-01 
Epoch 140 use: 487.68 second.

epoch 141 starting......
Epoch:  141 | train loss: 1.160213e-02 | valid loss: 9.881547e-03 
      	| train loss (relative): 2.843297e-01 | valid loss (relative): 2.417980e-01 
Epoch 141 use: 421.31 second.

epoch 142 starting......
Epoch:  142 | train loss: 1.160791e-02 | valid loss: 9.886676e-03 
      	| train loss (relative): 2.848839e-01 | valid loss (relative): 2.432296e-01 
Epoch 142 use: 436.19 second.

epoch 143 starting......
Epoch:  143 | train loss: 1.160172e-02 | valid loss: 9.924418e-03 
      	| train loss (relative): 2.840516e-01 | valid loss (relative): 2.451949e-01 
Epoch 143 use: 434.72 second.

epoch 144 starting......
Epoch:  144 | train loss: 1.161734e-02 | valid loss: 9.946940e-03 
      	| train loss (relative): 2.865572e-01 | valid loss (relative): 2.430698e-01 
Epoch 144 use: 417.79 second.

epoch 145 starting......
Epoch:  145 | train loss: 1.161733e-02 | valid loss: 9.898654e-03 
      	| train loss (relative): 2.858821e-01 | valid loss (relative): 2.434642e-01 
Epoch 145 use: 433.83 second.

epoch 146 starting......
Epoch:  146 | train loss: 1.160030e-02 | valid loss: 9.896867e-03 
      	| train loss (relative): 2.835948e-01 | valid loss (relative): 2.427394e-01 
Epoch 146 use: 433.74 second.

epoch 147 starting......
Epoch:  147 | train loss: 1.160234e-02 | valid loss: 9.909878e-03 
      	| train loss (relative): 2.840274e-01 | valid loss (relative): 2.430018e-01 
Epoch 147 use: 426.61 second.

epoch 148 starting......
Epoch:  148 | train loss: 1.160299e-02 | valid loss: 9.915051e-03 
      	| train loss (relative): 2.834258e-01 | valid loss (relative): 2.438625e-01 
Epoch 148 use: 439.91 second.

epoch 149 starting......
Epoch:  149 | train loss: 1.160070e-02 | valid loss: 9.879148e-03 
      	| train loss (relative): 2.845047e-01 | valid loss (relative): 2.416464e-01 
Epoch 149 use: 423.04 second.

epoch 150 starting......
Epoch:  150 | train loss: 1.160350e-02 | valid loss: 9.882589e-03 
      	| train loss (relative): 2.848070e-01 | valid loss (relative): 2.414754e-01 
Epoch 150 use: 436.66 second.

epoch 151 starting......
Epoch:  151 | train loss: 1.160728e-02 | valid loss: 9.862881e-03 
      	| train loss (relative): 2.837854e-01 | valid loss (relative): 2.405706e-01 
Epoch 151 use: 424.06 second.

epoch 152 starting......
Epoch:  152 | train loss: 1.160348e-02 | valid loss: 9.883513e-03 
      	| train loss (relative): 2.838414e-01 | valid loss (relative): 2.415231e-01 
Epoch 152 use: 429.65 second.

epoch 153 starting......
Epoch:  153 | train loss: 1.160128e-02 | valid loss: 9.904876e-03 
      	| train loss (relative): 2.835126e-01 | valid loss (relative): 2.427168e-01 
Epoch 153 use: 425.16 second.

epoch 154 starting......
Epoch:  154 | train loss: 1.160243e-02 | valid loss: 9.907876e-03 
      	| train loss (relative): 2.838437e-01 | valid loss (relative): 2.431221e-01 
Epoch 154 use: 416.77 second.

epoch 155 starting......
Epoch:  155 | train loss: 1.159907e-02 | valid loss: 9.876756e-03 
      	| train loss (relative): 2.851397e-01 | valid loss (relative): 2.417123e-01 
Epoch 155 use: 429.99 second.

epoch 156 starting......
Epoch:  156 | train loss: 1.160108e-02 | valid loss: 9.872711e-03 
      	| train loss (relative): 2.838943e-01 | valid loss (relative): 2.410496e-01 
Epoch 156 use: 418.88 second.

epoch 157 starting......
Epoch:  157 | train loss: 1.160059e-02 | valid loss: 9.876136e-03 
      	| train loss (relative): 2.840460e-01 | valid loss (relative): 2.416407e-01 
Epoch 157 use: 424.43 second.

epoch 158 starting......
Epoch:  158 | train loss: 1.160476e-02 | valid loss: 9.894951e-03 
      	| train loss (relative): 2.848368e-01 | valid loss (relative): 2.425639e-01 
Epoch 158 use: 433.34 second.

epoch 159 starting......
Epoch:  159 | train loss: 1.160533e-02 | valid loss: 9.879184e-03 
      	| train loss (relative): 2.842317e-01 | valid loss (relative): 2.409588e-01 
Epoch 159 use: 419.00 second.

epoch 160 starting......
Epoch:  160 | train loss: 1.160289e-02 | valid loss: 9.897205e-03 
      	| train loss (relative): 2.833727e-01 | valid loss (relative): 2.427013e-01 
Epoch 160 use: 426.88 second.

epoch 161 starting......
Epoch:  161 | train loss: 1.160335e-02 | valid loss: 9.875830e-03 
      	| train loss (relative): 2.850592e-01 | valid loss (relative): 2.409384e-01 
Epoch 161 use: 416.75 second.

epoch 162 starting......
Epoch:  162 | train loss: 1.159993e-02 | valid loss: 9.881859e-03 
      	| train loss (relative): 2.832809e-01 | valid loss (relative): 2.414966e-01 
Epoch 162 use: 435.35 second.

epoch 163 starting......
Epoch:  163 | train loss: 1.160276e-02 | valid loss: 9.891528e-03 
      	| train loss (relative): 2.839239e-01 | valid loss (relative): 2.420757e-01 
Epoch 163 use: 432.27 second.

epoch 164 starting......
Epoch:  164 | train loss: 1.159923e-02 | valid loss: 9.889068e-03 
      	| train loss (relative): 2.834247e-01 | valid loss (relative): 2.430450e-01 
Epoch 164 use: 415.02 second.

epoch 165 starting......
Epoch:  165 | train loss: 1.160030e-02 | valid loss: 9.897650e-03 
      	| train loss (relative): 2.835480e-01 | valid loss (relative): 2.424614e-01 
Epoch 165 use: 439.44 second.

epoch 166 starting......
Epoch:  166 | train loss: 1.160673e-02 | valid loss: 9.925033e-03 
      	| train loss (relative): 2.850657e-01 | valid loss (relative): 2.448624e-01 
Epoch 166 use: 411.73 second.

epoch 167 starting......
Epoch:  167 | train loss: 1.160608e-02 | valid loss: 9.912438e-03 
      	| train loss (relative): 2.852982e-01 | valid loss (relative): 2.435436e-01 
Epoch 167 use: 440.33 second.

epoch 168 starting......
Epoch:  168 | train loss: 1.159949e-02 | valid loss: 9.879304e-03 
      	| train loss (relative): 2.847799e-01 | valid loss (relative): 2.420379e-01 
Epoch 168 use: 425.84 second.

epoch 169 starting......
Epoch:  169 | train loss: 1.160080e-02 | valid loss: 9.890831e-03 
      	| train loss (relative): 2.836297e-01 | valid loss (relative): 2.423956e-01 
Epoch 169 use: 416.07 second.

epoch 170 starting......
Epoch:  170 | train loss: 1.160338e-02 | valid loss: 9.883824e-03 
      	| train loss (relative): 2.845299e-01 | valid loss (relative): 2.423461e-01 
Epoch 170 use: 437.37 second.

epoch 171 starting......
Epoch:  171 | train loss: 1.160445e-02 | valid loss: 9.889184e-03 
      	| train loss (relative): 2.853316e-01 | valid loss (relative): 2.421058e-01 
Epoch 171 use: 418.80 second.

epoch 172 starting......
Epoch:  172 | train loss: 1.162268e-02 | valid loss: 9.880732e-03 
      	| train loss (relative): 2.850807e-01 | valid loss (relative): 2.402878e-01 
Epoch 172 use: 421.24 second.

epoch 173 starting......
Epoch:  173 | train loss: 1.160308e-02 | valid loss: 9.901044e-03 
      	| train loss (relative): 2.830738e-01 | valid loss (relative): 2.423303e-01 
Epoch 173 use: 433.17 second.

epoch 174 starting......
Epoch:  174 | train loss: 1.160420e-02 | valid loss: 9.887986e-03 
      	| train loss (relative): 2.843887e-01 | valid loss (relative): 2.423302e-01 
Epoch 174 use: 423.29 second.

epoch 175 starting......
Epoch:  175 | train loss: 1.160453e-02 | valid loss: 9.893240e-03 
      	| train loss (relative): 2.846607e-01 | valid loss (relative): 2.425183e-01 
Epoch 175 use: 432.02 second.

epoch 176 starting......
Epoch:  176 | train loss: 1.160191e-02 | valid loss: 9.881131e-03 
      	| train loss (relative): 2.839402e-01 | valid loss (relative): 2.423076e-01 
Epoch 176 use: 422.11 second.

epoch 177 starting......
Epoch:  177 | train loss: 1.160011e-02 | valid loss: 9.892136e-03 
      	| train loss (relative): 2.835867e-01 | valid loss (relative): 2.426354e-01 
Epoch 177 use: 424.38 second.

epoch 178 starting......
Epoch:  178 | train loss: 1.160006e-02 | valid loss: 9.868236e-03 
      	| train loss (relative): 2.853363e-01 | valid loss (relative): 2.408193e-01 
Epoch 178 use: 426.57 second.

epoch 179 starting......
Epoch:  179 | train loss: 1.160282e-02 | valid loss: 9.905836e-03 
      	| train loss (relative): 2.830239e-01 | valid loss (relative): 2.433706e-01 
Epoch 179 use: 426.10 second.

epoch 180 starting......
Epoch:  180 | train loss: 1.160195e-02 | valid loss: 9.916565e-03 
      	| train loss (relative): 2.841340e-01 | valid loss (relative): 2.440776e-01 
Epoch 180 use: 437.26 second.

epoch 181 starting......
Epoch:  181 | train loss: 1.160158e-02 | valid loss: 9.866615e-03 
      	| train loss (relative): 2.855925e-01 | valid loss (relative): 2.405365e-01 
Epoch 181 use: 420.58 second.

epoch 182 starting......
Epoch:  182 | train loss: 1.160548e-02 | valid loss: 9.911156e-03 
      	| train loss (relative): 2.826902e-01 | valid loss (relative): 2.437997e-01 
Epoch 182 use: 398.14 second.

epoch 183 starting......
Epoch:  183 | train loss: 1.160156e-02 | valid loss: 9.890079e-03 
      	| train loss (relative): 2.851734e-01 | valid loss (relative): 2.420842e-01 
Epoch 183 use: 453.13 second.

epoch 184 starting......
Epoch:  184 | train loss: 1.159985e-02 | valid loss: 9.906270e-03 
      	| train loss (relative): 2.824130e-01 | valid loss (relative): 2.439608e-01 
Epoch 184 use: 447.31 second.

epoch 185 starting......
Epoch:  185 | train loss: 1.160096e-02 | valid loss: 9.871114e-03 
      	| train loss (relative): 2.851606e-01 | valid loss (relative): 2.410787e-01 
Epoch 185 use: 414.76 second.

epoch 186 starting......
Epoch:  186 | train loss: 1.160135e-02 | valid loss: 9.885536e-03 
      	| train loss (relative): 2.833398e-01 | valid loss (relative): 2.422809e-01 
Epoch 186 use: 427.47 second.

epoch 187 starting......
Epoch:  187 | train loss: 1.160519e-02 | valid loss: 9.908739e-03 
      	| train loss (relative): 2.855209e-01 | valid loss (relative): 2.436263e-01 
Epoch 187 use: 413.79 second.

epoch 188 starting......
Epoch:  188 | train loss: 1.160283e-02 | valid loss: 9.909282e-03 
      	| train loss (relative): 2.835991e-01 | valid loss (relative): 2.439975e-01 
Epoch 188 use: 398.97 second.

epoch 189 starting......
Epoch:  189 | train loss: 1.159827e-02 | valid loss: 9.885160e-03 
      	| train loss (relative): 2.850869e-01 | valid loss (relative): 2.424169e-01 
Epoch 189 use: 425.27 second.

epoch 190 starting......
Epoch:  190 | train loss: 1.160111e-02 | valid loss: 9.889977e-03 
      	| train loss (relative): 2.834252e-01 | valid loss (relative): 2.425165e-01 
Epoch 190 use: 404.62 second.

epoch 191 starting......
Epoch:  191 | train loss: 1.160323e-02 | valid loss: 9.880919e-03 
      	| train loss (relative): 2.838514e-01 | valid loss (relative): 2.416418e-01 
Epoch 191 use: 500.56 second.

epoch 192 starting......
Epoch:  192 | train loss: 1.160291e-02 | valid loss: 9.903189e-03 
      	| train loss (relative): 2.838987e-01 | valid loss (relative): 2.430760e-01 
Epoch 192 use: 411.34 second.

epoch 193 starting......
Epoch:  193 | train loss: 1.160616e-02 | valid loss: 9.877156e-03 
      	| train loss (relative): 2.865689e-01 | valid loss (relative): 2.422597e-01 
Epoch 193 use: 406.92 second.

epoch 194 starting......
Epoch:  194 | train loss: 1.160834e-02 | valid loss: 9.901272e-03 
      	| train loss (relative): 2.837707e-01 | valid loss (relative): 2.415219e-01 
Epoch 194 use: 405.84 second.

epoch 195 starting......
Epoch:  195 | train loss: 1.161722e-02 | valid loss: 9.872642e-03 
      	| train loss (relative): 2.847274e-01 | valid loss (relative): 2.436184e-01 
Epoch 195 use: 405.23 second.

epoch 196 starting......
Epoch:  196 | train loss: 1.160396e-02 | valid loss: 9.877541e-03 
      	| train loss (relative): 2.839039e-01 | valid loss (relative): 2.413753e-01 
Epoch 196 use: 400.93 second.

epoch 197 starting......
Epoch:  197 | train loss: 1.160249e-02 | valid loss: 9.927253e-03 
      	| train loss (relative): 2.832755e-01 | valid loss (relative): 2.446635e-01 
Epoch 197 use: 420.83 second.

epoch 198 starting......
Epoch:  198 | train loss: 1.160512e-02 | valid loss: 9.884704e-03 
      	| train loss (relative): 2.849818e-01 | valid loss (relative): 2.423137e-01 
Epoch 198 use: 413.00 second.

epoch 199 starting......
Epoch:  199 | train loss: 1.160398e-02 | valid loss: 9.859209e-03 
      	| train loss (relative): 2.840665e-01 | valid loss (relative): 2.402976e-01 
Epoch 199 use: 476.42 second.

test MSE Error: 1.115408e-02 | relative MSE Error: 2.718573e-01 
 Total time used for training: 19.39 hour.
MESLoss saved to  /rds/general/user/jy220/home/results/Variational_False_Changelr_False_MSELoss_Latent_128_nearest_neighbouring_True_SFC_nums_3_startlr_0.0005_n_epoches_200.txt
relative MSELoss saved to  /rds/general/user/jy220/home/results/Variational_False_Changelr_False_reMSELoss_Latent_128_nearest_neighbouring_True_SFC_nums_3_startlr_0.0005_n_epoches_200.txt
model saved to /rds/general/user/jy220/home/results/Variational_False_Changelr_False_Latent_128_Nearest_neighbouring_True_SFC_nums_3_startlr_0.0005_n_epoches_200.pth
model_dict saved to /rds/general/user/jy220/home/results/Variational_False_Changelr_False_Latent_128_Nearest_neighbouring_True_SFC_nums_3_startlr_0.0005_n_epoches_200_dict.pth
... Training slugflow data completed, Run finished Sun  1 Aug 16:45:56 BST 2021 ...
{'vtu_dir': '/rds/general/user/jy220/home/slugflow/', 'data_dir': '/rds/general/user/jy220/home/tensors/', 'data_type': 'tensors', 'vtu_fields': 'Component1::ComponentMassFractionPhase1, phase1::Velocity', 'variational': 'False', 'structured': 'False', 'changing_lr': 'False', 'self_concat': '2', 'dimension': '3', 'sfc_nums': '3', 'components': '4', 'nearest_neighbouring': 'True', 'dims_latent': '128', 'activation': 'Tanh', 'sfc_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/sfcs.pt', 'inv_sfc_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/inv_sfcs.pt', 'coords_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/coords.pt', 'cells_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/cells.npy', 'state_load': '/rds/general/user/jy220/home/results/Variational_False_Changelr_False_Latent_128_Nearest_neighbouring_True_SFC_nums_3_startlr_0.0005_n_epoches_200_dict.pth', 'batch_size': '16', 'lr': '0.0005', 'n_epoches': '200', 'seed': '15', 'visualize': 'False', 'reconstructed_path': 'None', 'reconstruct_start_index': '0', 'reconstruct_end_index': '50', 'save_path': '/rds/general/user/jy220/home/results/'}
[[ 2.50825269e-01 -1.67256990e-02 -1.22268730e-03]
 [ 2.38611959e-01 -5.50627777e-03 -5.06451913e-03]
 [ 2.39454874e-01 -1.59563257e-02  3.96778323e-03]
 ...
 [ 9.11361096e+00 -2.21908381e-02  2.22384902e-03]
 [ 9.11847310e+00 -2.84346931e-02 -2.34839590e-04]
 [ 9.11039661e+00 -2.51498573e-02 -8.70661044e-03]]
{'tetra': array([[      0,       1,       2,       3],
       [      4,       5,       6,       7],
       [      8,       9,      10,      11],
       ...,
       [1342744, 1342745, 1342746, 1342747],
       [1342748, 1342749, 1342750, 1342751],
       [1342752, 1342753, 1342754, 1342755]])}
structured  False 
 activation  Tanh() 
 self concat  2 
 sfc_nums  3 

dims_latent  128 
 components  4 
 nearest_neighbouring  True 

visualize  False 
 sample number  1706 

reading sfc nums......
[array([  41684,   41685,   41674, ..., 1235560, 1235564, 1235558]), array([954289, 954290, 954291, ..., 420685, 420687, 420688]), array([  40156,   40155,   40158, ..., 1123478, 1123475, 1123467])]
reading inverse sfc nums......
[array([   8146,    8145,    8144, ..., 1274444, 1274445, 1273166]), array([547280, 547283, 593037, ..., 933933, 933932, 864488]), array([ 66437,  66439, 122512, ..., 459416, 415130, 459418])]
Computing min and max......

tk:  tensor([0.7581, 0.0418, 0.1840, 0.1464], dtype=torch.float64) 

tb:  tensor([-0.3816, -0.3618, -0.0308,  0.1446], dtype=torch.float64) 

length of train set: 1505 

length of valid set: 100 

length of test set: 101 

torch device num: 4 

Let's use 4 GPUs!
epoch 200 starting......
Epoch:  200 | train loss: 1.167919e-02 | valid loss: 1.046158e-02 
      	| train loss (relative): 2.859598e-01 | valid loss (relative): 2.563586e-01 
Epoch 200 use: 429.23 second.

epoch 201 starting......
Epoch:  201 | train loss: 1.167323e-02 | valid loss: 1.044772e-02 
      	| train loss (relative): 2.862266e-01 | valid loss (relative): 2.562047e-01 
Epoch 201 use: 414.46 second.

epoch 202 starting......
Epoch:  202 | train loss: 1.167541e-02 | valid loss: 1.043456e-02 
      	| train loss (relative): 2.885025e-01 | valid loss (relative): 2.547478e-01 
Epoch 202 use: 405.21 second.

epoch 203 starting......
Epoch:  203 | train loss: 1.167399e-02 | valid loss: 1.043673e-02 
      	| train loss (relative): 2.860397e-01 | valid loss (relative): 2.558648e-01 
Epoch 203 use: 372.40 second.

epoch 204 starting......
Epoch:  204 | train loss: 1.167020e-02 | valid loss: 1.044277e-02 
      	| train loss (relative): 2.873800e-01 | valid loss (relative): 2.555883e-01 
Epoch 204 use: 405.24 second.

epoch 205 starting......
Epoch:  205 | train loss: 1.166903e-02 | valid loss: 1.044518e-02 
      	| train loss (relative): 2.856519e-01 | valid loss (relative): 2.553359e-01 
Epoch 205 use: 395.94 second.

epoch 206 starting......
Epoch:  206 | train loss: 1.167101e-02 | valid loss: 1.043738e-02 
      	| train loss (relative): 2.863018e-01 | valid loss (relative): 2.553278e-01 
Epoch 206 use: 396.38 second.

epoch 207 starting......
Epoch:  207 | train loss: 1.167000e-02 | valid loss: 1.045750e-02 
      	| train loss (relative): 2.877029e-01 | valid loss (relative): 2.582794e-01 
Epoch 207 use: 393.48 second.

epoch 208 starting......
Epoch:  208 | train loss: 1.167208e-02 | valid loss: 1.044960e-02 
      	| train loss (relative): 2.865169e-01 | valid loss (relative): 2.564521e-01 
Epoch 208 use: 407.72 second.

epoch 209 starting......
Epoch:  209 | train loss: 1.166887e-02 | valid loss: 1.044109e-02 
      	| train loss (relative): 2.855119e-01 | valid loss (relative): 2.556805e-01 
Epoch 209 use: 389.98 second.

epoch 210 starting......
Epoch:  210 | train loss: 1.167214e-02 | valid loss: 1.046559e-02 
      	| train loss (relative): 2.867190e-01 | valid loss (relative): 2.574525e-01 
Epoch 210 use: 390.65 second.

epoch 211 starting......
Epoch:  211 | train loss: 1.167390e-02 | valid loss: 1.047377e-02 
      	| train loss (relative): 2.863201e-01 | valid loss (relative): 2.579168e-01 
Epoch 211 use: 416.27 second.

epoch 212 starting......
Epoch:  212 | train loss: 1.167267e-02 | valid loss: 1.044555e-02 
      	| train loss (relative): 2.864266e-01 | valid loss (relative): 2.570581e-01 
Epoch 212 use: 368.59 second.

epoch 213 starting......
Epoch:  213 | train loss: 1.167584e-02 | valid loss: 1.044498e-02 
      	| train loss (relative): 2.876618e-01 | valid loss (relative): 2.571213e-01 
Epoch 213 use: 377.99 second.

epoch 214 starting......
Epoch:  214 | train loss: 1.167033e-02 | valid loss: 1.045852e-02 
      	| train loss (relative): 2.874738e-01 | valid loss (relative): 2.573542e-01 
Epoch 214 use: 389.93 second.

epoch 215 starting......
Epoch:  215 | train loss: 1.167042e-02 | valid loss: 1.045338e-02 
      	| train loss (relative): 2.860597e-01 | valid loss (relative): 2.569422e-01 
Epoch 215 use: 394.27 second.

epoch 216 starting......
Epoch:  216 | train loss: 1.167532e-02 | valid loss: 1.046696e-02 
      	| train loss (relative): 2.861693e-01 | valid loss (relative): 2.575852e-01 
Epoch 216 use: 375.63 second.

epoch 217 starting......
Epoch:  217 | train loss: 1.166995e-02 | valid loss: 1.046634e-02 
      	| train loss (relative): 2.860011e-01 | valid loss (relative): 2.569628e-01 
Epoch 217 use: 378.47 second.

epoch 218 starting......
Epoch:  218 | train loss: 1.167526e-02 | valid loss: 1.043155e-02 
      	| train loss (relative): 2.874601e-01 | valid loss (relative): 2.548765e-01 
Epoch 218 use: 396.10 second.

epoch 219 starting......
Epoch:  219 | train loss: 1.167187e-02 | valid loss: 1.044941e-02 
      	| train loss (relative): 2.853561e-01 | valid loss (relative): 2.554416e-01 
Epoch 219 use: 386.75 second.

epoch 220 starting......
Epoch:  220 | train loss: 1.166994e-02 | valid loss: 1.046551e-02 
      	| train loss (relative): 2.846913e-01 | valid loss (relative): 2.570177e-01 
Epoch 220 use: 393.68 second.

epoch 221 starting......
Epoch:  221 | train loss: 1.166980e-02 | valid loss: 1.046223e-02 
      	| train loss (relative): 2.867583e-01 | valid loss (relative): 2.568013e-01 
Epoch 221 use: 389.11 second.

epoch 222 starting......
Epoch:  222 | train loss: 1.167054e-02 | valid loss: 1.043319e-02 
      	| train loss (relative): 2.858205e-01 | valid loss (relative): 2.561891e-01 
Epoch 222 use: 380.75 second.

epoch 223 starting......
Epoch:  223 | train loss: 1.167136e-02 | valid loss: 1.046329e-02 
      	| train loss (relative): 2.862400e-01 | valid loss (relative): 2.562420e-01 
Epoch 223 use: 385.95 second.

epoch 224 starting......
Epoch:  224 | train loss: 1.167254e-02 | valid loss: 1.044226e-02 
      	| train loss (relative): 2.864307e-01 | valid loss (relative): 2.561499e-01 
Epoch 224 use: 369.02 second.

epoch 225 starting......
Epoch:  225 | train loss: 1.167079e-02 | valid loss: 1.043895e-02 
      	| train loss (relative): 2.863429e-01 | valid loss (relative): 2.554981e-01 
Epoch 225 use: 370.51 second.

epoch 226 starting......
Epoch:  226 | train loss: 1.167213e-02 | valid loss: 1.045772e-02 
      	| train loss (relative): 2.855476e-01 | valid loss (relative): 2.568156e-01 
Epoch 226 use: 371.88 second.

epoch 227 starting......
Epoch:  227 | train loss: 1.167055e-02 | valid loss: 1.043558e-02 
      	| train loss (relative): 2.860366e-01 | valid loss (relative): 2.554124e-01 
Epoch 227 use: 378.72 second.

epoch 228 starting......
Epoch:  228 | train loss: 1.166993e-02 | valid loss: 1.047741e-02 
      	| train loss (relative): 2.880196e-01 | valid loss (relative): 2.577455e-01 
Epoch 228 use: 375.86 second.

epoch 229 starting......
Epoch:  229 | train loss: 1.166947e-02 | valid loss: 1.045623e-02 
      	| train loss (relative): 2.863233e-01 | valid loss (relative): 2.563489e-01 
Epoch 229 use: 387.28 second.

epoch 230 starting......
Epoch:  230 | train loss: 1.167301e-02 | valid loss: 1.046065e-02 
      	| train loss (relative): 2.861720e-01 | valid loss (relative): 2.562899e-01 
Epoch 230 use: 359.83 second.

epoch 231 starting......
Epoch:  231 | train loss: 1.167061e-02 | valid loss: 1.048793e-02 
      	| train loss (relative): 2.850023e-01 | valid loss (relative): 2.574875e-01 
Epoch 231 use: 386.82 second.

epoch 232 starting......
Epoch:  232 | train loss: 1.167396e-02 | valid loss: 1.049765e-02 
      	| train loss (relative): 2.860700e-01 | valid loss (relative): 2.588748e-01 
Epoch 232 use: 388.10 second.

epoch 233 starting......
Epoch:  233 | train loss: 1.168607e-02 | valid loss: 1.046958e-02 
      	| train loss (relative): 2.887145e-01 | valid loss (relative): 2.581403e-01 
Epoch 233 use: 370.09 second.

epoch 234 starting......
Epoch:  234 | train loss: 1.168521e-02 | valid loss: 1.051264e-02 
      	| train loss (relative): 2.866680e-01 | valid loss (relative): 2.591774e-01 
Epoch 234 use: 377.99 second.

epoch 235 starting......
Epoch:  235 | train loss: 1.168506e-02 | valid loss: 1.045270e-02 
      	| train loss (relative): 2.890068e-01 | valid loss (relative): 2.551598e-01 
Epoch 235 use: 359.39 second.

epoch 236 starting......
Epoch:  236 | train loss: 1.166982e-02 | valid loss: 1.046310e-02 
      	| train loss (relative): 2.864572e-01 | valid loss (relative): 2.561786e-01 
Epoch 236 use: 367.11 second.

epoch 237 starting......
Epoch:  237 | train loss: 1.167026e-02 | valid loss: 1.043365e-02 
      	| train loss (relative): 2.858318e-01 | valid loss (relative): 2.568498e-01 
Epoch 237 use: 376.83 second.

epoch 238 starting......
Epoch:  238 | train loss: 1.167388e-02 | valid loss: 1.045522e-02 
      	| train loss (relative): 2.856028e-01 | valid loss (relative): 2.567649e-01 
Epoch 238 use: 361.87 second.

epoch 239 starting......
Epoch:  239 | train loss: 1.167357e-02 | valid loss: 1.046704e-02 
      	| train loss (relative): 2.859499e-01 | valid loss (relative): 2.570142e-01 
Epoch 239 use: 373.38 second.

epoch 240 starting......
Epoch:  240 | train loss: 1.167610e-02 | valid loss: 1.049165e-02 
      	| train loss (relative): 2.878817e-01 | valid loss (relative): 2.597290e-01 
Epoch 240 use: 381.07 second.

epoch 241 starting......
Epoch:  241 | train loss: 1.167334e-02 | valid loss: 1.045951e-02 
      	| train loss (relative): 2.873978e-01 | valid loss (relative): 2.568869e-01 
Epoch 241 use: 353.70 second.

epoch 242 starting......
Epoch:  242 | train loss: 1.167016e-02 | valid loss: 1.048959e-02 
      	| train loss (relative): 2.862927e-01 | valid loss (relative): 2.581896e-01 
Epoch 242 use: 370.02 second.

epoch 243 starting......
Epoch:  243 | train loss: 1.167068e-02 | valid loss: 1.046288e-02 
      	| train loss (relative): 2.865990e-01 | valid loss (relative): 2.568044e-01 
Epoch 243 use: 385.62 second.

epoch 244 starting......
Epoch:  244 | train loss: 1.166870e-02 | valid loss: 1.046479e-02 
      	| train loss (relative): 2.862641e-01 | valid loss (relative): 2.570534e-01 
Epoch 244 use: 367.72 second.

epoch 245 starting......
Epoch:  245 | train loss: 1.167089e-02 | valid loss: 1.045988e-02 
      	| train loss (relative): 2.852415e-01 | valid loss (relative): 2.569169e-01 
Epoch 245 use: 358.39 second.

epoch 246 starting......
Epoch:  246 | train loss: 1.167745e-02 | valid loss: 1.043523e-02 
      	| train loss (relative): 2.873524e-01 | valid loss (relative): 2.561080e-01 
Epoch 246 use: 375.68 second.

epoch 247 starting......
Epoch:  247 | train loss: 1.167273e-02 | valid loss: 1.044500e-02 
      	| train loss (relative): 2.880753e-01 | valid loss (relative): 2.567741e-01 
Epoch 247 use: 364.54 second.

epoch 248 starting......
Epoch:  248 | train loss: 1.167117e-02 | valid loss: 1.050302e-02 
      	| train loss (relative): 2.857363e-01 | valid loss (relative): 2.587484e-01 
Epoch 248 use: 374.26 second.

epoch 249 starting......
Epoch:  249 | train loss: 1.167358e-02 | valid loss: 1.044360e-02 
      	| train loss (relative): 2.867575e-01 | valid loss (relative): 2.565687e-01 
Epoch 249 use: 370.67 second.

epoch 250 starting......
Epoch:  250 | train loss: 1.166945e-02 | valid loss: 1.044363e-02 
      	| train loss (relative): 2.861216e-01 | valid loss (relative): 2.555641e-01 
Epoch 250 use: 349.88 second.

epoch 251 starting......
Epoch:  251 | train loss: 1.167140e-02 | valid loss: 1.045283e-02 
      	| train loss (relative): 2.856511e-01 | valid loss (relative): 2.566804e-01 
Epoch 251 use: 351.29 second.

epoch 252 starting......
Epoch:  252 | train loss: 1.167216e-02 | valid loss: 1.046561e-02 
      	| train loss (relative): 2.859595e-01 | valid loss (relative): 2.566302e-01 
Epoch 252 use: 366.40 second.

epoch 253 starting......
Epoch:  253 | train loss: 1.167296e-02 | valid loss: 1.048032e-02 
      	| train loss (relative): 2.859021e-01 | valid loss (relative): 2.583342e-01 
Epoch 253 use: 355.24 second.

epoch 254 starting......
Epoch:  254 | train loss: 1.167095e-02 | valid loss: 1.045741e-02 
      	| train loss (relative): 2.861776e-01 | valid loss (relative): 2.569880e-01 
Epoch 254 use: 349.17 second.

epoch 255 starting......
Epoch:  255 | train loss: 1.166877e-02 | valid loss: 1.046012e-02 
      	| train loss (relative): 2.868282e-01 | valid loss (relative): 2.565887e-01 
Epoch 255 use: 361.17 second.

epoch 256 starting......
Epoch:  256 | train loss: 1.167157e-02 | valid loss: 1.044180e-02 
      	| train loss (relative): 2.859913e-01 | valid loss (relative): 2.558266e-01 
Epoch 256 use: 374.21 second.

epoch 257 starting......
Epoch:  257 | train loss: 1.166926e-02 | valid loss: 1.043066e-02 
      	| train loss (relative): 2.866826e-01 | valid loss (relative): 2.549699e-01 
Epoch 257 use: 356.95 second.

epoch 258 starting......
Epoch:  258 | train loss: 1.167245e-02 | valid loss: 1.045528e-02 
      	| train loss (relative): 2.852615e-01 | valid loss (relative): 2.572274e-01 
Epoch 258 use: 366.32 second.

epoch 259 starting......
Epoch:  259 | train loss: 1.166981e-02 | valid loss: 1.047415e-02 
      	| train loss (relative): 2.869394e-01 | valid loss (relative): 2.582467e-01 
Epoch 259 use: 377.29 second.

epoch 260 starting......
Epoch:  260 | train loss: 1.167281e-02 | valid loss: 1.042577e-02 
      	| train loss (relative): 2.873114e-01 | valid loss (relative): 2.544613e-01 
Epoch 260 use: 392.04 second.

epoch 261 starting......
Epoch:  261 | train loss: 1.167037e-02 | valid loss: 1.047596e-02 
      	| train loss (relative): 2.841062e-01 | valid loss (relative): 2.573018e-01 
Epoch 261 use: 352.15 second.

epoch 262 starting......
Epoch:  262 | train loss: 1.167046e-02 | valid loss: 1.044524e-02 
      	| train loss (relative): 2.864448e-01 | valid loss (relative): 2.551395e-01 
Epoch 262 use: 359.43 second.

epoch 263 starting......
Epoch:  263 | train loss: 1.167291e-02 | valid loss: 1.045765e-02 
      	| train loss (relative): 2.865928e-01 | valid loss (relative): 2.567635e-01 
Epoch 263 use: 350.51 second.

epoch 264 starting......
Epoch:  264 | train loss: 1.167556e-02 | valid loss: 1.044050e-02 
      	| train loss (relative): 2.879207e-01 | valid loss (relative): 2.558496e-01 
Epoch 264 use: 385.90 second.

epoch 265 starting......
Epoch:  265 | train loss: 1.167028e-02 | valid loss: 1.046735e-02 
      	| train loss (relative): 2.854861e-01 | valid loss (relative): 2.574760e-01 
Epoch 265 use: 370.81 second.

epoch 266 starting......
Epoch:  266 | train loss: 1.167317e-02 | valid loss: 1.042718e-02 
      	| train loss (relative): 2.873867e-01 | valid loss (relative): 2.547598e-01 
Epoch 266 use: 356.16 second.

epoch 267 starting......
Epoch:  267 | train loss: 1.167036e-02 | valid loss: 1.045422e-02 
      	| train loss (relative): 2.855485e-01 | valid loss (relative): 2.571718e-01 
Epoch 267 use: 358.90 second.

epoch 268 starting......
Epoch:  268 | train loss: 1.167047e-02 | valid loss: 1.043630e-02 
      	| train loss (relative): 2.868012e-01 | valid loss (relative): 2.550817e-01 
Epoch 268 use: 367.99 second.

epoch 269 starting......
Epoch:  269 | train loss: 1.167010e-02 | valid loss: 1.041493e-02 
      	| train loss (relative): 2.864108e-01 | valid loss (relative): 2.544644e-01 
Epoch 269 use: 362.11 second.

epoch 270 starting......
Epoch:  270 | train loss: 1.167290e-02 | valid loss: 1.040763e-02 
      	| train loss (relative): 2.858207e-01 | valid loss (relative): 2.548451e-01 
Epoch 270 use: 365.61 second.

epoch 271 starting......
Epoch:  271 | train loss: 1.167504e-02 | valid loss: 1.045489e-02 
      	| train loss (relative): 2.852486e-01 | valid loss (relative): 2.572148e-01 
Epoch 271 use: 364.37 second.

epoch 272 starting......
Epoch:  272 | train loss: 1.166958e-02 | valid loss: 1.046689e-02 
      	| train loss (relative): 2.860577e-01 | valid loss (relative): 2.567849e-01 
Epoch 272 use: 370.40 second.

epoch 273 starting......
Epoch:  273 | train loss: 1.166934e-02 | valid loss: 1.046925e-02 
      	| train loss (relative): 2.857656e-01 | valid loss (relative): 2.562521e-01 
Epoch 273 use: 376.39 second.

epoch 274 starting......
Epoch:  274 | train loss: 1.167152e-02 | valid loss: 1.048863e-02 
      	| train loss (relative): 2.850876e-01 | valid loss (relative): 2.588411e-01 
Epoch 274 use: 362.06 second.

epoch 275 starting......
Epoch:  275 | train loss: 1.166946e-02 | valid loss: 1.045960e-02 
      	| train loss (relative): 2.868862e-01 | valid loss (relative): 2.563494e-01 
Epoch 275 use: 358.41 second.

epoch 276 starting......
Epoch:  276 | train loss: 1.167045e-02 | valid loss: 1.047285e-02 
      	| train loss (relative): 2.863338e-01 | valid loss (relative): 2.574952e-01 
Epoch 276 use: 363.07 second.

epoch 277 starting......
Epoch:  277 | train loss: 1.167106e-02 | valid loss: 1.046267e-02 
      	| train loss (relative): 2.858576e-01 | valid loss (relative): 2.567267e-01 
Epoch 277 use: 370.47 second.

epoch 278 starting......
Epoch:  278 | train loss: 1.166933e-02 | valid loss: 1.047473e-02 
      	| train loss (relative): 2.861967e-01 | valid loss (relative): 2.568721e-01 
Epoch 278 use: 363.40 second.

epoch 279 starting......
Epoch:  279 | train loss: 1.166841e-02 | valid loss: 1.045443e-02 
      	| train loss (relative): 2.859271e-01 | valid loss (relative): 2.564094e-01 
Epoch 279 use: 351.20 second.

epoch 280 starting......
Epoch:  280 | train loss: 1.167139e-02 | valid loss: 1.042000e-02 
      	| train loss (relative): 2.871452e-01 | valid loss (relative): 2.553270e-01 
Epoch 280 use: 368.14 second.

epoch 281 starting......
Epoch:  281 | train loss: 1.167937e-02 | valid loss: 1.046148e-02 
      	| train loss (relative): 2.847971e-01 | valid loss (relative): 2.562209e-01 
Epoch 281 use: 371.81 second.

epoch 282 starting......
Epoch:  282 | train loss: 1.167236e-02 | valid loss: 1.046696e-02 
      	| train loss (relative): 2.867284e-01 | valid loss (relative): 2.572550e-01 
Epoch 282 use: 363.66 second.

epoch 283 starting......
Epoch:  283 | train loss: 1.167012e-02 | valid loss: 1.049676e-02 
      	| train loss (relative): 2.847072e-01 | valid loss (relative): 2.585121e-01 
Epoch 283 use: 364.95 second.

epoch 284 starting......
Epoch:  284 | train loss: 1.166983e-02 | valid loss: 1.046784e-02 
      	| train loss (relative): 2.862380e-01 | valid loss (relative): 2.578472e-01 
Epoch 284 use: 361.62 second.

epoch 285 starting......
Epoch:  285 | train loss: 1.166974e-02 | valid loss: 1.044413e-02 
      	| train loss (relative): 2.874934e-01 | valid loss (relative): 2.565109e-01 
Epoch 285 use: 367.67 second.

epoch 286 starting......
Epoch:  286 | train loss: 1.167380e-02 | valid loss: 1.040784e-02 
      	| train loss (relative): 2.865487e-01 | valid loss (relative): 2.544884e-01 
Epoch 286 use: 370.45 second.

epoch 287 starting......
Epoch:  287 | train loss: 1.167348e-02 | valid loss: 1.041993e-02 
      	| train loss (relative): 2.863095e-01 | valid loss (relative): 2.545347e-01 
Epoch 287 use: 365.53 second.

epoch 288 starting......
Epoch:  288 | train loss: 1.167221e-02 | valid loss: 1.044360e-02 
      	| train loss (relative): 2.862576e-01 | valid loss (relative): 2.559659e-01 
Epoch 288 use: 348.57 second.

epoch 289 starting......
Epoch:  289 | train loss: 1.167436e-02 | valid loss: 1.044845e-02 
      	| train loss (relative): 2.854395e-01 | valid loss (relative): 2.560458e-01 
Epoch 289 use: 358.88 second.

epoch 290 starting......
Epoch:  290 | train loss: 1.166919e-02 | valid loss: 1.047262e-02 
      	| train loss (relative): 2.860683e-01 | valid loss (relative): 2.576980e-01 
Epoch 290 use: 366.14 second.

epoch 291 starting......
Epoch:  291 | train loss: 1.166824e-02 | valid loss: 1.047430e-02 
      	| train loss (relative): 2.852232e-01 | valid loss (relative): 2.574315e-01 
Epoch 291 use: 359.52 second.

epoch 292 starting......
Epoch:  292 | train loss: 1.166818e-02 | valid loss: 1.045063e-02 
      	| train loss (relative): 2.865863e-01 | valid loss (relative): 2.563536e-01 
Epoch 292 use: 354.47 second.

epoch 293 starting......
Epoch:  293 | train loss: 1.167001e-02 | valid loss: 1.043189e-02 
      	| train loss (relative): 2.872955e-01 | valid loss (relative): 2.546799e-01 
Epoch 293 use: 352.20 second.

epoch 294 starting......
Epoch:  294 | train loss: 1.167750e-02 | valid loss: 1.045614e-02 
      	| train loss (relative): 2.852961e-01 | valid loss (relative): 2.568167e-01 
Epoch 294 use: 373.35 second.

epoch 295 starting......
Epoch:  295 | train loss: 1.167225e-02 | valid loss: 1.047398e-02 
      	| train loss (relative): 2.859002e-01 | valid loss (relative): 2.580097e-01 
Epoch 295 use: 353.42 second.

epoch 296 starting......
Epoch:  296 | train loss: 1.166732e-02 | valid loss: 1.045078e-02 
      	| train loss (relative): 2.868897e-01 | valid loss (relative): 2.556576e-01 
Epoch 296 use: 364.03 second.

epoch 297 starting......
Epoch:  297 | train loss: 1.166894e-02 | valid loss: 1.043135e-02 
      	| train loss (relative): 2.859657e-01 | valid loss (relative): 2.554669e-01 
Epoch 297 use: 380.47 second.

epoch 298 starting......
Epoch:  298 | train loss: 1.167350e-02 | valid loss: 1.045606e-02 
      	| train loss (relative): 2.863422e-01 | valid loss (relative): 2.569642e-01 
Epoch 298 use: 369.48 second.

epoch 299 starting......
Epoch:  299 | train loss: 1.167349e-02 | valid loss: 1.044359e-02 
      	| train loss (relative): 2.874959e-01 | valid loss (relative): 2.559226e-01 
Epoch 299 use: 365.11 second.

epoch 300 starting......
Epoch:  300 | train loss: 1.166995e-02 | valid loss: 1.045500e-02 
      	| train loss (relative): 2.855603e-01 | valid loss (relative): 2.566830e-01 
Epoch 300 use: 355.16 second.

epoch 301 starting......
Epoch:  301 | train loss: 1.166881e-02 | valid loss: 1.044143e-02 
      	| train loss (relative): 2.863028e-01 | valid loss (relative): 2.561693e-01 
Epoch 301 use: 370.90 second.

epoch 302 starting......
Epoch:  302 | train loss: 1.167139e-02 | valid loss: 1.044138e-02 
      	| train loss (relative): 2.881568e-01 | valid loss (relative): 2.550324e-01 
Epoch 302 use: 367.62 second.

epoch 303 starting......
Epoch:  303 | train loss: 1.167172e-02 | valid loss: 1.044667e-02 
      	| train loss (relative): 2.858528e-01 | valid loss (relative): 2.554816e-01 
Epoch 303 use: 355.50 second.

epoch 304 starting......
Epoch:  304 | train loss: 1.167167e-02 | valid loss: 1.044883e-02 
      	| train loss (relative): 2.847315e-01 | valid loss (relative): 2.560525e-01 
Epoch 304 use: 364.52 second.

epoch 305 starting......
Epoch:  305 | train loss: 1.167124e-02 | valid loss: 1.046653e-02 
      	| train loss (relative): 2.864125e-01 | valid loss (relative): 2.576402e-01 
Epoch 305 use: 362.79 second.

epoch 306 starting......
Epoch:  306 | train loss: 1.167400e-02 | valid loss: 1.046451e-02 
      	| train loss (relative): 2.852954e-01 | valid loss (relative): 2.568040e-01 
Epoch 306 use: 358.94 second.

epoch 307 starting......
Epoch:  307 | train loss: 1.167178e-02 | valid loss: 1.046597e-02 
      	| train loss (relative): 2.864431e-01 | valid loss (relative): 2.572458e-01 
Epoch 307 use: 361.56 second.

epoch 308 starting......
Epoch:  308 | train loss: 1.167248e-02 | valid loss: 1.043405e-02 
      	| train loss (relative): 2.876574e-01 | valid loss (relative): 2.557912e-01 
Epoch 308 use: 371.73 second.

epoch 309 starting......
Epoch:  309 | train loss: 1.167374e-02 | valid loss: 1.043355e-02 
      	| train loss (relative): 2.866922e-01 | valid loss (relative): 2.536880e-01 
Epoch 309 use: 372.49 second.

epoch 310 starting......
Epoch:  310 | train loss: 1.167191e-02 | valid loss: 1.045568e-02 
      	| train loss (relative): 2.848877e-01 | valid loss (relative): 2.569181e-01 
Epoch 310 use: 380.14 second.

epoch 311 starting......
Epoch:  311 | train loss: 1.166975e-02 | valid loss: 1.045695e-02 
      	| train loss (relative): 2.858411e-01 | valid loss (relative): 2.567304e-01 
Epoch 311 use: 377.26 second.

epoch 312 starting......
Epoch:  312 | train loss: 1.167012e-02 | valid loss: 1.043584e-02 
      	| train loss (relative): 2.875162e-01 | valid loss (relative): 2.555076e-01 
Epoch 312 use: 384.12 second.

epoch 313 starting......
Epoch:  313 | train loss: 1.166928e-02 | valid loss: 1.041527e-02 
      	| train loss (relative): 2.869257e-01 | valid loss (relative): 2.542027e-01 
Epoch 313 use: 374.15 second.

epoch 314 starting......
Epoch:  314 | train loss: 1.166938e-02 | valid loss: 1.046790e-02 
      	| train loss (relative): 2.849905e-01 | valid loss (relative): 2.569104e-01 
Epoch 314 use: 380.75 second.

epoch 315 starting......
Epoch:  315 | train loss: 1.167068e-02 | valid loss: 1.046806e-02 
      	| train loss (relative): 2.874262e-01 | valid loss (relative): 2.568339e-01 
Epoch 315 use: 370.13 second.

epoch 316 starting......
Epoch:  316 | train loss: 1.167263e-02 | valid loss: 1.045891e-02 
      	| train loss (relative): 2.872502e-01 | valid loss (relative): 2.566930e-01 
Epoch 316 use: 357.87 second.

epoch 317 starting......
Epoch:  317 | train loss: 1.167560e-02 | valid loss: 1.045529e-02 
      	| train loss (relative): 2.861432e-01 | valid loss (relative): 2.561332e-01 
Epoch 317 use: 359.77 second.

epoch 318 starting......
Epoch:  318 | train loss: 1.167031e-02 | valid loss: 1.046139e-02 
      	| train loss (relative): 2.873925e-01 | valid loss (relative): 2.571125e-01 
Epoch 318 use: 375.02 second.

epoch 319 starting......
Epoch:  319 | train loss: 1.166923e-02 | valid loss: 1.044395e-02 
      	| train loss (relative): 2.867533e-01 | valid loss (relative): 2.561415e-01 
Epoch 319 use: 365.29 second.

epoch 320 starting......
Epoch:  320 | train loss: 1.167040e-02 | valid loss: 1.045326e-02 
      	| train loss (relative): 2.861782e-01 | valid loss (relative): 2.558655e-01 
Epoch 320 use: 376.12 second.

epoch 321 starting......
Epoch:  321 | train loss: 1.167093e-02 | valid loss: 1.047533e-02 
      	| train loss (relative): 2.849617e-01 | valid loss (relative): 2.574723e-01 
Epoch 321 use: 356.59 second.

epoch 322 starting......
Epoch:  322 | train loss: 1.166863e-02 | valid loss: 1.043257e-02 
      	| train loss (relative): 2.874213e-01 | valid loss (relative): 2.547802e-01 
Epoch 322 use: 361.63 second.

epoch 323 starting......
Epoch:  323 | train loss: 1.167253e-02 | valid loss: 1.044052e-02 
      	| train loss (relative): 2.858898e-01 | valid loss (relative): 2.554841e-01 
Epoch 323 use: 377.00 second.

epoch 324 starting......
Epoch:  324 | train loss: 1.167338e-02 | valid loss: 1.044325e-02 
      	| train loss (relative): 2.855270e-01 | valid loss (relative): 2.560494e-01 
Epoch 324 use: 380.78 second.

epoch 325 starting......
Epoch:  325 | train loss: 1.167153e-02 | valid loss: 1.043879e-02 
      	| train loss (relative): 2.859737e-01 | valid loss (relative): 2.560297e-01 
Epoch 325 use: 398.77 second.

epoch 326 starting......
Epoch:  326 | train loss: 1.167167e-02 | valid loss: 1.046727e-02 
      	| train loss (relative): 2.858469e-01 | valid loss (relative): 2.566598e-01 
Epoch 326 use: 375.94 second.

epoch 327 starting......
Epoch:  327 | train loss: 1.166993e-02 | valid loss: 1.045640e-02 
      	| train loss (relative): 2.858594e-01 | valid loss (relative): 2.560754e-01 
Epoch 327 use: 371.41 second.

epoch 328 starting......
Epoch:  328 | train loss: 1.166883e-02 | valid loss: 1.046847e-02 
      	| train loss (relative): 2.851023e-01 | valid loss (relative): 2.573740e-01 
Epoch 328 use: 364.07 second.

epoch 329 starting......
Epoch:  329 | train loss: 1.166781e-02 | valid loss: 1.047659e-02 
      	| train loss (relative): 2.855583e-01 | valid loss (relative): 2.574375e-01 
Epoch 329 use: 360.45 second.

epoch 330 starting......
Epoch:  330 | train loss: 1.167112e-02 | valid loss: 1.042870e-02 
      	| train loss (relative): 2.871521e-01 | valid loss (relative): 2.550220e-01 
Epoch 330 use: 386.39 second.

epoch 331 starting......
Epoch:  331 | train loss: 1.167015e-02 | valid loss: 1.043964e-02 
      	| train loss (relative): 2.857606e-01 | valid loss (relative): 2.552177e-01 
Epoch 331 use: 373.84 second.

epoch 332 starting......
Epoch:  332 | train loss: 1.166961e-02 | valid loss: 1.044457e-02 
      	| train loss (relative): 2.863816e-01 | valid loss (relative): 2.557000e-01 
Epoch 332 use: 370.83 second.

epoch 333 starting......
Epoch:  333 | train loss: 1.166945e-02 | valid loss: 1.042650e-02 
      	| train loss (relative): 2.861941e-01 | valid loss (relative): 2.550456e-01 
Epoch 333 use: 381.85 second.

epoch 334 starting......
Epoch:  334 | train loss: 1.166985e-02 | valid loss: 1.047060e-02 
      	| train loss (relative): 2.854055e-01 | valid loss (relative): 2.566773e-01 
Epoch 334 use: 364.80 second.

epoch 335 starting......
Epoch:  335 | train loss: 1.167128e-02 | valid loss: 1.044342e-02 
      	| train loss (relative): 2.864172e-01 | valid loss (relative): 2.553207e-01 
Epoch 335 use: 365.34 second.

epoch 336 starting......
Epoch:  336 | train loss: 1.167150e-02 | valid loss: 1.046363e-02 
      	| train loss (relative): 2.855092e-01 | valid loss (relative): 2.571588e-01 
Epoch 336 use: 375.46 second.

epoch 337 starting......
Epoch:  337 | train loss: 1.167260e-02 | valid loss: 1.047823e-02 
      	| train loss (relative): 2.856163e-01 | valid loss (relative): 2.565379e-01 
Epoch 337 use: 366.35 second.

epoch 338 starting......
Epoch:  338 | train loss: 1.167418e-02 | valid loss: 1.042111e-02 
      	| train loss (relative): 2.864415e-01 | valid loss (relative): 2.555369e-01 
Epoch 338 use: 363.97 second.

epoch 339 starting......
Epoch:  339 | train loss: 1.167100e-02 | valid loss: 1.046071e-02 
      	| train loss (relative): 2.864249e-01 | valid loss (relative): 2.569748e-01 
Epoch 339 use: 369.83 second.

epoch 340 starting......
Epoch:  340 | train loss: 1.166871e-02 | valid loss: 1.045503e-02 
      	| train loss (relative): 2.870297e-01 | valid loss (relative): 2.552563e-01 
Epoch 340 use: 366.94 second.

epoch 341 starting......
Epoch:  341 | train loss: 1.166846e-02 | valid loss: 1.049186e-02 
      	| train loss (relative): 2.841448e-01 | valid loss (relative): 2.586222e-01 
Epoch 341 use: 365.34 second.

epoch 342 starting......
Epoch:  342 | train loss: 1.167478e-02 | valid loss: 1.052795e-02 
      	| train loss (relative): 2.872807e-01 | valid loss (relative): 2.585532e-01 
Epoch 342 use: 372.66 second.

epoch 343 starting......
Epoch:  343 | train loss: 1.167274e-02 | valid loss: 1.042499e-02 
      	| train loss (relative): 2.883168e-01 | valid loss (relative): 2.542954e-01 
Epoch 343 use: 372.48 second.

epoch 344 starting......
Epoch:  344 | train loss: 1.167705e-02 | valid loss: 1.045767e-02 
      	| train loss (relative): 2.854475e-01 | valid loss (relative): 2.540951e-01 
Epoch 344 use: 375.16 second.

epoch 345 starting......
Epoch:  345 | train loss: 1.167407e-02 | valid loss: 1.043861e-02 
      	| train loss (relative): 2.862147e-01 | valid loss (relative): 2.555190e-01 
Epoch 345 use: 372.64 second.

epoch 346 starting......
Epoch:  346 | train loss: 1.167217e-02 | valid loss: 1.042917e-02 
      	| train loss (relative): 2.870339e-01 | valid loss (relative): 2.545626e-01 
Epoch 346 use: 364.86 second.

epoch 347 starting......
Epoch:  347 | train loss: 1.167330e-02 | valid loss: 1.044629e-02 
      	| train loss (relative): 2.855820e-01 | valid loss (relative): 2.557505e-01 
Epoch 347 use: 363.88 second.

epoch 348 starting......
Epoch:  348 | train loss: 1.167174e-02 | valid loss: 1.042838e-02 
      	| train loss (relative): 2.866420e-01 | valid loss (relative): 2.546920e-01 
Epoch 348 use: 358.35 second.

epoch 349 starting......
Epoch:  349 | train loss: 1.166915e-02 | valid loss: 1.045682e-02 
      	| train loss (relative): 2.858578e-01 | valid loss (relative): 2.563246e-01 
Epoch 349 use: 367.63 second.

epoch 350 starting......
Epoch:  350 | train loss: 1.167373e-02 | valid loss: 1.045821e-02 
      	| train loss (relative): 2.876057e-01 | valid loss (relative): 2.567017e-01 
Epoch 350 use: 360.65 second.

epoch 351 starting......
Epoch:  351 | train loss: 1.167173e-02 | valid loss: 1.046997e-02 
      	| train loss (relative): 2.860103e-01 | valid loss (relative): 2.572420e-01 
Epoch 351 use: 365.03 second.

epoch 352 starting......
Epoch:  352 | train loss: 1.166988e-02 | valid loss: 1.044780e-02 
      	| train loss (relative): 2.860783e-01 | valid loss (relative): 2.568801e-01 
Epoch 352 use: 381.72 second.

epoch 353 starting......
Epoch:  353 | train loss: 1.167411e-02 | valid loss: 1.046418e-02 
      	| train loss (relative): 2.859146e-01 | valid loss (relative): 2.576548e-01 
Epoch 353 use: 370.94 second.

epoch 354 starting......
Epoch:  354 | train loss: 1.166963e-02 | valid loss: 1.044175e-02 
      	| train loss (relative): 2.857474e-01 | valid loss (relative): 2.563899e-01 
Epoch 354 use: 361.18 second.

epoch 355 starting......
Epoch:  355 | train loss: 1.167230e-02 | valid loss: 1.045837e-02 
      	| train loss (relative): 2.879362e-01 | valid loss (relative): 2.576423e-01 
Epoch 355 use: 364.65 second.

epoch 356 starting......
Epoch:  356 | train loss: 1.166917e-02 | valid loss: 1.046697e-02 
      	| train loss (relative): 2.872346e-01 | valid loss (relative): 2.567255e-01 
Epoch 356 use: 355.71 second.

epoch 357 starting......
Epoch:  357 | train loss: 1.166942e-02 | valid loss: 1.046835e-02 
      	| train loss (relative): 2.860219e-01 | valid loss (relative): 2.579782e-01 
Epoch 357 use: 357.18 second.

epoch 358 starting......
Epoch:  358 | train loss: 1.166943e-02 | valid loss: 1.045461e-02 
      	| train loss (relative): 2.875670e-01 | valid loss (relative): 2.566616e-01 
Epoch 358 use: 359.68 second.

epoch 359 starting......
Epoch:  359 | train loss: 1.166859e-02 | valid loss: 1.044585e-02 
      	| train loss (relative): 2.867645e-01 | valid loss (relative): 2.554921e-01 
Epoch 359 use: 380.25 second.

epoch 360 starting......
Epoch:  360 | train loss: 1.167094e-02 | valid loss: 1.043333e-02 
      	| train loss (relative): 2.861107e-01 | valid loss (relative): 2.545387e-01 
Epoch 360 use: 360.60 second.

epoch 361 starting......
Epoch:  361 | train loss: 1.166934e-02 | valid loss: 1.043887e-02 
      	| train loss (relative): 2.852981e-01 | valid loss (relative): 2.562830e-01 
Epoch 361 use: 371.83 second.

epoch 362 starting......
Epoch:  362 | train loss: 1.167159e-02 | valid loss: 1.043842e-02 
      	| train loss (relative): 2.864480e-01 | valid loss (relative): 2.555245e-01 
Epoch 362 use: 383.89 second.

epoch 363 starting......
Epoch:  363 | train loss: 1.166992e-02 | valid loss: 1.044224e-02 
      	| train loss (relative): 2.862293e-01 | valid loss (relative): 2.554094e-01 
Epoch 363 use: 364.96 second.

epoch 364 starting......
Epoch:  364 | train loss: 1.167509e-02 | valid loss: 1.044851e-02 
      	| train loss (relative): 2.879613e-01 | valid loss (relative): 2.589098e-01 
Epoch 364 use: 393.14 second.

epoch 365 starting......
Epoch:  365 | train loss: 1.170270e-02 | valid loss: 1.042692e-02 
      	| train loss (relative): 2.880461e-01 | valid loss (relative): 2.541448e-01 
Epoch 365 use: 371.97 second.

epoch 366 starting......
Epoch:  366 | train loss: 1.167424e-02 | valid loss: 1.043587e-02 
      	| train loss (relative): 2.852157e-01 | valid loss (relative): 2.544291e-01 
Epoch 366 use: 378.67 second.

epoch 367 starting......
Epoch:  367 | train loss: 1.167148e-02 | valid loss: 1.045252e-02 
      	| train loss (relative): 2.850413e-01 | valid loss (relative): 2.563592e-01 
Epoch 367 use: 346.65 second.

epoch 368 starting......
Epoch:  368 | train loss: 1.167032e-02 | valid loss: 1.046434e-02 
      	| train loss (relative): 2.862742e-01 | valid loss (relative): 2.563077e-01 
Epoch 368 use: 381.65 second.

epoch 369 starting......
Epoch:  369 | train loss: 1.167376e-02 | valid loss: 1.045569e-02 
      	| train loss (relative): 2.864184e-01 | valid loss (relative): 2.565642e-01 
Epoch 369 use: 390.66 second.

epoch 370 starting......
Epoch:  370 | train loss: 1.167493e-02 | valid loss: 1.046631e-02 
      	| train loss (relative): 2.869510e-01 | valid loss (relative): 2.575064e-01 
Epoch 370 use: 402.04 second.

epoch 371 starting......
Epoch:  371 | train loss: 1.166743e-02 | valid loss: 1.040701e-02 
      	| train loss (relative): 2.876912e-01 | valid loss (relative): 2.543673e-01 
Epoch 371 use: 372.43 second.

epoch 372 starting......
Epoch:  372 | train loss: 1.166967e-02 | valid loss: 1.043295e-02 
      	| train loss (relative): 2.846625e-01 | valid loss (relative): 2.556341e-01 
Epoch 372 use: 361.93 second.

epoch 373 starting......
Epoch:  373 | train loss: 1.167192e-02 | valid loss: 1.045658e-02 
      	| train loss (relative): 2.856245e-01 | valid loss (relative): 2.563812e-01 
Epoch 373 use: 367.69 second.

epoch 374 starting......
Epoch:  374 | train loss: 1.167023e-02 | valid loss: 1.047625e-02 
      	| train loss (relative): 2.863545e-01 | valid loss (relative): 2.572522e-01 
Epoch 374 use: 360.75 second.

epoch 375 starting......
Epoch:  375 | train loss: 1.166780e-02 | valid loss: 1.045930e-02 
      	| train loss (relative): 2.862826e-01 | valid loss (relative): 2.563927e-01 
Epoch 375 use: 365.18 second.

epoch 376 starting......
Epoch:  376 | train loss: 1.167659e-02 | valid loss: 1.043622e-02 
      	| train loss (relative): 2.853152e-01 | valid loss (relative): 2.555678e-01 
Epoch 376 use: 376.18 second.

epoch 377 starting......
Epoch:  377 | train loss: 1.166841e-02 | valid loss: 1.044410e-02 
      	| train loss (relative): 2.871719e-01 | valid loss (relative): 2.568510e-01 
Epoch 377 use: 371.19 second.

epoch 378 starting......
Epoch:  378 | train loss: 1.167063e-02 | valid loss: 1.049456e-02 
      	| train loss (relative): 2.879710e-01 | valid loss (relative): 2.583490e-01 
Epoch 378 use: 383.14 second.

epoch 379 starting......
Epoch:  379 | train loss: 1.167248e-02 | valid loss: 1.044656e-02 
      	| train loss (relative): 2.864883e-01 | valid loss (relative): 2.555742e-01 
Epoch 379 use: 360.04 second.

epoch 380 starting......
Epoch:  380 | train loss: 1.167029e-02 | valid loss: 1.043879e-02 
      	| train loss (relative): 2.865138e-01 | valid loss (relative): 2.553093e-01 
Epoch 380 use: 367.11 second.

epoch 381 starting......
Epoch:  381 | train loss: 1.167003e-02 | valid loss: 1.044785e-02 
      	| train loss (relative): 2.861642e-01 | valid loss (relative): 2.559974e-01 
Epoch 381 use: 380.18 second.

epoch 382 starting......
Epoch:  382 | train loss: 1.167239e-02 | valid loss: 1.045483e-02 
      	| train loss (relative): 2.855037e-01 | valid loss (relative): 2.560543e-01 
Epoch 382 use: 374.32 second.

epoch 383 starting......
Epoch:  383 | train loss: 1.166861e-02 | valid loss: 1.045903e-02 
      	| train loss (relative): 2.860671e-01 | valid loss (relative): 2.571249e-01 
Epoch 383 use: 379.65 second.

epoch 384 starting......
Epoch:  384 | train loss: 1.167512e-02 | valid loss: 1.048983e-02 
      	| train loss (relative): 2.884995e-01 | valid loss (relative): 2.581511e-01 
Epoch 384 use: 362.85 second.

epoch 385 starting......
Epoch:  385 | train loss: 1.166891e-02 | valid loss: 1.046112e-02 
      	| train loss (relative): 2.865682e-01 | valid loss (relative): 2.566520e-01 
Epoch 385 use: 370.59 second.

epoch 386 starting......
Epoch:  386 | train loss: 1.166903e-02 | valid loss: 1.044923e-02 
      	| train loss (relative): 2.870912e-01 | valid loss (relative): 2.556685e-01 
Epoch 386 use: 369.40 second.

epoch 387 starting......
Epoch:  387 | train loss: 1.167349e-02 | valid loss: 1.042945e-02 
      	| train loss (relative): 2.848348e-01 | valid loss (relative): 2.558409e-01 
Epoch 387 use: 391.25 second.

epoch 388 starting......
Epoch:  388 | train loss: 1.167286e-02 | valid loss: 1.048822e-02 
      	| train loss (relative): 2.881946e-01 | valid loss (relative): 2.586800e-01 
Epoch 388 use: 367.15 second.

epoch 389 starting......
Epoch:  389 | train loss: 1.167109e-02 | valid loss: 1.049981e-02 
      	| train loss (relative): 2.851473e-01 | valid loss (relative): 2.592821e-01 
Epoch 389 use: 361.39 second.

epoch 390 starting......
Epoch:  390 | train loss: 1.167686e-02 | valid loss: 1.043248e-02 
      	| train loss (relative): 2.909837e-01 | valid loss (relative): 2.569349e-01 
Epoch 390 use: 375.98 second.

epoch 391 starting......
Epoch:  391 | train loss: 1.167038e-02 | valid loss: 1.043826e-02 
      	| train loss (relative): 2.856708e-01 | valid loss (relative): 2.552309e-01 
Epoch 391 use: 380.19 second.

epoch 392 starting......
Epoch:  392 | train loss: 1.166867e-02 | valid loss: 1.046783e-02 
      	| train loss (relative): 2.857853e-01 | valid loss (relative): 2.573576e-01 
Epoch 392 use: 370.99 second.

epoch 393 starting......
Epoch:  393 | train loss: 1.167250e-02 | valid loss: 1.047018e-02 
      	| train loss (relative): 2.878344e-01 | valid loss (relative): 2.577647e-01 
Epoch 393 use: 363.70 second.

epoch 394 starting......
Epoch:  394 | train loss: 1.166893e-02 | valid loss: 1.045783e-02 
      	| train loss (relative): 2.863483e-01 | valid loss (relative): 2.569304e-01 
Epoch 394 use: 385.31 second.

epoch 395 starting......
Epoch:  395 | train loss: 1.166923e-02 | valid loss: 1.047720e-02 
      	| train loss (relative): 2.858260e-01 | valid loss (relative): 2.569427e-01 
Epoch 395 use: 357.79 second.

epoch 396 starting......
Epoch:  396 | train loss: 1.166942e-02 | valid loss: 1.045024e-02 
      	| train loss (relative): 2.862847e-01 | valid loss (relative): 2.552474e-01 
Epoch 396 use: 388.39 second.

epoch 397 starting......
Epoch:  397 | train loss: 1.166868e-02 | valid loss: 1.045917e-02 
      	| train loss (relative): 2.856898e-01 | valid loss (relative): 2.574982e-01 
Epoch 397 use: 368.03 second.

epoch 398 starting......
Epoch:  398 | train loss: 1.166988e-02 | valid loss: 1.045910e-02 
      	| train loss (relative): 2.875257e-01 | valid loss (relative): 2.571071e-01 
Epoch 398 use: 388.09 second.

epoch 399 starting......
Epoch:  399 | train loss: 1.166745e-02 | valid loss: 1.044760e-02 
      	| train loss (relative): 2.865469e-01 | valid loss (relative): 2.563722e-01 
Epoch 399 use: 380.00 second.

test MSE Error: 9.598188e-03 | relative MSE Error: 2.355285e-01 
 Total time used for training: 20.67 hour.
MESLoss saved to  /rds/general/user/jy220/home/results/Variational_False_Changelr_False_MSELoss_Latent_128_nearest_neighbouring_True_SFC_nums_3_startlr_0.0005_n_epoches_400.txt
relative MSELoss saved to  /rds/general/user/jy220/home/results/Variational_False_Changelr_False_reMSELoss_Latent_128_nearest_neighbouring_True_SFC_nums_3_startlr_0.0005_n_epoches_400.txt
model saved to /rds/general/user/jy220/home/results/Variational_False_Changelr_False_Latent_128_Nearest_neighbouring_True_SFC_nums_3_startlr_0.0005_n_epoches_400.pth
model_dict saved to /rds/general/user/jy220/home/results/Variational_False_Changelr_False_Latent_128_Nearest_neighbouring_True_SFC_nums_3_startlr_0.0005_n_epoches_400_dict.pth
... Training slugflow data completed, Run finished Tue  3 Aug 05:32:08 BST 2021 ...
{'vtu_dir': '/rds/general/user/jy220/home/slugflow/', 'data_dir': '/rds/general/user/jy220/home/tensors/', 'data_type': 'tensors', 'vtu_fields': 'Component1::ComponentMassFractionPhase1, phase1::Velocity', 'variational': 'True', 'structured': 'False', 'changing_lr': 'False', 'self_concat': '2', 'dimension': '3', 'sfc_nums': '3', 'components': '4', 'nearest_neighbouring': 'True', 'dims_latent': '128', 'activation': 'Tanh', 'sfc_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/sfcs.pt', 'inv_sfc_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/inv_sfcs.pt', 'coords_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/coords.pt', 'cells_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/cells.npy', 'state_load': 'None', 'batch_size': '16', 'lr': '0.005', 'n_epoches': '80', 'seed': '15', 'visualize': 'False', 'reconstructed_path': 'None', 'reconstruct_start_index': '0', 'reconstruct_end_index': '50', 'save_path': '/rds/general/user/jy220/home/new_results/'}
[[ 2.50825269e-01 -1.67256990e-02 -1.22268730e-03]
 [ 2.38611959e-01 -5.50627777e-03 -5.06451913e-03]
 [ 2.39454874e-01 -1.59563257e-02  3.96778323e-03]
 ...
 [ 9.11361096e+00 -2.21908381e-02  2.22384902e-03]
 [ 9.11847310e+00 -2.84346931e-02 -2.34839590e-04]
 [ 9.11039661e+00 -2.51498573e-02 -8.70661044e-03]]
{'tetra': array([[      0,       1,       2,       3],
       [      4,       5,       6,       7],
       [      8,       9,      10,      11],
       ...,
       [1342744, 1342745, 1342746, 1342747],
       [1342748, 1342749, 1342750, 1342751],
       [1342752, 1342753, 1342754, 1342755]])}
structured  False 
 activation  Tanh() 
 self concat  2 
 sfc_nums  3 

dims_latent  128 
 components  4 
 nearest_neighbouring  True 

visualize  False 
 sample number  1706 

reading sfc nums......
[array([  41684,   41685,   41674, ..., 1235560, 1235564, 1235558]), array([954289, 954290, 954291, ..., 420685, 420687, 420688]), array([  40156,   40155,   40158, ..., 1123478, 1123475, 1123467])]
reading inverse sfc nums......
[array([   8146,    8145,    8144, ..., 1274444, 1274445, 1273166]), array([547280, 547283, 593037, ..., 933933, 933932, 864488]), array([ 66437,  66439, 122512, ..., 459416, 415130, 459418])]
Computing min and max......

tk:  tensor([0.7581, 0.0418, 0.1840, 0.1464], dtype=torch.float64) 

tb:  tensor([-0.3816, -0.3618, -0.0308,  0.1446], dtype=torch.float64) 

length of train set: 1505 

length of valid set: 100 

length of test set: 101 

torch device num: 4 

Let's use 4 GPUs!
epoch 0 starting......
... Training slugflow data completed, Run finished Wed  4 Aug 18:39:03 BST 2021 ...
{'vtu_dir': '/rds/general/user/jy220/home/slugflow/', 'data_dir': '/rds/general/user/jy220/home/tensors/', 'data_type': 'tensors', 'vtu_fields': 'Component1::ComponentMassFractionPhase1, phase1::Velocity', 'variational': 'False', 'structured': 'False', 'changing_lr': 'False', 'self_concat': '2', 'dimension': '3', 'sfc_nums': '3', 'components': '4', 'nearest_neighbouring': 'True', 'dims_latent': '128', 'activation': 'Tanh', 'sfc_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/sfcs.pt', 'inv_sfc_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/inv_sfcs.pt', 'coords_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/coords.pt', 'cells_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/cells.npy', 'state_load': '/rds/general/user/jy220/home/results/Variational_False_Changelr_False_Latent_128_Nearest_neighbouring_True_SFC_nums_3_startlr_0.0005_n_epoches_400_dict.pth', 'batch_size': '16', 'lr': '0.005', 'n_epoches': '200', 'seed': '15', 'visualize': 'False', 'reconstructed_path': 'None', 'reconstruct_start_index': '0', 'reconstruct_end_index': '50', 'save_path': '/rds/general/user/jy220/home/new_results/'}
[[ 2.50825269e-01 -1.67256990e-02 -1.22268730e-03]
 [ 2.38611959e-01 -5.50627777e-03 -5.06451913e-03]
 [ 2.39454874e-01 -1.59563257e-02  3.96778323e-03]
 ...
 [ 9.11361096e+00 -2.21908381e-02  2.22384902e-03]
 [ 9.11847310e+00 -2.84346931e-02 -2.34839590e-04]
 [ 9.11039661e+00 -2.51498573e-02 -8.70661044e-03]]
{'tetra': array([[      0,       1,       2,       3],
       [      4,       5,       6,       7],
       [      8,       9,      10,      11],
       ...,
       [1342744, 1342745, 1342746, 1342747],
       [1342748, 1342749, 1342750, 1342751],
       [1342752, 1342753, 1342754, 1342755]])}
structured  False 
 activation  Tanh() 
 self concat  2 
 sfc_nums  3 

dims_latent  128 
 components  4 
 nearest_neighbouring  True 

visualize  False 
 sample number  1706 

reading sfc nums......
[array([  41684,   41685,   41674, ..., 1235560, 1235564, 1235558]), array([954289, 954290, 954291, ..., 420685, 420687, 420688]), array([  40156,   40155,   40158, ..., 1123478, 1123475, 1123467])]
reading inverse sfc nums......
[array([   8146,    8145,    8144, ..., 1274444, 1274445, 1273166]), array([547280, 547283, 593037, ..., 933933, 933932, 864488]), array([ 66437,  66439, 122512, ..., 459416, 415130, 459418])]
Computing min and max......

tk:  tensor([0.7581, 0.0418, 0.1840, 0.1464], dtype=torch.float64) 

tb:  tensor([-0.3816, -0.3618, -0.0308,  0.1446], dtype=torch.float64) 

length of train set: 1505 

length of valid set: 100 

length of test set: 101 

torch device num: 4 

Let's use 4 GPUs!
epoch 400 starting......
Epoch:  400 | train loss: 1.168611e-02 | valid loss: 9.154138e-03 
      	| train loss (relative): 2.853060e-01 | valid loss (relative): 2.259625e-01 
Epoch 400 use: 438.04 second.

epoch 401 starting......
Epoch:  401 | train loss: 1.168668e-02 | valid loss: 9.156918e-03 
      	| train loss (relative): 2.886288e-01 | valid loss (relative): 2.263687e-01 
Epoch 401 use: 415.58 second.

epoch 402 starting......
Epoch:  402 | train loss: 1.168162e-02 | valid loss: 9.107536e-03 
      	| train loss (relative): 2.871377e-01 | valid loss (relative): 2.231857e-01 
Epoch 402 use: 408.31 second.

epoch 403 starting......
Epoch:  403 | train loss: 1.168366e-02 | valid loss: 9.097345e-03 
      	| train loss (relative): 2.869263e-01 | valid loss (relative): 2.232239e-01 
Epoch 403 use: 398.47 second.

epoch 404 starting......
Epoch:  404 | train loss: 1.168454e-02 | valid loss: 9.105787e-03 
      	| train loss (relative): 2.870996e-01 | valid loss (relative): 2.231344e-01 
Epoch 404 use: 398.24 second.

epoch 405 starting......
Epoch:  405 | train loss: 1.167916e-02 | valid loss: 9.119911e-03 
      	| train loss (relative): 2.860894e-01 | valid loss (relative): 2.235867e-01 
Epoch 405 use: 391.33 second.

epoch 406 starting......
Epoch:  406 | train loss: 1.167869e-02 | valid loss: 9.096048e-03 
      	| train loss (relative): 2.866843e-01 | valid loss (relative): 2.228338e-01 
Epoch 406 use: 383.46 second.

epoch 407 starting......
Epoch:  407 | train loss: 1.168181e-02 | valid loss: 9.139679e-03 
      	| train loss (relative): 2.858057e-01 | valid loss (relative): 2.251405e-01 
Epoch 407 use: 387.30 second.

epoch 408 starting......
Epoch:  408 | train loss: 1.168002e-02 | valid loss: 9.109571e-03 
      	| train loss (relative): 2.870885e-01 | valid loss (relative): 2.236850e-01 
Epoch 408 use: 429.76 second.

epoch 409 starting......
Epoch:  409 | train loss: 1.168135e-02 | valid loss: 9.121753e-03 
      	| train loss (relative): 2.867364e-01 | valid loss (relative): 2.239646e-01 
Epoch 409 use: 405.28 second.

epoch 410 starting......
Epoch:  410 | train loss: 1.168046e-02 | valid loss: 9.123393e-03 
      	| train loss (relative): 2.860555e-01 | valid loss (relative): 2.243009e-01 
Epoch 410 use: 412.88 second.

epoch 411 starting......
Epoch:  411 | train loss: 1.168403e-02 | valid loss: 9.137633e-03 
      	| train loss (relative): 2.882021e-01 | valid loss (relative): 2.248514e-01 
Epoch 411 use: 436.73 second.

epoch 412 starting......
Epoch:  412 | train loss: 1.168011e-02 | valid loss: 9.095164e-03 
      	| train loss (relative): 2.869664e-01 | valid loss (relative): 2.232085e-01 
Epoch 412 use: 421.66 second.

epoch 413 starting......
Epoch:  413 | train loss: 1.168059e-02 | valid loss: 9.090594e-03 
      	| train loss (relative): 2.866630e-01 | valid loss (relative): 2.226590e-01 
Epoch 413 use: 393.16 second.

epoch 414 starting......
Epoch:  414 | train loss: 1.168463e-02 | valid loss: 9.111238e-03 
      	| train loss (relative): 2.857114e-01 | valid loss (relative): 2.233520e-01 
Epoch 414 use: 431.62 second.

epoch 415 starting......
Epoch:  415 | train loss: 1.167895e-02 | valid loss: 9.080069e-03 
      	| train loss (relative): 2.873446e-01 | valid loss (relative): 2.224886e-01 
Epoch 415 use: 389.59 second.

epoch 416 starting......
Epoch:  416 | train loss: 1.168044e-02 | valid loss: 9.116554e-03 
      	| train loss (relative): 2.853903e-01 | valid loss (relative): 2.235740e-01 
Epoch 416 use: 379.81 second.

epoch 417 starting......
Epoch:  417 | train loss: 1.168282e-02 | valid loss: 9.107986e-03 
      	| train loss (relative): 2.875579e-01 | valid loss (relative): 2.237739e-01 
Epoch 417 use: 379.40 second.

epoch 418 starting......
Epoch:  418 | train loss: 1.169010e-02 | valid loss: 9.156670e-03 
      	| train loss (relative): 2.877469e-01 | valid loss (relative): 2.251817e-01 
Epoch 418 use: 403.57 second.

epoch 419 starting......
Epoch:  419 | train loss: 1.168104e-02 | valid loss: 9.127743e-03 
      	| train loss (relative): 2.868489e-01 | valid loss (relative): 2.246161e-01 
Epoch 419 use: 423.26 second.

epoch 420 starting......
Epoch:  420 | train loss: 1.167986e-02 | valid loss: 9.115755e-03 
      	| train loss (relative): 2.871342e-01 | valid loss (relative): 2.243587e-01 
Epoch 420 use: 444.80 second.

epoch 421 starting......
Epoch:  421 | train loss: 1.167988e-02 | valid loss: 9.131555e-03 
      	| train loss (relative): 2.854213e-01 | valid loss (relative): 2.246064e-01 
Epoch 421 use: 399.74 second.

epoch 422 starting......
Epoch:  422 | train loss: 1.168368e-02 | valid loss: 9.138661e-03 
      	| train loss (relative): 2.878844e-01 | valid loss (relative): 2.249990e-01 
Epoch 422 use: 365.44 second.

epoch 423 starting......
Epoch:  423 | train loss: 1.168443e-02 | valid loss: 9.154471e-03 
      	| train loss (relative): 2.861409e-01 | valid loss (relative): 2.256373e-01 
Epoch 423 use: 376.73 second.

epoch 424 starting......
Epoch:  424 | train loss: 1.168274e-02 | valid loss: 9.124943e-03 
      	| train loss (relative): 2.883388e-01 | valid loss (relative): 2.248723e-01 
Epoch 424 use: 393.22 second.

epoch 425 starting......
Epoch:  425 | train loss: 1.168062e-02 | valid loss: 9.123457e-03 
      	| train loss (relative): 2.859381e-01 | valid loss (relative): 2.242343e-01 
Epoch 425 use: 392.51 second.

epoch 426 starting......
Epoch:  426 | train loss: 1.168252e-02 | valid loss: 9.073420e-03 
      	| train loss (relative): 2.876422e-01 | valid loss (relative): 2.218528e-01 
Epoch 426 use: 428.35 second.

epoch 427 starting......
Epoch:  427 | train loss: 1.168291e-02 | valid loss: 9.094810e-03 
      	| train loss (relative): 2.861320e-01 | valid loss (relative): 2.227789e-01 
Epoch 427 use: 410.89 second.

epoch 428 starting......
Epoch:  428 | train loss: 1.168999e-02 | valid loss: 9.086368e-03 
      	| train loss (relative): 2.900901e-01 | valid loss (relative): 2.227969e-01 
Epoch 428 use: 423.20 second.

epoch 429 starting......
Epoch:  429 | train loss: 1.168464e-02 | valid loss: 9.122352e-03 
      	| train loss (relative): 2.858832e-01 | valid loss (relative): 2.238292e-01 
Epoch 429 use: 408.29 second.

epoch 430 starting......
Epoch:  430 | train loss: 1.168259e-02 | valid loss: 9.116240e-03 
      	| train loss (relative): 2.870986e-01 | valid loss (relative): 2.239276e-01 
Epoch 430 use: 389.46 second.

epoch 431 starting......
Epoch:  431 | train loss: 1.168308e-02 | valid loss: 9.112838e-03 
      	| train loss (relative): 2.852833e-01 | valid loss (relative): 2.236695e-01 
Epoch 431 use: 419.15 second.

epoch 432 starting......
Epoch:  432 | train loss: 1.168474e-02 | valid loss: 9.129498e-03 
      	| train loss (relative): 2.869855e-01 | valid loss (relative): 2.245855e-01 
Epoch 432 use: 477.66 second.

epoch 433 starting......
Epoch:  433 | train loss: 1.168370e-02 | valid loss: 9.100893e-03 
      	| train loss (relative): 2.862102e-01 | valid loss (relative): 2.234176e-01 
Epoch 433 use: 415.67 second.

epoch 434 starting......
Epoch:  434 | train loss: 1.167883e-02 | valid loss: 9.104348e-03 
      	| train loss (relative): 2.872300e-01 | valid loss (relative): 2.236251e-01 
Epoch 434 use: 442.75 second.

epoch 435 starting......
Epoch:  435 | train loss: 1.167990e-02 | valid loss: 9.102202e-03 
      	| train loss (relative): 2.864473e-01 | valid loss (relative): 2.236567e-01 
Epoch 435 use: 455.93 second.

epoch 436 starting......
Epoch:  436 | train loss: 1.168185e-02 | valid loss: 9.082189e-03 
      	| train loss (relative): 2.871510e-01 | valid loss (relative): 2.223392e-01 
Epoch 436 use: 436.31 second.

epoch 437 starting......
Epoch:  437 | train loss: 1.168471e-02 | valid loss: 9.125639e-03 
      	| train loss (relative): 2.877254e-01 | valid loss (relative): 2.240775e-01 
Epoch 437 use: 420.65 second.

epoch 438 starting......
Epoch:  438 | train loss: 1.167891e-02 | valid loss: 9.107123e-03 
      	| train loss (relative): 2.870336e-01 | valid loss (relative): 2.234790e-01 
Epoch 438 use: 423.97 second.

epoch 439 starting......
Epoch:  439 | train loss: 1.167982e-02 | valid loss: 9.112913e-03 
      	| train loss (relative): 2.867751e-01 | valid loss (relative): 2.242201e-01 
Epoch 439 use: 408.88 second.

epoch 440 starting......
Epoch:  440 | train loss: 1.168117e-02 | valid loss: 9.129186e-03 
      	| train loss (relative): 2.866516e-01 | valid loss (relative): 2.242221e-01 
Epoch 440 use: 418.87 second.

epoch 441 starting......
Epoch:  441 | train loss: 1.168857e-02 | valid loss: 9.160924e-03 
      	| train loss (relative): 2.878514e-01 | valid loss (relative): 2.257410e-01 
Epoch 441 use: 393.56 second.

epoch 442 starting......
Epoch:  442 | train loss: 1.168161e-02 | valid loss: 9.131818e-03 
      	| train loss (relative): 2.874360e-01 | valid loss (relative): 2.243448e-01 
Epoch 442 use: 407.78 second.

epoch 443 starting......
Epoch:  443 | train loss: 1.168352e-02 | valid loss: 9.084405e-03 
      	| train loss (relative): 2.869521e-01 | valid loss (relative): 2.221272e-01 
Epoch 443 use: 438.86 second.

epoch 444 starting......
Epoch:  444 | train loss: 1.168246e-02 | valid loss: 9.119447e-03 
      	| train loss (relative): 2.870497e-01 | valid loss (relative): 2.244227e-01 
Epoch 444 use: 419.22 second.

epoch 445 starting......
Epoch:  445 | train loss: 1.168067e-02 | valid loss: 9.114116e-03 
      	| train loss (relative): 2.872566e-01 | valid loss (relative): 2.237239e-01 
Epoch 445 use: 399.10 second.

epoch 446 starting......
Epoch:  446 | train loss: 1.167998e-02 | valid loss: 9.111363e-03 
      	| train loss (relative): 2.865434e-01 | valid loss (relative): 2.238533e-01 
Epoch 446 use: 408.73 second.

epoch 447 starting......
Epoch:  447 | train loss: 1.168219e-02 | valid loss: 9.134430e-03 
      	| train loss (relative): 2.868180e-01 | valid loss (relative): 2.246069e-01 
Epoch 447 use: 403.24 second.

epoch 448 starting......
Epoch:  448 | train loss: 1.168200e-02 | valid loss: 9.114574e-03 
      	| train loss (relative): 2.866521e-01 | valid loss (relative): 2.243599e-01 
Epoch 448 use: 400.76 second.

epoch 449 starting......
Epoch:  449 | train loss: 1.168117e-02 | valid loss: 9.119825e-03 
      	| train loss (relative): 2.868580e-01 | valid loss (relative): 2.241605e-01 
Epoch 449 use: 410.89 second.

epoch 450 starting......
Epoch:  450 | train loss: 1.169199e-02 | valid loss: 9.142488e-03 
      	| train loss (relative): 2.890600e-01 | valid loss (relative): 2.246267e-01 
Epoch 450 use: 438.34 second.

epoch 451 starting......
Epoch:  451 | train loss: 1.167989e-02 | valid loss: 9.096107e-03 
      	| train loss (relative): 2.868020e-01 | valid loss (relative): 2.232017e-01 
Epoch 451 use: 411.43 second.

epoch 452 starting......
Epoch:  452 | train loss: 1.168307e-02 | valid loss: 9.094228e-03 
      	| train loss (relative): 2.869530e-01 | valid loss (relative): 2.226785e-01 
Epoch 452 use: 410.91 second.

epoch 453 starting......
Epoch:  453 | train loss: 1.168341e-02 | valid loss: 9.118359e-03 
      	| train loss (relative): 2.860758e-01 | valid loss (relative): 2.239385e-01 
Epoch 453 use: 453.48 second.

epoch 454 starting......
Epoch:  454 | train loss: 1.168159e-02 | valid loss: 9.120977e-03 
      	| train loss (relative): 2.865071e-01 | valid loss (relative): 2.238475e-01 
Epoch 454 use: 446.46 second.

epoch 455 starting......
Epoch:  455 | train loss: 1.168032e-02 | valid loss: 9.095668e-03 
      	| train loss (relative): 2.867815e-01 | valid loss (relative): 2.225219e-01 
Epoch 455 use: 419.63 second.

epoch 456 starting......
Epoch:  456 | train loss: 1.168067e-02 | valid loss: 9.097393e-03 
      	| train loss (relative): 2.865546e-01 | valid loss (relative): 2.230809e-01 
Epoch 456 use: 418.15 second.

epoch 457 starting......
Epoch:  457 | train loss: 1.168170e-02 | valid loss: 9.103828e-03 
      	| train loss (relative): 2.858732e-01 | valid loss (relative): 2.238011e-01 
Epoch 457 use: 449.09 second.

epoch 458 starting......
Epoch:  458 | train loss: 1.168283e-02 | valid loss: 9.113207e-03 
      	| train loss (relative): 2.876815e-01 | valid loss (relative): 2.240270e-01 
Epoch 458 use: 414.78 second.

epoch 459 starting......
Epoch:  459 | train loss: 1.168503e-02 | valid loss: 9.145849e-03 
      	| train loss (relative): 2.862976e-01 | valid loss (relative): 2.251924e-01 
Epoch 459 use: 402.01 second.

epoch 460 starting......
Epoch:  460 | train loss: 1.167992e-02 | valid loss: 9.123109e-03 
      	| train loss (relative): 2.860097e-01 | valid loss (relative): 2.245500e-01 
Epoch 460 use: 434.34 second.

epoch 461 starting......
Epoch:  461 | train loss: 1.168768e-02 | valid loss: 9.137276e-03 
      	| train loss (relative): 2.894456e-01 | valid loss (relative): 2.242182e-01 
Epoch 461 use: 408.95 second.

epoch 462 starting......
Epoch:  462 | train loss: 1.168371e-02 | valid loss: 9.123914e-03 
      	| train loss (relative): 2.870798e-01 | valid loss (relative): 2.237379e-01 
Epoch 462 use: 436.62 second.

epoch 463 starting......
Epoch:  463 | train loss: 1.168014e-02 | valid loss: 9.089768e-03 
      	| train loss (relative): 2.868441e-01 | valid loss (relative): 2.225993e-01 
Epoch 463 use: 396.28 second.

epoch 464 starting......
Epoch:  464 | train loss: 1.168019e-02 | valid loss: 9.164885e-03 
      	| train loss (relative): 2.851343e-01 | valid loss (relative): 2.258559e-01 
Epoch 464 use: 419.77 second.

epoch 465 starting......
Epoch:  465 | train loss: 1.168139e-02 | valid loss: 9.089599e-03 
      	| train loss (relative): 2.877364e-01 | valid loss (relative): 2.226747e-01 
Epoch 465 use: 455.46 second.

epoch 466 starting......
Epoch:  466 | train loss: 1.168196e-02 | valid loss: 9.091119e-03 
      	| train loss (relative): 2.867540e-01 | valid loss (relative): 2.226181e-01 
Epoch 466 use: 399.73 second.

epoch 467 starting......
Epoch:  467 | train loss: 1.168017e-02 | valid loss: 9.103769e-03 
      	| train loss (relative): 2.862852e-01 | valid loss (relative): 2.226610e-01 
Epoch 467 use: 395.92 second.

epoch 468 starting......
Epoch:  468 | train loss: 1.168152e-02 | valid loss: 9.119208e-03 
      	| train loss (relative): 2.869561e-01 | valid loss (relative): 2.237858e-01 
Epoch 468 use: 404.94 second.

epoch 469 starting......
Epoch:  469 | train loss: 1.168059e-02 | valid loss: 9.086712e-03 
      	| train loss (relative): 2.860612e-01 | valid loss (relative): 2.228711e-01 
Epoch 469 use: 426.72 second.

epoch 470 starting......
Epoch:  470 | train loss: 1.168060e-02 | valid loss: 9.115945e-03 
      	| train loss (relative): 2.859485e-01 | valid loss (relative): 2.238075e-01 
Epoch 470 use: 456.62 second.

epoch 471 starting......
Epoch:  471 | train loss: 1.168172e-02 | valid loss: 9.134158e-03 
      	| train loss (relative): 2.865642e-01 | valid loss (relative): 2.250897e-01 
Epoch 471 use: 399.60 second.

epoch 472 starting......
Epoch:  472 | train loss: 1.167886e-02 | valid loss: 9.091093e-03 
      	| train loss (relative): 2.872937e-01 | valid loss (relative): 2.228325e-01 
Epoch 472 use: 407.23 second.

epoch 473 starting......
Epoch:  473 | train loss: 1.168335e-02 | valid loss: 9.098551e-03 
      	| train loss (relative): 2.862565e-01 | valid loss (relative): 2.235993e-01 
Epoch 473 use: 403.81 second.

epoch 474 starting......
Epoch:  474 | train loss: 1.168647e-02 | valid loss: 9.115642e-03 
      	| train loss (relative): 2.873369e-01 | valid loss (relative): 2.235122e-01 
Epoch 474 use: 415.59 second.

epoch 475 starting......
Epoch:  475 | train loss: 1.167952e-02 | valid loss: 9.125044e-03 
      	| train loss (relative): 2.855267e-01 | valid loss (relative): 2.240295e-01 
Epoch 475 use: 423.78 second.

epoch 476 starting......
Epoch:  476 | train loss: 1.168283e-02 | valid loss: 9.117824e-03 
      	| train loss (relative): 2.870709e-01 | valid loss (relative): 2.238223e-01 
Epoch 476 use: 436.51 second.

epoch 477 starting......
Epoch:  477 | train loss: 1.168253e-02 | valid loss: 9.121543e-03 
      	| train loss (relative): 2.858604e-01 | valid loss (relative): 2.240268e-01 
Epoch 477 use: 396.09 second.

epoch 478 starting......
Epoch:  478 | train loss: 1.168406e-02 | valid loss: 9.123026e-03 
      	| train loss (relative): 2.875036e-01 | valid loss (relative): 2.243221e-01 
Epoch 478 use: 397.89 second.

epoch 479 starting......
Epoch:  479 | train loss: 1.168394e-02 | valid loss: 9.124348e-03 
      	| train loss (relative): 2.859635e-01 | valid loss (relative): 2.244051e-01 
Epoch 479 use: 442.10 second.

epoch 480 starting......
Epoch:  480 | train loss: 1.168952e-02 | valid loss: 9.120670e-03 
      	| train loss (relative): 2.900406e-01 | valid loss (relative): 2.245173e-01 
Epoch 480 use: 404.23 second.

epoch 481 starting......
Epoch:  481 | train loss: 1.168010e-02 | valid loss: 9.104733e-03 
      	| train loss (relative): 2.865045e-01 | valid loss (relative): 2.224798e-01 
Epoch 481 use: 408.09 second.

epoch 482 starting......
Epoch:  482 | train loss: 1.168030e-02 | valid loss: 9.101967e-03 
      	| train loss (relative): 2.861201e-01 | valid loss (relative): 2.233612e-01 
Epoch 482 use: 399.66 second.

epoch 483 starting......
Epoch:  483 | train loss: 1.168206e-02 | valid loss: 9.115264e-03 
      	| train loss (relative): 2.866346e-01 | valid loss (relative): 2.241312e-01 
Epoch 483 use: 471.51 second.

epoch 484 starting......
Epoch:  484 | train loss: 1.168188e-02 | valid loss: 9.099768e-03 
      	| train loss (relative): 2.860393e-01 | valid loss (relative): 2.234633e-01 
Epoch 484 use: 398.64 second.

epoch 485 starting......
Epoch:  485 | train loss: 1.168155e-02 | valid loss: 9.134844e-03 
      	| train loss (relative): 2.863377e-01 | valid loss (relative): 2.251415e-01 
Epoch 485 use: 419.65 second.

epoch 486 starting......
Epoch:  486 | train loss: 1.168015e-02 | valid loss: 9.103782e-03 
      	| train loss (relative): 2.861178e-01 | valid loss (relative): 2.225983e-01 
Epoch 486 use: 429.77 second.

epoch 487 starting......
Epoch:  487 | train loss: 1.168065e-02 | valid loss: 9.131717e-03 
      	| train loss (relative): 2.863336e-01 | valid loss (relative): 2.243059e-01 
Epoch 487 use: 399.07 second.

epoch 488 starting......
Epoch:  488 | train loss: 1.168313e-02 | valid loss: 9.123463e-03 
      	| train loss (relative): 2.862357e-01 | valid loss (relative): 2.244137e-01 
Epoch 488 use: 398.60 second.

epoch 489 starting......
Epoch:  489 | train loss: 1.168322e-02 | valid loss: 9.130490e-03 
      	| train loss (relative): 2.874463e-01 | valid loss (relative): 2.236266e-01 
Epoch 489 use: 437.88 second.

epoch 490 starting......
Epoch:  490 | train loss: 1.168185e-02 | valid loss: 9.104828e-03 
      	| train loss (relative): 2.866657e-01 | valid loss (relative): 2.233070e-01 
Epoch 490 use: 405.73 second.

epoch 491 starting......
Epoch:  491 | train loss: 1.167870e-02 | valid loss: 9.134069e-03 
      	| train loss (relative): 2.861030e-01 | valid loss (relative): 2.245499e-01 
Epoch 491 use: 390.89 second.

epoch 492 starting......
Epoch:  492 | train loss: 1.167918e-02 | valid loss: 9.088212e-03 
      	| train loss (relative): 2.874767e-01 | valid loss (relative): 2.227804e-01 
Epoch 492 use: 397.11 second.

epoch 493 starting......
Epoch:  493 | train loss: 1.168163e-02 | valid loss: 9.129956e-03 
      	| train loss (relative): 2.859758e-01 | valid loss (relative): 2.249075e-01 
Epoch 493 use: 404.79 second.

epoch 494 starting......
Epoch:  494 | train loss: 1.168139e-02 | valid loss: 9.127320e-03 
      	| train loss (relative): 2.874188e-01 | valid loss (relative): 2.245917e-01 
Epoch 494 use: 418.94 second.

epoch 495 starting......
Epoch:  495 | train loss: 1.168263e-02 | valid loss: 9.098243e-03 
      	| train loss (relative): 2.864209e-01 | valid loss (relative): 2.225869e-01 
Epoch 495 use: 399.74 second.

epoch 496 starting......
Epoch:  496 | train loss: 1.167927e-02 | valid loss: 9.096834e-03 
      	| train loss (relative): 2.869309e-01 | valid loss (relative): 2.224307e-01 
Epoch 496 use: 398.18 second.

epoch 497 starting......
Epoch:  497 | train loss: 1.168206e-02 | valid loss: 9.111968e-03 
      	| train loss (relative): 2.863249e-01 | valid loss (relative): 2.236213e-01 
Epoch 497 use: 407.97 second.

epoch 498 starting......
Epoch:  498 | train loss: 1.168713e-02 | valid loss: 9.132606e-03 
      	| train loss (relative): 2.862086e-01 | valid loss (relative): 2.249924e-01 
Epoch 498 use: 424.61 second.

epoch 499 starting......
Epoch:  499 | train loss: 1.168019e-02 | valid loss: 9.085031e-03 
      	| train loss (relative): 2.866458e-01 | valid loss (relative): 2.224207e-01 
Epoch 499 use: 474.40 second.

epoch 500 starting......
Epoch:  500 | train loss: 1.168174e-02 | valid loss: 9.097154e-03 
      	| train loss (relative): 2.857309e-01 | valid loss (relative): 2.231930e-01 
Epoch 500 use: 408.06 second.

epoch 501 starting......
Epoch:  501 | train loss: 1.168157e-02 | valid loss: 9.097257e-03 
      	| train loss (relative): 2.871270e-01 | valid loss (relative): 2.231360e-01 
Epoch 501 use: 403.73 second.

epoch 502 starting......
Epoch:  502 | train loss: 1.168524e-02 | valid loss: 9.159590e-03 
      	| train loss (relative): 2.854871e-01 | valid loss (relative): 2.260475e-01 
Epoch 502 use: 401.95 second.

epoch 503 starting......
Epoch:  503 | train loss: 1.168629e-02 | valid loss: 9.154262e-03 
      	| train loss (relative): 2.859962e-01 | valid loss (relative): 2.250429e-01 
Epoch 503 use: 398.57 second.

epoch 504 starting......
Epoch:  504 | train loss: 1.168182e-02 | valid loss: 9.108349e-03 
      	| train loss (relative): 2.878329e-01 | valid loss (relative): 2.236762e-01 
Epoch 504 use: 429.29 second.

epoch 505 starting......
Epoch:  505 | train loss: 1.168192e-02 | valid loss: 9.133670e-03 
      	| train loss (relative): 2.870505e-01 | valid loss (relative): 2.249652e-01 
Epoch 505 use: 411.99 second.

epoch 506 starting......
Epoch:  506 | train loss: 1.168783e-02 | valid loss: 9.134707e-03 
      	| train loss (relative): 2.877239e-01 | valid loss (relative): 2.243093e-01 
Epoch 506 use: 434.66 second.

epoch 507 starting......
Epoch:  507 | train loss: 1.167824e-02 | valid loss: 9.129146e-03 
      	| train loss (relative): 2.857611e-01 | valid loss (relative): 2.244621e-01 
Epoch 507 use: 398.47 second.

epoch 508 starting......
Epoch:  508 | train loss: 1.168146e-02 | valid loss: 9.075911e-03 
      	| train loss (relative): 2.880174e-01 | valid loss (relative): 2.220200e-01 
Epoch 508 use: 492.97 second.

epoch 509 starting......
Epoch:  509 | train loss: 1.168165e-02 | valid loss: 9.096399e-03 
      	| train loss (relative): 2.862071e-01 | valid loss (relative): 2.229806e-01 
Epoch 509 use: 497.01 second.

epoch 510 starting......
Epoch:  510 | train loss: 1.168042e-02 | valid loss: 9.080848e-03 
      	| train loss (relative): 2.868757e-01 | valid loss (relative): 2.222240e-01 
Epoch 510 use: 431.90 second.

epoch 511 starting......
Epoch:  511 | train loss: 1.167986e-02 | valid loss: 9.135270e-03 
      	| train loss (relative): 2.847371e-01 | valid loss (relative): 2.248938e-01 
Epoch 511 use: 401.66 second.

epoch 512 starting......
Epoch:  512 | train loss: 1.168292e-02 | valid loss: 9.126646e-03 
      	| train loss (relative): 2.874566e-01 | valid loss (relative): 2.243561e-01 
Epoch 512 use: 427.50 second.

epoch 513 starting......
Epoch:  513 | train loss: 1.168138e-02 | valid loss: 9.094827e-03 
      	| train loss (relative): 2.877423e-01 | valid loss (relative): 2.226507e-01 
Epoch 513 use: 475.91 second.

epoch 514 starting......
Epoch:  514 | train loss: 1.168406e-02 | valid loss: 9.105249e-03 
      	| train loss (relative): 2.860643e-01 | valid loss (relative): 2.231751e-01 
Epoch 514 use: 434.91 second.

epoch 515 starting......
Epoch:  515 | train loss: 1.167938e-02 | valid loss: 9.092385e-03 
      	| train loss (relative): 2.864148e-01 | valid loss (relative): 2.229875e-01 
Epoch 515 use: 418.45 second.

epoch 516 starting......
Epoch:  516 | train loss: 1.167916e-02 | valid loss: 9.119627e-03 
      	| train loss (relative): 2.861396e-01 | valid loss (relative): 2.236509e-01 
Epoch 516 use: 415.37 second.

epoch 517 starting......
Epoch:  517 | train loss: 1.168280e-02 | valid loss: 9.097762e-03 
      	| train loss (relative): 2.871759e-01 | valid loss (relative): 2.227523e-01 
Epoch 517 use: 414.82 second.

epoch 518 starting......
Epoch:  518 | train loss: 1.168204e-02 | valid loss: 9.130565e-03 
      	| train loss (relative): 2.852302e-01 | valid loss (relative): 2.248859e-01 
Epoch 518 use: 476.16 second.

epoch 519 starting......
Epoch:  519 | train loss: 1.168613e-02 | valid loss: 9.110019e-03 
      	| train loss (relative): 2.897171e-01 | valid loss (relative): 2.238090e-01 
Epoch 519 use: 473.13 second.

epoch 520 starting......
Epoch:  520 | train loss: 1.168793e-02 | valid loss: 9.111967e-03 
      	| train loss (relative): 2.867678e-01 | valid loss (relative): 2.241528e-01 
Epoch 520 use: 437.16 second.

epoch 521 starting......
Epoch:  521 | train loss: 1.168776e-02 | valid loss: 9.108569e-03 
      	| train loss (relative): 2.862218e-01 | valid loss (relative): 2.247239e-01 
Epoch 521 use: 432.48 second.

epoch 522 starting......
Epoch:  522 | train loss: 1.168187e-02 | valid loss: 9.086057e-03 
      	| train loss (relative): 2.882948e-01 | valid loss (relative): 2.227978e-01 
Epoch 522 use: 431.94 second.

epoch 523 starting......
Epoch:  523 | train loss: 1.168249e-02 | valid loss: 9.102855e-03 
      	| train loss (relative): 2.856383e-01 | valid loss (relative): 2.225811e-01 
Epoch 523 use: 454.94 second.

epoch 524 starting......
Epoch:  524 | train loss: 1.167995e-02 | valid loss: 9.119761e-03 
      	| train loss (relative): 2.865281e-01 | valid loss (relative): 2.240133e-01 
Epoch 524 use: 466.47 second.

epoch 525 starting......
Epoch:  525 | train loss: 1.168047e-02 | valid loss: 9.153697e-03 
      	| train loss (relative): 2.860733e-01 | valid loss (relative): 2.253041e-01 
Epoch 525 use: 421.46 second.

epoch 526 starting......
Epoch:  526 | train loss: 1.168343e-02 | valid loss: 9.128905e-03 
      	| train loss (relative): 2.863443e-01 | valid loss (relative): 2.249023e-01 
Epoch 526 use: 438.53 second.

epoch 527 starting......
Epoch:  527 | train loss: 1.168194e-02 | valid loss: 9.108028e-03 
      	| train loss (relative): 2.890605e-01 | valid loss (relative): 2.244792e-01 
Epoch 527 use: 452.95 second.

epoch 528 starting......
Epoch:  528 | train loss: 1.168127e-02 | valid loss: 9.111296e-03 
      	| train loss (relative): 2.866205e-01 | valid loss (relative): 2.234432e-01 
Epoch 528 use: 415.09 second.

epoch 529 starting......
Epoch:  529 | train loss: 1.168300e-02 | valid loss: 9.108406e-03 
      	| train loss (relative): 2.867802e-01 | valid loss (relative): 2.235074e-01 
Epoch 529 use: 475.25 second.

epoch 530 starting......
Epoch:  530 | train loss: 1.167958e-02 | valid loss: 9.144587e-03 
      	| train loss (relative): 2.866694e-01 | valid loss (relative): 2.248524e-01 
Epoch 530 use: 449.11 second.

epoch 531 starting......
Epoch:  531 | train loss: 1.168288e-02 | valid loss: 9.132353e-03 
      	| train loss (relative): 2.878588e-01 | valid loss (relative): 2.247400e-01 
Epoch 531 use: 419.41 second.

epoch 532 starting......
Epoch:  532 | train loss: 1.168617e-02 | valid loss: 9.106353e-03 
      	| train loss (relative): 2.874423e-01 | valid loss (relative): 2.238805e-01 
Epoch 532 use: 427.37 second.

epoch 533 starting......
Epoch:  533 | train loss: 1.168184e-02 | valid loss: 9.109136e-03 
      	| train loss (relative): 2.868502e-01 | valid loss (relative): 2.240486e-01 
Epoch 533 use: 443.72 second.

epoch 534 starting......
Epoch:  534 | train loss: 1.167868e-02 | valid loss: 9.078122e-03 
      	| train loss (relative): 2.864202e-01 | valid loss (relative): 2.223014e-01 
Epoch 534 use: 471.94 second.

epoch 535 starting......
Epoch:  535 | train loss: 1.168172e-02 | valid loss: 9.127398e-03 
      	| train loss (relative): 2.859956e-01 | valid loss (relative): 2.244749e-01 
Epoch 535 use: 426.20 second.

epoch 536 starting......
Epoch:  536 | train loss: 1.168127e-02 | valid loss: 9.086360e-03 
      	| train loss (relative): 2.868849e-01 | valid loss (relative): 2.226163e-01 
Epoch 536 use: 417.09 second.

epoch 537 starting......
Epoch:  537 | train loss: 1.168317e-02 | valid loss: 9.128476e-03 
      	| train loss (relative): 2.858151e-01 | valid loss (relative): 2.244704e-01 
Epoch 537 use: 447.10 second.

epoch 538 starting......
Epoch:  538 | train loss: 1.168143e-02 | valid loss: 9.080678e-03 
      	| train loss (relative): 2.868389e-01 | valid loss (relative): 2.223684e-01 
Epoch 538 use: 424.39 second.

epoch 539 starting......
Epoch:  539 | train loss: 1.168110e-02 | valid loss: 9.114096e-03 
      	| train loss (relative): 2.866402e-01 | valid loss (relative): 2.236529e-01 
Epoch 539 use: 459.81 second.

epoch 540 starting......
Epoch:  540 | train loss: 1.168179e-02 | valid loss: 9.085018e-03 
      	| train loss (relative): 2.866447e-01 | valid loss (relative): 2.221372e-01 
Epoch 540 use: 502.41 second.

epoch 541 starting......
Epoch:  541 | train loss: 1.168173e-02 | valid loss: 9.134986e-03 
      	| train loss (relative): 2.862450e-01 | valid loss (relative): 2.252374e-01 
Epoch 541 use: 476.13 second.

epoch 542 starting......
Epoch:  542 | train loss: 1.168291e-02 | valid loss: 9.138228e-03 
      	| train loss (relative): 2.892570e-01 | valid loss (relative): 2.247499e-01 
Epoch 542 use: 425.78 second.

epoch 543 starting......
Epoch:  543 | train loss: 1.169082e-02 | valid loss: 9.162124e-03 
      	| train loss (relative): 2.883560e-01 | valid loss (relative): 2.254216e-01 
Epoch 543 use: 480.48 second.

epoch 544 starting......
Epoch:  544 | train loss: 1.168052e-02 | valid loss: 9.088843e-03 
      	| train loss (relative): 2.881555e-01 | valid loss (relative): 2.227362e-01 
Epoch 544 use: 442.84 second.

epoch 545 starting......
Epoch:  545 | train loss: 1.168061e-02 | valid loss: 9.129826e-03 
      	| train loss (relative): 2.850404e-01 | valid loss (relative): 2.235607e-01 
Epoch 545 use: 446.03 second.

epoch 546 starting......
Epoch:  546 | train loss: 1.168143e-02 | valid loss: 9.118663e-03 
      	| train loss (relative): 2.867831e-01 | valid loss (relative): 2.243375e-01 
Epoch 546 use: 464.64 second.

epoch 547 starting......
Epoch:  547 | train loss: 1.167978e-02 | valid loss: 9.085798e-03 
      	| train loss (relative): 2.882823e-01 | valid loss (relative): 2.228878e-01 
Epoch 547 use: 402.62 second.

epoch 548 starting......
Epoch:  548 | train loss: 1.168266e-02 | valid loss: 9.086768e-03 
      	| train loss (relative): 2.866907e-01 | valid loss (relative): 2.223407e-01 
Epoch 548 use: 416.91 second.

epoch 549 starting......
Epoch:  549 | train loss: 1.168069e-02 | valid loss: 9.095580e-03 
      	| train loss (relative): 2.861957e-01 | valid loss (relative): 2.231235e-01 
Epoch 549 use: 419.46 second.

epoch 550 starting......
Epoch:  550 | train loss: 1.168172e-02 | valid loss: 9.074822e-03 
      	| train loss (relative): 2.871885e-01 | valid loss (relative): 2.219635e-01 
Epoch 550 use: 441.62 second.

epoch 551 starting......
Epoch:  551 | train loss: 1.167817e-02 | valid loss: 9.127686e-03 
      	| train loss (relative): 2.856129e-01 | valid loss (relative): 2.244294e-01 
Epoch 551 use: 510.07 second.

epoch 552 starting......
Epoch:  552 | train loss: 1.168131e-02 | valid loss: 9.118438e-03 
      	| train loss (relative): 2.872569e-01 | valid loss (relative): 2.239206e-01 
Epoch 552 use: 464.08 second.

epoch 553 starting......
Epoch:  553 | train loss: 1.168110e-02 | valid loss: 9.136286e-03 
      	| train loss (relative): 2.872280e-01 | valid loss (relative): 2.251774e-01 
Epoch 553 use: 447.98 second.

epoch 554 starting......
Epoch:  554 | train loss: 1.168747e-02 | valid loss: 9.104466e-03 
      	| train loss (relative): 2.880286e-01 | valid loss (relative): 2.232302e-01 
Epoch 554 use: 484.85 second.

epoch 555 starting......
Epoch:  555 | train loss: 1.168075e-02 | valid loss: 9.101604e-03 
      	| train loss (relative): 2.870204e-01 | valid loss (relative): 2.233068e-01 
Epoch 555 use: 430.93 second.

epoch 556 starting......
Epoch:  556 | train loss: 1.167878e-02 | valid loss: 9.095527e-03 
      	| train loss (relative): 2.871874e-01 | valid loss (relative): 2.231451e-01 
Epoch 556 use: 500.86 second.

epoch 557 starting......
Epoch:  557 | train loss: 1.168164e-02 | valid loss: 9.142785e-03 
      	| train loss (relative): 2.863335e-01 | valid loss (relative): 2.250849e-01 
Epoch 557 use: 433.24 second.

epoch 558 starting......
Epoch:  558 | train loss: 1.168045e-02 | valid loss: 9.125904e-03 
      	| train loss (relative): 2.874219e-01 | valid loss (relative): 2.243323e-01 
Epoch 558 use: 457.45 second.

epoch 559 starting......
Epoch:  559 | train loss: 1.168048e-02 | valid loss: 9.109800e-03 
      	| train loss (relative): 2.863641e-01 | valid loss (relative): 2.235056e-01 
Epoch 559 use: 407.70 second.

epoch 560 starting......
Epoch:  560 | train loss: 1.168062e-02 | valid loss: 9.111981e-03 
      	| train loss (relative): 2.863968e-01 | valid loss (relative): 2.242095e-01 
Epoch 560 use: 430.58 second.

epoch 561 starting......
Epoch:  561 | train loss: 1.168270e-02 | valid loss: 9.092592e-03 
      	| train loss (relative): 2.892527e-01 | valid loss (relative): 2.236428e-01 
Epoch 561 use: 512.18 second.

epoch 562 starting......
Epoch:  562 | train loss: 1.168263e-02 | valid loss: 9.105588e-03 
      	| train loss (relative): 2.853638e-01 | valid loss (relative): 2.240247e-01 
Epoch 562 use: 435.30 second.

epoch 563 starting......
Epoch:  563 | train loss: 1.168534e-02 | valid loss: 9.133919e-03 
      	| train loss (relative): 2.872439e-01 | valid loss (relative): 2.250530e-01 
Epoch 563 use: 454.16 second.

epoch 564 starting......
Epoch:  564 | train loss: 1.168353e-02 | valid loss: 9.112249e-03 
      	| train loss (relative): 2.867731e-01 | valid loss (relative): 2.245286e-01 
Epoch 564 use: 425.57 second.

epoch 565 starting......
Epoch:  565 | train loss: 1.168771e-02 | valid loss: 9.148150e-03 
      	| train loss (relative): 2.882643e-01 | valid loss (relative): 2.252947e-01 
Epoch 565 use: 474.59 second.

epoch 566 starting......
Epoch:  566 | train loss: 1.168183e-02 | valid loss: 9.103677e-03 
      	| train loss (relative): 2.872601e-01 | valid loss (relative): 2.230380e-01 
Epoch 566 use: 444.25 second.

epoch 567 starting......
Epoch:  567 | train loss: 1.168369e-02 | valid loss: 9.097418e-03 
      	| train loss (relative): 2.861505e-01 | valid loss (relative): 2.226557e-01 
Epoch 567 use: 419.54 second.

epoch 568 starting......
Epoch:  568 | train loss: 1.168472e-02 | valid loss: 9.079680e-03 
      	| train loss (relative): 2.869124e-01 | valid loss (relative): 2.214529e-01 
Epoch 568 use: 431.69 second.

epoch 569 starting......
Epoch:  569 | train loss: 1.167997e-02 | valid loss: 9.105373e-03 
      	| train loss (relative): 2.860615e-01 | valid loss (relative): 2.236340e-01 
Epoch 569 use: 415.43 second.

epoch 570 starting......
Epoch:  570 | train loss: 1.167747e-02 | valid loss: 9.117401e-03 
      	| train loss (relative): 2.857921e-01 | valid loss (relative): 2.237172e-01 
Epoch 570 use: 415.82 second.

epoch 571 starting......
Epoch:  571 | train loss: 1.168067e-02 | valid loss: 9.060648e-03 
      	| train loss (relative): 2.875830e-01 | valid loss (relative): 2.211878e-01 
Epoch 571 use: 498.18 second.

epoch 572 starting......
Epoch:  572 | train loss: 1.168147e-02 | valid loss: 9.084924e-03 
      	| train loss (relative): 2.868375e-01 | valid loss (relative): 2.221762e-01 
Epoch 572 use: 443.91 second.

epoch 573 starting......
Epoch:  573 | train loss: 1.168234e-02 | valid loss: 9.118150e-03 
      	| train loss (relative): 2.856550e-01 | valid loss (relative): 2.241960e-01 
Epoch 573 use: 429.38 second.

epoch 574 starting......
Epoch:  574 | train loss: 1.168413e-02 | valid loss: 9.145216e-03 
      	| train loss (relative): 2.862839e-01 | valid loss (relative): 2.250781e-01 
Epoch 574 use: 443.90 second.

epoch 575 starting......
Epoch:  575 | train loss: 1.167992e-02 | valid loss: 9.092694e-03 
      	| train loss (relative): 2.871719e-01 | valid loss (relative): 2.229355e-01 
Epoch 575 use: 482.77 second.

epoch 576 starting......
Epoch:  576 | train loss: 1.168225e-02 | valid loss: 9.126909e-03 
      	| train loss (relative): 2.859914e-01 | valid loss (relative): 2.244458e-01 
Epoch 576 use: 436.79 second.

epoch 577 starting......
Epoch:  577 | train loss: 1.168139e-02 | valid loss: 9.113678e-03 
      	| train loss (relative): 2.864349e-01 | valid loss (relative): 2.234529e-01 
Epoch 577 use: 435.13 second.

epoch 578 starting......
Epoch:  578 | train loss: 1.168258e-02 | valid loss: 9.089176e-03 
      	| train loss (relative): 2.871203e-01 | valid loss (relative): 2.219889e-01 
Epoch 578 use: 417.29 second.

epoch 579 starting......
Epoch:  579 | train loss: 1.168121e-02 | valid loss: 9.077529e-03 
      	| train loss (relative): 2.865591e-01 | valid loss (relative): 2.218419e-01 
Epoch 579 use: 461.30 second.

epoch 580 starting......
Epoch:  580 | train loss: 1.168117e-02 | valid loss: 9.101143e-03 
      	| train loss (relative): 2.856359e-01 | valid loss (relative): 2.235206e-01 
Epoch 580 use: 414.71 second.

epoch 581 starting......
Epoch:  581 | train loss: 1.168174e-02 | valid loss: 9.124937e-03 
      	| train loss (relative): 2.863538e-01 | valid loss (relative): 2.245743e-01 
Epoch 581 use: 414.52 second.

epoch 582 starting......
Epoch:  582 | train loss: 1.168072e-02 | valid loss: 9.114966e-03 
      	| train loss (relative): 2.867438e-01 | valid loss (relative): 2.244827e-01 
Epoch 582 use: 421.01 second.

epoch 583 starting......
Epoch:  583 | train loss: 1.168311e-02 | valid loss: 9.099150e-03 
      	| train loss (relative): 2.878130e-01 | valid loss (relative): 2.229942e-01 
Epoch 583 use: 414.49 second.

epoch 584 starting......
Epoch:  584 | train loss: 1.168011e-02 | valid loss: 9.126626e-03 
      	| train loss (relative): 2.861523e-01 | valid loss (relative): 2.248345e-01 
Epoch 584 use: 403.93 second.

epoch 585 starting......
Epoch:  585 | train loss: 1.168012e-02 | valid loss: 9.102636e-03 
      	| train loss (relative): 2.871390e-01 | valid loss (relative): 2.233685e-01 
Epoch 585 use: 424.93 second.

epoch 586 starting......
Epoch:  586 | train loss: 1.168706e-02 | valid loss: 9.114682e-03 
      	| train loss (relative): 2.868076e-01 | valid loss (relative): 2.237720e-01 
Epoch 586 use: 438.74 second.

epoch 587 starting......
Epoch:  587 | train loss: 1.168371e-02 | valid loss: 9.115987e-03 
      	| train loss (relative): 2.857715e-01 | valid loss (relative): 2.235484e-01 
Epoch 587 use: 439.99 second.

epoch 588 starting......
Epoch:  588 | train loss: 1.167876e-02 | valid loss: 9.113386e-03 
      	| train loss (relative): 2.865044e-01 | valid loss (relative): 2.240428e-01 
Epoch 588 use: 424.16 second.

epoch 589 starting......
Epoch:  589 | train loss: 1.168280e-02 | valid loss: 9.132666e-03 
      	| train loss (relative): 2.860846e-01 | valid loss (relative): 2.244435e-01 
Epoch 589 use: 462.78 second.

epoch 590 starting......
Epoch:  590 | train loss: 1.168434e-02 | valid loss: 9.081010e-03 
      	| train loss (relative): 2.873845e-01 | valid loss (relative): 2.213586e-01 
Epoch 590 use: 493.18 second.

epoch 591 starting......
Epoch:  591 | train loss: 1.168321e-02 | valid loss: 9.121571e-03 
      	| train loss (relative): 2.860541e-01 | valid loss (relative): 2.237114e-01 
Epoch 591 use: 446.09 second.

epoch 592 starting......
Epoch:  592 | train loss: 1.168408e-02 | valid loss: 9.139691e-03 
      	| train loss (relative): 2.860088e-01 | valid loss (relative): 2.252475e-01 
Epoch 592 use: 411.69 second.

epoch 593 starting......
Epoch:  593 | train loss: 1.168667e-02 | valid loss: 9.124713e-03 
      	| train loss (relative): 2.871513e-01 | valid loss (relative): 2.240036e-01 
Epoch 593 use: 448.77 second.

epoch 594 starting......
Epoch:  594 | train loss: 1.167973e-02 | valid loss: 9.112007e-03 
      	| train loss (relative): 2.861159e-01 | valid loss (relative): 2.238631e-01 
Epoch 594 use: 412.20 second.

epoch 595 starting......
Epoch:  595 | train loss: 1.168284e-02 | valid loss: 9.114649e-03 
      	| train loss (relative): 2.871757e-01 | valid loss (relative): 2.236912e-01 
Epoch 595 use: 410.77 second.

epoch 596 starting......
Epoch:  596 | train loss: 1.168182e-02 | valid loss: 9.104406e-03 
      	| train loss (relative): 2.866518e-01 | valid loss (relative): 2.229951e-01 
Epoch 596 use: 404.84 second.

epoch 597 starting......
Epoch:  597 | train loss: 1.168508e-02 | valid loss: 9.111719e-03 
      	| train loss (relative): 2.865675e-01 | valid loss (relative): 2.238335e-01 
Epoch 597 use: 413.87 second.

epoch 598 starting......
Epoch:  598 | train loss: 1.168067e-02 | valid loss: 9.111732e-03 
      	| train loss (relative): 2.863313e-01 | valid loss (relative): 2.238366e-01 
Epoch 598 use: 410.75 second.

epoch 599 starting......
Epoch:  599 | train loss: 1.168130e-02 | valid loss: 9.099552e-03 
      	| train loss (relative): 2.869950e-01 | valid loss (relative): 2.227563e-01 
Epoch 599 use: 406.53 second.

test MSE Error: 1.075490e-02 | relative MSE Error: 2.632792e-01 
 Total time used for training: 23.73 hour.
MESLoss saved to  /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_MSELoss_Latent_128_nearest_neighbouring_True_SFC_nums_3_startlr_0.0005_n_epoches_600.txt
relative MSELoss saved to  /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_reMSELoss_Latent_128_nearest_neighbouring_True_SFC_nums_3_startlr_0.0005_n_epoches_600.txt
model saved to /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_128_Nearest_neighbouring_True_SFC_nums_3_startlr_0.0005_n_epoches_600.pth
model_dict saved to /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_128_Nearest_neighbouring_True_SFC_nums_3_startlr_0.0005_n_epoches_600_dict.pth
... Training slugflow data completed, Run finished Thu  5 Aug 19:52:35 BST 2021 ...
{'vtu_dir': '/rds/general/user/jy220/home/slugflow/', 'data_dir': '/rds/general/user/jy220/home/tensors/', 'data_type': 'tensors', 'vtu_fields': 'Component1::ComponentMassFractionPhase1, phase1::Velocity', 'variational': 'False', 'optimizer': 'Adamax', 'structured': 'False', 'changing_lr': 'False', 'self_concat': '2', 'dimension': '3', 'sfc_nums': '3', 'components': '4', 'nearest_neighbouring': 'True', 'dims_latent': '128', 'activation': 'Tanh', 'sfc_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/sfcs.pt', 'inv_sfc_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/inv_sfcs.pt', 'coords_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/coords.pt', 'cells_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/cells.npy', 'state_load': 'None', 'batch_size': '16', 'lr': '0.001', 'n_epoches': '100', 'seed': '15', 'visualize': 'False', 'reconstructed_path': 'None', 'reconstruct_start_index': '0', 'reconstruct_end_index': '50', 'save_path': '/rds/general/user/jy220/home/new_results/'}
[[ 2.50825269e-01 -1.67256990e-02 -1.22268730e-03]
 [ 2.38611959e-01 -5.50627777e-03 -5.06451913e-03]
 [ 2.39454874e-01 -1.59563257e-02  3.96778323e-03]
 ...
 [ 9.11361096e+00 -2.21908381e-02  2.22384902e-03]
 [ 9.11847310e+00 -2.84346931e-02 -2.34839590e-04]
 [ 9.11039661e+00 -2.51498573e-02 -8.70661044e-03]]
{'tetra': array([[      0,       1,       2,       3],
       [      4,       5,       6,       7],
       [      8,       9,      10,      11],
       ...,
       [1342744, 1342745, 1342746, 1342747],
       [1342748, 1342749, 1342750, 1342751],
       [1342752, 1342753, 1342754, 1342755]])}
structured  False 
 activation  Tanh() 
 self concat  2 
 sfc_nums  3 

dims_latent  128 
 components  4 
 nearest_neighbouring  True 

visualize  False 
 sample number  1706 

reading sfc nums......
[array([  41684,   41685,   41674, ..., 1235560, 1235564, 1235558]), array([954289, 954290, 954291, ..., 420685, 420687, 420688]), array([  40156,   40155,   40158, ..., 1123478, 1123475, 1123467])]
reading inverse sfc nums......
[array([   8146,    8145,    8144, ..., 1274444, 1274445, 1273166]), array([547280, 547283, 593037, ..., 933933, 933932, 864488]), array([ 66437,  66439, 122512, ..., 459416, 415130, 459418])]
Computing min and max......

tk:  tensor([0.7581, 0.0418, 0.1840, 0.1464], dtype=torch.float64) 

tb:  tensor([-0.3816, -0.3618, -0.0308,  0.1446], dtype=torch.float64) 

length of train set: 1505 

length of valid set: 100 

length of test set: 101 

torch device num: 4 

Let's use 4 GPUs!
epoch 0 starting......
Epoch:  0 | train loss: 2.214813e-02 | valid loss: 1.085394e-02 
      	| train loss (relative): 4.097538e+00 | valid loss (relative): 3.324902e-01 
Epoch 0 use: 421.36 second.

epoch 1 starting......
Epoch:  1 | train loss: 7.470678e-03 | valid loss: 6.572424e-03 
      	| train loss (relative): 1.772772e-01 | valid loss (relative): 1.404594e-01 
Epoch 1 use: 382.43 second.

epoch 2 starting......
Epoch:  2 | train loss: 6.154080e-03 | valid loss: 6.156956e-03 
      	| train loss (relative): 1.328067e-01 | valid loss (relative): 1.314029e-01 
Epoch 2 use: 371.28 second.

epoch 3 starting......
Epoch:  3 | train loss: 5.765370e-03 | valid loss: 5.689471e-03 
      	| train loss (relative): 1.242123e-01 | valid loss (relative): 1.231164e-01 
Epoch 3 use: 403.26 second.

epoch 4 starting......
Epoch:  4 | train loss: 5.331276e-03 | valid loss: 5.340013e-03 
      	| train loss (relative): 1.145423e-01 | valid loss (relative): 1.128864e-01 
Epoch 4 use: 357.25 second.

epoch 5 starting......
Epoch:  5 | train loss: 5.074737e-03 | valid loss: 5.057245e-03 
      	| train loss (relative): 1.082125e-01 | valid loss (relative): 1.075309e-01 
Epoch 5 use: 387.72 second.

epoch 6 starting......
Epoch:  6 | train loss: 4.843139e-03 | valid loss: 4.861261e-03 
      	| train loss (relative): 1.027830e-01 | valid loss (relative): 1.010656e-01 
Epoch 6 use: 555.06 second.

epoch 7 starting......
Epoch:  7 | train loss: 4.626058e-03 | valid loss: 4.668351e-03 
      	| train loss (relative): 9.727705e-02 | valid loss (relative): 9.832377e-02 
Epoch 7 use: 480.99 second.

epoch 8 starting......
Epoch:  8 | train loss: 4.524548e-03 | valid loss: 4.596117e-03 
      	| train loss (relative): 9.463648e-02 | valid loss (relative): 9.671831e-02 
Epoch 8 use: 376.13 second.

epoch 9 starting......
Epoch:  9 | train loss: 4.471921e-03 | valid loss: 4.566591e-03 
      	| train loss (relative): 9.338039e-02 | valid loss (relative): 9.442402e-02 
Epoch 9 use: 406.44 second.

epoch 10 starting......
Epoch:  10 | train loss: 4.373059e-03 | valid loss: 4.449611e-03 
      	| train loss (relative): 9.109942e-02 | valid loss (relative): 9.125795e-02 
Epoch 10 use: 394.90 second.

epoch 11 starting......
Epoch:  11 | train loss: 4.304685e-03 | valid loss: 4.363482e-03 
      	| train loss (relative): 8.949755e-02 | valid loss (relative): 8.986650e-02 
Epoch 11 use: 360.40 second.

epoch 12 starting......
Epoch:  12 | train loss: 4.232943e-03 | valid loss: 4.294990e-03 
      	| train loss (relative): 8.797960e-02 | valid loss (relative): 8.979332e-02 
Epoch 12 use: 393.40 second.

epoch 13 starting......
Epoch:  13 | train loss: 4.161126e-03 | valid loss: 4.242943e-03 
      	| train loss (relative): 8.637583e-02 | valid loss (relative): 8.927020e-02 
Epoch 13 use: 389.76 second.

epoch 14 starting......
Epoch:  14 | train loss: 4.096437e-03 | valid loss: 4.211611e-03 
      	| train loss (relative): 8.499183e-02 | valid loss (relative): 8.930688e-02 
Epoch 14 use: 396.48 second.

epoch 15 starting......
Epoch:  15 | train loss: 4.005941e-03 | valid loss: 4.025890e-03 
      	| train loss (relative): 8.304352e-02 | valid loss (relative): 8.267907e-02 
Epoch 15 use: 376.58 second.

epoch 16 starting......
Epoch:  16 | train loss: 3.914812e-03 | valid loss: 3.987930e-03 
      	| train loss (relative): 8.091927e-02 | valid loss (relative): 8.338892e-02 
Epoch 16 use: 356.73 second.

epoch 17 starting......
Epoch:  17 | train loss: 3.834712e-03 | valid loss: 3.923244e-03 
      	| train loss (relative): 7.913192e-02 | valid loss (relative): 8.242867e-02 
Epoch 17 use: 357.93 second.

epoch 18 starting......
Epoch:  18 | train loss: 3.763392e-03 | valid loss: 3.806120e-03 
      	| train loss (relative): 7.747591e-02 | valid loss (relative): 7.900607e-02 
Epoch 18 use: 364.78 second.

epoch 19 starting......
Epoch:  19 | train loss: 3.717334e-03 | valid loss: 3.758922e-03 
      	| train loss (relative): 7.642712e-02 | valid loss (relative): 7.680647e-02 
Epoch 19 use: 363.67 second.

epoch 20 starting......
Epoch:  20 | train loss: 3.638098e-03 | valid loss: 3.713021e-03 
      	| train loss (relative): 7.462817e-02 | valid loss (relative): 7.685336e-02 
Epoch 20 use: 377.75 second.

epoch 21 starting......
Epoch:  21 | train loss: 3.626816e-03 | valid loss: 3.759411e-03 
      	| train loss (relative): 7.434069e-02 | valid loss (relative): 7.868010e-02 
Epoch 21 use: 365.93 second.

epoch 22 starting......
Epoch:  22 | train loss: 3.588619e-03 | valid loss: 3.603069e-03 
      	| train loss (relative): 7.348651e-02 | valid loss (relative): 7.430295e-02 
Epoch 22 use: 356.73 second.

epoch 23 starting......
Epoch:  23 | train loss: 3.525922e-03 | valid loss: 3.603165e-03 
      	| train loss (relative): 7.213522e-02 | valid loss (relative): 7.448135e-02 
Epoch 23 use: 398.16 second.

epoch 24 starting......
Epoch:  24 | train loss: 3.500613e-03 | valid loss: 3.564390e-03 
      	| train loss (relative): 7.156438e-02 | valid loss (relative): 7.403459e-02 
Epoch 24 use: 388.85 second.

epoch 25 starting......
Epoch:  25 | train loss: 3.457291e-03 | valid loss: 3.512821e-03 
      	| train loss (relative): 7.065889e-02 | valid loss (relative): 7.088634e-02 
Epoch 25 use: 376.59 second.

epoch 26 starting......
Epoch:  26 | train loss: 3.423072e-03 | valid loss: 3.538493e-03 
      	| train loss (relative): 6.988889e-02 | valid loss (relative): 7.293569e-02 
Epoch 26 use: 384.17 second.

epoch 27 starting......
Epoch:  27 | train loss: 3.412015e-03 | valid loss: 3.470076e-03 
      	| train loss (relative): 6.966080e-02 | valid loss (relative): 7.108315e-02 
Epoch 27 use: 375.03 second.

epoch 28 starting......
Epoch:  28 | train loss: 3.382822e-03 | valid loss: 3.460640e-03 
      	| train loss (relative): 6.898984e-02 | valid loss (relative): 7.151971e-02 
Epoch 28 use: 371.66 second.

epoch 29 starting......
Epoch:  29 | train loss: 3.344323e-03 | valid loss: 3.409803e-03 
      	| train loss (relative): 6.816523e-02 | valid loss (relative): 7.015615e-02 
Epoch 29 use: 361.96 second.

epoch 30 starting......
Epoch:  30 | train loss: 3.312646e-03 | valid loss: 3.434270e-03 
      	| train loss (relative): 6.746119e-02 | valid loss (relative): 7.051790e-02 
Epoch 30 use: 357.36 second.

epoch 31 starting......
Epoch:  31 | train loss: 3.304667e-03 | valid loss: 3.401411e-03 
      	| train loss (relative): 6.731360e-02 | valid loss (relative): 6.975945e-02 
Epoch 31 use: 378.67 second.

epoch 32 starting......
Epoch:  32 | train loss: 3.282164e-03 | valid loss: 3.345788e-03 
      	| train loss (relative): 6.680246e-02 | valid loss (relative): 6.714842e-02 
Epoch 32 use: 384.46 second.

epoch 33 starting......
Epoch:  33 | train loss: 3.254361e-03 | valid loss: 3.328584e-03 
      	| train loss (relative): 6.618202e-02 | valid loss (relative): 6.805046e-02 
Epoch 33 use: 368.73 second.

epoch 34 starting......
Epoch:  34 | train loss: 3.229533e-03 | valid loss: 3.298640e-03 
      	| train loss (relative): 6.566461e-02 | valid loss (relative): 6.686802e-02 
Epoch 34 use: 475.96 second.

epoch 35 starting......
Epoch:  35 | train loss: 3.214507e-03 | valid loss: 3.285338e-03 
      	| train loss (relative): 6.531817e-02 | valid loss (relative): 6.686925e-02 
Epoch 35 use: 492.15 second.

epoch 36 starting......
Epoch:  36 | train loss: 3.198801e-03 | valid loss: 3.257028e-03 
      	| train loss (relative): 6.497890e-02 | valid loss (relative): 6.654690e-02 
Epoch 36 use: 482.95 second.

epoch 37 starting......
Epoch:  37 | train loss: 3.183282e-03 | valid loss: 3.257087e-03 
      	| train loss (relative): 6.463495e-02 | valid loss (relative): 6.735374e-02 
Epoch 37 use: 496.84 second.

epoch 38 starting......
Epoch:  38 | train loss: 3.175180e-03 | valid loss: 3.278890e-03 
      	| train loss (relative): 6.447694e-02 | valid loss (relative): 6.722770e-02 
Epoch 38 use: 510.69 second.

epoch 39 starting......
Epoch:  39 | train loss: 3.175511e-03 | valid loss: 3.201535e-03 
      	| train loss (relative): 6.450761e-02 | valid loss (relative): 6.490326e-02 
Epoch 39 use: 470.92 second.

epoch 40 starting......
Epoch:  40 | train loss: 3.118807e-03 | valid loss: 3.187113e-03 
      	| train loss (relative): 6.326066e-02 | valid loss (relative): 6.447887e-02 
Epoch 40 use: 498.31 second.

epoch 41 starting......
Epoch:  41 | train loss: 3.097876e-03 | valid loss: 3.156801e-03 
      	| train loss (relative): 6.280408e-02 | valid loss (relative): 6.421893e-02 
Epoch 41 use: 502.86 second.

epoch 42 starting......
Epoch:  42 | train loss: 3.090912e-03 | valid loss: 3.195234e-03 
      	| train loss (relative): 6.267626e-02 | valid loss (relative): 6.489486e-02 
Epoch 42 use: 506.07 second.

epoch 43 starting......
Epoch:  43 | train loss: 3.089679e-03 | valid loss: 3.167471e-03 
      	| train loss (relative): 6.262238e-02 | valid loss (relative): 6.438529e-02 
Epoch 43 use: 515.63 second.

epoch 44 starting......
Epoch:  44 | train loss: 3.083645e-03 | valid loss: 3.146477e-03 
      	| train loss (relative): 6.249824e-02 | valid loss (relative): 6.439404e-02 
Epoch 44 use: 524.63 second.

epoch 45 starting......
Epoch:  45 | train loss: 3.069947e-03 | valid loss: 3.141463e-03 
      	| train loss (relative): 6.222887e-02 | valid loss (relative): 6.445288e-02 
Epoch 45 use: 515.92 second.

epoch 46 starting......
Epoch:  46 | train loss: 3.026570e-03 | valid loss: 3.098918e-03 
      	| train loss (relative): 6.127712e-02 | valid loss (relative): 6.325518e-02 
Epoch 46 use: 522.62 second.

epoch 47 starting......
Epoch:  47 | train loss: 3.020776e-03 | valid loss: 3.098360e-03 
      	| train loss (relative): 6.115982e-02 | valid loss (relative): 6.283635e-02 
Epoch 47 use: 516.22 second.

epoch 48 starting......
Epoch:  48 | train loss: 3.002430e-03 | valid loss: 3.072577e-03 
      	| train loss (relative): 6.077925e-02 | valid loss (relative): 6.271064e-02 
Epoch 48 use: 527.92 second.

epoch 49 starting......
Epoch:  49 | train loss: 2.994747e-03 | valid loss: 3.104657e-03 
      	| train loss (relative): 6.063562e-02 | valid loss (relative): 6.292869e-02 
Epoch 49 use: 514.71 second.

epoch 50 starting......
Epoch:  50 | train loss: 2.984977e-03 | valid loss: 3.047284e-03 
      	| train loss (relative): 6.038778e-02 | valid loss (relative): 6.130179e-02 
Epoch 50 use: 517.67 second.

epoch 51 starting......
Epoch:  51 | train loss: 2.962937e-03 | valid loss: 3.034120e-03 
      	| train loss (relative): 5.992613e-02 | valid loss (relative): 6.171301e-02 
Epoch 51 use: 492.08 second.

epoch 52 starting......
Epoch:  52 | train loss: 2.939433e-03 | valid loss: 3.075005e-03 
      	| train loss (relative): 5.944009e-02 | valid loss (relative): 6.236029e-02 
Epoch 52 use: 506.01 second.

epoch 53 starting......
Epoch:  53 | train loss: 2.927475e-03 | valid loss: 2.980384e-03 
      	| train loss (relative): 5.917198e-02 | valid loss (relative): 5.986701e-02 
Epoch 53 use: 519.96 second.

epoch 54 starting......
Epoch:  54 | train loss: 2.902806e-03 | valid loss: 2.972769e-03 
      	| train loss (relative): 5.864586e-02 | valid loss (relative): 6.036817e-02 
Epoch 54 use: 628.88 second.

epoch 55 starting......
Epoch:  55 | train loss: 2.898825e-03 | valid loss: 3.001587e-03 
      	| train loss (relative): 5.856419e-02 | valid loss (relative): 6.172562e-02 
Epoch 55 use: 506.98 second.

epoch 56 starting......
Epoch:  56 | train loss: 2.891521e-03 | valid loss: 2.951056e-03 
      	| train loss (relative): 5.841903e-02 | valid loss (relative): 5.972422e-02 
Epoch 56 use: 519.16 second.

epoch 57 starting......
Epoch:  57 | train loss: 2.866955e-03 | valid loss: 2.966757e-03 
      	| train loss (relative): 5.789297e-02 | valid loss (relative): 5.976240e-02 
Epoch 57 use: 489.63 second.

epoch 58 starting......
Epoch:  58 | train loss: 2.862145e-03 | valid loss: 2.935507e-03 
      	| train loss (relative): 5.778338e-02 | valid loss (relative): 5.911797e-02 
Epoch 58 use: 495.25 second.

epoch 59 starting......
Epoch:  59 | train loss: 2.837133e-03 | valid loss: 2.909701e-03 
      	| train loss (relative): 5.726853e-02 | valid loss (relative): 5.855183e-02 
Epoch 59 use: 489.28 second.

epoch 60 starting......
Epoch:  60 | train loss: 2.821764e-03 | valid loss: 2.887517e-03 
      	| train loss (relative): 5.693700e-02 | valid loss (relative): 5.833413e-02 
Epoch 60 use: 483.38 second.

epoch 61 starting......
Epoch:  61 | train loss: 2.806134e-03 | valid loss: 2.899576e-03 
      	| train loss (relative): 5.659937e-02 | valid loss (relative): 5.781196e-02 
Epoch 61 use: 496.51 second.

epoch 62 starting......
Epoch:  62 | train loss: 2.795201e-03 | valid loss: 2.859932e-03 
      	| train loss (relative): 5.636756e-02 | valid loss (relative): 5.758744e-02 
Epoch 62 use: 499.47 second.

epoch 63 starting......
Epoch:  63 | train loss: 2.779949e-03 | valid loss: 2.869043e-03 
      	| train loss (relative): 5.604996e-02 | valid loss (relative): 5.852310e-02 
Epoch 63 use: 503.02 second.

epoch 64 starting......
Epoch:  64 | train loss: 2.777227e-03 | valid loss: 2.856129e-03 
      	| train loss (relative): 5.599326e-02 | valid loss (relative): 5.753396e-02 
Epoch 64 use: 521.83 second.

epoch 65 starting......
Epoch:  65 | train loss: 2.763820e-03 | valid loss: 2.830272e-03 
      	| train loss (relative): 5.572236e-02 | valid loss (relative): 5.713818e-02 
Epoch 65 use: 501.63 second.

epoch 66 starting......
Epoch:  66 | train loss: 2.747075e-03 | valid loss: 2.811481e-03 
      	| train loss (relative): 5.533668e-02 | valid loss (relative): 5.678467e-02 
Epoch 66 use: 565.31 second.

epoch 67 starting......
Epoch:  67 | train loss: 2.729484e-03 | valid loss: 2.806705e-03 
      	| train loss (relative): 5.498293e-02 | valid loss (relative): 5.678254e-02 
Epoch 67 use: 650.66 second.

epoch 68 starting......
Epoch:  68 | train loss: 2.715102e-03 | valid loss: 2.795419e-03 
      	| train loss (relative): 5.468018e-02 | valid loss (relative): 5.760594e-02 
Epoch 68 use: 652.73 second.

epoch 69 starting......
Epoch:  69 | train loss: 2.708256e-03 | valid loss: 2.783501e-03 
      	| train loss (relative): 5.453679e-02 | valid loss (relative): 5.601666e-02 
Epoch 69 use: 526.94 second.

epoch 70 starting......
Epoch:  70 | train loss: 2.693448e-03 | valid loss: 2.799413e-03 
      	| train loss (relative): 5.422423e-02 | valid loss (relative): 5.701771e-02 
Epoch 70 use: 573.74 second.

epoch 71 starting......
Epoch:  71 | train loss: 2.688636e-03 | valid loss: 2.727083e-03 
      	| train loss (relative): 5.411244e-02 | valid loss (relative): 5.491259e-02 
Epoch 71 use: 568.18 second.

epoch 72 starting......
Epoch:  72 | train loss: 2.654882e-03 | valid loss: 2.718378e-03 
      	| train loss (relative): 5.341313e-02 | valid loss (relative): 5.488546e-02 
Epoch 72 use: 522.33 second.

epoch 73 starting......
Epoch:  73 | train loss: 2.643560e-03 | valid loss: 2.722344e-03 
      	| train loss (relative): 5.318953e-02 | valid loss (relative): 5.440336e-02 
Epoch 73 use: 504.23 second.

epoch 74 starting......
Epoch:  74 | train loss: 2.632155e-03 | valid loss: 2.689108e-03 
      	| train loss (relative): 5.292714e-02 | valid loss (relative): 5.458689e-02 
Epoch 74 use: 513.88 second.

epoch 75 starting......
Epoch:  75 | train loss: 2.623556e-03 | valid loss: 2.685107e-03 
      	| train loss (relative): 5.274471e-02 | valid loss (relative): 5.420825e-02 
Epoch 75 use: 523.17 second.

epoch 76 starting......
Epoch:  76 | train loss: 2.614945e-03 | valid loss: 2.712427e-03 
      	| train loss (relative): 5.255822e-02 | valid loss (relative): 5.472787e-02 
Epoch 76 use: 542.68 second.

epoch 77 starting......
Epoch:  77 | train loss: 2.617225e-03 | valid loss: 2.704845e-03 
      	| train loss (relative): 5.260335e-02 | valid loss (relative): 5.325418e-02 
Epoch 77 use: 517.86 second.

epoch 78 starting......
Epoch:  78 | train loss: 2.606529e-03 | valid loss: 2.652863e-03 
      	| train loss (relative): 5.237596e-02 | valid loss (relative): 5.320188e-02 
Epoch 78 use: 556.46 second.

epoch 79 starting......
Epoch:  79 | train loss: 2.576031e-03 | valid loss: 2.633465e-03 
      	| train loss (relative): 5.173625e-02 | valid loss (relative): 5.308963e-02 
Epoch 79 use: 531.98 second.

epoch 80 starting......
Epoch:  80 | train loss: 2.563363e-03 | valid loss: 2.639454e-03 
      	| train loss (relative): 5.147362e-02 | valid loss (relative): 5.277711e-02 
Epoch 80 use: 530.53 second.

epoch 81 starting......
Epoch:  81 | train loss: 2.558196e-03 | valid loss: 2.642202e-03 
      	| train loss (relative): 5.135813e-02 | valid loss (relative): 5.338452e-02 
Epoch 81 use: 547.53 second.

epoch 82 starting......
Epoch:  82 | train loss: 2.552693e-03 | valid loss: 2.624539e-03 
      	| train loss (relative): 5.123515e-02 | valid loss (relative): 5.263245e-02 
Epoch 82 use: 508.08 second.

epoch 83 starting......
Epoch:  83 | train loss: 2.548090e-03 | valid loss: 2.607963e-03 
      	| train loss (relative): 5.115819e-02 | valid loss (relative): 5.288428e-02 
Epoch 83 use: 526.25 second.

epoch 84 starting......
Epoch:  84 | train loss: 2.537371e-03 | valid loss: 2.596579e-03 
      	| train loss (relative): 5.092639e-02 | valid loss (relative): 5.193295e-02 
Epoch 84 use: 547.26 second.

epoch 85 starting......
Epoch:  85 | train loss: 2.521879e-03 | valid loss: 2.585480e-03 
      	| train loss (relative): 5.059592e-02 | valid loss (relative): 5.199233e-02 
Epoch 85 use: 505.88 second.

epoch 86 starting......
Epoch:  86 | train loss: 2.515203e-03 | valid loss: 2.607563e-03 
      	| train loss (relative): 5.043706e-02 | valid loss (relative): 5.230341e-02 
Epoch 86 use: 544.88 second.

epoch 87 starting......
Epoch:  87 | train loss: 2.509266e-03 | valid loss: 2.575438e-03 
      	| train loss (relative): 5.032522e-02 | valid loss (relative): 5.182444e-02 
Epoch 87 use: 555.26 second.

epoch 88 starting......
Epoch:  88 | train loss: 2.498744e-03 | valid loss: 2.574801e-03 
      	| train loss (relative): 5.010546e-02 | valid loss (relative): 5.143121e-02 
Epoch 88 use: 501.95 second.

epoch 89 starting......
Epoch:  89 | train loss: 2.499349e-03 | valid loss: 2.574873e-03 
      	| train loss (relative): 5.012126e-02 | valid loss (relative): 5.149983e-02 
Epoch 89 use: 533.35 second.

epoch 90 starting......
Epoch:  90 | train loss: 2.498468e-03 | valid loss: 2.550287e-03 
      	| train loss (relative): 5.008478e-02 | valid loss (relative): 5.109354e-02 
Epoch 90 use: 549.23 second.

epoch 91 starting......
Epoch:  91 | train loss: 2.477427e-03 | valid loss: 2.541296e-03 
      	| train loss (relative): 4.965052e-02 | valid loss (relative): 5.067780e-02 
Epoch 91 use: 490.38 second.

epoch 92 starting......
Epoch:  92 | train loss: 2.463109e-03 | valid loss: 2.523434e-03 
      	| train loss (relative): 4.935884e-02 | valid loss (relative): 5.080427e-02 
Epoch 92 use: 525.68 second.

epoch 93 starting......
Epoch:  93 | train loss: 2.456165e-03 | valid loss: 2.525765e-03 
      	| train loss (relative): 4.920394e-02 | valid loss (relative): 5.094285e-02 
Epoch 93 use: 510.62 second.

epoch 94 starting......
Epoch:  94 | train loss: 2.459844e-03 | valid loss: 2.525387e-03 
      	| train loss (relative): 4.927289e-02 | valid loss (relative): 5.064285e-02 
Epoch 94 use: 491.34 second.

epoch 95 starting......
Epoch:  95 | train loss: 2.451060e-03 | valid loss: 2.528923e-03 
      	| train loss (relative): 4.910176e-02 | valid loss (relative): 5.060108e-02 
Epoch 95 use: 526.21 second.

epoch 96 starting......
Epoch:  96 | train loss: 2.443952e-03 | valid loss: 2.516029e-03 
      	| train loss (relative): 4.894574e-02 | valid loss (relative): 5.037859e-02 
Epoch 96 use: 513.51 second.

epoch 97 starting......
Epoch:  97 | train loss: 2.435766e-03 | valid loss: 2.490495e-03 
      	| train loss (relative): 4.876736e-02 | valid loss (relative): 4.993982e-02 
Epoch 97 use: 521.42 second.

epoch 98 starting......
Epoch:  98 | train loss: 2.431561e-03 | valid loss: 2.527016e-03 
      	| train loss (relative): 4.868534e-02 | valid loss (relative): 4.979141e-02 
Epoch 98 use: 508.42 second.

epoch 99 starting......
Epoch:  99 | train loss: 2.428992e-03 | valid loss: 2.489429e-03 
      	| train loss (relative): 4.863103e-02 | valid loss (relative): 4.971453e-02 
Epoch 99 use: 527.30 second.

test MSE Error: 2.423461e-03 | relative MSE Error: 4.802922e-02 
 Total time used for training: 13.23 hour.
MESLoss saved to  /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_MSELoss_Latent_128_nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_100.txt
relative MSELoss saved to  /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_reMSELoss_Latent_128_nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_100.txt
model saved to /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_128_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_100.pth
model_dict saved to /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_128_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_100_dict.pth
... Training slugflow data completed, Run finished Tue 10 Aug 06:48:38 BST 2021 ...
{'vtu_dir': '/rds/general/user/jy220/home/slugflow/', 'data_dir': '/rds/general/user/jy220/home/tensors/', 'data_type': 'tensors', 'vtu_fields': 'Component1::ComponentMassFractionPhase1, phase1::Velocity', 'variational': 'False', 'optimizer': 'Adamax', 'structured': 'False', 'changing_lr': 'False', 'self_concat': '2', 'dimension': '3', 'sfc_nums': '3', 'components': '4', 'nearest_neighbouring': 'True', 'dims_latent': '128', 'activation': 'Tanh', 'sfc_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/sfcs.pt', 'inv_sfc_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/inv_sfcs.pt', 'coords_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/coords.pt', 'cells_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/cells.npy', 'state_load': '/rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_128_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_100_dict.pth', 'batch_size': '16', 'lr': '0.001', 'n_epoches': '200', 'seed': '15', 'visualize': 'False', 'reconstructed_path': 'None', 'reconstruct_start_index': '0', 'reconstruct_end_index': '50', 'save_path': '/rds/general/user/jy220/home/new_results/'}
[[ 2.50825269e-01 -1.67256990e-02 -1.22268730e-03]
 [ 2.38611959e-01 -5.50627777e-03 -5.06451913e-03]
 [ 2.39454874e-01 -1.59563257e-02  3.96778323e-03]
 ...
 [ 9.11361096e+00 -2.21908381e-02  2.22384902e-03]
 [ 9.11847310e+00 -2.84346931e-02 -2.34839590e-04]
 [ 9.11039661e+00 -2.51498573e-02 -8.70661044e-03]]
{'tetra': array([[      0,       1,       2,       3],
       [      4,       5,       6,       7],
       [      8,       9,      10,      11],
       ...,
       [1342744, 1342745, 1342746, 1342747],
       [1342748, 1342749, 1342750, 1342751],
       [1342752, 1342753, 1342754, 1342755]])}
structured  False 
 activation  Tanh() 
 self concat  2 
 sfc_nums  3 

dims_latent  128 
 components  4 
 nearest_neighbouring  True 

visualize  False 
 sample number  1706 

reading sfc nums......
[array([  41684,   41685,   41674, ..., 1235560, 1235564, 1235558]), array([954289, 954290, 954291, ..., 420685, 420687, 420688]), array([  40156,   40155,   40158, ..., 1123478, 1123475, 1123467])]
reading inverse sfc nums......
[array([   8146,    8145,    8144, ..., 1274444, 1274445, 1273166]), array([547280, 547283, 593037, ..., 933933, 933932, 864488]), array([ 66437,  66439, 122512, ..., 459416, 415130, 459418])]
Computing min and max......

tk:  tensor([0.7581, 0.0418, 0.1840, 0.1464], dtype=torch.float64) 

tb:  tensor([-0.3816, -0.3618, -0.0308,  0.1446], dtype=torch.float64) 

length of train set: 1505 

length of valid set: 100 

length of test set: 101 

torch device num: 4 

Let's use 4 GPUs!
epoch 100 starting......
Epoch:  100 | train loss: 3.182635e-03 | valid loss: 2.372757e-03 
      	| train loss (relative): 6.443124e-02 | valid loss (relative): 4.733379e-02 
Epoch 100 use: 703.68 second.

epoch 101 starting......
Epoch:  101 | train loss: 2.438022e-03 | valid loss: 2.325597e-03 
      	| train loss (relative): 4.882974e-02 | valid loss (relative): 4.644235e-02 
Epoch 101 use: 491.95 second.

epoch 102 starting......
Epoch:  102 | train loss: 2.408934e-03 | valid loss: 2.314467e-03 
      	| train loss (relative): 4.821631e-02 | valid loss (relative): 4.613114e-02 
Epoch 102 use: 463.00 second.

epoch 103 starting......
Epoch:  103 | train loss: 2.396375e-03 | valid loss: 2.308113e-03 
      	| train loss (relative): 4.795562e-02 | valid loss (relative): 4.612376e-02 
Epoch 103 use: 547.13 second.

epoch 104 starting......
Epoch:  104 | train loss: 2.388566e-03 | valid loss: 2.302758e-03 
      	| train loss (relative): 4.778935e-02 | valid loss (relative): 4.588904e-02 
Epoch 104 use: 906.40 second.

epoch 105 starting......
Epoch:  105 | train loss: 2.380534e-03 | valid loss: 2.295279e-03 
      	| train loss (relative): 4.761924e-02 | valid loss (relative): 4.571993e-02 
Epoch 105 use: 1191.90 second.

epoch 106 starting......
Epoch:  106 | train loss: 2.375098e-03 | valid loss: 2.289899e-03 
      	| train loss (relative): 4.750264e-02 | valid loss (relative): 4.566010e-02 
Epoch 106 use: 1103.72 second.

epoch 107 starting......
Epoch:  107 | train loss: 2.369716e-03 | valid loss: 2.290059e-03 
      	| train loss (relative): 4.739671e-02 | valid loss (relative): 4.551979e-02 
Epoch 107 use: 793.40 second.

epoch 108 starting......
Epoch:  108 | train loss: 2.368969e-03 | valid loss: 2.282607e-03 
      	| train loss (relative): 4.736324e-02 | valid loss (relative): 4.536072e-02 
Epoch 108 use: 635.71 second.

epoch 109 starting......
Epoch:  109 | train loss: 2.360397e-03 | valid loss: 2.279328e-03 
      	| train loss (relative): 4.718802e-02 | valid loss (relative): 4.546993e-02 
Epoch 109 use: 995.92 second.

epoch 110 starting......
Epoch:  110 | train loss: 2.356604e-03 | valid loss: 2.276539e-03 
      	| train loss (relative): 4.711583e-02 | valid loss (relative): 4.543279e-02 
Epoch 110 use: 1060.41 second.

epoch 111 starting......
Epoch:  111 | train loss: 2.350570e-03 | valid loss: 2.272187e-03 
      	| train loss (relative): 4.697576e-02 | valid loss (relative): 4.524969e-02 
Epoch 111 use: 738.83 second.

epoch 112 starting......
Epoch:  112 | train loss: 2.346827e-03 | valid loss: 2.270426e-03 
      	| train loss (relative): 4.689940e-02 | valid loss (relative): 4.527697e-02 
Epoch 112 use: 510.96 second.

epoch 113 starting......
Epoch:  113 | train loss: 2.344344e-03 | valid loss: 2.267644e-03 
      	| train loss (relative): 4.684321e-02 | valid loss (relative): 4.529269e-02 
Epoch 113 use: 353.77 second.

epoch 114 starting......
Epoch:  114 | train loss: 2.343180e-03 | valid loss: 2.271682e-03 
      	| train loss (relative): 4.682382e-02 | valid loss (relative): 4.542780e-02 
Epoch 114 use: 445.49 second.

epoch 115 starting......
Epoch:  115 | train loss: 2.341446e-03 | valid loss: 2.267607e-03 
      	| train loss (relative): 4.678796e-02 | valid loss (relative): 4.516752e-02 
Epoch 115 use: 378.71 second.

epoch 116 starting......
Epoch:  116 | train loss: 2.338408e-03 | valid loss: 2.262054e-03 
      	| train loss (relative): 4.672031e-02 | valid loss (relative): 4.509546e-02 
Epoch 116 use: 526.76 second.

epoch 117 starting......
Epoch:  117 | train loss: 2.332611e-03 | valid loss: 2.263872e-03 
      	| train loss (relative): 4.659693e-02 | valid loss (relative): 4.492135e-02 
Epoch 117 use: 455.63 second.

epoch 118 starting......
Epoch:  118 | train loss: 2.338368e-03 | valid loss: 2.267858e-03 
      	| train loss (relative): 4.670839e-02 | valid loss (relative): 4.492254e-02 
Epoch 118 use: 362.65 second.

epoch 119 starting......
Epoch:  119 | train loss: 2.334444e-03 | valid loss: 2.256416e-03 
      	| train loss (relative): 4.662359e-02 | valid loss (relative): 4.492885e-02 
Epoch 119 use: 357.89 second.

epoch 120 starting......
Epoch:  120 | train loss: 2.328564e-03 | valid loss: 2.253827e-03 
      	| train loss (relative): 4.651171e-02 | valid loss (relative): 4.467297e-02 
Epoch 120 use: 595.06 second.

epoch 121 starting......
Epoch:  121 | train loss: 2.323309e-03 | valid loss: 2.248660e-03 
      	| train loss (relative): 4.639618e-02 | valid loss (relative): 4.447091e-02 
Epoch 121 use: 569.59 second.

epoch 122 starting......
Epoch:  122 | train loss: 2.326128e-03 | valid loss: 2.259091e-03 
      	| train loss (relative): 4.645207e-02 | valid loss (relative): 4.492141e-02 
Epoch 122 use: 467.55 second.

epoch 123 starting......
Epoch:  123 | train loss: 2.324656e-03 | valid loss: 2.259389e-03 
      	| train loss (relative): 4.642202e-02 | valid loss (relative): 4.490117e-02 
Epoch 123 use: 706.63 second.

epoch 124 starting......
Epoch:  124 | train loss: 2.327143e-03 | valid loss: 2.260356e-03 
      	| train loss (relative): 4.646573e-02 | valid loss (relative): 4.496524e-02 
Epoch 124 use: 782.41 second.

epoch 125 starting......
Epoch:  125 | train loss: 2.316700e-03 | valid loss: 2.241854e-03 
      	| train loss (relative): 4.624734e-02 | valid loss (relative): 4.452583e-02 
Epoch 125 use: 520.27 second.

epoch 126 starting......
Epoch:  126 | train loss: 2.313763e-03 | valid loss: 2.243490e-03 
      	| train loss (relative): 4.619279e-02 | valid loss (relative): 4.470178e-02 
Epoch 126 use: 539.99 second.

epoch 127 starting......
Epoch:  127 | train loss: 2.311671e-03 | valid loss: 2.249820e-03 
      	| train loss (relative): 4.614812e-02 | valid loss (relative): 4.466400e-02 
Epoch 127 use: 333.95 second.

epoch 128 starting......
Epoch:  128 | train loss: 2.307920e-03 | valid loss: 2.247219e-03 
      	| train loss (relative): 4.606111e-02 | valid loss (relative): 4.452117e-02 
Epoch 128 use: 454.36 second.

epoch 129 starting......
Epoch:  129 | train loss: 2.300811e-03 | valid loss: 2.232947e-03 
      	| train loss (relative): 4.591044e-02 | valid loss (relative): 4.400505e-02 
Epoch 129 use: 1025.33 second.

epoch 130 starting......
Epoch:  130 | train loss: 2.302651e-03 | valid loss: 2.250294e-03 
      	| train loss (relative): 4.595674e-02 | valid loss (relative): 4.468895e-02 
Epoch 130 use: 776.27 second.

epoch 131 starting......
Epoch:  131 | train loss: 2.308083e-03 | valid loss: 2.265490e-03 
      	| train loss (relative): 4.606505e-02 | valid loss (relative): 4.549735e-02 
Epoch 131 use: 1174.53 second.

epoch 132 starting......
Epoch:  132 | train loss: 2.302018e-03 | valid loss: 2.228498e-03 
      	| train loss (relative): 4.593524e-02 | valid loss (relative): 4.416243e-02 
Epoch 132 use: 1191.48 second.

epoch 133 starting......
Epoch:  133 | train loss: 2.288020e-03 | valid loss: 2.215680e-03 
      	| train loss (relative): 4.564824e-02 | valid loss (relative): 4.389650e-02 
Epoch 133 use: 497.59 second.

epoch 134 starting......
Epoch:  134 | train loss: 2.280347e-03 | valid loss: 2.213551e-03 
      	| train loss (relative): 4.549129e-02 | valid loss (relative): 4.435664e-02 
Epoch 134 use: 418.64 second.

epoch 135 starting......
Epoch:  135 | train loss: 2.279748e-03 | valid loss: 2.214192e-03 
      	| train loss (relative): 4.548525e-02 | valid loss (relative): 4.398933e-02 
Epoch 135 use: 343.08 second.

epoch 136 starting......
Epoch:  136 | train loss: 2.280041e-03 | valid loss: 2.221635e-03 
      	| train loss (relative): 4.548039e-02 | valid loss (relative): 4.402031e-02 
Epoch 136 use: 400.22 second.

epoch 137 starting......
Epoch:  137 | train loss: 2.280012e-03 | valid loss: 2.210995e-03 
      	| train loss (relative): 4.547470e-02 | valid loss (relative): 4.400015e-02 
Epoch 137 use: 357.09 second.

epoch 138 starting......
Epoch:  138 | train loss: 2.272210e-03 | valid loss: 2.203410e-03 
      	| train loss (relative): 4.531226e-02 | valid loss (relative): 4.371609e-02 
Epoch 138 use: 374.87 second.

epoch 139 starting......
Epoch:  139 | train loss: 2.266604e-03 | valid loss: 2.223993e-03 
      	| train loss (relative): 4.520041e-02 | valid loss (relative): 4.390976e-02 
Epoch 139 use: 371.01 second.

epoch 140 starting......
Epoch:  140 | train loss: 2.268246e-03 | valid loss: 2.207139e-03 
      	| train loss (relative): 4.523071e-02 | valid loss (relative): 4.385723e-02 
Epoch 140 use: 398.54 second.

epoch 141 starting......
Epoch:  141 | train loss: 2.261294e-03 | valid loss: 2.202498e-03 
      	| train loss (relative): 4.508041e-02 | valid loss (relative): 4.387598e-02 
Epoch 141 use: 365.52 second.

epoch 142 starting......
Epoch:  142 | train loss: 2.258791e-03 | valid loss: 2.232354e-03 
      	| train loss (relative): 4.503583e-02 | valid loss (relative): 4.479627e-02 
Epoch 142 use: 377.21 second.

epoch 143 starting......
Epoch:  143 | train loss: 2.269882e-03 | valid loss: 2.207620e-03 
      	| train loss (relative): 4.526972e-02 | valid loss (relative): 4.363456e-02 
Epoch 143 use: 405.33 second.

epoch 144 starting......
Epoch:  144 | train loss: 2.259642e-03 | valid loss: 2.188927e-03 
      	| train loss (relative): 4.505165e-02 | valid loss (relative): 4.333842e-02 
Epoch 144 use: 340.28 second.

epoch 145 starting......
Epoch:  145 | train loss: 2.242739e-03 | valid loss: 2.177118e-03 
      	| train loss (relative): 4.470664e-02 | valid loss (relative): 4.355029e-02 
Epoch 145 use: 362.88 second.

epoch 146 starting......
Epoch:  146 | train loss: 2.242993e-03 | valid loss: 2.207000e-03 
      	| train loss (relative): 4.470717e-02 | valid loss (relative): 4.446270e-02 
Epoch 146 use: 343.86 second.

epoch 147 starting......
Epoch:  147 | train loss: 2.246511e-03 | valid loss: 2.182923e-03 
      	| train loss (relative): 4.477239e-02 | valid loss (relative): 4.360031e-02 
Epoch 147 use: 366.78 second.

epoch 148 starting......
Epoch:  148 | train loss: 2.235462e-03 | valid loss: 2.177675e-03 
      	| train loss (relative): 4.454547e-02 | valid loss (relative): 4.335649e-02 
Epoch 148 use: 380.80 second.

epoch 149 starting......
Epoch:  149 | train loss: 2.239937e-03 | valid loss: 2.190538e-03 
      	| train loss (relative): 4.462297e-02 | valid loss (relative): 4.345319e-02 
Epoch 149 use: 370.56 second.

epoch 150 starting......
Epoch:  150 | train loss: 2.245960e-03 | valid loss: 2.169932e-03 
      	| train loss (relative): 4.474967e-02 | valid loss (relative): 4.312085e-02 
Epoch 150 use: 367.03 second.

epoch 151 starting......
Epoch:  151 | train loss: 2.222450e-03 | valid loss: 2.166048e-03 
      	| train loss (relative): 4.427626e-02 | valid loss (relative): 4.323970e-02 
Epoch 151 use: 339.55 second.

epoch 152 starting......
Epoch:  152 | train loss: 2.219110e-03 | valid loss: 2.159054e-03 
      	| train loss (relative): 4.421009e-02 | valid loss (relative): 4.287611e-02 
Epoch 152 use: 360.87 second.

epoch 153 starting......
Epoch:  153 | train loss: 2.219696e-03 | valid loss: 2.168180e-03 
      	| train loss (relative): 4.421953e-02 | valid loss (relative): 4.290145e-02 
Epoch 153 use: 325.18 second.

epoch 154 starting......
Epoch:  154 | train loss: 2.225562e-03 | valid loss: 2.175043e-03 
      	| train loss (relative): 4.432766e-02 | valid loss (relative): 4.339040e-02 
Epoch 154 use: 324.13 second.

epoch 155 starting......
Epoch:  155 | train loss: 2.221190e-03 | valid loss: 2.154532e-03 
      	| train loss (relative): 4.424817e-02 | valid loss (relative): 4.288427e-02 
Epoch 155 use: 333.82 second.

epoch 156 starting......
Epoch:  156 | train loss: 2.216931e-03 | valid loss: 2.161208e-03 
      	| train loss (relative): 4.415187e-02 | valid loss (relative): 4.344143e-02 
Epoch 156 use: 346.48 second.

epoch 157 starting......
Epoch:  157 | train loss: 2.224063e-03 | valid loss: 2.169323e-03 
      	| train loss (relative): 4.430522e-02 | valid loss (relative): 4.339795e-02 
Epoch 157 use: 340.11 second.

epoch 158 starting......
Epoch:  158 | train loss: 2.214371e-03 | valid loss: 2.144932e-03 
      	| train loss (relative): 4.411133e-02 | valid loss (relative): 4.255763e-02 
Epoch 158 use: 369.49 second.

epoch 159 starting......
Epoch:  159 | train loss: 2.204196e-03 | valid loss: 2.149889e-03 
      	| train loss (relative): 4.388610e-02 | valid loss (relative): 4.272930e-02 
Epoch 159 use: 345.10 second.

epoch 160 starting......
Epoch:  160 | train loss: 2.204221e-03 | valid loss: 2.153514e-03 
      	| train loss (relative): 4.389485e-02 | valid loss (relative): 4.313443e-02 
Epoch 160 use: 338.46 second.

epoch 161 starting......
Epoch:  161 | train loss: 2.203487e-03 | valid loss: 2.147176e-03 
      	| train loss (relative): 4.387686e-02 | valid loss (relative): 4.276865e-02 
Epoch 161 use: 333.07 second.

epoch 162 starting......
Epoch:  162 | train loss: 2.204179e-03 | valid loss: 2.149250e-03 
      	| train loss (relative): 4.388422e-02 | valid loss (relative): 4.280229e-02 
Epoch 162 use: 333.60 second.

epoch 163 starting......
Epoch:  163 | train loss: 2.203504e-03 | valid loss: 2.146371e-03 
      	| train loss (relative): 4.387268e-02 | valid loss (relative): 4.288599e-02 
Epoch 163 use: 345.56 second.

epoch 164 starting......
Epoch:  164 | train loss: 2.197783e-03 | valid loss: 2.141503e-03 
      	| train loss (relative): 4.374956e-02 | valid loss (relative): 4.255905e-02 
Epoch 164 use: 351.77 second.

epoch 165 starting......
Epoch:  165 | train loss: 2.190008e-03 | valid loss: 2.152396e-03 
      	| train loss (relative): 4.359707e-02 | valid loss (relative): 4.264647e-02 
Epoch 165 use: 355.82 second.

epoch 166 starting......
Epoch:  166 | train loss: 2.188904e-03 | valid loss: 2.139104e-03 
      	| train loss (relative): 4.356507e-02 | valid loss (relative): 4.226150e-02 
Epoch 166 use: 348.84 second.

epoch 167 starting......
Epoch:  167 | train loss: 2.198822e-03 | valid loss: 2.139832e-03 
      	| train loss (relative): 4.376382e-02 | valid loss (relative): 4.231105e-02 
Epoch 167 use: 347.49 second.

epoch 168 starting......
Epoch:  168 | train loss: 2.187409e-03 | valid loss: 2.122993e-03 
      	| train loss (relative): 4.353864e-02 | valid loss (relative): 4.229248e-02 
Epoch 168 use: 345.76 second.

epoch 169 starting......
Epoch:  169 | train loss: 2.173599e-03 | valid loss: 2.122853e-03 
      	| train loss (relative): 4.324886e-02 | valid loss (relative): 4.242396e-02 
Epoch 169 use: 341.20 second.

epoch 170 starting......
Epoch:  170 | train loss: 2.172319e-03 | valid loss: 2.117787e-03 
      	| train loss (relative): 4.322246e-02 | valid loss (relative): 4.183777e-02 
Epoch 170 use: 342.86 second.

epoch 171 starting......
Epoch:  171 | train loss: 2.173486e-03 | valid loss: 2.123465e-03 
      	| train loss (relative): 4.324664e-02 | valid loss (relative): 4.195754e-02 
Epoch 171 use: 340.64 second.

epoch 172 starting......
Epoch:  172 | train loss: 2.177402e-03 | valid loss: 2.115622e-03 
      	| train loss (relative): 4.331864e-02 | valid loss (relative): 4.193686e-02 
Epoch 172 use: 340.01 second.

epoch 173 starting......
Epoch:  173 | train loss: 2.170037e-03 | valid loss: 2.125818e-03 
      	| train loss (relative): 4.317817e-02 | valid loss (relative): 4.240236e-02 
Epoch 173 use: 341.47 second.

epoch 174 starting......
Epoch:  174 | train loss: 2.175926e-03 | valid loss: 2.121013e-03 
      	| train loss (relative): 4.329609e-02 | valid loss (relative): 4.193725e-02 
Epoch 174 use: 333.31 second.

epoch 175 starting......
Epoch:  175 | train loss: 2.174949e-03 | valid loss: 2.129015e-03 
      	| train loss (relative): 4.327359e-02 | valid loss (relative): 4.194811e-02 
Epoch 175 use: 318.67 second.

epoch 176 starting......
Epoch:  176 | train loss: 2.169948e-03 | valid loss: 2.112403e-03 
      	| train loss (relative): 4.317016e-02 | valid loss (relative): 4.195624e-02 
Epoch 176 use: 346.14 second.

epoch 177 starting......
Epoch:  177 | train loss: 2.159705e-03 | valid loss: 2.111612e-03 
      	| train loss (relative): 4.295662e-02 | valid loss (relative): 4.172012e-02 
Epoch 177 use: 356.56 second.

epoch 178 starting......
Epoch:  178 | train loss: 2.160457e-03 | valid loss: 2.105999e-03 
      	| train loss (relative): 4.296831e-02 | valid loss (relative): 4.176483e-02 
Epoch 178 use: 446.78 second.

epoch 179 starting......
Epoch:  179 | train loss: 2.152875e-03 | valid loss: 2.110270e-03 
      	| train loss (relative): 4.281615e-02 | valid loss (relative): 4.153139e-02 
Epoch 179 use: 383.52 second.

epoch 180 starting......
Epoch:  180 | train loss: 2.152602e-03 | valid loss: 2.105768e-03 
      	| train loss (relative): 4.281273e-02 | valid loss (relative): 4.137682e-02 
Epoch 180 use: 447.14 second.

epoch 181 starting......
Epoch:  181 | train loss: 2.159316e-03 | valid loss: 2.093704e-03 
      	| train loss (relative): 4.293660e-02 | valid loss (relative): 4.147233e-02 
Epoch 181 use: 368.78 second.

epoch 182 starting......
Epoch:  182 | train loss: 2.142360e-03 | valid loss: 2.090733e-03 
      	| train loss (relative): 4.260152e-02 | valid loss (relative): 4.138486e-02 
Epoch 182 use: 419.37 second.

epoch 183 starting......
Epoch:  183 | train loss: 2.148596e-03 | valid loss: 2.097367e-03 
      	| train loss (relative): 4.272135e-02 | valid loss (relative): 4.147175e-02 
Epoch 183 use: 456.97 second.

epoch 184 starting......
Epoch:  184 | train loss: 2.149633e-03 | valid loss: 2.104384e-03 
      	| train loss (relative): 4.274343e-02 | valid loss (relative): 4.111356e-02 
Epoch 184 use: 490.82 second.

epoch 185 starting......
Epoch:  185 | train loss: 2.139586e-03 | valid loss: 2.083432e-03 
      	| train loss (relative): 4.254074e-02 | valid loss (relative): 4.109575e-02 
Epoch 185 use: 482.79 second.

epoch 186 starting......
Epoch:  186 | train loss: 2.136955e-03 | valid loss: 2.087572e-03 
      	| train loss (relative): 4.248160e-02 | valid loss (relative): 4.129902e-02 
Epoch 186 use: 448.11 second.

epoch 187 starting......
Epoch:  187 | train loss: 2.136671e-03 | valid loss: 2.101303e-03 
      	| train loss (relative): 4.247390e-02 | valid loss (relative): 4.182959e-02 
Epoch 187 use: 429.10 second.

epoch 188 starting......
Epoch:  188 | train loss: 2.149198e-03 | valid loss: 2.093190e-03 
      	| train loss (relative): 4.273201e-02 | valid loss (relative): 4.153592e-02 
Epoch 188 use: 344.64 second.

epoch 189 starting......
Epoch:  189 | train loss: 2.131248e-03 | valid loss: 2.077562e-03 
      	| train loss (relative): 4.235395e-02 | valid loss (relative): 4.146879e-02 
Epoch 189 use: 344.45 second.

epoch 190 starting......
Epoch:  190 | train loss: 2.124183e-03 | valid loss: 2.069735e-03 
      	| train loss (relative): 4.221852e-02 | valid loss (relative): 4.105181e-02 
Epoch 190 use: 435.40 second.

epoch 191 starting......
Epoch:  191 | train loss: 2.124582e-03 | valid loss: 2.084143e-03 
      	| train loss (relative): 4.223108e-02 | valid loss (relative): 4.117029e-02 
Epoch 191 use: 742.73 second.

epoch 192 starting......
Epoch:  192 | train loss: 2.130733e-03 | valid loss: 2.075206e-03 
      	| train loss (relative): 4.234029e-02 | valid loss (relative): 4.106940e-02 
Epoch 192 use: 882.03 second.

epoch 193 starting......
Epoch:  193 | train loss: 2.126373e-03 | valid loss: 2.082749e-03 
      	| train loss (relative): 4.224966e-02 | valid loss (relative): 4.127249e-02 
Epoch 193 use: 352.15 second.

epoch 194 starting......
Epoch:  194 | train loss: 2.129329e-03 | valid loss: 2.090788e-03 
      	| train loss (relative): 4.231035e-02 | valid loss (relative): 4.154255e-02 
Epoch 194 use: 397.43 second.

epoch 195 starting......
Epoch:  195 | train loss: 2.130157e-03 | valid loss: 2.076590e-03 
      	| train loss (relative): 4.232045e-02 | valid loss (relative): 4.137236e-02 
Epoch 195 use: 340.32 second.

epoch 196 starting......
Epoch:  196 | train loss: 2.121795e-03 | valid loss: 2.072695e-03 
      	| train loss (relative): 4.217028e-02 | valid loss (relative): 4.097931e-02 
Epoch 196 use: 369.52 second.

epoch 197 starting......
Epoch:  197 | train loss: 2.125933e-03 | valid loss: 2.069445e-03 
      	| train loss (relative): 4.224548e-02 | valid loss (relative): 4.090171e-02 
Epoch 197 use: 339.26 second.

epoch 198 starting......
Epoch:  198 | train loss: 2.112863e-03 | valid loss: 2.058819e-03 
      	| train loss (relative): 4.198494e-02 | valid loss (relative): 4.060875e-02 
Epoch 198 use: 330.21 second.

epoch 199 starting......
Epoch:  199 | train loss: 2.110883e-03 | valid loss: 2.073907e-03 
      	| train loss (relative): 4.194184e-02 | valid loss (relative): 4.140425e-02 
Epoch 199 use: 337.35 second.

epoch 200 starting......
Epoch:  200 | train loss: 2.113097e-03 | valid loss: 2.077042e-03 
      	| train loss (relative): 4.198559e-02 | valid loss (relative): 4.106570e-02 
Epoch 200 use: 377.52 second.

epoch 201 starting......
Epoch:  201 | train loss: 2.120764e-03 | valid loss: 2.064348e-03 
      	| train loss (relative): 4.212947e-02 | valid loss (relative): 4.109453e-02 
Epoch 201 use: 347.31 second.

epoch 202 starting......
Epoch:  202 | train loss: 2.107240e-03 | valid loss: 2.054935e-03 
      	| train loss (relative): 4.186584e-02 | valid loss (relative): 4.076167e-02 
Epoch 202 use: 362.04 second.

epoch 203 starting......
Epoch:  203 | train loss: 2.103601e-03 | valid loss: 2.053706e-03 
      	| train loss (relative): 4.178522e-02 | valid loss (relative): 4.058615e-02 
Epoch 203 use: 382.83 second.

epoch 204 starting......
Epoch:  204 | train loss: 2.106644e-03 | valid loss: 2.084430e-03 
      	| train loss (relative): 4.184201e-02 | valid loss (relative): 4.087756e-02 
Epoch 204 use: 384.09 second.

epoch 205 starting......
Epoch:  205 | train loss: 2.114831e-03 | valid loss: 2.058224e-03 
      	| train loss (relative): 4.201142e-02 | valid loss (relative): 4.051291e-02 
Epoch 205 use: 438.99 second.

epoch 206 starting......
Epoch:  206 | train loss: 2.097933e-03 | valid loss: 2.046644e-03 
      	| train loss (relative): 4.168202e-02 | valid loss (relative): 4.048108e-02 
Epoch 206 use: 383.19 second.

epoch 207 starting......
Epoch:  207 | train loss: 2.097938e-03 | valid loss: 2.046529e-03 
      	| train loss (relative): 4.166123e-02 | valid loss (relative): 4.056988e-02 
Epoch 207 use: 390.36 second.

epoch 208 starting......
Epoch:  208 | train loss: 2.099307e-03 | valid loss: 2.073348e-03 
      	| train loss (relative): 4.168727e-02 | valid loss (relative): 4.142525e-02 
Epoch 208 use: 387.60 second.

epoch 209 starting......
Epoch:  209 | train loss: 2.102381e-03 | valid loss: 2.048729e-03 
      	| train loss (relative): 4.175146e-02 | valid loss (relative): 4.057647e-02 
Epoch 209 use: 347.40 second.

epoch 210 starting......
Epoch:  210 | train loss: 2.092831e-03 | valid loss: 2.042673e-03 
      	| train loss (relative): 4.156598e-02 | valid loss (relative): 4.056517e-02 
Epoch 210 use: 335.12 second.

epoch 211 starting......
Epoch:  211 | train loss: 2.088605e-03 | valid loss: 2.063258e-03 
      	| train loss (relative): 4.146678e-02 | valid loss (relative): 4.169339e-02 
Epoch 211 use: 338.89 second.

epoch 212 starting......
Epoch:  212 | train loss: 2.087663e-03 | valid loss: 2.040513e-03 
      	| train loss (relative): 4.146287e-02 | valid loss (relative): 4.049196e-02 
Epoch 212 use: 328.56 second.

epoch 213 starting......
Epoch:  213 | train loss: 2.084086e-03 | valid loss: 2.038447e-03 
      	| train loss (relative): 4.137932e-02 | valid loss (relative): 4.036426e-02 
Epoch 213 use: 374.01 second.

epoch 214 starting......
Epoch:  214 | train loss: 2.081693e-03 | valid loss: 2.036293e-03 
      	| train loss (relative): 4.132981e-02 | valid loss (relative): 4.024555e-02 
Epoch 214 use: 344.19 second.

epoch 215 starting......
Epoch:  215 | train loss: 2.083758e-03 | valid loss: 2.036297e-03 
      	| train loss (relative): 4.136842e-02 | valid loss (relative): 4.022456e-02 
Epoch 215 use: 330.69 second.

epoch 216 starting......
Epoch:  216 | train loss: 2.086672e-03 | valid loss: 2.038553e-03 
      	| train loss (relative): 4.143088e-02 | valid loss (relative): 4.033175e-02 
Epoch 216 use: 340.17 second.

epoch 217 starting......
Epoch:  217 | train loss: 2.086457e-03 | valid loss: 2.034443e-03 
      	| train loss (relative): 4.142166e-02 | valid loss (relative): 4.043340e-02 
Epoch 217 use: 376.30 second.

epoch 218 starting......
Epoch:  218 | train loss: 2.086063e-03 | valid loss: 2.032544e-03 
      	| train loss (relative): 4.141572e-02 | valid loss (relative): 4.049816e-02 
Epoch 218 use: 339.56 second.

epoch 219 starting......
Epoch:  219 | train loss: 2.076671e-03 | valid loss: 2.030865e-03 
      	| train loss (relative): 4.122760e-02 | valid loss (relative): 3.975421e-02 
Epoch 219 use: 338.18 second.

epoch 220 starting......
Epoch:  220 | train loss: 2.075149e-03 | valid loss: 2.030561e-03 
      	| train loss (relative): 4.118603e-02 | valid loss (relative): 4.026378e-02 
Epoch 220 use: 327.78 second.

epoch 221 starting......
Epoch:  221 | train loss: 2.072660e-03 | valid loss: 2.032046e-03 
      	| train loss (relative): 4.114146e-02 | valid loss (relative): 4.055402e-02 
Epoch 221 use: 336.61 second.

epoch 222 starting......
Epoch:  222 | train loss: 2.075675e-03 | valid loss: 2.036158e-03 
      	| train loss (relative): 4.119717e-02 | valid loss (relative): 4.015622e-02 
Epoch 222 use: 330.34 second.

epoch 223 starting......
Epoch:  223 | train loss: 2.074630e-03 | valid loss: 2.016964e-03 
      	| train loss (relative): 4.117823e-02 | valid loss (relative): 3.985911e-02 
Epoch 223 use: 358.28 second.

epoch 224 starting......
Epoch:  224 | train loss: 2.074408e-03 | valid loss: 2.017922e-03 
      	| train loss (relative): 4.116056e-02 | valid loss (relative): 3.996574e-02 
Epoch 224 use: 337.16 second.

epoch 225 starting......
Epoch:  225 | train loss: 2.064397e-03 | valid loss: 2.011237e-03 
      	| train loss (relative): 4.096655e-02 | valid loss (relative): 3.977990e-02 
Epoch 225 use: 367.27 second.

epoch 226 starting......
Epoch:  226 | train loss: 2.064819e-03 | valid loss: 2.028589e-03 
      	| train loss (relative): 4.098419e-02 | valid loss (relative): 4.021905e-02 
Epoch 226 use: 536.42 second.

epoch 227 starting......
Epoch:  227 | train loss: 2.068120e-03 | valid loss: 2.022782e-03 
      	| train loss (relative): 4.103995e-02 | valid loss (relative): 4.000840e-02 
Epoch 227 use: 1032.66 second.

epoch 228 starting......
Epoch:  228 | train loss: 2.068325e-03 | valid loss: 2.019701e-03 
      	| train loss (relative): 4.104316e-02 | valid loss (relative): 3.980749e-02 
Epoch 228 use: 936.11 second.

epoch 229 starting......
Epoch:  229 | train loss: 2.066848e-03 | valid loss: 2.023343e-03 
      	| train loss (relative): 4.101327e-02 | valid loss (relative): 4.014506e-02 
Epoch 229 use: 560.27 second.

epoch 230 starting......
Epoch:  230 | train loss: 2.064525e-03 | valid loss: 2.017098e-03 
      	| train loss (relative): 4.096325e-02 | valid loss (relative): 3.981045e-02 
Epoch 230 use: 356.92 second.

epoch 231 starting......
Epoch:  231 | train loss: 2.061788e-03 | valid loss: 2.013701e-03 
      	| train loss (relative): 4.090778e-02 | valid loss (relative): 3.973573e-02 
Epoch 231 use: 362.53 second.

epoch 232 starting......
Epoch:  232 | train loss: 2.056682e-03 | valid loss: 2.004077e-03 
      	| train loss (relative): 4.080459e-02 | valid loss (relative): 3.951406e-02 
Epoch 232 use: 343.76 second.

epoch 233 starting......
Epoch:  233 | train loss: 2.054201e-03 | valid loss: 2.013018e-03 
      	| train loss (relative): 4.076114e-02 | valid loss (relative): 3.950850e-02 
Epoch 233 use: 342.36 second.

epoch 234 starting......
Epoch:  234 | train loss: 2.049527e-03 | valid loss: 2.002306e-03 
      	| train loss (relative): 4.065355e-02 | valid loss (relative): 3.965543e-02 
Epoch 234 use: 329.17 second.

epoch 235 starting......
Epoch:  235 | train loss: 2.061643e-03 | valid loss: 2.010170e-03 
      	| train loss (relative): 4.090484e-02 | valid loss (relative): 3.984591e-02 
Epoch 235 use: 330.44 second.

epoch 236 starting......
Epoch:  236 | train loss: 2.049023e-03 | valid loss: 2.003322e-03 
      	| train loss (relative): 4.064345e-02 | valid loss (relative): 3.988623e-02 
Epoch 236 use: 325.79 second.

epoch 237 starting......
Epoch:  237 | train loss: 2.048526e-03 | valid loss: 2.016199e-03 
      	| train loss (relative): 4.063059e-02 | valid loss (relative): 4.014144e-02 
Epoch 237 use: 423.32 second.

epoch 238 starting......
Epoch:  238 | train loss: 2.051820e-03 | valid loss: 2.035782e-03 
      	| train loss (relative): 4.069709e-02 | valid loss (relative): 4.078372e-02 
Epoch 238 use: 359.76 second.

epoch 239 starting......
Epoch:  239 | train loss: 2.044917e-03 | valid loss: 1.998783e-03 
      	| train loss (relative): 4.057739e-02 | valid loss (relative): 3.951845e-02 
Epoch 239 use: 348.95 second.

epoch 240 starting......
Epoch:  240 | train loss: 2.036200e-03 | valid loss: 1.990889e-03 
      	| train loss (relative): 4.038907e-02 | valid loss (relative): 3.920251e-02 
Epoch 240 use: 333.67 second.

epoch 241 starting......
Epoch:  241 | train loss: 2.038816e-03 | valid loss: 2.001932e-03 
      	| train loss (relative): 4.043319e-02 | valid loss (relative): 3.956712e-02 
Epoch 241 use: 320.12 second.

epoch 242 starting......
Epoch:  242 | train loss: 2.038529e-03 | valid loss: 2.009497e-03 
      	| train loss (relative): 4.043019e-02 | valid loss (relative): 3.975580e-02 
Epoch 242 use: 337.05 second.

epoch 243 starting......
Epoch:  243 | train loss: 2.038689e-03 | valid loss: 1.994795e-03 
      	| train loss (relative): 4.042790e-02 | valid loss (relative): 3.921563e-02 
Epoch 243 use: 335.01 second.

epoch 244 starting......
Epoch:  244 | train loss: 2.037252e-03 | valid loss: 2.003310e-03 
      	| train loss (relative): 4.040045e-02 | valid loss (relative): 3.967389e-02 
Epoch 244 use: 339.36 second.

epoch 245 starting......
Epoch:  245 | train loss: 2.036438e-03 | valid loss: 1.988918e-03 
      	| train loss (relative): 4.038682e-02 | valid loss (relative): 3.944657e-02 
Epoch 245 use: 340.51 second.

epoch 246 starting......
Epoch:  246 | train loss: 2.031798e-03 | valid loss: 2.008536e-03 
      	| train loss (relative): 4.028406e-02 | valid loss (relative): 4.000754e-02 
Epoch 246 use: 331.77 second.

epoch 247 starting......
Epoch:  247 | train loss: 2.033164e-03 | valid loss: 1.987416e-03 
      	| train loss (relative): 4.031441e-02 | valid loss (relative): 3.949098e-02 
Epoch 247 use: 335.22 second.

epoch 248 starting......
Epoch:  248 | train loss: 2.025360e-03 | valid loss: 1.981550e-03 
      	| train loss (relative): 4.015328e-02 | valid loss (relative): 3.960725e-02 
Epoch 248 use: 345.88 second.

epoch 249 starting......
Epoch:  249 | train loss: 2.025904e-03 | valid loss: 1.984592e-03 
      	| train loss (relative): 4.016731e-02 | valid loss (relative): 3.893836e-02 
Epoch 249 use: 345.20 second.

epoch 250 starting......
Epoch:  250 | train loss: 2.025098e-03 | valid loss: 1.976900e-03 
      	| train loss (relative): 4.014799e-02 | valid loss (relative): 3.891288e-02 
Epoch 250 use: 369.39 second.

epoch 251 starting......
Epoch:  251 | train loss: 2.020278e-03 | valid loss: 1.976196e-03 
      	| train loss (relative): 4.005016e-02 | valid loss (relative): 3.903237e-02 
Epoch 251 use: 364.11 second.

epoch 252 starting......
Epoch:  252 | train loss: 2.016625e-03 | valid loss: 1.971712e-03 
      	| train loss (relative): 3.996951e-02 | valid loss (relative): 3.914667e-02 
Epoch 252 use: 330.58 second.

epoch 253 starting......
Epoch:  253 | train loss: 2.021616e-03 | valid loss: 1.983730e-03 
      	| train loss (relative): 4.007636e-02 | valid loss (relative): 3.900104e-02 
Epoch 253 use: 358.91 second.

epoch 254 starting......
Epoch:  254 | train loss: 2.019498e-03 | valid loss: 1.975701e-03 
      	| train loss (relative): 4.002237e-02 | valid loss (relative): 3.912093e-02 
Epoch 254 use: 328.70 second.

epoch 255 starting......
Epoch:  255 | train loss: 2.017692e-03 | valid loss: 1.980655e-03 
      	| train loss (relative): 3.999304e-02 | valid loss (relative): 3.899376e-02 
Epoch 255 use: 361.39 second.

epoch 256 starting......
Epoch:  256 | train loss: 2.021361e-03 | valid loss: 1.968456e-03 
      	| train loss (relative): 4.005932e-02 | valid loss (relative): 3.906582e-02 
Epoch 256 use: 326.55 second.

epoch 257 starting......
Epoch:  257 | train loss: 2.011199e-03 | valid loss: 1.964911e-03 
      	| train loss (relative): 3.986649e-02 | valid loss (relative): 3.879190e-02 
Epoch 257 use: 326.35 second.

epoch 258 starting......
Epoch:  258 | train loss: 2.015171e-03 | valid loss: 1.968986e-03 
      	| train loss (relative): 3.993918e-02 | valid loss (relative): 3.884052e-02 
Epoch 258 use: 331.88 second.

epoch 259 starting......
Epoch:  259 | train loss: 2.012117e-03 | valid loss: 1.977580e-03 
      	| train loss (relative): 3.987738e-02 | valid loss (relative): 3.879455e-02 
Epoch 259 use: 322.67 second.

epoch 260 starting......
Epoch:  260 | train loss: 2.011367e-03 | valid loss: 1.966530e-03 
      	| train loss (relative): 3.986079e-02 | valid loss (relative): 3.899680e-02 
Epoch 260 use: 337.82 second.

epoch 261 starting......
Epoch:  261 | train loss: 2.005807e-03 | valid loss: 1.965532e-03 
      	| train loss (relative): 3.974874e-02 | valid loss (relative): 3.901650e-02 
Epoch 261 use: 342.16 second.

epoch 262 starting......
Epoch:  262 | train loss: 2.005304e-03 | valid loss: 1.965167e-03 
      	| train loss (relative): 3.973768e-02 | valid loss (relative): 3.886105e-02 
Epoch 262 use: 321.87 second.

epoch 263 starting......
Epoch:  263 | train loss: 2.002362e-03 | valid loss: 1.958885e-03 
      	| train loss (relative): 3.967611e-02 | valid loss (relative): 3.874987e-02 
Epoch 263 use: 359.71 second.

epoch 264 starting......
Epoch:  264 | train loss: 2.003517e-03 | valid loss: 1.962226e-03 
      	| train loss (relative): 3.969776e-02 | valid loss (relative): 3.900981e-02 
Epoch 264 use: 350.53 second.

epoch 265 starting......
Epoch:  265 | train loss: 2.004018e-03 | valid loss: 1.984460e-03 
      	| train loss (relative): 3.970899e-02 | valid loss (relative): 3.861828e-02 
Epoch 265 use: 337.81 second.

epoch 266 starting......
Epoch:  266 | train loss: 2.008803e-03 | valid loss: 1.983215e-03 
      	| train loss (relative): 3.979982e-02 | valid loss (relative): 3.932396e-02 
Epoch 266 use: 332.72 second.

epoch 267 starting......
Epoch:  267 | train loss: 1.998615e-03 | valid loss: 1.948635e-03 
      	| train loss (relative): 3.960247e-02 | valid loss (relative): 3.841584e-02 
Epoch 267 use: 335.68 second.

epoch 268 starting......
Epoch:  268 | train loss: 1.990377e-03 | valid loss: 1.948004e-03 
      	| train loss (relative): 3.943025e-02 | valid loss (relative): 3.856044e-02 
Epoch 268 use: 329.55 second.

epoch 269 starting......
Epoch:  269 | train loss: 1.996013e-03 | valid loss: 1.960338e-03 
      	| train loss (relative): 3.954170e-02 | valid loss (relative): 3.866745e-02 
Epoch 269 use: 352.22 second.

epoch 270 starting......
Epoch:  270 | train loss: 1.999369e-03 | valid loss: 1.955262e-03 
      	| train loss (relative): 3.961582e-02 | valid loss (relative): 3.838047e-02 
Epoch 270 use: 331.51 second.

epoch 271 starting......
Epoch:  271 | train loss: 1.988018e-03 | valid loss: 1.944343e-03 
      	| train loss (relative): 3.937678e-02 | valid loss (relative): 3.845713e-02 
Epoch 271 use: 330.96 second.

epoch 272 starting......
Epoch:  272 | train loss: 1.987695e-03 | valid loss: 1.954712e-03 
      	| train loss (relative): 3.937973e-02 | valid loss (relative): 3.836936e-02 
Epoch 272 use: 337.87 second.

epoch 273 starting......
Epoch:  273 | train loss: 1.987334e-03 | valid loss: 1.949690e-03 
      	| train loss (relative): 3.936334e-02 | valid loss (relative): 3.857002e-02 
Epoch 273 use: 356.83 second.

epoch 274 starting......
Epoch:  274 | train loss: 1.992167e-03 | valid loss: 1.960725e-03 
      	| train loss (relative): 3.946469e-02 | valid loss (relative): 3.865630e-02 
Epoch 274 use: 340.66 second.

epoch 275 starting......
Epoch:  275 | train loss: 1.989361e-03 | valid loss: 1.948496e-03 
      	| train loss (relative): 3.940116e-02 | valid loss (relative): 3.851620e-02 
Epoch 275 use: 354.44 second.

epoch 276 starting......
Epoch:  276 | train loss: 1.984128e-03 | valid loss: 1.947565e-03 
      	| train loss (relative): 3.929872e-02 | valid loss (relative): 3.829364e-02 
Epoch 276 use: 333.68 second.

epoch 277 starting......
Epoch:  277 | train loss: 1.983243e-03 | valid loss: 1.946527e-03 
      	| train loss (relative): 3.927655e-02 | valid loss (relative): 3.823203e-02 
Epoch 277 use: 383.30 second.

epoch 278 starting......
Epoch:  278 | train loss: 1.987359e-03 | valid loss: 1.953124e-03 
      	| train loss (relative): 3.935834e-02 | valid loss (relative): 3.848251e-02 
Epoch 278 use: 336.43 second.

epoch 279 starting......
Epoch:  279 | train loss: 1.982252e-03 | valid loss: 1.947442e-03 
      	| train loss (relative): 3.926136e-02 | valid loss (relative): 3.826866e-02 
Epoch 279 use: 373.10 second.

epoch 280 starting......
Epoch:  280 | train loss: 1.987222e-03 | valid loss: 1.941182e-03 
      	| train loss (relative): 3.935215e-02 | valid loss (relative): 3.829908e-02 
Epoch 280 use: 398.72 second.

epoch 281 starting......
Epoch:  281 | train loss: 1.974762e-03 | valid loss: 1.941332e-03 
      	| train loss (relative): 3.910678e-02 | valid loss (relative): 3.845794e-02 
Epoch 281 use: 363.38 second.

epoch 282 starting......
Epoch:  282 | train loss: 1.977970e-03 | valid loss: 1.943153e-03 
      	| train loss (relative): 3.916837e-02 | valid loss (relative): 3.789331e-02 
Epoch 282 use: 421.75 second.

epoch 283 starting......
Epoch:  283 | train loss: 1.972695e-03 | valid loss: 1.940780e-03 
      	| train loss (relative): 3.906013e-02 | valid loss (relative): 3.835844e-02 
Epoch 283 use: 385.36 second.

epoch 284 starting......
Epoch:  284 | train loss: 1.974749e-03 | valid loss: 1.936538e-03 
      	| train loss (relative): 3.909945e-02 | valid loss (relative): 3.801628e-02 
Epoch 284 use: 397.99 second.

epoch 285 starting......
Epoch:  285 | train loss: 1.969455e-03 | valid loss: 1.923323e-03 
      	| train loss (relative): 3.898906e-02 | valid loss (relative): 3.802414e-02 
Epoch 285 use: 378.65 second.

epoch 286 starting......
Epoch:  286 | train loss: 1.962563e-03 | valid loss: 1.923722e-03 
      	| train loss (relative): 3.885892e-02 | valid loss (relative): 3.785693e-02 
Epoch 286 use: 420.07 second.

epoch 287 starting......
Epoch:  287 | train loss: 1.968479e-03 | valid loss: 1.954122e-03 
      	| train loss (relative): 3.897534e-02 | valid loss (relative): 3.825178e-02 
Epoch 287 use: 425.09 second.

epoch 288 starting......
Epoch:  288 | train loss: 1.971212e-03 | valid loss: 1.930405e-03 
      	| train loss (relative): 3.901488e-02 | valid loss (relative): 3.823392e-02 
Epoch 288 use: 373.44 second.

epoch 289 starting......
Epoch:  289 | train loss: 1.964158e-03 | valid loss: 1.921520e-03 
      	| train loss (relative): 3.888534e-02 | valid loss (relative): 3.803584e-02 
Epoch 289 use: 380.09 second.

epoch 290 starting......
Epoch:  290 | train loss: 1.962734e-03 | valid loss: 1.926478e-03 
      	| train loss (relative): 3.885379e-02 | valid loss (relative): 3.816587e-02 
Epoch 290 use: 368.65 second.

epoch 291 starting......
Epoch:  291 | train loss: 1.964131e-03 | valid loss: 1.937823e-03 
      	| train loss (relative): 3.889064e-02 | valid loss (relative): 3.793826e-02 
Epoch 291 use: 374.65 second.

epoch 292 starting......
Epoch:  292 | train loss: 1.962213e-03 | valid loss: 1.923791e-03 
      	| train loss (relative): 3.883286e-02 | valid loss (relative): 3.774761e-02 
Epoch 292 use: 446.97 second.

epoch 293 starting......
Epoch:  293 | train loss: 1.956206e-03 | valid loss: 1.929336e-03 
      	| train loss (relative): 3.872411e-02 | valid loss (relative): 3.761123e-02 
Epoch 293 use: 370.55 second.

epoch 294 starting......
Epoch:  294 | train loss: 1.957475e-03 | valid loss: 1.906543e-03 
      	| train loss (relative): 3.874537e-02 | valid loss (relative): 3.769764e-02 
Epoch 294 use: 382.96 second.

epoch 295 starting......
Epoch:  295 | train loss: 1.947768e-03 | valid loss: 1.912752e-03 
      	| train loss (relative): 3.855025e-02 | valid loss (relative): 3.789498e-02 
Epoch 295 use: 406.25 second.

epoch 296 starting......
Epoch:  296 | train loss: 1.954167e-03 | valid loss: 1.916791e-03 
      	| train loss (relative): 3.867225e-02 | valid loss (relative): 3.771778e-02 
Epoch 296 use: 447.73 second.

epoch 297 starting......
Epoch:  297 | train loss: 1.954952e-03 | valid loss: 1.918053e-03 
      	| train loss (relative): 3.869836e-02 | valid loss (relative): 3.781325e-02 
Epoch 297 use: 424.26 second.

epoch 298 starting......
Epoch:  298 | train loss: 1.952704e-03 | valid loss: 1.919220e-03 
      	| train loss (relative): 3.864438e-02 | valid loss (relative): 3.797682e-02 
Epoch 298 use: 373.66 second.

epoch 299 starting......
Epoch:  299 | train loss: 1.953507e-03 | valid loss: 1.918244e-03 
      	| train loss (relative): 3.866132e-02 | valid loss (relative): 3.801460e-02 
Epoch 299 use: 385.13 second.

test MSE Error: 1.944561e-03 | relative MSE Error: 3.854830e-02 
 Total time used for training: 23.71 hour.
MESLoss saved to  /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_MSELoss_Latent_128_nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_300.txt
relative MSELoss saved to  /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_reMSELoss_Latent_128_nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_300.txt
model saved to /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_128_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_300.pth
model_dict saved to /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_128_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_300_dict.pth
... Training slugflow data completed, Run finished Wed 11 Aug 11:26:36 BST 2021 ...
{'vtu_dir': '/rds/general/user/jy220/home/slugflow/', 'data_dir': '/rds/general/user/jy220/home/tensors/', 'data_type': 'tensors', 'vtu_fields': 'Component1::ComponentMassFractionPhase1, phase1::Velocity', 'variational': 'False', 'optimizer': 'Adamax', 'structured': 'False', 'changing_lr': 'False', 'self_concat': '2', 'dimension': '3', 'sfc_nums': '3', 'components': '4', 'nearest_neighbouring': 'True', 'dims_latent': '128', 'activation': 'Tanh', 'sfc_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/sfcs.pt', 'inv_sfc_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/inv_sfcs.pt', 'coords_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/coords.pt', 'cells_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/cells.npy', 'state_load': '/rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_128_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_300_dict.pth', 'batch_size': '16', 'lr': '0.001', 'n_epoches': '0', 'seed': '15', 'visualize': 'False', 'reconstructed_path': '/rds/general/user/jy220/home/reconstructed', 'reconstruct_start_index': '0', 'reconstruct_end_index': '100', 'save_path': '/rds/general/user/jy220/home/new_results/'}
[[ 2.50825269e-01 -1.67256990e-02 -1.22268730e-03]
 [ 2.38611959e-01 -5.50627777e-03 -5.06451913e-03]
 [ 2.39454874e-01 -1.59563257e-02  3.96778323e-03]
 ...
 [ 9.11361096e+00 -2.21908381e-02  2.22384902e-03]
 [ 9.11847310e+00 -2.84346931e-02 -2.34839590e-04]
 [ 9.11039661e+00 -2.51498573e-02 -8.70661044e-03]]
{'tetra': array([[      0,       1,       2,       3],
       [      4,       5,       6,       7],
       [      8,       9,      10,      11],
       ...,
       [1342744, 1342745, 1342746, 1342747],
       [1342748, 1342749, 1342750, 1342751],
       [1342752, 1342753, 1342754, 1342755]])}
structured  False 
 activation  Tanh() 
 self concat  2 
 sfc_nums  3 

dims_latent  128 
 components  4 
 nearest_neighbouring  True 

visualize  False 
 sample number  1706 

reading sfc nums......
[array([  41684,   41685,   41674, ..., 1235560, 1235564, 1235558]), array([954289, 954290, 954291, ..., 420685, 420687, 420688]), array([  40156,   40155,   40158, ..., 1123478, 1123475, 1123467])]
reading inverse sfc nums......
[array([   8146,    8145,    8144, ..., 1274444, 1274445, 1273166]), array([547280, 547283, 593037, ..., 933933, 933932, 864488]), array([ 66437,  66439, 122512, ..., 459416, 415130, 459418])]
Computing min and max......

tk:  tensor([0.7581, 0.0418, 0.1840, 0.1464], dtype=torch.float64) 

tb:  tensor([-0.3816, -0.3618, -0.0308,  0.1446], dtype=torch.float64) 

length of train set: 1505 

length of valid set: 100 

length of test set: 101 

torch device num: 4 

Let's use 4 GPUs!
test MSE Error: 1.921762e-03 | relative MSE Error: 3.819534e-02 
 Total time used for training: 0.01 hour.
MESLoss saved to  /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_MSELoss_Latent_128_nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_300.txt
relative MSELoss saved to  /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_reMSELoss_Latent_128_nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_300.txt
model saved to /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_128_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_300.pth
model_dict saved to /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_128_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_300_dict.pth
... Training slugflow data completed, Run finished Wed 11 Aug 12:54:43 BST 2021 ...
{'vtu_dir': '/rds/general/user/jy220/home/slugflow/', 'data_dir': '/rds/general/user/jy220/home/tensors/', 'data_type': 'tensors', 'vtu_fields': 'Component1::ComponentMassFractionPhase1, phase1::Velocity', 'variational': 'False', 'optimizer': 'Adamax', 'structured': 'False', 'changing_lr': 'False', 'self_concat': '2', 'dimension': '3', 'sfc_nums': '3', 'components': '4', 'nearest_neighbouring': 'True', 'dims_latent': '128', 'activation': 'Tanh', 'sfc_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/sfcs.pt', 'inv_sfc_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/inv_sfcs.pt', 'coords_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/coords.pt', 'cells_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/cells.npy', 'state_load': '/rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_128_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_300_dict.pth', 'batch_size': '16', 'lr': '0.001', 'n_epoches': '0', 'seed': '15', 'visualize': 'False', 'reconstructed_path': '/rds/general/user/jy220/home/reconstructed', 'reconstruct_start_index': '0', 'reconstruct_end_index': '100', 'save_path': '/rds/general/user/jy220/home/new_results/'}
[[ 2.50825269e-01 -1.67256990e-02 -1.22268730e-03]
 [ 2.38611959e-01 -5.50627777e-03 -5.06451913e-03]
 [ 2.39454874e-01 -1.59563257e-02  3.96778323e-03]
 ...
 [ 9.11361096e+00 -2.21908381e-02  2.22384902e-03]
 [ 9.11847310e+00 -2.84346931e-02 -2.34839590e-04]
 [ 9.11039661e+00 -2.51498573e-02 -8.70661044e-03]]
{'tetra': array([[      0,       1,       2,       3],
       [      4,       5,       6,       7],
       [      8,       9,      10,      11],
       ...,
       [1342744, 1342745, 1342746, 1342747],
       [1342748, 1342749, 1342750, 1342751],
       [1342752, 1342753, 1342754, 1342755]])}
structured  False 
 activation  Tanh() 
 self concat  2 
 sfc_nums  3 

dims_latent  128 
 components  4 
 nearest_neighbouring  True 

visualize  False 
 sample number  1706 

reading sfc nums......
[array([  41684,   41685,   41674, ..., 1235560, 1235564, 1235558]), array([954289, 954290, 954291, ..., 420685, 420687, 420688]), array([  40156,   40155,   40158, ..., 1123478, 1123475, 1123467])]
reading inverse sfc nums......
[array([   8146,    8145,    8144, ..., 1274444, 1274445, 1273166]), array([547280, 547283, 593037, ..., 933933, 933932, 864488]), array([ 66437,  66439, 122512, ..., 459416, 415130, 459418])]
Computing min and max......

tk:  tensor([0.7581, 0.0418, 0.1840, 0.1464], dtype=torch.float64) 

tb:  tensor([-0.3816, -0.3618, -0.0308,  0.1446], dtype=torch.float64) 

length of train set: 1505 

length of valid set: 100 

length of test set: 101 

torch device num: 4 

Let's use 4 GPUs!
test MSE Error: 1.972502e-03 | relative MSE Error: 3.914318e-02 
 Total time used for training: 0.01 hour.
MESLoss saved to  /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_MSELoss_Latent_128_nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_300.txt
relative MSELoss saved to  /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_reMSELoss_Latent_128_nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_300.txt
model saved to /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_128_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_300.pth
model_dict saved to /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_128_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_300_dict.pth
file_prefix: /rds/general/user/jy220/home/slugflow/slug_255_exp_projected_compressed_, file_format: .vtu
Write vtu Data......

... Training slugflow data completed, Run finished Wed 11 Aug 13:43:25 BST 2021 ...
{'vtu_dir': '/rds/general/user/jy220/home/slugflow/', 'data_dir': '/rds/general/user/jy220/home/tensors/', 'data_type': 'tensors', 'vtu_fields': 'Component1::ComponentMassFractionPhase1, phase1::Velocity', 'variational': 'False', 'optimizer': 'Adamax', 'structured': 'False', 'changing_lr': 'False', 'self_concat': '2', 'dimension': '3', 'sfc_nums': '3', 'components': '4', 'nearest_neighbouring': 'True', 'dims_latent': '128', 'activation': 'Tanh', 'sfc_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/sfcs.pt', 'inv_sfc_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/inv_sfcs.pt', 'coords_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/coords.pt', 'cells_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/cells.npy', 'state_load': '/rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_128_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_300_dict.pth', 'batch_size': '16', 'lr': '0.001', 'n_epoches': '0', 'seed': '15', 'visualize': 'False', 'reconstructed_path': '/rds/general/user/jy220/home/reconstructed', 'reconstruct_start_index': '0', 'reconstruct_end_index': '100', 'save_path': 'None'}
[[ 2.50825269e-01 -1.67256990e-02 -1.22268730e-03]
 [ 2.38611959e-01 -5.50627777e-03 -5.06451913e-03]
 [ 2.39454874e-01 -1.59563257e-02  3.96778323e-03]
 ...
 [ 9.11361096e+00 -2.21908381e-02  2.22384902e-03]
 [ 9.11847310e+00 -2.84346931e-02 -2.34839590e-04]
 [ 9.11039661e+00 -2.51498573e-02 -8.70661044e-03]]
{'tetra': array([[      0,       1,       2,       3],
       [      4,       5,       6,       7],
       [      8,       9,      10,      11],
       ...,
       [1342744, 1342745, 1342746, 1342747],
       [1342748, 1342749, 1342750, 1342751],
       [1342752, 1342753, 1342754, 1342755]])}
vtu fields:  ['Component1::ComponentMassFractionPhase1', 'phase1::Velocity'] 
 structured  False 
 activation  Tanh() 
 self concat  2 
 sfc_nums  3 

dims_latent  128 
 components  4 
 nearest_neighbouring  True 

visualize  False 
 sample number  1706 

reading sfc nums......
[array([  41684,   41685,   41674, ..., 1235560, 1235564, 1235558]), array([954289, 954290, 954291, ..., 420685, 420687, 420688]), array([  40156,   40155,   40158, ..., 1123478, 1123475, 1123467])]
reading inverse sfc nums......
[array([   8146,    8145,    8144, ..., 1274444, 1274445, 1273166]), array([547280, 547283, 593037, ..., 933933, 933932, 864488]), array([ 66437,  66439, 122512, ..., 459416, 415130, 459418])]
Computing min and max......

tk:  tensor([0.7581, 0.0418, 0.1840, 0.1464], dtype=torch.float64) 

tb:  tensor([-0.3816, -0.3618, -0.0308,  0.1446], dtype=torch.float64) 

length of train set: 1505 

length of valid set: 100 

length of test set: 101 

torch device num: 4 

Let's use 4 GPUs!
test MSE Error: 1.928382e-03 | relative MSE Error: 3.824467e-02 
 Total time used for training: 0.01 hour.
file_prefix: /rds/general/user/jy220/home/slugflow/slug_255_exp_projected_compressed_, file_format: .vtu
Write vtu Data......

error for snapshot 0: 0.001839
torch.Size([1342756, 4])
error for snapshot 1: 0.001529
torch.Size([1342756, 4])
error for snapshot 2: 0.001210
torch.Size([1342756, 4])
error for snapshot 3: 0.001024
torch.Size([1342756, 4])
error for snapshot 4: 0.000936
torch.Size([1342756, 4])
error for snapshot 5: 0.000885
torch.Size([1342756, 4])
error for snapshot 6: 0.000873
torch.Size([1342756, 4])
error for snapshot 7: 0.000871
torch.Size([1342756, 4])
error for snapshot 8: 0.000867
torch.Size([1342756, 4])
error for snapshot 9: 0.000848
torch.Size([1342756, 4])
error for snapshot 10: 0.000838
torch.Size([1342756, 4])
error for snapshot 11: 0.000839
torch.Size([1342756, 4])
error for snapshot 12: 0.000838
torch.Size([1342756, 4])
error for snapshot 13: 0.000836
torch.Size([1342756, 4])
error for snapshot 14: 0.000854
torch.Size([1342756, 4])
error for snapshot 15: 0.000857
torch.Size([1342756, 4])
error for snapshot 16: 0.000888
torch.Size([1342756, 4])
error for snapshot 17: 0.000914
torch.Size([1342756, 4])
error for snapshot 18: 0.000944
torch.Size([1342756, 4])
error for snapshot 19: 0.000997
torch.Size([1342756, 4])
error for snapshot 20: 0.001000
torch.Size([1342756, 4])
error for snapshot 21: 0.001022
torch.Size([1342756, 4])
error for snapshot 22: 0.001004
torch.Size([1342756, 4])
error for snapshot 23: 0.000959
torch.Size([1342756, 4])
error for snapshot 24: 0.000858
torch.Size([1342756, 4])
error for snapshot 25: 0.000880
torch.Size([1342756, 4])
error for snapshot 26: 0.000990
torch.Size([1342756, 4])
error for snapshot 27: 0.001069
torch.Size([1342756, 4])
error for snapshot 28: 0.001136
torch.Size([1342756, 4])
error for snapshot 29: 0.001166
torch.Size([1342756, 4])
error for snapshot 30: 0.001165
torch.Size([1342756, 4])
error for snapshot 31: 0.001159
torch.Size([1342756, 4])
error for snapshot 32: 0.001148
torch.Size([1342756, 4])
error for snapshot 33: 0.001151
torch.Size([1342756, 4])
error for snapshot 34: 0.001144
torch.Size([1342756, 4])
error for snapshot 35: 0.001119
torch.Size([1342756, 4])
error for snapshot 36: 0.001114
torch.Size([1342756, 4])
error for snapshot 37: 0.001110
torch.Size([1342756, 4])
error for snapshot 38: 0.001110
torch.Size([1342756, 4])
error for snapshot 39: 0.001122
torch.Size([1342756, 4])
error for snapshot 40: 0.001138
torch.Size([1342756, 4])
error for snapshot 41: 0.001125
torch.Size([1342756, 4])
error for snapshot 42: 0.001100
torch.Size([1342756, 4])
error for snapshot 43: 0.001078
torch.Size([1342756, 4])
error for snapshot 44: 0.001044
torch.Size([1342756, 4])
error for snapshot 45: 0.001045
torch.Size([1342756, 4])
error for snapshot 46: 0.001051
torch.Size([1342756, 4])
error for snapshot 47: 0.001085
torch.Size([1342756, 4])
error for snapshot 48: 0.001103
torch.Size([1342756, 4])
error for snapshot 49: 0.001109
torch.Size([1342756, 4])
error for snapshot 50: 0.001103
torch.Size([1342756, 4])
error for snapshot 51: 0.001046
torch.Size([1342756, 4])
error for snapshot 52: 0.001022
torch.Size([1342756, 4])
error for snapshot 53: 0.001137
torch.Size([1342756, 4])
error for snapshot 54: 0.001268
torch.Size([1342756, 4])
error for snapshot 55: 0.001385
torch.Size([1342756, 4])
error for snapshot 56: 0.001441
torch.Size([1342756, 4])
error for snapshot 57: 0.001457
torch.Size([1342756, 4])
error for snapshot 58: 0.001483
torch.Size([1342756, 4])
error for snapshot 59: 0.001443
torch.Size([1342756, 4])
error for snapshot 60: 0.001433
torch.Size([1342756, 4])
error for snapshot 61: 0.001395
torch.Size([1342756, 4])
error for snapshot 62: 0.001330
torch.Size([1342756, 4])
error for snapshot 63: 0.001236
torch.Size([1342756, 4])
error for snapshot 64: 0.001207
torch.Size([1342756, 4])
error for snapshot 65: 0.001188
torch.Size([1342756, 4])
error for snapshot 66: 0.001124
torch.Size([1342756, 4])
error for snapshot 67: 0.001066
torch.Size([1342756, 4])
error for snapshot 68: 0.001043
torch.Size([1342756, 4])
error for snapshot 69: 0.001023
torch.Size([1342756, 4])
error for snapshot 70: 0.001004
torch.Size([1342756, 4])
error for snapshot 71: 0.001008
torch.Size([1342756, 4])
error for snapshot 72: 0.001012
torch.Size([1342756, 4])
error for snapshot 73: 0.001017
torch.Size([1342756, 4])
error for snapshot 74: 0.001041
torch.Size([1342756, 4])
error for snapshot 75: 0.001082
torch.Size([1342756, 4])
error for snapshot 76: 0.001125
torch.Size([1342756, 4])
error for snapshot 77: 0.001166
torch.Size([1342756, 4])
error for snapshot 78: 0.001186
torch.Size([1342756, 4])
error for snapshot 79: 0.001143
torch.Size([1342756, 4])
error for snapshot 80: 0.001055
torch.Size([1342756, 4])
error for snapshot 81: 0.000963
torch.Size([1342756, 4])
error for snapshot 82: 0.001076
torch.Size([1342756, 4])
error for snapshot 83: 0.001189
torch.Size([1342756, 4])
error for snapshot 84: 0.001220
torch.Size([1342756, 4])
error for snapshot 85: 0.001199
torch.Size([1342756, 4])
error for snapshot 86: 0.001152
torch.Size([1342756, 4])
error for snapshot 87: 0.001091
torch.Size([1342756, 4])
error for snapshot 88: 0.001041
torch.Size([1342756, 4])
error for snapshot 89: 0.001001
torch.Size([1342756, 4])
error for snapshot 90: 0.000985
torch.Size([1342756, 4])
error for snapshot 91: 0.000960
torch.Size([1342756, 4])
error for snapshot 92: 0.000957
torch.Size([1342756, 4])
error for snapshot 93: 0.000942
torch.Size([1342756, 4])
error for snapshot 94: 0.000928
torch.Size([1342756, 4])
error for snapshot 95: 0.000919
torch.Size([1342756, 4])
error for snapshot 96: 0.000907
torch.Size([1342756, 4])
error for snapshot 97: 0.000900
torch.Size([1342756, 4])
error for snapshot 98: 0.000899
torch.Size([1342756, 4])
error for snapshot 99: 0.000918
torch.Size([1342756, 4])

 Finished writing vtu files.
... Training slugflow data completed, Run finished Wed 11 Aug 15:47:32 BST 2021 ...
{'vtu_dir': '/rds/general/user/jy220/home/slugflow/', 'data_dir': '/rds/general/user/jy220/home/tensors/', 'data_type': 'tensors', 'vtu_fields': 'Component1::ComponentMassFractionPhase1, phase1::Velocity', 'variational': 'False', 'optimizer': 'Adamax', 'structured': 'False', 'changing_lr': 'False', 'self_concat': '2', 'dimension': '3', 'sfc_nums': '3', 'components': '4', 'nearest_neighbouring': 'True', 'dims_latent': '128', 'activation': 'Tanh', 'sfc_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/sfcs.pt', 'inv_sfc_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/inv_sfcs.pt', 'coords_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/coords.pt', 'cells_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/cells.npy', 'state_load': '/rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_128_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_300_dict.pth', 'batch_size': '16', 'lr': '0.001', 'n_epoches': '100', 'seed': '15', 'visualize': 'False', 'reconstructed_path': 'None', 'reconstruct_start_index': '0', 'reconstruct_end_index': '100', 'save_path': '/rds/general/user/jy220/home/new_results/'}
[[ 2.50825269e-01 -1.67256990e-02 -1.22268730e-03]
 [ 2.38611959e-01 -5.50627777e-03 -5.06451913e-03]
 [ 2.39454874e-01 -1.59563257e-02  3.96778323e-03]
 ...
 [ 9.11361096e+00 -2.21908381e-02  2.22384902e-03]
 [ 9.11847310e+00 -2.84346931e-02 -2.34839590e-04]
 [ 9.11039661e+00 -2.51498573e-02 -8.70661044e-03]]
{'tetra': array([[      0,       1,       2,       3],
       [      4,       5,       6,       7],
       [      8,       9,      10,      11],
       ...,
       [1342744, 1342745, 1342746, 1342747],
       [1342748, 1342749, 1342750, 1342751],
       [1342752, 1342753, 1342754, 1342755]])}
vtu fields:  ['Component1::ComponentMassFractionPhase1', 'phase1::Velocity'] 
 structured  False 
 activation  Tanh() 
 self concat  2 
 sfc_nums  3 

dims_latent  128 
 components  4 
 nearest_neighbouring  True 

visualize  False 
 sample number  1706 

reading sfc nums......
[array([  41684,   41685,   41674, ..., 1235560, 1235564, 1235558]), array([954289, 954290, 954291, ..., 420685, 420687, 420688]), array([  40156,   40155,   40158, ..., 1123478, 1123475, 1123467])]
reading inverse sfc nums......
[array([   8146,    8145,    8144, ..., 1274444, 1274445, 1273166]), array([547280, 547283, 593037, ..., 933933, 933932, 864488]), array([ 66437,  66439, 122512, ..., 459416, 415130, 459418])]
Computing min and max......

tk:  tensor([0.7581, 0.0418, 0.1840, 0.1464], dtype=torch.float64) 

tb:  tensor([-0.3816, -0.3618, -0.0308,  0.1446], dtype=torch.float64) 

length of train set: 1505 

length of valid set: 100 

length of test set: 101 

torch device num: 4 

Let's use 4 GPUs!
epoch 300 starting......
Epoch:  300 | train loss: 2.321039e-03 | valid loss: 1.928936e-03 
      	| train loss (relative): 4.553246e-02 | valid loss (relative): 3.813889e-02 
Epoch 300 use: 560.24 second.

epoch 301 starting......
Epoch:  301 | train loss: 1.951891e-03 | valid loss: 1.901958e-03 
      	| train loss (relative): 3.864941e-02 | valid loss (relative): 3.750269e-02 
Epoch 301 use: 522.05 second.

epoch 302 starting......
Epoch:  302 | train loss: 1.933753e-03 | valid loss: 1.892724e-03 
      	| train loss (relative): 3.827884e-02 | valid loss (relative): 3.731508e-02 
Epoch 302 use: 461.09 second.

epoch 303 starting......
Epoch:  303 | train loss: 1.926208e-03 | valid loss: 1.889676e-03 
      	| train loss (relative): 3.812362e-02 | valid loss (relative): 3.724656e-02 
Epoch 303 use: 468.65 second.

epoch 304 starting......
Epoch:  304 | train loss: 1.922446e-03 | valid loss: 1.888698e-03 
      	| train loss (relative): 3.803668e-02 | valid loss (relative): 3.722878e-02 
Epoch 304 use: 674.18 second.

epoch 305 starting......
Epoch:  305 | train loss: 1.919854e-03 | valid loss: 1.887859e-03 
      	| train loss (relative): 3.798494e-02 | valid loss (relative): 3.722850e-02 
Epoch 305 use: 1123.89 second.

epoch 306 starting......
Epoch:  306 | train loss: 1.918273e-03 | valid loss: 1.886961e-03 
      	| train loss (relative): 3.795571e-02 | valid loss (relative): 3.720944e-02 
Epoch 306 use: 1174.12 second.

epoch 307 starting......
Epoch:  307 | train loss: 1.916614e-03 | valid loss: 1.886712e-03 
      	| train loss (relative): 3.791280e-02 | valid loss (relative): 3.721225e-02 
Epoch 307 use: 970.58 second.

epoch 308 starting......
Epoch:  308 | train loss: 1.917459e-03 | valid loss: 1.886272e-03 
      	| train loss (relative): 3.793250e-02 | valid loss (relative): 3.722135e-02 
Epoch 308 use: 581.02 second.

epoch 309 starting......
Epoch:  309 | train loss: 1.914927e-03 | valid loss: 1.885658e-03 
      	| train loss (relative): 3.788427e-02 | valid loss (relative): 3.723765e-02 
Epoch 309 use: 532.34 second.

epoch 310 starting......
Epoch:  310 | train loss: 1.914904e-03 | valid loss: 1.888859e-03 
      	| train loss (relative): 3.788450e-02 | valid loss (relative): 3.721357e-02 
Epoch 310 use: 439.09 second.

epoch 311 starting......
Epoch:  311 | train loss: 1.914469e-03 | valid loss: 1.889251e-03 
      	| train loss (relative): 3.786359e-02 | valid loss (relative): 3.724714e-02 
Epoch 311 use: 468.76 second.

epoch 312 starting......
Epoch:  312 | train loss: 1.916763e-03 | valid loss: 1.891425e-03 
      	| train loss (relative): 3.791597e-02 | valid loss (relative): 3.736394e-02 
Epoch 312 use: 470.69 second.

epoch 313 starting......
Epoch:  313 | train loss: 1.916383e-03 | valid loss: 1.894333e-03 
      	| train loss (relative): 3.791082e-02 | valid loss (relative): 3.737129e-02 
Epoch 313 use: 480.19 second.

epoch 314 starting......
Epoch:  314 | train loss: 1.916943e-03 | valid loss: 1.891810e-03 
      	| train loss (relative): 3.791992e-02 | valid loss (relative): 3.748154e-02 
Epoch 314 use: 433.10 second.

epoch 315 starting......
Epoch:  315 | train loss: 1.916482e-03 | valid loss: 1.896373e-03 
      	| train loss (relative): 3.791180e-02 | valid loss (relative): 3.729049e-02 
Epoch 315 use: 460.37 second.

epoch 316 starting......
Epoch:  316 | train loss: 1.916194e-03 | valid loss: 1.894237e-03 
      	| train loss (relative): 3.789564e-02 | valid loss (relative): 3.745288e-02 
Epoch 316 use: 432.98 second.

epoch 317 starting......
Epoch:  317 | train loss: 1.915337e-03 | valid loss: 1.899334e-03 
      	| train loss (relative): 3.789398e-02 | valid loss (relative): 3.745436e-02 
Epoch 317 use: 464.56 second.

epoch 318 starting......
Epoch:  318 | train loss: 1.917008e-03 | valid loss: 1.893100e-03 
      	| train loss (relative): 3.792408e-02 | valid loss (relative): 3.739779e-02 
Epoch 318 use: 428.24 second.

epoch 319 starting......
Epoch:  319 | train loss: 1.915251e-03 | valid loss: 1.896399e-03 
      	| train loss (relative): 3.788269e-02 | valid loss (relative): 3.740434e-02 
Epoch 319 use: 438.03 second.

epoch 320 starting......
Epoch:  320 | train loss: 1.914200e-03 | valid loss: 1.896968e-03 
      	| train loss (relative): 3.785706e-02 | valid loss (relative): 3.739799e-02 
Epoch 320 use: 395.35 second.

epoch 321 starting......
Epoch:  321 | train loss: 1.911566e-03 | valid loss: 1.887778e-03 
      	| train loss (relative): 3.780173e-02 | valid loss (relative): 3.713163e-02 
Epoch 321 use: 411.63 second.

epoch 322 starting......
Epoch:  322 | train loss: 1.908206e-03 | valid loss: 1.893670e-03 
      	| train loss (relative): 3.772981e-02 | valid loss (relative): 3.744008e-02 
Epoch 322 use: 411.15 second.

epoch 323 starting......
Epoch:  323 | train loss: 1.913888e-03 | valid loss: 1.889043e-03 
      	| train loss (relative): 3.784831e-02 | valid loss (relative): 3.733359e-02 
Epoch 323 use: 432.35 second.

epoch 324 starting......
Epoch:  324 | train loss: 1.908491e-03 | valid loss: 1.888184e-03 
      	| train loss (relative): 3.774417e-02 | valid loss (relative): 3.711038e-02 
Epoch 324 use: 465.56 second.

epoch 325 starting......
Epoch:  325 | train loss: 1.906642e-03 | valid loss: 1.893167e-03 
      	| train loss (relative): 3.770434e-02 | valid loss (relative): 3.724384e-02 
Epoch 325 use: 437.88 second.

epoch 326 starting......
Epoch:  326 | train loss: 1.913975e-03 | valid loss: 1.883598e-03 
      	| train loss (relative): 3.785012e-02 | valid loss (relative): 3.700256e-02 
Epoch 326 use: 501.35 second.

epoch 327 starting......
Epoch:  327 | train loss: 1.899788e-03 | valid loss: 1.877907e-03 
      	| train loss (relative): 3.756444e-02 | valid loss (relative): 3.699219e-02 
Epoch 327 use: 562.59 second.

epoch 328 starting......
Epoch:  328 | train loss: 1.903580e-03 | valid loss: 1.890863e-03 
      	| train loss (relative): 3.763293e-02 | valid loss (relative): 3.709666e-02 
Epoch 328 use: 557.52 second.

epoch 329 starting......
Epoch:  329 | train loss: 1.900833e-03 | valid loss: 1.891534e-03 
      	| train loss (relative): 3.758166e-02 | valid loss (relative): 3.745857e-02 
Epoch 329 use: 684.06 second.

epoch 330 starting......
Epoch:  330 | train loss: 1.897045e-03 | valid loss: 1.878943e-03 
      	| train loss (relative): 3.750806e-02 | valid loss (relative): 3.697249e-02 
Epoch 330 use: 460.84 second.

epoch 331 starting......
Epoch:  331 | train loss: 1.899018e-03 | valid loss: 1.879327e-03 
      	| train loss (relative): 3.754432e-02 | valid loss (relative): 3.695375e-02 
Epoch 331 use: 416.35 second.

epoch 332 starting......
Epoch:  332 | train loss: 1.893859e-03 | valid loss: 1.883132e-03 
      	| train loss (relative): 3.743706e-02 | valid loss (relative): 3.725081e-02 
Epoch 332 use: 466.53 second.

epoch 333 starting......
Epoch:  333 | train loss: 1.898797e-03 | valid loss: 1.875733e-03 
      	| train loss (relative): 3.754105e-02 | valid loss (relative): 3.678447e-02 
Epoch 333 use: 442.17 second.

epoch 334 starting......
Epoch:  334 | train loss: 1.895487e-03 | valid loss: 1.907230e-03 
      	| train loss (relative): 3.747546e-02 | valid loss (relative): 3.790686e-02 
Epoch 334 use: 407.05 second.

epoch 335 starting......
Epoch:  335 | train loss: 1.909661e-03 | valid loss: 1.880779e-03 
      	| train loss (relative): 3.773965e-02 | valid loss (relative): 3.724148e-02 
Epoch 335 use: 403.72 second.

epoch 336 starting......
Epoch:  336 | train loss: 1.897390e-03 | valid loss: 1.882279e-03 
      	| train loss (relative): 3.750369e-02 | valid loss (relative): 3.693870e-02 
Epoch 336 use: 403.20 second.

epoch 337 starting......
Epoch:  337 | train loss: 1.892478e-03 | valid loss: 1.875899e-03 
      	| train loss (relative): 3.740599e-02 | valid loss (relative): 3.713172e-02 
Epoch 337 use: 421.56 second.

epoch 338 starting......
Epoch:  338 | train loss: 1.890044e-03 | valid loss: 1.878370e-03 
      	| train loss (relative): 3.736490e-02 | valid loss (relative): 3.695732e-02 
Epoch 338 use: 445.42 second.

epoch 339 starting......
Epoch:  339 | train loss: 1.890262e-03 | valid loss: 1.881598e-03 
      	| train loss (relative): 3.736509e-02 | valid loss (relative): 3.725181e-02 
Epoch 339 use: 406.54 second.

epoch 340 starting......
Epoch:  340 | train loss: 1.889257e-03 | valid loss: 1.876435e-03 
      	| train loss (relative): 3.734320e-02 | valid loss (relative): 3.694434e-02 
Epoch 340 use: 392.48 second.

epoch 341 starting......
Epoch:  341 | train loss: 1.888478e-03 | valid loss: 1.876769e-03 
      	| train loss (relative): 3.732936e-02 | valid loss (relative): 3.699750e-02 
Epoch 341 use: 398.34 second.

epoch 342 starting......
Epoch:  342 | train loss: 1.886152e-03 | valid loss: 1.874846e-03 
      	| train loss (relative): 3.727637e-02 | valid loss (relative): 3.700717e-02 
Epoch 342 use: 414.12 second.

epoch 343 starting......
Epoch:  343 | train loss: 1.884380e-03 | valid loss: 1.873956e-03 
      	| train loss (relative): 3.723357e-02 | valid loss (relative): 3.715879e-02 
Epoch 343 use: 406.47 second.

epoch 344 starting......
Epoch:  344 | train loss: 1.882971e-03 | valid loss: 1.870515e-03 
      	| train loss (relative): 3.721200e-02 | valid loss (relative): 3.684955e-02 
Epoch 344 use: 405.47 second.

epoch 345 starting......
Epoch:  345 | train loss: 1.886978e-03 | valid loss: 1.875669e-03 
      	| train loss (relative): 3.729512e-02 | valid loss (relative): 3.683200e-02 
Epoch 345 use: 399.19 second.

epoch 346 starting......
Epoch:  346 | train loss: 1.887751e-03 | valid loss: 1.878193e-03 
      	| train loss (relative): 3.730124e-02 | valid loss (relative): 3.698818e-02 
Epoch 346 use: 416.14 second.

epoch 347 starting......
Epoch:  347 | train loss: 1.881884e-03 | valid loss: 1.873014e-03 
      	| train loss (relative): 3.719411e-02 | valid loss (relative): 3.699787e-02 
Epoch 347 use: 410.89 second.

epoch 348 starting......
Epoch:  348 | train loss: 1.884457e-03 | valid loss: 1.883916e-03 
      	| train loss (relative): 3.723945e-02 | valid loss (relative): 3.673529e-02 
Epoch 348 use: 478.78 second.

epoch 349 starting......
Epoch:  349 | train loss: 1.878029e-03 | valid loss: 1.858797e-03 
      	| train loss (relative): 3.710547e-02 | valid loss (relative): 3.656700e-02 
Epoch 349 use: 409.46 second.

epoch 350 starting......
Epoch:  350 | train loss: 1.874307e-03 | valid loss: 1.867105e-03 
      	| train loss (relative): 3.703843e-02 | valid loss (relative): 3.669545e-02 
Epoch 350 use: 405.86 second.

epoch 351 starting......
Epoch:  351 | train loss: 1.877736e-03 | valid loss: 1.858766e-03 
      	| train loss (relative): 3.709805e-02 | valid loss (relative): 3.656084e-02 
Epoch 351 use: 410.97 second.

epoch 352 starting......
Epoch:  352 | train loss: 1.873656e-03 | valid loss: 1.866622e-03 
      	| train loss (relative): 3.702052e-02 | valid loss (relative): 3.686958e-02 
Epoch 352 use: 422.41 second.

epoch 353 starting......
Epoch:  353 | train loss: 1.874745e-03 | valid loss: 1.865606e-03 
      	| train loss (relative): 3.704349e-02 | valid loss (relative): 3.698692e-02 
Epoch 353 use: 397.17 second.

epoch 354 starting......
Epoch:  354 | train loss: 1.876389e-03 | valid loss: 1.860019e-03 
      	| train loss (relative): 3.707199e-02 | valid loss (relative): 3.666764e-02 
Epoch 354 use: 393.12 second.

epoch 355 starting......
Epoch:  355 | train loss: 1.876081e-03 | valid loss: 1.865578e-03 
      	| train loss (relative): 3.706699e-02 | valid loss (relative): 3.638626e-02 
Epoch 355 use: 407.32 second.

epoch 356 starting......
Epoch:  356 | train loss: 1.877162e-03 | valid loss: 1.864550e-03 
      	| train loss (relative): 3.708932e-02 | valid loss (relative): 3.690784e-02 
Epoch 356 use: 410.17 second.

epoch 357 starting......
Epoch:  357 | train loss: 1.872901e-03 | valid loss: 1.856375e-03 
      	| train loss (relative): 3.699742e-02 | valid loss (relative): 3.665425e-02 
Epoch 357 use: 380.21 second.

epoch 358 starting......
Epoch:  358 | train loss: 1.870635e-03 | valid loss: 1.868300e-03 
      	| train loss (relative): 3.696015e-02 | valid loss (relative): 3.673176e-02 
Epoch 358 use: 391.50 second.

epoch 359 starting......
Epoch:  359 | train loss: 1.871052e-03 | valid loss: 1.855874e-03 
      	| train loss (relative): 3.697045e-02 | valid loss (relative): 3.644510e-02 
Epoch 359 use: 398.52 second.

epoch 360 starting......
Epoch:  360 | train loss: 1.867182e-03 | valid loss: 1.863362e-03 
      	| train loss (relative): 3.688140e-02 | valid loss (relative): 3.651711e-02 
Epoch 360 use: 381.92 second.

epoch 361 starting......
Epoch:  361 | train loss: 1.873726e-03 | valid loss: 1.859740e-03 
      	| train loss (relative): 3.701013e-02 | valid loss (relative): 3.657440e-02 
Epoch 361 use: 379.51 second.

epoch 362 starting......
Epoch:  362 | train loss: 1.866030e-03 | valid loss: 1.859767e-03 
      	| train loss (relative): 3.685946e-02 | valid loss (relative): 3.637796e-02 
Epoch 362 use: 403.77 second.

epoch 363 starting......
Epoch:  363 | train loss: 1.863144e-03 | valid loss: 1.852439e-03 
      	| train loss (relative): 3.680436e-02 | valid loss (relative): 3.640805e-02 
Epoch 363 use: 389.80 second.

epoch 364 starting......
Epoch:  364 | train loss: 1.866245e-03 | valid loss: 1.852707e-03 
      	| train loss (relative): 3.685750e-02 | valid loss (relative): 3.664696e-02 
Epoch 364 use: 387.99 second.

epoch 365 starting......
Epoch:  365 | train loss: 1.856886e-03 | valid loss: 1.847277e-03 
      	| train loss (relative): 3.668189e-02 | valid loss (relative): 3.606898e-02 
Epoch 365 use: 402.72 second.

epoch 366 starting......
Epoch:  366 | train loss: 1.856138e-03 | valid loss: 1.849414e-03 
      	| train loss (relative): 3.665331e-02 | valid loss (relative): 3.673130e-02 
Epoch 366 use: 386.75 second.

epoch 367 starting......
Epoch:  367 | train loss: 1.861466e-03 | valid loss: 1.851758e-03 
      	| train loss (relative): 3.676965e-02 | valid loss (relative): 3.648483e-02 
Epoch 367 use: 389.97 second.

epoch 368 starting......
Epoch:  368 | train loss: 1.861309e-03 | valid loss: 1.855225e-03 
      	| train loss (relative): 3.675726e-02 | valid loss (relative): 3.643211e-02 
Epoch 368 use: 387.59 second.

epoch 369 starting......
Epoch:  369 | train loss: 1.859131e-03 | valid loss: 1.852316e-03 
      	| train loss (relative): 3.671598e-02 | valid loss (relative): 3.644539e-02 
Epoch 369 use: 387.62 second.

epoch 370 starting......
Epoch:  370 | train loss: 1.856658e-03 | valid loss: 1.845245e-03 
      	| train loss (relative): 3.666104e-02 | valid loss (relative): 3.611194e-02 
Epoch 370 use: 397.25 second.

epoch 371 starting......
Epoch:  371 | train loss: 1.854149e-03 | valid loss: 1.847195e-03 
      	| train loss (relative): 3.661959e-02 | valid loss (relative): 3.618243e-02 
Epoch 371 use: 386.17 second.

epoch 372 starting......
Epoch:  372 | train loss: 1.858016e-03 | valid loss: 1.859340e-03 
      	| train loss (relative): 3.669114e-02 | valid loss (relative): 3.668192e-02 
Epoch 372 use: 388.24 second.

epoch 373 starting......
Epoch:  373 | train loss: 1.851019e-03 | valid loss: 1.831363e-03 
      	| train loss (relative): 3.655737e-02 | valid loss (relative): 3.598995e-02 
Epoch 373 use: 404.01 second.

epoch 374 starting......
Epoch:  374 | train loss: 1.843117e-03 | valid loss: 1.849421e-03 
      	| train loss (relative): 3.639542e-02 | valid loss (relative): 3.662902e-02 
Epoch 374 use: 422.84 second.

epoch 375 starting......
Epoch:  375 | train loss: 1.846348e-03 | valid loss: 1.844447e-03 
      	| train loss (relative): 3.646682e-02 | valid loss (relative): 3.618680e-02 
Epoch 375 use: 383.82 second.

epoch 376 starting......
Epoch:  376 | train loss: 1.848595e-03 | valid loss: 1.847430e-03 
      	| train loss (relative): 3.650220e-02 | valid loss (relative): 3.635194e-02 
Epoch 376 use: 390.03 second.

epoch 377 starting......
Epoch:  377 | train loss: 1.845365e-03 | valid loss: 1.834968e-03 
      	| train loss (relative): 3.643862e-02 | valid loss (relative): 3.601959e-02 
Epoch 377 use: 393.83 second.

epoch 378 starting......
Epoch:  378 | train loss: 1.844787e-03 | valid loss: 1.838819e-03 
      	| train loss (relative): 3.642324e-02 | valid loss (relative): 3.631360e-02 
Epoch 378 use: 398.06 second.

epoch 379 starting......
Epoch:  379 | train loss: 1.844139e-03 | valid loss: 1.833671e-03 
      	| train loss (relative): 3.641004e-02 | valid loss (relative): 3.610074e-02 
Epoch 379 use: 403.05 second.

epoch 380 starting......
Epoch:  380 | train loss: 1.839094e-03 | valid loss: 1.841479e-03 
      	| train loss (relative): 3.631023e-02 | valid loss (relative): 3.619632e-02 
Epoch 380 use: 403.13 second.

epoch 381 starting......
Epoch:  381 | train loss: 1.843720e-03 | valid loss: 1.842247e-03 
      	| train loss (relative): 3.640170e-02 | valid loss (relative): 3.620113e-02 
Epoch 381 use: 406.41 second.

epoch 382 starting......
Epoch:  382 | train loss: 1.843318e-03 | valid loss: 1.837737e-03 
      	| train loss (relative): 3.639811e-02 | valid loss (relative): 3.614496e-02 
Epoch 382 use: 404.72 second.

epoch 383 starting......
Epoch:  383 | train loss: 1.843425e-03 | valid loss: 1.830669e-03 
      	| train loss (relative): 3.639648e-02 | valid loss (relative): 3.619975e-02 
Epoch 383 use: 397.91 second.

epoch 384 starting......
Epoch:  384 | train loss: 1.835955e-03 | valid loss: 1.828667e-03 
      	| train loss (relative): 3.624628e-02 | valid loss (relative): 3.627381e-02 
Epoch 384 use: 416.94 second.

epoch 385 starting......
Epoch:  385 | train loss: 1.834557e-03 | valid loss: 1.829003e-03 
      	| train loss (relative): 3.621801e-02 | valid loss (relative): 3.613099e-02 
Epoch 385 use: 419.30 second.

epoch 386 starting......
Epoch:  386 | train loss: 1.833319e-03 | valid loss: 1.828973e-03 
      	| train loss (relative): 3.619127e-02 | valid loss (relative): 3.609632e-02 
Epoch 386 use: 451.97 second.

epoch 387 starting......
Epoch:  387 | train loss: 1.839697e-03 | valid loss: 1.822481e-03 
      	| train loss (relative): 3.630129e-02 | valid loss (relative): 3.600570e-02 
Epoch 387 use: 408.89 second.

epoch 388 starting......
Epoch:  388 | train loss: 1.831334e-03 | valid loss: 1.819398e-03 
      	| train loss (relative): 3.615743e-02 | valid loss (relative): 3.580345e-02 
Epoch 388 use: 401.71 second.

epoch 389 starting......
Epoch:  389 | train loss: 1.825903e-03 | valid loss: 1.820978e-03 
      	| train loss (relative): 3.603727e-02 | valid loss (relative): 3.585070e-02 
Epoch 389 use: 426.26 second.

epoch 390 starting......
Epoch:  390 | train loss: 1.827681e-03 | valid loss: 1.818958e-03 
      	| train loss (relative): 3.607210e-02 | valid loss (relative): 3.582314e-02 
Epoch 390 use: 461.66 second.

epoch 391 starting......
Epoch:  391 | train loss: 1.822733e-03 | valid loss: 1.823655e-03 
      	| train loss (relative): 3.597743e-02 | valid loss (relative): 3.582543e-02 
Epoch 391 use: 422.17 second.

epoch 392 starting......
Epoch:  392 | train loss: 1.826129e-03 | valid loss: 1.822602e-03 
      	| train loss (relative): 3.604115e-02 | valid loss (relative): 3.588137e-02 
Epoch 392 use: 395.74 second.

epoch 393 starting......
Epoch:  393 | train loss: 1.835211e-03 | valid loss: 1.811991e-03 
      	| train loss (relative): 3.622686e-02 | valid loss (relative): 3.572420e-02 
Epoch 393 use: 397.46 second.

epoch 394 starting......
Epoch:  394 | train loss: 1.817197e-03 | valid loss: 1.811463e-03 
      	| train loss (relative): 3.586517e-02 | valid loss (relative): 3.555697e-02 
Epoch 394 use: 409.26 second.

epoch 395 starting......
Epoch:  395 | train loss: 1.818127e-03 | valid loss: 1.818447e-03 
      	| train loss (relative): 3.587934e-02 | valid loss (relative): 3.586943e-02 
Epoch 395 use: 467.79 second.

epoch 396 starting......
Epoch:  396 | train loss: 1.817707e-03 | valid loss: 1.809412e-03 
      	| train loss (relative): 3.587732e-02 | valid loss (relative): 3.550173e-02 
Epoch 396 use: 443.97 second.

epoch 397 starting......
Epoch:  397 | train loss: 1.815941e-03 | valid loss: 1.816651e-03 
      	| train loss (relative): 3.583335e-02 | valid loss (relative): 3.561572e-02 
Epoch 397 use: 452.81 second.

epoch 398 starting......
Epoch:  398 | train loss: 1.819542e-03 | valid loss: 1.818677e-03 
      	| train loss (relative): 3.590262e-02 | valid loss (relative): 3.580951e-02 
Epoch 398 use: 439.60 second.

epoch 399 starting......
Epoch:  399 | train loss: 1.819597e-03 | valid loss: 1.810375e-03 
      	| train loss (relative): 3.590798e-02 | valid loss (relative): 3.570792e-02 
Epoch 399 use: 506.31 second.

test MSE Error: 1.957766e-03 | relative MSE Error: 3.882128e-02 
 Total time used for training: 12.58 hour.
MESLoss saved to  /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_MSELoss_Latent_128_nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_400.txt
relative MSELoss saved to  /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_reMSELoss_Latent_128_nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_400.txt
model saved to /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_128_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_400.pth
model_dict saved to /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_128_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_400_dict.pth
... Training slugflow data completed, Run finished Thu 12 Aug 04:58:35 BST 2021 ...
{'vtu_dir': '/rds/general/user/jy220/home/slugflow/', 'data_dir': '/rds/general/user/jy220/home/tensors/', 'data_type': 'tensors', 'vtu_fields': 'Component1::ComponentMassFractionPhase1, phase1::Velocity', 'variational': 'False', 'optimizer': 'Adamax', 'structured': 'False', 'changing_lr': 'False', 'self_concat': '2', 'dimension': '3', 'sfc_nums': '3', 'components': '4', 'nearest_neighbouring': 'True', 'dims_latent': '128', 'activation': 'Tanh', 'sfc_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/sfcs.pt', 'inv_sfc_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/inv_sfcs.pt', 'coords_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/coords.pt', 'cells_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/cells.npy', 'state_load': '/rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_128_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_400_dict.pth', 'batch_size': '16', 'lr': '0.001', 'n_epoches': '100', 'seed': '15', 'visualize': 'False', 'reconstructed_path': 'None', 'reconstruct_start_index': '0', 'reconstruct_end_index': '100', 'save_path': '/rds/general/user/jy220/home/new_results/'}
[[ 2.50825269e-01 -1.67256990e-02 -1.22268730e-03]
 [ 2.38611959e-01 -5.50627777e-03 -5.06451913e-03]
 [ 2.39454874e-01 -1.59563257e-02  3.96778323e-03]
 ...
 [ 9.11361096e+00 -2.21908381e-02  2.22384902e-03]
 [ 9.11847310e+00 -2.84346931e-02 -2.34839590e-04]
 [ 9.11039661e+00 -2.51498573e-02 -8.70661044e-03]]
{'tetra': array([[      0,       1,       2,       3],
       [      4,       5,       6,       7],
       [      8,       9,      10,      11],
       ...,
       [1342744, 1342745, 1342746, 1342747],
       [1342748, 1342749, 1342750, 1342751],
       [1342752, 1342753, 1342754, 1342755]])}
vtu fields:  ['Component1::ComponentMassFractionPhase1', 'phase1::Velocity'] 
 structured  False 
 activation  Tanh() 
 self concat  2 
 sfc_nums  3 

dims_latent  128 
 components  4 
 nearest_neighbouring  True 

visualize  False 
 sample number  1706 

reading sfc nums......
[array([  41684,   41685,   41674, ..., 1235560, 1235564, 1235558]), array([954289, 954290, 954291, ..., 420685, 420687, 420688]), array([  40156,   40155,   40158, ..., 1123478, 1123475, 1123467])]
reading inverse sfc nums......
[array([   8146,    8145,    8144, ..., 1274444, 1274445, 1273166]), array([547280, 547283, 593037, ..., 933933, 933932, 864488]), array([ 66437,  66439, 122512, ..., 459416, 415130, 459418])]
Computing min and max......

tk:  tensor([0.7581, 0.0418, 0.1840, 0.1464], dtype=torch.float64) 

tb:  tensor([-0.3816, -0.3618, -0.0308,  0.1446], dtype=torch.float64) 

length of train set: 1505 

length of valid set: 100 

length of test set: 101 

torch device num: 4 

Let's use 4 GPUs!
epoch 400 starting......
Epoch:  400 | train loss: 2.084446e-03 | valid loss: 1.862272e-03 
      	| train loss (relative): 4.103205e-02 | valid loss (relative): 3.688563e-02 
Epoch 400 use: 400.17 second.

epoch 401 starting......
Epoch:  401 | train loss: 1.826199e-03 | valid loss: 1.839298e-03 
      	| train loss (relative): 3.605715e-02 | valid loss (relative): 3.633147e-02 
Epoch 401 use: 382.48 second.

epoch 402 starting......
Epoch:  402 | train loss: 1.812666e-03 | valid loss: 1.835269e-03 
      	| train loss (relative): 3.578037e-02 | valid loss (relative): 3.628654e-02 
Epoch 402 use: 370.47 second.

epoch 403 starting......
Epoch:  403 | train loss: 1.808862e-03 | valid loss: 1.832898e-03 
      	| train loss (relative): 3.569859e-02 | valid loss (relative): 3.619394e-02 
Epoch 403 use: 395.61 second.

epoch 404 starting......
Epoch:  404 | train loss: 1.804786e-03 | valid loss: 1.832232e-03 
      	| train loss (relative): 3.561262e-02 | valid loss (relative): 3.624649e-02 
Epoch 404 use: 393.42 second.

epoch 405 starting......
Epoch:  405 | train loss: 1.802786e-03 | valid loss: 1.831755e-03 
      	| train loss (relative): 3.557580e-02 | valid loss (relative): 3.617246e-02 
Epoch 405 use: 385.05 second.

epoch 406 starting......
Epoch:  406 | train loss: 1.800860e-03 | valid loss: 1.830552e-03 
      	| train loss (relative): 3.553556e-02 | valid loss (relative): 3.614762e-02 
Epoch 406 use: 383.47 second.

epoch 407 starting......
Epoch:  407 | train loss: 1.800436e-03 | valid loss: 1.831608e-03 
      	| train loss (relative): 3.552215e-02 | valid loss (relative): 3.616590e-02 
Epoch 407 use: 389.04 second.

epoch 408 starting......
Epoch:  408 | train loss: 1.799922e-03 | valid loss: 1.834811e-03 
      	| train loss (relative): 3.551512e-02 | valid loss (relative): 3.626401e-02 
Epoch 408 use: 383.96 second.

epoch 409 starting......
Epoch:  409 | train loss: 1.801572e-03 | valid loss: 1.833246e-03 
      	| train loss (relative): 3.554561e-02 | valid loss (relative): 3.620737e-02 
Epoch 409 use: 386.19 second.

epoch 410 starting......
Epoch:  410 | train loss: 1.802276e-03 | valid loss: 1.839284e-03 
      	| train loss (relative): 3.556170e-02 | valid loss (relative): 3.639768e-02 
Epoch 410 use: 443.22 second.

epoch 411 starting......
Epoch:  411 | train loss: 1.805653e-03 | valid loss: 1.843831e-03 
      	| train loss (relative): 3.563101e-02 | valid loss (relative): 3.655709e-02 
Epoch 411 use: 372.87 second.

epoch 412 starting......
Epoch:  412 | train loss: 1.806526e-03 | valid loss: 1.838964e-03 
      	| train loss (relative): 3.564870e-02 | valid loss (relative): 3.639598e-02 
Epoch 412 use: 382.73 second.

epoch 413 starting......
Epoch:  413 | train loss: 1.802388e-03 | valid loss: 1.834407e-03 
      	| train loss (relative): 3.556042e-02 | valid loss (relative): 3.624259e-02 
Epoch 413 use: 364.88 second.

epoch 414 starting......
Epoch:  414 | train loss: 1.800839e-03 | valid loss: 1.833447e-03 
      	| train loss (relative): 3.552763e-02 | valid loss (relative): 3.612109e-02 
Epoch 414 use: 394.16 second.

epoch 415 starting......
Epoch:  415 | train loss: 1.801020e-03 | valid loss: 1.839425e-03 
      	| train loss (relative): 3.552791e-02 | valid loss (relative): 3.632029e-02 
Epoch 415 use: 373.96 second.

epoch 416 starting......
Epoch:  416 | train loss: 1.804398e-03 | valid loss: 1.840788e-03 
      	| train loss (relative): 3.560310e-02 | valid loss (relative): 3.631269e-02 
Epoch 416 use: 393.82 second.

epoch 417 starting......
Epoch:  417 | train loss: 1.802051e-03 | valid loss: 1.835046e-03 
      	| train loss (relative): 3.554947e-02 | valid loss (relative): 3.618435e-02 
Epoch 417 use: 362.38 second.

epoch 418 starting......
Epoch:  418 | train loss: 1.799998e-03 | valid loss: 1.838958e-03 
      	| train loss (relative): 3.551020e-02 | valid loss (relative): 3.634770e-02 
Epoch 418 use: 357.51 second.

epoch 419 starting......
Epoch:  419 | train loss: 1.804460e-03 | valid loss: 1.848871e-03 
      	| train loss (relative): 3.559185e-02 | valid loss (relative): 3.655646e-02 
Epoch 419 use: 360.19 second.

epoch 420 starting......
Epoch:  420 | train loss: 1.803765e-03 | valid loss: 1.843433e-03 
      	| train loss (relative): 3.558119e-02 | valid loss (relative): 3.643584e-02 
Epoch 420 use: 388.43 second.

epoch 421 starting......
Epoch:  421 | train loss: 1.802894e-03 | valid loss: 1.839487e-03 
      	| train loss (relative): 3.556538e-02 | valid loss (relative): 3.645216e-02 
Epoch 421 use: 367.83 second.

epoch 422 starting......
Epoch:  422 | train loss: 1.799185e-03 | valid loss: 1.863294e-03 
      	| train loss (relative): 3.548537e-02 | valid loss (relative): 3.698985e-02 
Epoch 422 use: 361.55 second.

epoch 423 starting......
Epoch:  423 | train loss: 1.804534e-03 | valid loss: 1.839807e-03 
      	| train loss (relative): 3.559895e-02 | valid loss (relative): 3.632087e-02 
Epoch 423 use: 340.45 second.

epoch 424 starting......
Epoch:  424 | train loss: 1.793764e-03 | valid loss: 1.832994e-03 
      	| train loss (relative): 3.537900e-02 | valid loss (relative): 3.621582e-02 
Epoch 424 use: 357.88 second.

epoch 425 starting......
Epoch:  425 | train loss: 1.790135e-03 | valid loss: 1.828424e-03 
      	| train loss (relative): 3.530920e-02 | valid loss (relative): 3.611701e-02 
Epoch 425 use: 361.81 second.

epoch 426 starting......
Epoch:  426 | train loss: 1.790096e-03 | valid loss: 1.836406e-03 
      	| train loss (relative): 3.530464e-02 | valid loss (relative): 3.604766e-02 
Epoch 426 use: 344.36 second.

epoch 427 starting......
Epoch:  427 | train loss: 1.796305e-03 | valid loss: 1.875193e-03 
      	| train loss (relative): 3.542865e-02 | valid loss (relative): 3.735043e-02 
Epoch 427 use: 358.84 second.

epoch 428 starting......
Epoch:  428 | train loss: 1.802627e-03 | valid loss: 1.837499e-03 
      	| train loss (relative): 3.555445e-02 | valid loss (relative): 3.621524e-02 
Epoch 428 use: 354.79 second.

epoch 429 starting......
Epoch:  429 | train loss: 1.789806e-03 | valid loss: 1.835121e-03 
      	| train loss (relative): 3.529634e-02 | valid loss (relative): 3.611393e-02 
Epoch 429 use: 356.92 second.

epoch 430 starting......
Epoch:  430 | train loss: 1.790512e-03 | valid loss: 1.830580e-03 
      	| train loss (relative): 3.531212e-02 | valid loss (relative): 3.621542e-02 
Epoch 430 use: 349.39 second.

epoch 431 starting......
Epoch:  431 | train loss: 1.785661e-03 | valid loss: 1.830191e-03 
      	| train loss (relative): 3.521110e-02 | valid loss (relative): 3.606947e-02 
Epoch 431 use: 351.91 second.

epoch 432 starting......
Epoch:  432 | train loss: 1.785656e-03 | valid loss: 1.833704e-03 
      	| train loss (relative): 3.520989e-02 | valid loss (relative): 3.601978e-02 
Epoch 432 use: 360.69 second.

epoch 433 starting......
Epoch:  433 | train loss: 1.785679e-03 | valid loss: 1.825178e-03 
      	| train loss (relative): 3.521077e-02 | valid loss (relative): 3.624536e-02 
Epoch 433 use: 347.93 second.

epoch 434 starting......
Epoch:  434 | train loss: 1.785850e-03 | valid loss: 1.830281e-03 
      	| train loss (relative): 3.521556e-02 | valid loss (relative): 3.622827e-02 
Epoch 434 use: 354.61 second.

epoch 435 starting......
Epoch:  435 | train loss: 1.786699e-03 | valid loss: 1.827530e-03 
      	| train loss (relative): 3.523520e-02 | valid loss (relative): 3.626062e-02 
Epoch 435 use: 350.94 second.

epoch 436 starting......
Epoch:  436 | train loss: 1.786732e-03 | valid loss: 1.837917e-03 
      	| train loss (relative): 3.522645e-02 | valid loss (relative): 3.591522e-02 
Epoch 436 use: 343.37 second.

epoch 437 starting......
Epoch:  437 | train loss: 1.786466e-03 | valid loss: 1.825988e-03 
      	| train loss (relative): 3.521981e-02 | valid loss (relative): 3.590776e-02 
Epoch 437 use: 364.15 second.

epoch 438 starting......
Epoch:  438 | train loss: 1.781205e-03 | valid loss: 1.823512e-03 
      	| train loss (relative): 3.512279e-02 | valid loss (relative): 3.582131e-02 
Epoch 438 use: 386.79 second.

epoch 439 starting......
Epoch:  439 | train loss: 1.781125e-03 | valid loss: 1.827581e-03 
      	| train loss (relative): 3.511468e-02 | valid loss (relative): 3.584470e-02 
Epoch 439 use: 355.92 second.

epoch 440 starting......
Epoch:  440 | train loss: 1.778949e-03 | valid loss: 1.825857e-03 
      	| train loss (relative): 3.507394e-02 | valid loss (relative): 3.607564e-02 
Epoch 440 use: 420.29 second.

epoch 441 starting......
Epoch:  441 | train loss: 1.784220e-03 | valid loss: 1.830207e-03 
      	| train loss (relative): 3.518336e-02 | valid loss (relative): 3.631246e-02 
Epoch 441 use: 454.42 second.

epoch 442 starting......
Epoch:  442 | train loss: 1.784000e-03 | valid loss: 1.825830e-03 
      	| train loss (relative): 3.517180e-02 | valid loss (relative): 3.626779e-02 
Epoch 442 use: 362.63 second.

epoch 443 starting......
Epoch:  443 | train loss: 1.778910e-03 | valid loss: 1.835959e-03 
      	| train loss (relative): 3.507856e-02 | valid loss (relative): 3.652237e-02 
Epoch 443 use: 361.24 second.

epoch 444 starting......
Epoch:  444 | train loss: 1.779872e-03 | valid loss: 1.822851e-03 
      	| train loss (relative): 3.508887e-02 | valid loss (relative): 3.610995e-02 
Epoch 444 use: 389.78 second.

epoch 445 starting......
Epoch:  445 | train loss: 1.782855e-03 | valid loss: 1.821792e-03 
      	| train loss (relative): 3.514914e-02 | valid loss (relative): 3.569516e-02 
Epoch 445 use: 377.26 second.

epoch 446 starting......
Epoch:  446 | train loss: 1.776673e-03 | valid loss: 1.831957e-03 
      	| train loss (relative): 3.502440e-02 | valid loss (relative): 3.640859e-02 
Epoch 446 use: 405.02 second.

epoch 447 starting......
Epoch:  447 | train loss: 1.782229e-03 | valid loss: 1.829972e-03 
      	| train loss (relative): 3.513718e-02 | valid loss (relative): 3.617482e-02 
Epoch 447 use: 389.34 second.

epoch 448 starting......
Epoch:  448 | train loss: 1.772407e-03 | valid loss: 1.813647e-03 
      	| train loss (relative): 3.493274e-02 | valid loss (relative): 3.582450e-02 
Epoch 448 use: 389.38 second.

epoch 449 starting......
Epoch:  449 | train loss: 1.775067e-03 | valid loss: 1.833638e-03 
      	| train loss (relative): 3.499130e-02 | valid loss (relative): 3.595048e-02 
Epoch 449 use: 533.80 second.

epoch 450 starting......
Epoch:  450 | train loss: 1.774061e-03 | valid loss: 1.814364e-03 
      	| train loss (relative): 3.496689e-02 | valid loss (relative): 3.579809e-02 
Epoch 450 use: 696.20 second.

epoch 451 starting......
Epoch:  451 | train loss: 1.766311e-03 | valid loss: 1.813174e-03 
      	| train loss (relative): 3.481009e-02 | valid loss (relative): 3.573494e-02 
Epoch 451 use: 390.84 second.

epoch 452 starting......
Epoch:  452 | train loss: 1.766244e-03 | valid loss: 1.811702e-03 
      	| train loss (relative): 3.481318e-02 | valid loss (relative): 3.580162e-02 
Epoch 452 use: 452.51 second.

epoch 453 starting......
Epoch:  453 | train loss: 1.766365e-03 | valid loss: 1.821405e-03 
      	| train loss (relative): 3.481922e-02 | valid loss (relative): 3.581268e-02 
Epoch 453 use: 395.75 second.

epoch 454 starting......
Epoch:  454 | train loss: 1.770041e-03 | valid loss: 1.808891e-03 
      	| train loss (relative): 3.489063e-02 | valid loss (relative): 3.557498e-02 
Epoch 454 use: 649.09 second.

epoch 455 starting......
Epoch:  455 | train loss: 1.761765e-03 | valid loss: 1.804408e-03 
      	| train loss (relative): 3.472933e-02 | valid loss (relative): 3.550559e-02 
Epoch 455 use: 881.07 second.

epoch 456 starting......
Epoch:  456 | train loss: 1.759861e-03 | valid loss: 1.810757e-03 
      	| train loss (relative): 3.468553e-02 | valid loss (relative): 3.580484e-02 
Epoch 456 use: 906.49 second.

epoch 457 starting......
Epoch:  457 | train loss: 1.762480e-03 | valid loss: 1.813188e-03 
      	| train loss (relative): 3.473476e-02 | valid loss (relative): 3.605620e-02 
Epoch 457 use: 957.08 second.

epoch 458 starting......
Epoch:  458 | train loss: 1.766934e-03 | valid loss: 1.803386e-03 
      	| train loss (relative): 3.482776e-02 | valid loss (relative): 3.553614e-02 
Epoch 458 use: 927.41 second.

epoch 459 starting......
Epoch:  459 | train loss: 1.759144e-03 | valid loss: 1.806479e-03 
      	| train loss (relative): 3.466675e-02 | valid loss (relative): 3.579045e-02 
Epoch 459 use: 934.35 second.

epoch 460 starting......
Epoch:  460 | train loss: 1.760227e-03 | valid loss: 1.814130e-03 
      	| train loss (relative): 3.468959e-02 | valid loss (relative): 3.597344e-02 
Epoch 460 use: 842.92 second.

epoch 461 starting......
Epoch:  461 | train loss: 1.761716e-03 | valid loss: 1.838207e-03 
      	| train loss (relative): 3.471971e-02 | valid loss (relative): 3.674258e-02 
Epoch 461 use: 975.54 second.

epoch 462 starting......
Epoch:  462 | train loss: 1.768050e-03 | valid loss: 1.800736e-03 
      	| train loss (relative): 3.484907e-02 | valid loss (relative): 3.551133e-02 
Epoch 462 use: 546.91 second.

epoch 463 starting......
Epoch:  463 | train loss: 1.752227e-03 | valid loss: 1.798593e-03 
      	| train loss (relative): 3.452799e-02 | valid loss (relative): 3.555211e-02 
Epoch 463 use: 466.50 second.

epoch 464 starting......
Epoch:  464 | train loss: 1.748731e-03 | valid loss: 1.798852e-03 
      	| train loss (relative): 3.445849e-02 | valid loss (relative): 3.570366e-02 
Epoch 464 use: 326.63 second.

epoch 465 starting......
Epoch:  465 | train loss: 1.752598e-03 | valid loss: 1.809396e-03 
      	| train loss (relative): 3.453397e-02 | valid loss (relative): 3.599516e-02 
Epoch 465 use: 340.77 second.

epoch 466 starting......
Epoch:  466 | train loss: 1.758130e-03 | valid loss: 1.816029e-03 
      	| train loss (relative): 3.465007e-02 | valid loss (relative): 3.561457e-02 
Epoch 466 use: 358.98 second.

epoch 467 starting......
Epoch:  467 | train loss: 1.756178e-03 | valid loss: 1.809146e-03 
      	| train loss (relative): 3.460359e-02 | valid loss (relative): 3.594116e-02 
Epoch 467 use: 356.38 second.

epoch 468 starting......
Epoch:  468 | train loss: 1.755537e-03 | valid loss: 1.802164e-03 
      	| train loss (relative): 3.459654e-02 | valid loss (relative): 3.545777e-02 
Epoch 468 use: 373.92 second.

epoch 469 starting......
Epoch:  469 | train loss: 1.750939e-03 | valid loss: 1.802041e-03 
      	| train loss (relative): 3.450019e-02 | valid loss (relative): 3.566772e-02 
Epoch 469 use: 340.78 second.

epoch 470 starting......
Epoch:  470 | train loss: 1.752970e-03 | valid loss: 1.818171e-03 
      	| train loss (relative): 3.453315e-02 | valid loss (relative): 3.601591e-02 
Epoch 470 use: 329.04 second.

epoch 471 starting......
Epoch:  471 | train loss: 1.752509e-03 | valid loss: 1.807558e-03 
      	| train loss (relative): 3.453328e-02 | valid loss (relative): 3.569808e-02 
Epoch 471 use: 334.42 second.

epoch 472 starting......
Epoch:  472 | train loss: 1.752281e-03 | valid loss: 1.796994e-03 
      	| train loss (relative): 3.452820e-02 | valid loss (relative): 3.540153e-02 
Epoch 472 use: 333.65 second.

epoch 473 starting......
Epoch:  473 | train loss: 1.747574e-03 | valid loss: 1.794618e-03 
      	| train loss (relative): 3.443252e-02 | valid loss (relative): 3.552458e-02 
Epoch 473 use: 334.34 second.

epoch 474 starting......
Epoch:  474 | train loss: 1.747860e-03 | valid loss: 1.805504e-03 
      	| train loss (relative): 3.443531e-02 | valid loss (relative): 3.554969e-02 
Epoch 474 use: 315.42 second.

epoch 475 starting......
Epoch:  475 | train loss: 1.751849e-03 | valid loss: 1.812119e-03 
      	| train loss (relative): 3.451675e-02 | valid loss (relative): 3.540801e-02 
Epoch 475 use: 356.41 second.

epoch 476 starting......
Epoch:  476 | train loss: 1.751389e-03 | valid loss: 1.816818e-03 
      	| train loss (relative): 3.449911e-02 | valid loss (relative): 3.549663e-02 
Epoch 476 use: 358.77 second.

epoch 477 starting......
Epoch:  477 | train loss: 1.749370e-03 | valid loss: 1.811366e-03 
      	| train loss (relative): 3.446357e-02 | valid loss (relative): 3.594466e-02 
Epoch 477 use: 361.00 second.

epoch 478 starting......
Epoch:  478 | train loss: 1.744044e-03 | valid loss: 1.787189e-03 
      	| train loss (relative): 3.436160e-02 | valid loss (relative): 3.511928e-02 
Epoch 478 use: 345.51 second.

epoch 479 starting......
Epoch:  479 | train loss: 1.737932e-03 | valid loss: 1.797616e-03 
      	| train loss (relative): 3.423495e-02 | valid loss (relative): 3.524540e-02 
Epoch 479 use: 357.34 second.

epoch 480 starting......
Epoch:  480 | train loss: 1.746221e-03 | valid loss: 1.794547e-03 
      	| train loss (relative): 3.439653e-02 | valid loss (relative): 3.539537e-02 
Epoch 480 use: 351.29 second.

epoch 481 starting......
Epoch:  481 | train loss: 1.743361e-03 | valid loss: 1.803388e-03 
      	| train loss (relative): 3.434958e-02 | valid loss (relative): 3.554013e-02 
Epoch 481 use: 342.38 second.

epoch 482 starting......
Epoch:  482 | train loss: 1.745470e-03 | valid loss: 1.788679e-03 
      	| train loss (relative): 3.438363e-02 | valid loss (relative): 3.551548e-02 
Epoch 482 use: 346.91 second.

epoch 483 starting......
Epoch:  483 | train loss: 1.740873e-03 | valid loss: 1.797914e-03 
      	| train loss (relative): 3.429304e-02 | valid loss (relative): 3.572936e-02 
Epoch 483 use: 383.20 second.

epoch 484 starting......
Epoch:  484 | train loss: 1.740018e-03 | valid loss: 1.783992e-03 
      	| train loss (relative): 3.427637e-02 | valid loss (relative): 3.541166e-02 
Epoch 484 use: 385.76 second.

epoch 485 starting......
Epoch:  485 | train loss: 1.730599e-03 | valid loss: 1.780872e-03 
      	| train loss (relative): 3.409729e-02 | valid loss (relative): 3.526688e-02 
Epoch 485 use: 380.99 second.

epoch 486 starting......
Epoch:  486 | train loss: 1.734135e-03 | valid loss: 1.792524e-03 
      	| train loss (relative): 3.416309e-02 | valid loss (relative): 3.523837e-02 
Epoch 486 use: 377.06 second.

epoch 487 starting......
Epoch:  487 | train loss: 1.734127e-03 | valid loss: 1.788804e-03 
      	| train loss (relative): 3.416176e-02 | valid loss (relative): 3.523881e-02 
Epoch 487 use: 416.76 second.

epoch 488 starting......
Epoch:  488 | train loss: 1.734458e-03 | valid loss: 1.783839e-03 
      	| train loss (relative): 3.416064e-02 | valid loss (relative): 3.520095e-02 
Epoch 488 use: 354.74 second.

epoch 489 starting......
Epoch:  489 | train loss: 1.735241e-03 | valid loss: 1.787375e-03 
      	| train loss (relative): 3.417730e-02 | valid loss (relative): 3.515814e-02 
Epoch 489 use: 351.68 second.

epoch 490 starting......
Epoch:  490 | train loss: 1.736850e-03 | valid loss: 1.787805e-03 
      	| train loss (relative): 3.420728e-02 | valid loss (relative): 3.553737e-02 
Epoch 490 use: 389.74 second.

epoch 491 starting......
Epoch:  491 | train loss: 1.736637e-03 | valid loss: 1.787808e-03 
      	| train loss (relative): 3.420802e-02 | valid loss (relative): 3.526706e-02 
Epoch 491 use: 427.13 second.

epoch 492 starting......
Epoch:  492 | train loss: 1.733701e-03 | valid loss: 1.781736e-03 
      	| train loss (relative): 3.414178e-02 | valid loss (relative): 3.515915e-02 
Epoch 492 use: 391.92 second.

epoch 493 starting......
Epoch:  493 | train loss: 1.729463e-03 | valid loss: 1.797195e-03 
      	| train loss (relative): 3.406821e-02 | valid loss (relative): 3.515117e-02 
Epoch 493 use: 744.12 second.

epoch 494 starting......
Epoch:  494 | train loss: 1.737200e-03 | valid loss: 1.774216e-03 
      	| train loss (relative): 3.421439e-02 | valid loss (relative): 3.494075e-02 
Epoch 494 use: 461.05 second.

epoch 495 starting......
Epoch:  495 | train loss: 1.722997e-03 | valid loss: 1.769820e-03 
      	| train loss (relative): 3.392573e-02 | valid loss (relative): 3.480250e-02 
Epoch 495 use: 547.02 second.

epoch 496 starting......
Epoch:  496 | train loss: 1.718472e-03 | valid loss: 1.770836e-03 
      	| train loss (relative): 3.384170e-02 | valid loss (relative): 3.480549e-02 
Epoch 496 use: 403.19 second.

epoch 497 starting......
Epoch:  497 | train loss: 1.721065e-03 | valid loss: 1.773371e-03 
      	| train loss (relative): 3.389534e-02 | valid loss (relative): 3.484222e-02 
Epoch 497 use: 404.03 second.

epoch 498 starting......
Epoch:  498 | train loss: 1.720703e-03 | valid loss: 1.773358e-03 
      	| train loss (relative): 3.388148e-02 | valid loss (relative): 3.493949e-02 
Epoch 498 use: 608.48 second.

epoch 499 starting......
Epoch:  499 | train loss: 1.724954e-03 | valid loss: 1.782891e-03 
      	| train loss (relative): 3.396919e-02 | valid loss (relative): 3.492745e-02 
Epoch 499 use: 428.66 second.

test MSE Error: 1.773919e-03 | relative MSE Error: 3.472137e-02 
 Total time used for training: 11.94 hour.
MESLoss saved to  /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_MSELoss_Latent_128_nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_500.txt
relative MSELoss saved to  /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_reMSELoss_Latent_128_nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_500.txt
model saved to /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_128_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_500.pth
model_dict saved to /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_128_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_500_dict.pth
... Training slugflow data completed, Run finished Thu 12 Aug 20:11:06 BST 2021 ...
{'vtu_dir': '/rds/general/user/jy220/home/slugflow/', 'data_dir': '/rds/general/user/jy220/home/tensors/', 'data_type': 'tensors', 'vtu_fields': 'Component1::ComponentMassFractionPhase1, phase1::Velocity', 'variational': 'False', 'optimizer': 'Adamax', 'structured': 'False', 'changing_lr': 'False', 'self_concat': '2', 'dimension': '3', 'sfc_nums': '3', 'components': '4', 'nearest_neighbouring': 'True', 'dims_latent': '128', 'activation': 'Tanh', 'sfc_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/sfcs.pt', 'inv_sfc_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/inv_sfcs.pt', 'coords_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/coords.pt', 'cells_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/cells.npy', 'state_load': '/rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_128_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_500_dict.pth', 'batch_size': '16', 'lr': '0.001', 'n_epoches': '200', 'seed': '15', 'visualize': 'False', 'reconstructed_path': 'None', 'reconstruct_start_index': '0', 'reconstruct_end_index': '100', 'save_path': '/rds/general/user/jy220/home/new_results/'}
[[ 2.50825269e-01 -1.67256990e-02 -1.22268730e-03]
 [ 2.38611959e-01 -5.50627777e-03 -5.06451913e-03]
 [ 2.39454874e-01 -1.59563257e-02  3.96778323e-03]
 ...
 [ 9.11361096e+00 -2.21908381e-02  2.22384902e-03]
 [ 9.11847310e+00 -2.84346931e-02 -2.34839590e-04]
 [ 9.11039661e+00 -2.51498573e-02 -8.70661044e-03]]
{'tetra': array([[      0,       1,       2,       3],
       [      4,       5,       6,       7],
       [      8,       9,      10,      11],
       ...,
       [1342744, 1342745, 1342746, 1342747],
       [1342748, 1342749, 1342750, 1342751],
       [1342752, 1342753, 1342754, 1342755]])}
vtu fields:  ['Component1::ComponentMassFractionPhase1', 'phase1::Velocity'] 
 structured  False 
 activation  Tanh() 
 self concat  2 
 sfc_nums  3 

dims_latent  128 
 components  4 
 nearest_neighbouring  True 

visualize  False 
 sample number  1706 

reading sfc nums......
[array([  41684,   41685,   41674, ..., 1235560, 1235564, 1235558]), array([954289, 954290, 954291, ..., 420685, 420687, 420688]), array([  40156,   40155,   40158, ..., 1123478, 1123475, 1123467])]
reading inverse sfc nums......
[array([   8146,    8145,    8144, ..., 1274444, 1274445, 1273166]), array([547280, 547283, 593037, ..., 933933, 933932, 864488]), array([ 66437,  66439, 122512, ..., 459416, 415130, 459418])]
Computing min and max......

tk:  tensor([0.7581, 0.0418, 0.1840, 0.1464], dtype=torch.float64) 

tb:  tensor([-0.3816, -0.3618, -0.0308,  0.1446], dtype=torch.float64) 

length of train set: 1505 

length of valid set: 100 

length of test set: 101 

torch device num: 4 

Let's use 4 GPUs!
epoch 500 starting......
Epoch:  500 | train loss: 2.004045e-03 | valid loss: 1.820841e-03 
      	| train loss (relative): 3.939158e-02 | valid loss (relative): 3.597388e-02 
Epoch 500 use: 514.53 second.

epoch 501 starting......
Epoch:  501 | train loss: 1.728168e-03 | valid loss: 1.800375e-03 
      	| train loss (relative): 3.405152e-02 | valid loss (relative): 3.555682e-02 
Epoch 501 use: 507.69 second.

epoch 502 starting......
Epoch:  502 | train loss: 1.713328e-03 | valid loss: 1.795332e-03 
      	| train loss (relative): 3.374215e-02 | valid loss (relative): 3.539622e-02 
Epoch 502 use: 548.83 second.

epoch 503 starting......
Epoch:  503 | train loss: 1.708440e-03 | valid loss: 1.795170e-03 
      	| train loss (relative): 3.364042e-02 | valid loss (relative): 3.544813e-02 
Epoch 503 use: 506.55 second.

epoch 504 starting......
Epoch:  504 | train loss: 1.706426e-03 | valid loss: 1.793665e-03 
      	| train loss (relative): 3.360212e-02 | valid loss (relative): 3.541756e-02 
Epoch 504 use: 483.03 second.

epoch 505 starting......
Epoch:  505 | train loss: 1.703950e-03 | valid loss: 1.792819e-03 
      	| train loss (relative): 3.354554e-02 | valid loss (relative): 3.538526e-02 
Epoch 505 use: 616.26 second.

epoch 506 starting......
Epoch:  506 | train loss: 1.702406e-03 | valid loss: 1.792832e-03 
      	| train loss (relative): 3.351826e-02 | valid loss (relative): 3.537824e-02 
Epoch 506 use: 556.27 second.

epoch 507 starting......
Epoch:  507 | train loss: 1.701400e-03 | valid loss: 1.793562e-03 
      	| train loss (relative): 3.349393e-02 | valid loss (relative): 3.534582e-02 
Epoch 507 use: 441.37 second.

epoch 508 starting......
Epoch:  508 | train loss: 1.700621e-03 | valid loss: 1.793244e-03 
      	| train loss (relative): 3.347818e-02 | valid loss (relative): 3.537707e-02 
Epoch 508 use: 500.15 second.

epoch 509 starting......
Epoch:  509 | train loss: 1.699397e-03 | valid loss: 1.795333e-03 
      	| train loss (relative): 3.344942e-02 | valid loss (relative): 3.552934e-02 
Epoch 509 use: 471.46 second.

epoch 510 starting......
Epoch:  510 | train loss: 1.699223e-03 | valid loss: 1.793400e-03 
      	| train loss (relative): 3.345031e-02 | valid loss (relative): 3.543273e-02 
Epoch 510 use: 439.15 second.

epoch 511 starting......
Epoch:  511 | train loss: 1.699158e-03 | valid loss: 1.795428e-03 
      	| train loss (relative): 3.344746e-02 | valid loss (relative): 3.543735e-02 
Epoch 511 use: 438.24 second.

epoch 512 starting......
Epoch:  512 | train loss: 1.699062e-03 | valid loss: 1.798786e-03 
      	| train loss (relative): 3.344860e-02 | valid loss (relative): 3.546559e-02 
Epoch 512 use: 458.00 second.

epoch 513 starting......
Epoch:  513 | train loss: 1.702548e-03 | valid loss: 1.800787e-03 
      	| train loss (relative): 3.351402e-02 | valid loss (relative): 3.555613e-02 
Epoch 513 use: 439.60 second.

epoch 514 starting......
Epoch:  514 | train loss: 1.702801e-03 | valid loss: 1.799570e-03 
      	| train loss (relative): 3.351884e-02 | valid loss (relative): 3.552296e-02 
Epoch 514 use: 434.82 second.

epoch 515 starting......
Epoch:  515 | train loss: 1.702344e-03 | valid loss: 1.799143e-03 
      	| train loss (relative): 3.351314e-02 | valid loss (relative): 3.560325e-02 
Epoch 515 use: 468.00 second.

epoch 516 starting......
Epoch:  516 | train loss: 1.703061e-03 | valid loss: 1.802779e-03 
      	| train loss (relative): 3.352475e-02 | valid loss (relative): 3.552448e-02 
Epoch 516 use: 440.32 second.

epoch 517 starting......
Epoch:  517 | train loss: 1.705360e-03 | valid loss: 1.811191e-03 
      	| train loss (relative): 3.356662e-02 | valid loss (relative): 3.578413e-02 
Epoch 517 use: 434.77 second.

epoch 518 starting......
Epoch:  518 | train loss: 1.706719e-03 | valid loss: 1.808095e-03 
      	| train loss (relative): 3.359802e-02 | valid loss (relative): 3.580746e-02 
Epoch 518 use: 469.58 second.

epoch 519 starting......
Epoch:  519 | train loss: 1.707193e-03 | valid loss: 1.806597e-03 
      	| train loss (relative): 3.360791e-02 | valid loss (relative): 3.563590e-02 
Epoch 519 use: 448.51 second.

epoch 520 starting......
Epoch:  520 | train loss: 1.711098e-03 | valid loss: 1.812355e-03 
      	| train loss (relative): 3.368402e-02 | valid loss (relative): 3.569718e-02 
Epoch 520 use: 449.11 second.

epoch 521 starting......
Epoch:  521 | train loss: 1.703636e-03 | valid loss: 1.804608e-03 
      	| train loss (relative): 3.352809e-02 | valid loss (relative): 3.577058e-02 
Epoch 521 use: 434.38 second.

epoch 522 starting......
Epoch:  522 | train loss: 1.703698e-03 | valid loss: 1.806634e-03 
      	| train loss (relative): 3.353950e-02 | valid loss (relative): 3.552799e-02 
Epoch 522 use: 465.21 second.

epoch 523 starting......
Epoch:  523 | train loss: 1.701809e-03 | valid loss: 1.811321e-03 
      	| train loss (relative): 3.349572e-02 | valid loss (relative): 3.568833e-02 
Epoch 523 use: 447.34 second.

epoch 524 starting......
Epoch:  524 | train loss: 1.704618e-03 | valid loss: 1.803054e-03 
      	| train loss (relative): 3.355139e-02 | valid loss (relative): 3.558799e-02 
Epoch 524 use: 430.41 second.

epoch 525 starting......
Epoch:  525 | train loss: 1.703006e-03 | valid loss: 1.808920e-03 
      	| train loss (relative): 3.351384e-02 | valid loss (relative): 3.560152e-02 
Epoch 525 use: 441.37 second.

epoch 526 starting......
Epoch:  526 | train loss: 1.704459e-03 | valid loss: 1.815486e-03 
      	| train loss (relative): 3.354305e-02 | valid loss (relative): 3.601111e-02 
Epoch 526 use: 447.05 second.

epoch 527 starting......
Epoch:  527 | train loss: 1.705126e-03 | valid loss: 1.806093e-03 
      	| train loss (relative): 3.355951e-02 | valid loss (relative): 3.553056e-02 
Epoch 527 use: 448.56 second.

epoch 528 starting......
Epoch:  528 | train loss: 1.698653e-03 | valid loss: 1.797545e-03 
      	| train loss (relative): 3.343045e-02 | valid loss (relative): 3.564836e-02 
Epoch 528 use: 437.81 second.

epoch 529 starting......
Epoch:  529 | train loss: 1.695332e-03 | valid loss: 1.808372e-03 
      	| train loss (relative): 3.336345e-02 | valid loss (relative): 3.583675e-02 
Epoch 529 use: 440.71 second.

epoch 530 starting......
Epoch:  530 | train loss: 1.701822e-03 | valid loss: 1.805994e-03 
      	| train loss (relative): 3.349403e-02 | valid loss (relative): 3.559711e-02 
Epoch 530 use: 453.31 second.

epoch 531 starting......
Epoch:  531 | train loss: 1.697717e-03 | valid loss: 1.796826e-03 
      	| train loss (relative): 3.340735e-02 | valid loss (relative): 3.547165e-02 
Epoch 531 use: 437.35 second.

epoch 532 starting......
Epoch:  532 | train loss: 1.697808e-03 | valid loss: 1.806676e-03 
      	| train loss (relative): 3.341029e-02 | valid loss (relative): 3.560683e-02 
Epoch 532 use: 478.47 second.

epoch 533 starting......
Epoch:  533 | train loss: 1.698229e-03 | valid loss: 1.824017e-03 
      	| train loss (relative): 3.342178e-02 | valid loss (relative): 3.587627e-02 
Epoch 533 use: 456.19 second.

epoch 534 starting......
Epoch:  534 | train loss: 1.702044e-03 | valid loss: 1.804489e-03 
      	| train loss (relative): 3.349776e-02 | valid loss (relative): 3.560211e-02 
Epoch 534 use: 438.33 second.

epoch 535 starting......
Epoch:  535 | train loss: 1.697896e-03 | valid loss: 1.810643e-03 
      	| train loss (relative): 3.340803e-02 | valid loss (relative): 3.553263e-02 
Epoch 535 use: 457.74 second.

epoch 536 starting......
Epoch:  536 | train loss: 1.699323e-03 | valid loss: 1.807146e-03 
      	| train loss (relative): 3.344322e-02 | valid loss (relative): 3.552288e-02 
Epoch 536 use: 447.59 second.

epoch 537 starting......
Epoch:  537 | train loss: 1.696836e-03 | valid loss: 1.794315e-03 
      	| train loss (relative): 3.339480e-02 | valid loss (relative): 3.554632e-02 
Epoch 537 use: 508.01 second.

epoch 538 starting......
Epoch:  538 | train loss: 1.687569e-03 | valid loss: 1.790929e-03 
      	| train loss (relative): 3.319981e-02 | valid loss (relative): 3.558391e-02 
Epoch 538 use: 436.57 second.

epoch 539 starting......
Epoch:  539 | train loss: 1.689156e-03 | valid loss: 1.793671e-03 
      	| train loss (relative): 3.323246e-02 | valid loss (relative): 3.549295e-02 
Epoch 539 use: 451.33 second.

epoch 540 starting......
Epoch:  540 | train loss: 1.686039e-03 | valid loss: 1.800258e-03 
      	| train loss (relative): 3.317757e-02 | valid loss (relative): 3.529390e-02 
Epoch 540 use: 448.36 second.

epoch 541 starting......
Epoch:  541 | train loss: 1.689761e-03 | valid loss: 1.803662e-03 
      	| train loss (relative): 3.324802e-02 | valid loss (relative): 3.558588e-02 
Epoch 541 use: 436.57 second.

epoch 542 starting......
Epoch:  542 | train loss: 1.692534e-03 | valid loss: 1.802262e-03 
      	| train loss (relative): 3.330664e-02 | valid loss (relative): 3.551185e-02 
Epoch 542 use: 465.52 second.

epoch 543 starting......
Epoch:  543 | train loss: 1.691679e-03 | valid loss: 1.795451e-03 
      	| train loss (relative): 3.328704e-02 | valid loss (relative): 3.539342e-02 
Epoch 543 use: 476.60 second.

epoch 544 starting......
Epoch:  544 | train loss: 1.687325e-03 | valid loss: 1.800853e-03 
      	| train loss (relative): 3.320301e-02 | valid loss (relative): 3.541600e-02 
Epoch 544 use: 421.98 second.

epoch 545 starting......
Epoch:  545 | train loss: 1.688041e-03 | valid loss: 1.793054e-03 
      	| train loss (relative): 3.321339e-02 | valid loss (relative): 3.538490e-02 
Epoch 545 use: 435.89 second.

epoch 546 starting......
Epoch:  546 | train loss: 1.687548e-03 | valid loss: 1.800692e-03 
      	| train loss (relative): 3.320421e-02 | valid loss (relative): 3.560517e-02 
Epoch 546 use: 468.04 second.

epoch 547 starting......
Epoch:  547 | train loss: 1.690230e-03 | valid loss: 1.793052e-03 
      	| train loss (relative): 3.325263e-02 | valid loss (relative): 3.550054e-02 
Epoch 547 use: 430.21 second.

epoch 548 starting......
Epoch:  548 | train loss: 1.684582e-03 | valid loss: 1.795600e-03 
      	| train loss (relative): 3.314605e-02 | valid loss (relative): 3.546374e-02 
Epoch 548 use: 441.55 second.

epoch 549 starting......
Epoch:  549 | train loss: 1.683514e-03 | valid loss: 1.793978e-03 
      	| train loss (relative): 3.311601e-02 | valid loss (relative): 3.548148e-02 
Epoch 549 use: 433.13 second.

epoch 550 starting......
Epoch:  550 | train loss: 1.683602e-03 | valid loss: 1.794394e-03 
      	| train loss (relative): 3.312690e-02 | valid loss (relative): 3.553963e-02 
Epoch 550 use: 433.85 second.

epoch 551 starting......
Epoch:  551 | train loss: 1.688446e-03 | valid loss: 1.792938e-03 
      	| train loss (relative): 3.321680e-02 | valid loss (relative): 3.542561e-02 
Epoch 551 use: 431.24 second.

epoch 552 starting......
Epoch:  552 | train loss: 1.679663e-03 | valid loss: 1.790646e-03 
      	| train loss (relative): 3.304663e-02 | valid loss (relative): 3.531684e-02 
Epoch 552 use: 475.30 second.

epoch 553 starting......
Epoch:  553 | train loss: 1.683696e-03 | valid loss: 1.794218e-03 
      	| train loss (relative): 3.312918e-02 | valid loss (relative): 3.548072e-02 
Epoch 553 use: 484.77 second.

epoch 554 starting......
Epoch:  554 | train loss: 1.681789e-03 | valid loss: 1.804007e-03 
      	| train loss (relative): 3.308297e-02 | valid loss (relative): 3.574285e-02 
Epoch 554 use: 432.66 second.

epoch 555 starting......
Epoch:  555 | train loss: 1.682315e-03 | valid loss: 1.783878e-03 
      	| train loss (relative): 3.309494e-02 | valid loss (relative): 3.527902e-02 
Epoch 555 use: 473.76 second.

epoch 556 starting......
Epoch:  556 | train loss: 1.674379e-03 | valid loss: 1.787925e-03 
      	| train loss (relative): 3.293999e-02 | valid loss (relative): 3.516071e-02 
Epoch 556 use: 431.49 second.

epoch 557 starting......
Epoch:  557 | train loss: 1.679376e-03 | valid loss: 1.799553e-03 
      	| train loss (relative): 3.303977e-02 | valid loss (relative): 3.520752e-02 
Epoch 557 use: 463.82 second.

epoch 558 starting......
Epoch:  558 | train loss: 1.682189e-03 | valid loss: 1.790527e-03 
      	| train loss (relative): 3.308928e-02 | valid loss (relative): 3.555221e-02 
Epoch 558 use: 426.97 second.

epoch 559 starting......
Epoch:  559 | train loss: 1.677334e-03 | valid loss: 1.789056e-03 
      	| train loss (relative): 3.300025e-02 | valid loss (relative): 3.536778e-02 
Epoch 559 use: 446.51 second.

epoch 560 starting......
Epoch:  560 | train loss: 1.678244e-03 | valid loss: 1.793628e-03 
      	| train loss (relative): 3.301271e-02 | valid loss (relative): 3.537827e-02 
Epoch 560 use: 444.97 second.

epoch 561 starting......
Epoch:  561 | train loss: 1.683218e-03 | valid loss: 1.789267e-03 
      	| train loss (relative): 3.311223e-02 | valid loss (relative): 3.535340e-02 
Epoch 561 use: 496.19 second.

epoch 562 starting......
Epoch:  562 | train loss: 1.671600e-03 | valid loss: 1.777067e-03 
      	| train loss (relative): 3.287935e-02 | valid loss (relative): 3.502060e-02 
Epoch 562 use: 503.30 second.

epoch 563 starting......
Epoch:  563 | train loss: 1.668613e-03 | valid loss: 1.785389e-03 
      	| train loss (relative): 3.282380e-02 | valid loss (relative): 3.539678e-02 
Epoch 563 use: 438.66 second.

epoch 564 starting......
Epoch:  564 | train loss: 1.673044e-03 | valid loss: 1.781530e-03 
      	| train loss (relative): 3.290483e-02 | valid loss (relative): 3.513821e-02 
Epoch 564 use: 484.50 second.

epoch 565 starting......
Epoch:  565 | train loss: 1.678383e-03 | valid loss: 1.796248e-03 
      	| train loss (relative): 3.301607e-02 | valid loss (relative): 3.562851e-02 
Epoch 565 use: 443.30 second.

epoch 566 starting......
Epoch:  566 | train loss: 1.672198e-03 | valid loss: 1.775958e-03 
      	| train loss (relative): 3.289386e-02 | valid loss (relative): 3.500460e-02 
Epoch 566 use: 449.00 second.

epoch 567 starting......
Epoch:  567 | train loss: 1.667136e-03 | valid loss: 1.787496e-03 
      	| train loss (relative): 3.278454e-02 | valid loss (relative): 3.510943e-02 
Epoch 567 use: 484.05 second.

epoch 568 starting......
Epoch:  568 | train loss: 1.672268e-03 | valid loss: 1.790450e-03 
      	| train loss (relative): 3.289174e-02 | valid loss (relative): 3.520521e-02 
Epoch 568 use: 505.60 second.

epoch 569 starting......
Epoch:  569 | train loss: 1.671568e-03 | valid loss: 1.786621e-03 
      	| train loss (relative): 3.287970e-02 | valid loss (relative): 3.516803e-02 
Epoch 569 use: 488.88 second.

epoch 570 starting......
Epoch:  570 | train loss: 1.673767e-03 | valid loss: 1.786764e-03 
      	| train loss (relative): 3.292035e-02 | valid loss (relative): 3.514715e-02 
Epoch 570 use: 454.58 second.

epoch 571 starting......
Epoch:  571 | train loss: 1.669524e-03 | valid loss: 1.779806e-03 
      	| train loss (relative): 3.283858e-02 | valid loss (relative): 3.531631e-02 
Epoch 571 use: 452.97 second.

epoch 572 starting......
Epoch:  572 | train loss: 1.672287e-03 | valid loss: 1.779734e-03 
      	| train loss (relative): 3.289019e-02 | valid loss (relative): 3.512460e-02 
Epoch 572 use: 448.06 second.

epoch 573 starting......
Epoch:  573 | train loss: 1.668323e-03 | valid loss: 1.773649e-03 
      	| train loss (relative): 3.280742e-02 | valid loss (relative): 3.516800e-02 
Epoch 573 use: 438.87 second.

epoch 574 starting......
Epoch:  574 | train loss: 1.663475e-03 | valid loss: 1.774438e-03 
      	| train loss (relative): 3.271741e-02 | valid loss (relative): 3.486487e-02 
Epoch 574 use: 438.02 second.

epoch 575 starting......
Epoch:  575 | train loss: 1.663067e-03 | valid loss: 1.776004e-03 
      	| train loss (relative): 3.270954e-02 | valid loss (relative): 3.514379e-02 
Epoch 575 use: 451.03 second.

epoch 576 starting......
Epoch:  576 | train loss: 1.665195e-03 | valid loss: 1.778766e-03 
      	| train loss (relative): 3.275373e-02 | valid loss (relative): 3.514948e-02 
Epoch 576 use: 442.59 second.

epoch 577 starting......
Epoch:  577 | train loss: 1.669615e-03 | valid loss: 1.790219e-03 
      	| train loss (relative): 3.283816e-02 | valid loss (relative): 3.527331e-02 
Epoch 577 use: 435.07 second.

epoch 578 starting......
Epoch:  578 | train loss: 1.670197e-03 | valid loss: 1.777661e-03 
      	| train loss (relative): 3.284331e-02 | valid loss (relative): 3.534260e-02 
Epoch 578 use: 433.61 second.

epoch 579 starting......
Epoch:  579 | train loss: 1.659353e-03 | valid loss: 1.775648e-03 
      	| train loss (relative): 3.263765e-02 | valid loss (relative): 3.522414e-02 
Epoch 579 use: 434.04 second.

epoch 580 starting......
Epoch:  580 | train loss: 1.660626e-03 | valid loss: 1.773516e-03 
      	| train loss (relative): 3.266227e-02 | valid loss (relative): 3.473335e-02 
Epoch 580 use: 459.72 second.

epoch 581 starting......
Epoch:  581 | train loss: 1.657545e-03 | valid loss: 1.771372e-03 
      	| train loss (relative): 3.259391e-02 | valid loss (relative): 3.486928e-02 
Epoch 581 use: 443.78 second.

epoch 582 starting......
Epoch:  582 | train loss: 1.660945e-03 | valid loss: 1.774207e-03 
      	| train loss (relative): 3.266259e-02 | valid loss (relative): 3.477461e-02 
Epoch 582 use: 454.14 second.

epoch 583 starting......
Epoch:  583 | train loss: 1.662244e-03 | valid loss: 1.776808e-03 
      	| train loss (relative): 3.268652e-02 | valid loss (relative): 3.488638e-02 
Epoch 583 use: 440.52 second.

epoch 584 starting......
Epoch:  584 | train loss: 1.664094e-03 | valid loss: 1.784300e-03 
      	| train loss (relative): 3.272732e-02 | valid loss (relative): 3.490081e-02 
Epoch 584 use: 461.61 second.

epoch 585 starting......
Epoch:  585 | train loss: 1.661115e-03 | valid loss: 1.777140e-03 
      	| train loss (relative): 3.266157e-02 | valid loss (relative): 3.519938e-02 
Epoch 585 use: 436.21 second.

epoch 586 starting......
Epoch:  586 | train loss: 1.660817e-03 | valid loss: 1.781863e-03 
      	| train loss (relative): 3.265948e-02 | valid loss (relative): 3.546441e-02 
Epoch 586 use: 451.88 second.

epoch 587 starting......
Epoch:  587 | train loss: 1.664018e-03 | valid loss: 1.771147e-03 
      	| train loss (relative): 3.272135e-02 | valid loss (relative): 3.479947e-02 
Epoch 587 use: 464.31 second.

epoch 588 starting......
Epoch:  588 | train loss: 1.657008e-03 | valid loss: 1.772686e-03 
      	| train loss (relative): 3.258623e-02 | valid loss (relative): 3.478334e-02 
Epoch 588 use: 491.35 second.

epoch 589 starting......
Epoch:  589 | train loss: 1.654235e-03 | valid loss: 1.772791e-03 
      	| train loss (relative): 3.252713e-02 | valid loss (relative): 3.495697e-02 
Epoch 589 use: 466.01 second.

epoch 590 starting......
Epoch:  590 | train loss: 1.658901e-03 | valid loss: 1.766474e-03 
      	| train loss (relative): 3.261546e-02 | valid loss (relative): 3.486956e-02 
Epoch 590 use: 457.19 second.

epoch 591 starting......
Epoch:  591 | train loss: 1.651414e-03 | valid loss: 1.767434e-03 
      	| train loss (relative): 3.247112e-02 | valid loss (relative): 3.472853e-02 
Epoch 591 use: 508.57 second.

epoch 592 starting......
Epoch:  592 | train loss: 1.654494e-03 | valid loss: 1.773884e-03 
      	| train loss (relative): 3.253201e-02 | valid loss (relative): 3.512100e-02 
Epoch 592 use: 456.30 second.

epoch 593 starting......
Epoch:  593 | train loss: 1.657711e-03 | valid loss: 1.772316e-03 
      	| train loss (relative): 3.259782e-02 | valid loss (relative): 3.493489e-02 
Epoch 593 use: 454.51 second.

epoch 594 starting......
Epoch:  594 | train loss: 1.654170e-03 | valid loss: 1.762494e-03 
      	| train loss (relative): 3.252365e-02 | valid loss (relative): 3.460184e-02 
Epoch 594 use: 473.87 second.

epoch 595 starting......
Epoch:  595 | train loss: 1.650573e-03 | valid loss: 1.763692e-03 
      	| train loss (relative): 3.244656e-02 | valid loss (relative): 3.494157e-02 
Epoch 595 use: 471.36 second.

epoch 596 starting......
Epoch:  596 | train loss: 1.648157e-03 | valid loss: 1.770377e-03 
      	| train loss (relative): 3.240438e-02 | valid loss (relative): 3.484925e-02 
Epoch 596 use: 439.27 second.

epoch 597 starting......
Epoch:  597 | train loss: 1.652364e-03 | valid loss: 1.760465e-03 
      	| train loss (relative): 3.248497e-02 | valid loss (relative): 3.474695e-02 
Epoch 597 use: 438.14 second.

epoch 598 starting......
Epoch:  598 | train loss: 1.649575e-03 | valid loss: 1.768298e-03 
      	| train loss (relative): 3.243107e-02 | valid loss (relative): 3.492942e-02 
Epoch 598 use: 460.58 second.

epoch 599 starting......
Epoch:  599 | train loss: 1.654170e-03 | valid loss: 1.772895e-03 
      	| train loss (relative): 3.252521e-02 | valid loss (relative): 3.502278e-02 
Epoch 599 use: 436.53 second.

epoch 600 starting......
Epoch:  600 | train loss: 1.651799e-03 | valid loss: 1.771701e-03 
      	| train loss (relative): 3.247864e-02 | valid loss (relative): 3.496262e-02 
Epoch 600 use: 504.00 second.

epoch 601 starting......
Epoch:  601 | train loss: 1.647345e-03 | valid loss: 1.763766e-03 
      	| train loss (relative): 3.238341e-02 | valid loss (relative): 3.477895e-02 
Epoch 601 use: 448.61 second.

epoch 602 starting......
Epoch:  602 | train loss: 1.649298e-03 | valid loss: 1.762165e-03 
      	| train loss (relative): 3.242503e-02 | valid loss (relative): 3.461462e-02 
Epoch 602 use: 500.31 second.

epoch 603 starting......
Epoch:  603 | train loss: 1.650147e-03 | valid loss: 1.764506e-03 
      	| train loss (relative): 3.243423e-02 | valid loss (relative): 3.502186e-02 
Epoch 603 use: 493.31 second.

epoch 604 starting......
Epoch:  604 | train loss: 1.647134e-03 | valid loss: 1.758163e-03 
      	| train loss (relative): 3.238171e-02 | valid loss (relative): 3.457893e-02 
Epoch 604 use: 458.22 second.

epoch 605 starting......
Epoch:  605 | train loss: 1.644810e-03 | valid loss: 1.764553e-03 
      	| train loss (relative): 3.233416e-02 | valid loss (relative): 3.457266e-02 
Epoch 605 use: 708.99 second.

epoch 606 starting......
Epoch:  606 | train loss: 1.648775e-03 | valid loss: 1.760222e-03 
      	| train loss (relative): 3.240920e-02 | valid loss (relative): 3.470554e-02 
Epoch 606 use: 687.25 second.

epoch 607 starting......
Epoch:  607 | train loss: 1.644459e-03 | valid loss: 1.753209e-03 
      	| train loss (relative): 3.232199e-02 | valid loss (relative): 3.441925e-02 
Epoch 607 use: 469.16 second.

epoch 608 starting......
Epoch:  608 | train loss: 1.642168e-03 | valid loss: 1.758009e-03 
      	| train loss (relative): 3.228417e-02 | valid loss (relative): 3.461183e-02 
Epoch 608 use: 506.35 second.

epoch 609 starting......
Epoch:  609 | train loss: 1.644384e-03 | valid loss: 1.759516e-03 
      	| train loss (relative): 3.232394e-02 | valid loss (relative): 3.445052e-02 
Epoch 609 use: 489.81 second.

epoch 610 starting......
Epoch:  610 | train loss: 1.643212e-03 | valid loss: 1.754451e-03 
      	| train loss (relative): 3.229831e-02 | valid loss (relative): 3.470454e-02 
Epoch 610 use: 455.43 second.

epoch 611 starting......
Epoch:  611 | train loss: 1.640522e-03 | valid loss: 1.755653e-03 
      	| train loss (relative): 3.224698e-02 | valid loss (relative): 3.450679e-02 
Epoch 611 use: 435.80 second.

epoch 612 starting......
Epoch:  612 | train loss: 1.642440e-03 | valid loss: 1.758144e-03 
      	| train loss (relative): 3.228504e-02 | valid loss (relative): 3.479197e-02 
Epoch 612 use: 488.85 second.

epoch 613 starting......
Epoch:  613 | train loss: 1.638585e-03 | valid loss: 1.751512e-03 
      	| train loss (relative): 3.221465e-02 | valid loss (relative): 3.428524e-02 
Epoch 613 use: 418.97 second.

epoch 614 starting......
Epoch:  614 | train loss: 1.644352e-03 | valid loss: 1.754180e-03 
      	| train loss (relative): 3.231568e-02 | valid loss (relative): 3.455959e-02 
Epoch 614 use: 350.13 second.

epoch 615 starting......
Epoch:  615 | train loss: 1.637414e-03 | valid loss: 1.749604e-03 
      	| train loss (relative): 3.218646e-02 | valid loss (relative): 3.451820e-02 
Epoch 615 use: 334.01 second.

epoch 616 starting......
Epoch:  616 | train loss: 1.636384e-03 | valid loss: 1.752796e-03 
      	| train loss (relative): 3.216349e-02 | valid loss (relative): 3.462940e-02 
Epoch 616 use: 344.99 second.

epoch 617 starting......
Epoch:  617 | train loss: 1.635501e-03 | valid loss: 1.759997e-03 
      	| train loss (relative): 3.214581e-02 | valid loss (relative): 3.463797e-02 
Epoch 617 use: 354.27 second.

epoch 618 starting......
Epoch:  618 | train loss: 1.644666e-03 | valid loss: 1.754552e-03 
      	| train loss (relative): 3.232635e-02 | valid loss (relative): 3.462598e-02 
Epoch 618 use: 398.77 second.

epoch 619 starting......
Epoch:  619 | train loss: 1.641551e-03 | valid loss: 1.743179e-03 
      	| train loss (relative): 3.226787e-02 | valid loss (relative): 3.423729e-02 
Epoch 619 use: 385.47 second.

epoch 620 starting......
Epoch:  620 | train loss: 1.628975e-03 | valid loss: 1.741574e-03 
      	| train loss (relative): 3.201502e-02 | valid loss (relative): 3.426987e-02 
Epoch 620 use: 342.66 second.

epoch 621 starting......
Epoch:  621 | train loss: 1.628116e-03 | valid loss: 1.742207e-03 
      	| train loss (relative): 3.199199e-02 | valid loss (relative): 3.438603e-02 
Epoch 621 use: 324.14 second.

epoch 622 starting......
Epoch:  622 | train loss: 1.632556e-03 | valid loss: 1.744723e-03 
      	| train loss (relative): 3.208769e-02 | valid loss (relative): 3.428148e-02 
Epoch 622 use: 348.90 second.

epoch 623 starting......
Epoch:  623 | train loss: 1.634663e-03 | valid loss: 1.748845e-03 
      	| train loss (relative): 3.213004e-02 | valid loss (relative): 3.427196e-02 
Epoch 623 use: 333.81 second.

epoch 624 starting......
Epoch:  624 | train loss: 1.635022e-03 | valid loss: 1.751686e-03 
      	| train loss (relative): 3.213371e-02 | valid loss (relative): 3.444782e-02 
Epoch 624 use: 342.09 second.

epoch 625 starting......
Epoch:  625 | train loss: 1.633663e-03 | valid loss: 1.765130e-03 
      	| train loss (relative): 3.211068e-02 | valid loss (relative): 3.504165e-02 
Epoch 625 use: 336.18 second.

epoch 626 starting......
Epoch:  626 | train loss: 1.633788e-03 | valid loss: 1.739773e-03 
      	| train loss (relative): 3.211154e-02 | valid loss (relative): 3.431344e-02 
Epoch 626 use: 338.75 second.

epoch 627 starting......
Epoch:  627 | train loss: 1.625519e-03 | valid loss: 1.738148e-03 
      	| train loss (relative): 3.194539e-02 | valid loss (relative): 3.428221e-02 
Epoch 627 use: 333.09 second.

epoch 628 starting......
Epoch:  628 | train loss: 1.622731e-03 | valid loss: 1.742981e-03 
      	| train loss (relative): 3.188720e-02 | valid loss (relative): 3.428972e-02 
Epoch 628 use: 336.06 second.

epoch 629 starting......
Epoch:  629 | train loss: 1.629020e-03 | valid loss: 1.755668e-03 
      	| train loss (relative): 3.200753e-02 | valid loss (relative): 3.465870e-02 
Epoch 629 use: 321.53 second.

epoch 630 starting......
Epoch:  630 | train loss: 1.631718e-03 | valid loss: 1.739949e-03 
      	| train loss (relative): 3.206777e-02 | valid loss (relative): 3.438516e-02 
Epoch 630 use: 337.04 second.

epoch 631 starting......
Epoch:  631 | train loss: 1.626372e-03 | valid loss: 1.743988e-03 
      	| train loss (relative): 3.197006e-02 | valid loss (relative): 3.425023e-02 
Epoch 631 use: 331.70 second.

epoch 632 starting......
Epoch:  632 | train loss: 1.625592e-03 | valid loss: 1.740792e-03 
      	| train loss (relative): 3.194520e-02 | valid loss (relative): 3.426313e-02 
Epoch 632 use: 345.82 second.

epoch 633 starting......
Epoch:  633 | train loss: 1.623875e-03 | valid loss: 1.735579e-03 
      	| train loss (relative): 3.190877e-02 | valid loss (relative): 3.404145e-02 
Epoch 633 use: 341.06 second.

epoch 634 starting......
Epoch:  634 | train loss: 1.624391e-03 | valid loss: 1.745599e-03 
      	| train loss (relative): 3.192078e-02 | valid loss (relative): 3.443545e-02 
Epoch 634 use: 350.93 second.

epoch 635 starting......
Epoch:  635 | train loss: 1.628807e-03 | valid loss: 1.740062e-03 
      	| train loss (relative): 3.200873e-02 | valid loss (relative): 3.415545e-02 
Epoch 635 use: 366.57 second.

epoch 636 starting......
Epoch:  636 | train loss: 1.629081e-03 | valid loss: 1.734378e-03 
      	| train loss (relative): 3.200161e-02 | valid loss (relative): 3.419458e-02 
Epoch 636 use: 457.14 second.

epoch 637 starting......
Epoch:  637 | train loss: 1.618374e-03 | valid loss: 1.730095e-03 
      	| train loss (relative): 3.179989e-02 | valid loss (relative): 3.409404e-02 
Epoch 637 use: 392.04 second.

epoch 638 starting......
Epoch:  638 | train loss: 1.617933e-03 | valid loss: 1.729373e-03 
      	| train loss (relative): 3.179088e-02 | valid loss (relative): 3.394317e-02 
Epoch 638 use: 366.32 second.

epoch 639 starting......
Epoch:  639 | train loss: 1.618181e-03 | valid loss: 1.735552e-03 
      	| train loss (relative): 3.179700e-02 | valid loss (relative): 3.419242e-02 
Epoch 639 use: 337.15 second.

epoch 640 starting......
Epoch:  640 | train loss: 1.620519e-03 | valid loss: 1.734995e-03 
      	| train loss (relative): 3.184544e-02 | valid loss (relative): 3.419866e-02 
Epoch 640 use: 339.33 second.

epoch 641 starting......
Epoch:  641 | train loss: 1.624149e-03 | valid loss: 1.737275e-03 
      	| train loss (relative): 3.191521e-02 | valid loss (relative): 3.405960e-02 
Epoch 641 use: 326.74 second.

epoch 642 starting......
Epoch:  642 | train loss: 1.621056e-03 | valid loss: 1.736341e-03 
      	| train loss (relative): 3.185190e-02 | valid loss (relative): 3.410732e-02 
Epoch 642 use: 336.81 second.

epoch 643 starting......
Epoch:  643 | train loss: 1.623017e-03 | valid loss: 1.738339e-03 
      	| train loss (relative): 3.188464e-02 | valid loss (relative): 3.448740e-02 
Epoch 643 use: 337.96 second.

epoch 644 starting......
Epoch:  644 | train loss: 1.628356e-03 | valid loss: 1.737774e-03 
      	| train loss (relative): 3.199738e-02 | valid loss (relative): 3.416820e-02 
Epoch 644 use: 333.16 second.

epoch 645 starting......
Epoch:  645 | train loss: 1.619445e-03 | valid loss: 1.726525e-03 
      	| train loss (relative): 3.181683e-02 | valid loss (relative): 3.404270e-02 
Epoch 645 use: 339.03 second.

epoch 646 starting......
Epoch:  646 | train loss: 1.614834e-03 | valid loss: 1.728332e-03 
      	| train loss (relative): 3.172591e-02 | valid loss (relative): 3.405643e-02 
Epoch 646 use: 335.48 second.

epoch 647 starting......
Epoch:  647 | train loss: 1.615441e-03 | valid loss: 1.734076e-03 
      	| train loss (relative): 3.174405e-02 | valid loss (relative): 3.410767e-02 
Epoch 647 use: 325.63 second.

epoch 648 starting......
Epoch:  648 | train loss: 1.620406e-03 | valid loss: 1.739930e-03 
      	| train loss (relative): 3.183401e-02 | valid loss (relative): 3.438258e-02 
Epoch 648 use: 340.11 second.

epoch 649 starting......
Epoch:  649 | train loss: 1.622135e-03 | valid loss: 1.737236e-03 
      	| train loss (relative): 3.186965e-02 | valid loss (relative): 3.420956e-02 
Epoch 649 use: 371.39 second.

epoch 650 starting......
Epoch:  650 | train loss: 1.622647e-03 | valid loss: 1.735914e-03 
      	| train loss (relative): 3.188300e-02 | valid loss (relative): 3.393747e-02 
Epoch 650 use: 334.13 second.

epoch 651 starting......
Epoch:  651 | train loss: 1.620848e-03 | valid loss: 1.735222e-03 
      	| train loss (relative): 3.184095e-02 | valid loss (relative): 3.400631e-02 
Epoch 651 use: 350.05 second.

epoch 652 starting......
Epoch:  652 | train loss: 1.616990e-03 | valid loss: 1.733443e-03 
      	| train loss (relative): 3.177052e-02 | valid loss (relative): 3.402242e-02 
Epoch 652 use: 345.43 second.

epoch 653 starting......
Epoch:  653 | train loss: 1.614541e-03 | valid loss: 1.732003e-03 
      	| train loss (relative): 3.171490e-02 | valid loss (relative): 3.421867e-02 
Epoch 653 use: 382.43 second.

epoch 654 starting......
Epoch:  654 | train loss: 1.617416e-03 | valid loss: 1.724799e-03 
      	| train loss (relative): 3.177941e-02 | valid loss (relative): 3.410327e-02 
Epoch 654 use: 354.92 second.

epoch 655 starting......
Epoch:  655 | train loss: 1.610607e-03 | valid loss: 1.724401e-03 
      	| train loss (relative): 3.164028e-02 | valid loss (relative): 3.401498e-02 
Epoch 655 use: 331.72 second.

epoch 656 starting......
Epoch:  656 | train loss: 1.607045e-03 | valid loss: 1.719521e-03 
      	| train loss (relative): 3.157152e-02 | valid loss (relative): 3.392309e-02 
Epoch 656 use: 341.78 second.

epoch 657 starting......
Epoch:  657 | train loss: 1.609471e-03 | valid loss: 1.725162e-03 
      	| train loss (relative): 3.162098e-02 | valid loss (relative): 3.394404e-02 
Epoch 657 use: 384.47 second.

epoch 658 starting......
Epoch:  658 | train loss: 1.611508e-03 | valid loss: 1.728441e-03 
      	| train loss (relative): 3.165696e-02 | valid loss (relative): 3.405767e-02 
Epoch 658 use: 365.97 second.

epoch 659 starting......
Epoch:  659 | train loss: 1.606741e-03 | valid loss: 1.725886e-03 
      	| train loss (relative): 3.156490e-02 | valid loss (relative): 3.384050e-02 
Epoch 659 use: 336.61 second.

epoch 660 starting......
Epoch:  660 | train loss: 1.608539e-03 | valid loss: 1.720159e-03 
      	| train loss (relative): 3.159740e-02 | valid loss (relative): 3.397012e-02 
Epoch 660 use: 345.03 second.

epoch 661 starting......
Epoch:  661 | train loss: 1.606314e-03 | valid loss: 1.720197e-03 
      	| train loss (relative): 3.155459e-02 | valid loss (relative): 3.406684e-02 
Epoch 661 use: 336.11 second.

epoch 662 starting......
Epoch:  662 | train loss: 1.606420e-03 | valid loss: 1.735818e-03 
      	| train loss (relative): 3.155660e-02 | valid loss (relative): 3.436835e-02 
Epoch 662 use: 360.47 second.

epoch 663 starting......
Epoch:  663 | train loss: 1.615180e-03 | valid loss: 1.745375e-03 
      	| train loss (relative): 3.173088e-02 | valid loss (relative): 3.435151e-02 
Epoch 663 use: 342.59 second.

epoch 664 starting......
Epoch:  664 | train loss: 1.614843e-03 | valid loss: 1.724711e-03 
      	| train loss (relative): 3.172129e-02 | valid loss (relative): 3.389643e-02 
Epoch 664 use: 344.42 second.

epoch 665 starting......
Epoch:  665 | train loss: 1.607154e-03 | valid loss: 1.726801e-03 
      	| train loss (relative): 3.156732e-02 | valid loss (relative): 3.397308e-02 
Epoch 665 use: 334.29 second.

epoch 666 starting......
Epoch:  666 | train loss: 1.609506e-03 | valid loss: 1.723725e-03 
      	| train loss (relative): 3.161278e-02 | valid loss (relative): 3.393166e-02 
Epoch 666 use: 356.16 second.

epoch 667 starting......
Epoch:  667 | train loss: 1.606673e-03 | valid loss: 1.725728e-03 
      	| train loss (relative): 3.155955e-02 | valid loss (relative): 3.386435e-02 
Epoch 667 use: 335.43 second.

epoch 668 starting......
Epoch:  668 | train loss: 1.609464e-03 | valid loss: 1.723414e-03 
      	| train loss (relative): 3.161044e-02 | valid loss (relative): 3.416292e-02 
Epoch 668 use: 342.72 second.

epoch 669 starting......
Epoch:  669 | train loss: 1.606949e-03 | valid loss: 1.721356e-03 
      	| train loss (relative): 3.156290e-02 | valid loss (relative): 3.400564e-02 
Epoch 669 use: 332.97 second.

epoch 670 starting......
Epoch:  670 | train loss: 1.602660e-03 | valid loss: 1.712602e-03 
      	| train loss (relative): 3.147799e-02 | valid loss (relative): 3.366014e-02 
Epoch 670 use: 337.91 second.

epoch 671 starting......
Epoch:  671 | train loss: 1.602863e-03 | valid loss: 1.720200e-03 
      	| train loss (relative): 3.148731e-02 | valid loss (relative): 3.391786e-02 
Epoch 671 use: 340.05 second.

epoch 672 starting......
Epoch:  672 | train loss: 1.605450e-03 | valid loss: 1.721836e-03 
      	| train loss (relative): 3.153774e-02 | valid loss (relative): 3.385070e-02 
Epoch 672 use: 350.85 second.

epoch 673 starting......
Epoch:  673 | train loss: 1.603932e-03 | valid loss: 1.722435e-03 
      	| train loss (relative): 3.150299e-02 | valid loss (relative): 3.381369e-02 
Epoch 673 use: 342.34 second.

epoch 674 starting......
Epoch:  674 | train loss: 1.608403e-03 | valid loss: 1.719006e-03 
      	| train loss (relative): 3.159027e-02 | valid loss (relative): 3.389059e-02 
Epoch 674 use: 345.88 second.

epoch 675 starting......
Epoch:  675 | train loss: 1.603203e-03 | valid loss: 1.716290e-03 
      	| train loss (relative): 3.148877e-02 | valid loss (relative): 3.368396e-02 
Epoch 675 use: 344.03 second.

epoch 676 starting......
Epoch:  676 | train loss: 1.602417e-03 | valid loss: 1.718186e-03 
      	| train loss (relative): 3.147314e-02 | valid loss (relative): 3.384140e-02 
Epoch 676 use: 347.55 second.

epoch 677 starting......
Epoch:  677 | train loss: 1.602178e-03 | valid loss: 1.722120e-03 
      	| train loss (relative): 3.146411e-02 | valid loss (relative): 3.390206e-02 
Epoch 677 use: 337.22 second.

epoch 678 starting......
Epoch:  678 | train loss: 1.603974e-03 | valid loss: 1.723648e-03 
      	| train loss (relative): 3.150662e-02 | valid loss (relative): 3.392231e-02 
Epoch 678 use: 339.92 second.

epoch 679 starting......
Epoch:  679 | train loss: 1.604202e-03 | valid loss: 1.716009e-03 
      	| train loss (relative): 3.150519e-02 | valid loss (relative): 3.365030e-02 
Epoch 679 use: 339.86 second.

epoch 680 starting......
Epoch:  680 | train loss: 1.598199e-03 | valid loss: 1.714902e-03 
      	| train loss (relative): 3.138645e-02 | valid loss (relative): 3.385208e-02 
Epoch 680 use: 346.91 second.

epoch 681 starting......
Epoch:  681 | train loss: 1.599143e-03 | valid loss: 1.713904e-03 
      	| train loss (relative): 3.140572e-02 | valid loss (relative): 3.378099e-02 
Epoch 681 use: 341.10 second.

epoch 682 starting......
Epoch:  682 | train loss: 1.600462e-03 | valid loss: 1.722602e-03 
      	| train loss (relative): 3.143436e-02 | valid loss (relative): 3.391172e-02 
Epoch 682 use: 333.54 second.

epoch 683 starting......
Epoch:  683 | train loss: 1.603523e-03 | valid loss: 1.716602e-03 
      	| train loss (relative): 3.149366e-02 | valid loss (relative): 3.374260e-02 
Epoch 683 use: 337.53 second.

epoch 684 starting......
Epoch:  684 | train loss: 1.599120e-03 | valid loss: 1.717156e-03 
      	| train loss (relative): 3.140509e-02 | valid loss (relative): 3.402860e-02 
Epoch 684 use: 355.78 second.

epoch 685 starting......
Epoch:  685 | train loss: 1.600503e-03 | valid loss: 1.715763e-03 
      	| train loss (relative): 3.143114e-02 | valid loss (relative): 3.384758e-02 
Epoch 685 use: 336.65 second.

epoch 686 starting......
Epoch:  686 | train loss: 1.594646e-03 | valid loss: 1.707533e-03 
      	| train loss (relative): 3.131527e-02 | valid loss (relative): 3.356056e-02 
Epoch 686 use: 332.63 second.

epoch 687 starting......
Epoch:  687 | train loss: 1.594012e-03 | valid loss: 1.709657e-03 
      	| train loss (relative): 3.130229e-02 | valid loss (relative): 3.375418e-02 
Epoch 687 use: 354.72 second.

epoch 688 starting......
Epoch:  688 | train loss: 1.598136e-03 | valid loss: 1.716796e-03 
      	| train loss (relative): 3.138694e-02 | valid loss (relative): 3.408777e-02 
Epoch 688 use: 340.30 second.

epoch 689 starting......
Epoch:  689 | train loss: 1.600617e-03 | valid loss: 1.717655e-03 
      	| train loss (relative): 3.143408e-02 | valid loss (relative): 3.394465e-02 
Epoch 689 use: 341.42 second.

epoch 690 starting......
Epoch:  690 | train loss: 1.597765e-03 | valid loss: 1.710368e-03 
      	| train loss (relative): 3.137526e-02 | valid loss (relative): 3.350508e-02 
Epoch 690 use: 343.47 second.

epoch 691 starting......
Epoch:  691 | train loss: 1.596466e-03 | valid loss: 1.711024e-03 
      	| train loss (relative): 3.135005e-02 | valid loss (relative): 3.364739e-02 
Epoch 691 use: 330.93 second.

epoch 692 starting......
Epoch:  692 | train loss: 1.594611e-03 | valid loss: 1.711215e-03 
      	| train loss (relative): 3.131697e-02 | valid loss (relative): 3.346639e-02 
Epoch 692 use: 338.17 second.

epoch 693 starting......
Epoch:  693 | train loss: 1.595701e-03 | valid loss: 1.707810e-03 
      	| train loss (relative): 3.133064e-02 | valid loss (relative): 3.344523e-02 
Epoch 693 use: 345.15 second.

epoch 694 starting......
Epoch:  694 | train loss: 1.591431e-03 | valid loss: 1.714279e-03 
      	| train loss (relative): 3.124927e-02 | valid loss (relative): 3.368153e-02 
Epoch 694 use: 328.59 second.

epoch 695 starting......
Epoch:  695 | train loss: 1.592632e-03 | valid loss: 1.707125e-03 
      	| train loss (relative): 3.127475e-02 | valid loss (relative): 3.345762e-02 
Epoch 695 use: 340.42 second.

epoch 696 starting......
Epoch:  696 | train loss: 1.591690e-03 | valid loss: 1.702261e-03 
      	| train loss (relative): 3.125120e-02 | valid loss (relative): 3.357179e-02 
Epoch 696 use: 353.97 second.

epoch 697 starting......
Epoch:  697 | train loss: 1.586832e-03 | valid loss: 1.704263e-03 
      	| train loss (relative): 3.115774e-02 | valid loss (relative): 3.353419e-02 
Epoch 697 use: 344.46 second.

epoch 698 starting......
Epoch:  698 | train loss: 1.591727e-03 | valid loss: 1.704991e-03 
      	| train loss (relative): 3.125985e-02 | valid loss (relative): 3.363653e-02 
Epoch 698 use: 338.49 second.

epoch 699 starting......
Epoch:  699 | train loss: 1.586317e-03 | valid loss: 1.702140e-03 
      	| train loss (relative): 3.114753e-02 | valid loss (relative): 3.342763e-02 
Epoch 699 use: 352.01 second.

test MSE Error: 1.637549e-03 | relative MSE Error: 3.212721e-02 
 Total time used for training: 22.99 hour.
MESLoss saved to  /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_MSELoss_Latent_128_nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_700.txt
relative MSELoss saved to  /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_reMSELoss_Latent_128_nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_700.txt
model saved to /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_128_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_700.pth
model_dict saved to /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_128_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_700_dict.pth
... Training slugflow data completed, Run finished Fri 13 Aug 21:51:47 BST 2021 ...
{'vtu_dir': '/rds/general/user/jy220/home/slugflow/', 'data_dir': '/rds/general/user/jy220/home/tensors/', 'data_type': 'tensors', 'vtu_fields': 'Component1::ComponentMassFractionPhase1, phase1::Velocity', 'variational': 'False', 'optimizer': 'Adamax', 'structured': 'False', 'changing_lr': 'False', 'self_concat': '2', 'dimension': '3', 'sfc_nums': '3', 'components': '4', 'nearest_neighbouring': 'True', 'dims_latent': '128', 'activation': 'Tanh', 'sfc_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/sfcs.pt', 'inv_sfc_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/inv_sfcs.pt', 'coords_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/coords.pt', 'cells_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/cells.npy', 'state_load': 'None', 'batch_size': '16', 'lr': '0.0005', 'n_epoches': '100', 'seed': '15', 'visualize': 'False', 'reconstructed_path': 'None', 'reconstruct_start_index': '0', 'reconstruct_end_index': '100', 'save_path': '/rds/general/user/jy220/home/new_results/'}
[[ 2.50825269e-01 -1.67256990e-02 -1.22268730e-03]
 [ 2.38611959e-01 -5.50627777e-03 -5.06451913e-03]
 [ 2.39454874e-01 -1.59563257e-02  3.96778323e-03]
 ...
 [ 9.11361096e+00 -2.21908381e-02  2.22384902e-03]
 [ 9.11847310e+00 -2.84346931e-02 -2.34839590e-04]
 [ 9.11039661e+00 -2.51498573e-02 -8.70661044e-03]]
{'tetra': array([[      0,       1,       2,       3],
       [      4,       5,       6,       7],
       [      8,       9,      10,      11],
       ...,
       [1342744, 1342745, 1342746, 1342747],
       [1342748, 1342749, 1342750, 1342751],
       [1342752, 1342753, 1342754, 1342755]])}
vtu fields:  ['Component1::ComponentMassFractionPhase1', 'phase1::Velocity'] 
 structured  False 
 activation  Tanh() 
 self concat  2 
 sfc_nums  3 

dims_latent  128 
 components  4 
 nearest_neighbouring  True 

visualize  False 
 sample number  1706 

reading sfc nums......
[array([  41684,   41685,   41674, ..., 1235560, 1235564, 1235558]), array([954289, 954290, 954291, ..., 420685, 420687, 420688]), array([  40156,   40155,   40158, ..., 1123478, 1123475, 1123467])]
reading inverse sfc nums......
[array([   8146,    8145,    8144, ..., 1274444, 1274445, 1273166]), array([547280, 547283, 593037, ..., 933933, 933932, 864488]), array([ 66437,  66439, 122512, ..., 459416, 415130, 459418])]
Computing min and max......

tk:  tensor([0.7581, 0.0418, 0.1840, 0.1464], dtype=torch.float64) 

tb:  tensor([-0.3816, -0.3618, -0.0308,  0.1446], dtype=torch.float64) 

length of train set: 1505 

length of valid set: 100 

length of test set: 101 

torch device num: 4 

Let's use 4 GPUs!
epoch 0 starting......
Epoch:  0 | train loss: 2.747498e-02 | valid loss: 2.073699e-02 
      	| train loss (relative): 3.318618e+00 | valid loss (relative): 9.504987e-01 
Epoch 0 use: 415.95 second.

epoch 1 starting......
Epoch:  1 | train loss: 1.444631e-02 | valid loss: 7.737293e-03 
      	| train loss (relative): 5.456474e-01 | valid loss (relative): 1.951184e-01 
Epoch 1 use: 358.05 second.

epoch 2 starting......
Epoch:  2 | train loss: 6.958035e-03 | valid loss: 6.441392e-03 
      	| train loss (relative): 1.592208e-01 | valid loss (relative): 1.374765e-01 
Epoch 2 use: 506.36 second.

epoch 3 starting......
Epoch:  3 | train loss: 6.209908e-03 | valid loss: 6.028912e-03 
      	| train loss (relative): 1.341757e-01 | valid loss (relative): 1.313487e-01 
Epoch 3 use: 320.47 second.

epoch 4 starting......
Epoch:  4 | train loss: 5.981537e-03 | valid loss: 5.845837e-03 
      	| train loss (relative): 1.286842e-01 | valid loss (relative): 1.274256e-01 
Epoch 4 use: 337.40 second.

epoch 5 starting......
Epoch:  5 | train loss: 5.800765e-03 | valid loss: 5.664899e-03 
      	| train loss (relative): 1.246808e-01 | valid loss (relative): 1.233218e-01 
Epoch 5 use: 341.51 second.

epoch 6 starting......
Epoch:  6 | train loss: 5.623538e-03 | valid loss: 5.576916e-03 
      	| train loss (relative): 1.212323e-01 | valid loss (relative): 1.216151e-01 
Epoch 6 use: 342.19 second.

epoch 7 starting......
Epoch:  7 | train loss: 5.424156e-03 | valid loss: 5.271961e-03 
      	| train loss (relative): 1.172516e-01 | valid loss (relative): 1.125258e-01 
Epoch 7 use: 382.03 second.

epoch 8 starting......
Epoch:  8 | train loss: 5.166412e-03 | valid loss: 5.061606e-03 
      	| train loss (relative): 1.114242e-01 | valid loss (relative): 1.086126e-01 
Epoch 8 use: 369.55 second.

epoch 9 starting......
Epoch:  9 | train loss: 4.958219e-03 | valid loss: 4.838642e-03 
      	| train loss (relative): 1.063381e-01 | valid loss (relative): 1.026646e-01 
Epoch 9 use: 325.15 second.

epoch 10 starting......
Epoch:  10 | train loss: 4.756297e-03 | valid loss: 4.663718e-03 
      	| train loss (relative): 1.011967e-01 | valid loss (relative): 9.946746e-02 
Epoch 10 use: 321.36 second.

epoch 11 starting......
Epoch:  11 | train loss: 4.620975e-03 | valid loss: 4.586581e-03 
      	| train loss (relative): 9.755274e-02 | valid loss (relative): 9.795517e-02 
Epoch 11 use: 334.12 second.

epoch 12 starting......
Epoch:  12 | train loss: 4.551428e-03 | valid loss: 4.503175e-03 
      	| train loss (relative): 9.570654e-02 | valid loss (relative): 9.340592e-02 
Epoch 12 use: 334.41 second.

epoch 13 starting......
Epoch:  13 | train loss: 4.479165e-03 | valid loss: 4.439346e-03 
      	| train loss (relative): 9.384210e-02 | valid loss (relative): 9.330349e-02 
Epoch 13 use: 337.42 second.

epoch 14 starting......
Epoch:  14 | train loss: 4.407261e-03 | valid loss: 4.419398e-03 
      	| train loss (relative): 9.213000e-02 | valid loss (relative): 9.189819e-02 
Epoch 14 use: 320.53 second.

epoch 15 starting......
Epoch:  15 | train loss: 4.369786e-03 | valid loss: 4.371008e-03 
      	| train loss (relative): 9.123344e-02 | valid loss (relative): 9.134690e-02 
Epoch 15 use: 320.48 second.

epoch 16 starting......
Epoch:  16 | train loss: 4.327860e-03 | valid loss: 4.323290e-03 
      	| train loss (relative): 9.034415e-02 | valid loss (relative): 9.062917e-02 
Epoch 16 use: 314.27 second.

epoch 17 starting......
Epoch:  17 | train loss: 4.265720e-03 | valid loss: 4.297837e-03 
      	| train loss (relative): 8.892308e-02 | valid loss (relative): 8.994112e-02 
Epoch 17 use: 323.48 second.

epoch 18 starting......
Epoch:  18 | train loss: 4.214408e-03 | valid loss: 4.239369e-03 
      	| train loss (relative): 8.777554e-02 | valid loss (relative): 8.789844e-02 
Epoch 18 use: 322.70 second.

epoch 19 starting......
Epoch:  19 | train loss: 4.165933e-03 | valid loss: 4.178807e-03 
      	| train loss (relative): 8.665999e-02 | valid loss (relative): 8.710604e-02 
Epoch 19 use: 314.76 second.

epoch 20 starting......
Epoch:  20 | train loss: 4.114489e-03 | valid loss: 4.155960e-03 
      	| train loss (relative): 8.548758e-02 | valid loss (relative): 8.481223e-02 
Epoch 20 use: 322.05 second.

epoch 21 starting......
Epoch:  21 | train loss: 4.055000e-03 | valid loss: 4.086133e-03 
      	| train loss (relative): 8.416662e-02 | valid loss (relative): 8.631355e-02 
Epoch 21 use: 311.19 second.

epoch 22 starting......
Epoch:  22 | train loss: 3.997157e-03 | valid loss: 4.044860e-03 
      	| train loss (relative): 8.285484e-02 | valid loss (relative): 8.457367e-02 
Epoch 22 use: 335.66 second.

epoch 23 starting......
Epoch:  23 | train loss: 3.946499e-03 | valid loss: 3.982043e-03 
      	| train loss (relative): 8.174853e-02 | valid loss (relative): 8.343408e-02 
Epoch 23 use: 334.91 second.

epoch 24 starting......
Epoch:  24 | train loss: 3.882412e-03 | valid loss: 3.924924e-03 
      	| train loss (relative): 8.034261e-02 | valid loss (relative): 8.138536e-02 
Epoch 24 use: 332.73 second.

epoch 25 starting......
Epoch:  25 | train loss: 3.831670e-03 | valid loss: 3.871717e-03 
      	| train loss (relative): 7.915919e-02 | valid loss (relative): 8.125272e-02 
Epoch 25 use: 374.32 second.

epoch 26 starting......
Epoch:  26 | train loss: 3.775504e-03 | valid loss: 3.831292e-03 
      	| train loss (relative): 7.792502e-02 | valid loss (relative): 7.944331e-02 
Epoch 26 use: 446.88 second.

epoch 27 starting......
Epoch:  27 | train loss: 3.730874e-03 | valid loss: 3.777215e-03 
      	| train loss (relative): 7.689399e-02 | valid loss (relative): 7.719746e-02 
Epoch 27 use: 372.28 second.

epoch 28 starting......
Epoch:  28 | train loss: 3.693268e-03 | valid loss: 3.733488e-03 
      	| train loss (relative): 7.604712e-02 | valid loss (relative): 7.785534e-02 
Epoch 28 use: 365.04 second.

epoch 29 starting......
Epoch:  29 | train loss: 3.658627e-03 | valid loss: 3.719390e-03 
      	| train loss (relative): 7.525899e-02 | valid loss (relative): 7.683709e-02 
Epoch 29 use: 335.55 second.

epoch 30 starting......
Epoch:  30 | train loss: 3.633609e-03 | valid loss: 3.662813e-03 
      	| train loss (relative): 7.464994e-02 | valid loss (relative): 7.647352e-02 
Epoch 30 use: 316.93 second.

epoch 31 starting......
Epoch:  31 | train loss: 3.598811e-03 | valid loss: 3.634009e-03 
      	| train loss (relative): 7.389700e-02 | valid loss (relative): 7.495415e-02 
Epoch 31 use: 307.18 second.

epoch 32 starting......
Epoch:  32 | train loss: 3.560377e-03 | valid loss: 3.605646e-03 
      	| train loss (relative): 7.301906e-02 | valid loss (relative): 7.473204e-02 
Epoch 32 use: 319.99 second.

epoch 33 starting......
Epoch:  33 | train loss: 3.544708e-03 | valid loss: 3.587123e-03 
      	| train loss (relative): 7.264146e-02 | valid loss (relative): 7.346243e-02 
Epoch 33 use: 326.99 second.

epoch 34 starting......
Epoch:  34 | train loss: 3.517229e-03 | valid loss: 3.551871e-03 
      	| train loss (relative): 7.201879e-02 | valid loss (relative): 7.370770e-02 
Epoch 34 use: 324.57 second.

epoch 35 starting......
Epoch:  35 | train loss: 3.502643e-03 | valid loss: 3.573481e-03 
      	| train loss (relative): 7.167319e-02 | valid loss (relative): 7.353233e-02 
Epoch 35 use: 329.19 second.

epoch 36 starting......
Epoch:  36 | train loss: 3.481187e-03 | valid loss: 3.521881e-03 
      	| train loss (relative): 7.121294e-02 | valid loss (relative): 7.194393e-02 
Epoch 36 use: 331.93 second.

epoch 37 starting......
Epoch:  37 | train loss: 3.455760e-03 | valid loss: 3.500076e-03 
      	| train loss (relative): 7.064889e-02 | valid loss (relative): 7.164832e-02 
Epoch 37 use: 310.99 second.

epoch 38 starting......
Epoch:  38 | train loss: 3.438307e-03 | valid loss: 3.508999e-03 
      	| train loss (relative): 7.024819e-02 | valid loss (relative): 7.244469e-02 
Epoch 38 use: 332.44 second.

epoch 39 starting......
Epoch:  39 | train loss: 3.418313e-03 | valid loss: 3.458753e-03 
      	| train loss (relative): 6.977601e-02 | valid loss (relative): 7.157697e-02 
Epoch 39 use: 346.69 second.

epoch 40 starting......
Epoch:  40 | train loss: 3.392703e-03 | valid loss: 3.428937e-03 
      	| train loss (relative): 6.923032e-02 | valid loss (relative): 6.972431e-02 
Epoch 40 use: 361.61 second.

epoch 41 starting......
Epoch:  41 | train loss: 3.387322e-03 | valid loss: 3.450337e-03 
      	| train loss (relative): 6.911113e-02 | valid loss (relative): 6.937692e-02 
Epoch 41 use: 332.74 second.

epoch 42 starting......
Epoch:  42 | train loss: 3.370430e-03 | valid loss: 3.423688e-03 
      	| train loss (relative): 6.873133e-02 | valid loss (relative): 7.012779e-02 
Epoch 42 use: 333.78 second.

epoch 43 starting......
Epoch:  43 | train loss: 3.355798e-03 | valid loss: 3.400214e-03 
      	| train loss (relative): 6.840563e-02 | valid loss (relative): 6.957071e-02 
Epoch 43 use: 314.76 second.

epoch 44 starting......
Epoch:  44 | train loss: 3.338586e-03 | valid loss: 3.380015e-03 
      	| train loss (relative): 6.805125e-02 | valid loss (relative): 6.924337e-02 
Epoch 44 use: 363.99 second.

epoch 45 starting......
Epoch:  45 | train loss: 3.324570e-03 | valid loss: 3.373059e-03 
      	| train loss (relative): 6.772073e-02 | valid loss (relative): 6.948569e-02 
Epoch 45 use: 349.18 second.

epoch 46 starting......
Epoch:  46 | train loss: 3.316302e-03 | valid loss: 3.358199e-03 
      	| train loss (relative): 6.756071e-02 | valid loss (relative): 6.864096e-02 
Epoch 46 use: 345.96 second.

epoch 47 starting......
Epoch:  47 | train loss: 3.302832e-03 | valid loss: 3.342885e-03 
      	| train loss (relative): 6.727611e-02 | valid loss (relative): 6.758808e-02 
Epoch 47 use: 363.57 second.

epoch 48 starting......
Epoch:  48 | train loss: 3.290115e-03 | valid loss: 3.333836e-03 
      	| train loss (relative): 6.695852e-02 | valid loss (relative): 6.779435e-02 
Epoch 48 use: 361.64 second.

epoch 49 starting......
Epoch:  49 | train loss: 3.277735e-03 | valid loss: 3.320336e-03 
      	| train loss (relative): 6.668282e-02 | valid loss (relative): 6.777720e-02 
Epoch 49 use: 348.11 second.

epoch 50 starting......
Epoch:  50 | train loss: 3.269871e-03 | valid loss: 3.322352e-03 
      	| train loss (relative): 6.652360e-02 | valid loss (relative): 6.827293e-02 
Epoch 50 use: 328.84 second.

epoch 51 starting......
Epoch:  51 | train loss: 3.258304e-03 | valid loss: 3.310997e-03 
      	| train loss (relative): 6.630325e-02 | valid loss (relative): 6.820261e-02 
Epoch 51 use: 336.01 second.

epoch 52 starting......
Epoch:  52 | train loss: 3.240215e-03 | valid loss: 3.277076e-03 
      	| train loss (relative): 6.590374e-02 | valid loss (relative): 6.668131e-02 
Epoch 52 use: 319.44 second.

epoch 53 starting......
Epoch:  53 | train loss: 3.226220e-03 | valid loss: 3.275977e-03 
      	| train loss (relative): 6.559042e-02 | valid loss (relative): 6.657721e-02 
Epoch 53 use: 325.52 second.

epoch 54 starting......
Epoch:  54 | train loss: 3.219940e-03 | valid loss: 3.270194e-03 
      	| train loss (relative): 6.547198e-02 | valid loss (relative): 6.675103e-02 
Epoch 54 use: 330.75 second.

epoch 55 starting......
Epoch:  55 | train loss: 3.219337e-03 | valid loss: 3.255602e-03 
      	| train loss (relative): 6.544769e-02 | valid loss (relative): 6.687541e-02 
Epoch 55 use: 319.60 second.

epoch 56 starting......
Epoch:  56 | train loss: 3.196842e-03 | valid loss: 3.229540e-03 
      	| train loss (relative): 6.496642e-02 | valid loss (relative): 6.650675e-02 
Epoch 56 use: 325.01 second.

epoch 57 starting......
Epoch:  57 | train loss: 3.189309e-03 | valid loss: 3.220154e-03 
      	| train loss (relative): 6.480174e-02 | valid loss (relative): 6.562939e-02 
Epoch 57 use: 339.51 second.

epoch 58 starting......
Epoch:  58 | train loss: 3.177030e-03 | valid loss: 3.209818e-03 
      	| train loss (relative): 6.454998e-02 | valid loss (relative): 6.600903e-02 
Epoch 58 use: 318.38 second.

epoch 59 starting......
Epoch:  59 | train loss: 3.166015e-03 | valid loss: 3.212292e-03 
      	| train loss (relative): 6.430262e-02 | valid loss (relative): 6.644620e-02 
Epoch 59 use: 316.59 second.

epoch 60 starting......
Epoch:  60 | train loss: 3.167716e-03 | valid loss: 3.197144e-03 
      	| train loss (relative): 6.434961e-02 | valid loss (relative): 6.572342e-02 
Epoch 60 use: 315.04 second.

epoch 61 starting......
Epoch:  61 | train loss: 3.147742e-03 | valid loss: 3.183114e-03 
      	| train loss (relative): 6.392346e-02 | valid loss (relative): 6.480701e-02 
Epoch 61 use: 324.40 second.

epoch 62 starting......
Epoch:  62 | train loss: 3.141090e-03 | valid loss: 3.180484e-03 
      	| train loss (relative): 6.377776e-02 | valid loss (relative): 6.441307e-02 
Epoch 62 use: 318.48 second.

epoch 63 starting......
Epoch:  63 | train loss: 3.133923e-03 | valid loss: 3.182337e-03 
      	| train loss (relative): 6.362739e-02 | valid loss (relative): 6.477931e-02 
Epoch 63 use: 325.52 second.

epoch 64 starting......
Epoch:  64 | train loss: 3.127393e-03 | valid loss: 3.155291e-03 
      	| train loss (relative): 6.348821e-02 | valid loss (relative): 6.381978e-02 
Epoch 64 use: 319.82 second.

epoch 65 starting......
Epoch:  65 | train loss: 3.112211e-03 | valid loss: 3.152991e-03 
      	| train loss (relative): 6.315353e-02 | valid loss (relative): 6.406662e-02 
Epoch 65 use: 316.44 second.

epoch 66 starting......
Epoch:  66 | train loss: 3.102290e-03 | valid loss: 3.142770e-03 
      	| train loss (relative): 6.294635e-02 | valid loss (relative): 6.356449e-02 
Epoch 66 use: 337.26 second.

epoch 67 starting......
Epoch:  67 | train loss: 3.094520e-03 | valid loss: 3.133060e-03 
      	| train loss (relative): 6.277947e-02 | valid loss (relative): 6.441433e-02 
Epoch 67 use: 339.02 second.

epoch 68 starting......
Epoch:  68 | train loss: 3.084735e-03 | valid loss: 3.126025e-03 
      	| train loss (relative): 6.256570e-02 | valid loss (relative): 6.356245e-02 
Epoch 68 use: 321.32 second.

epoch 69 starting......
Epoch:  69 | train loss: 3.082723e-03 | valid loss: 3.123011e-03 
      	| train loss (relative): 6.252791e-02 | valid loss (relative): 6.391759e-02 
Epoch 69 use: 317.92 second.

epoch 70 starting......
Epoch:  70 | train loss: 3.071253e-03 | valid loss: 3.117865e-03 
      	| train loss (relative): 6.228383e-02 | valid loss (relative): 6.390598e-02 
Epoch 70 use: 324.73 second.

epoch 71 starting......
Epoch:  71 | train loss: 3.063300e-03 | valid loss: 3.094157e-03 
      	| train loss (relative): 6.211307e-02 | valid loss (relative): 6.258867e-02 
Epoch 71 use: 323.42 second.

epoch 72 starting......
Epoch:  72 | train loss: 3.052333e-03 | valid loss: 3.097643e-03 
      	| train loss (relative): 6.186292e-02 | valid loss (relative): 6.368941e-02 
Epoch 72 use: 324.77 second.

epoch 73 starting......
Epoch:  73 | train loss: 3.047413e-03 | valid loss: 3.092681e-03 
      	| train loss (relative): 6.174176e-02 | valid loss (relative): 6.326392e-02 
Epoch 73 use: 317.71 second.

epoch 74 starting......
Epoch:  74 | train loss: 3.034816e-03 | valid loss: 3.083593e-03 
      	| train loss (relative): 6.147204e-02 | valid loss (relative): 6.216995e-02 
Epoch 74 use: 318.77 second.

epoch 75 starting......
Epoch:  75 | train loss: 3.027362e-03 | valid loss: 3.060448e-03 
      	| train loss (relative): 6.133210e-02 | valid loss (relative): 6.255230e-02 
Epoch 75 use: 327.77 second.

epoch 76 starting......
Epoch:  76 | train loss: 3.019226e-03 | valid loss: 3.057427e-03 
      	| train loss (relative): 6.114142e-02 | valid loss (relative): 6.224585e-02 
Epoch 76 use: 344.15 second.

epoch 77 starting......
Epoch:  77 | train loss: 3.017129e-03 | valid loss: 3.076501e-03 
      	| train loss (relative): 6.109690e-02 | valid loss (relative): 6.301836e-02 
Epoch 77 use: 332.61 second.

epoch 78 starting......
Epoch:  78 | train loss: 3.009641e-03 | valid loss: 3.038772e-03 
      	| train loss (relative): 6.093608e-02 | valid loss (relative): 6.183755e-02 
Epoch 78 use: 314.81 second.

epoch 79 starting......
Epoch:  79 | train loss: 2.992268e-03 | valid loss: 3.035000e-03 
      	| train loss (relative): 6.055373e-02 | valid loss (relative): 6.160886e-02 
Epoch 79 use: 332.35 second.

epoch 80 starting......
Epoch:  80 | train loss: 2.984379e-03 | valid loss: 3.018954e-03 
      	| train loss (relative): 6.038217e-02 | valid loss (relative): 6.123474e-02 
Epoch 80 use: 323.06 second.

epoch 81 starting......
Epoch:  81 | train loss: 2.980561e-03 | valid loss: 3.014677e-03 
      	| train loss (relative): 6.030369e-02 | valid loss (relative): 6.099199e-02 
Epoch 81 use: 327.60 second.

epoch 82 starting......
Epoch:  82 | train loss: 2.968655e-03 | valid loss: 3.014989e-03 
      	| train loss (relative): 6.003765e-02 | valid loss (relative): 6.168833e-02 
Epoch 82 use: 323.21 second.

epoch 83 starting......
Epoch:  83 | train loss: 2.964080e-03 | valid loss: 3.032194e-03 
      	| train loss (relative): 5.994049e-02 | valid loss (relative): 6.139841e-02 
Epoch 83 use: 312.33 second.

epoch 84 starting......
Epoch:  84 | train loss: 2.956799e-03 | valid loss: 2.980715e-03 
      	| train loss (relative): 5.977648e-02 | valid loss (relative): 6.081135e-02 
Epoch 84 use: 331.31 second.

epoch 85 starting......
Epoch:  85 | train loss: 2.938085e-03 | valid loss: 2.978772e-03 
      	| train loss (relative): 5.938708e-02 | valid loss (relative): 6.047266e-02 
Epoch 85 use: 329.17 second.

epoch 86 starting......
Epoch:  86 | train loss: 2.934933e-03 | valid loss: 2.978317e-03 
      	| train loss (relative): 5.930897e-02 | valid loss (relative): 6.002409e-02 
Epoch 86 use: 328.12 second.

epoch 87 starting......
Epoch:  87 | train loss: 2.932718e-03 | valid loss: 2.978278e-03 
      	| train loss (relative): 5.926833e-02 | valid loss (relative): 6.005654e-02 
Epoch 87 use: 325.53 second.

epoch 88 starting......
Epoch:  88 | train loss: 2.923616e-03 | valid loss: 2.962620e-03 
      	| train loss (relative): 5.908279e-02 | valid loss (relative): 6.040561e-02 
Epoch 88 use: 327.93 second.

epoch 89 starting......
Epoch:  89 | train loss: 2.909916e-03 | valid loss: 2.949747e-03 
      	| train loss (relative): 5.878343e-02 | valid loss (relative): 6.018407e-02 
Epoch 89 use: 332.97 second.

epoch 90 starting......
Epoch:  90 | train loss: 2.908999e-03 | valid loss: 2.951709e-03 
      	| train loss (relative): 5.876328e-02 | valid loss (relative): 5.945736e-02 
Epoch 90 use: 316.61 second.

epoch 91 starting......
Epoch:  91 | train loss: 2.898969e-03 | valid loss: 2.936120e-03 
      	| train loss (relative): 5.854688e-02 | valid loss (relative): 5.946494e-02 
Epoch 91 use: 325.67 second.

epoch 92 starting......
Epoch:  92 | train loss: 2.891466e-03 | valid loss: 2.928968e-03 
      	| train loss (relative): 5.837852e-02 | valid loss (relative): 5.951748e-02 
Epoch 92 use: 329.96 second.

epoch 93 starting......
Epoch:  93 | train loss: 2.880673e-03 | valid loss: 2.912445e-03 
      	| train loss (relative): 5.815277e-02 | valid loss (relative): 5.902600e-02 
Epoch 93 use: 346.56 second.

epoch 94 starting......
Epoch:  94 | train loss: 2.866831e-03 | valid loss: 2.910218e-03 
      	| train loss (relative): 5.787734e-02 | valid loss (relative): 5.875977e-02 
Epoch 94 use: 343.06 second.

epoch 95 starting......
Epoch:  95 | train loss: 2.861642e-03 | valid loss: 2.922468e-03 
      	| train loss (relative): 5.775752e-02 | valid loss (relative): 5.937776e-02 
Epoch 95 use: 327.47 second.

epoch 96 starting......
Epoch:  96 | train loss: 2.855976e-03 | valid loss: 2.906750e-03 
      	| train loss (relative): 5.764424e-02 | valid loss (relative): 5.913503e-02 
Epoch 96 use: 335.74 second.

epoch 97 starting......
Epoch:  97 | train loss: 2.846550e-03 | valid loss: 2.907602e-03 
      	| train loss (relative): 5.743978e-02 | valid loss (relative): 5.941694e-02 
Epoch 97 use: 332.98 second.

epoch 98 starting......
Epoch:  98 | train loss: 2.841180e-03 | valid loss: 2.880122e-03 
      	| train loss (relative): 5.733251e-02 | valid loss (relative): 5.775211e-02 
Epoch 98 use: 367.86 second.

epoch 99 starting......
Epoch:  99 | train loss: 2.827705e-03 | valid loss: 2.877838e-03 
      	| train loss (relative): 5.704241e-02 | valid loss (relative): 5.762016e-02 
Epoch 99 use: 360.51 second.

test MSE Error: 2.891569e-03 | relative MSE Error: 5.792256e-02 
 Total time used for training: 9.34 hour.
MESLoss saved to  /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_MSELoss_Latent_128_nearest_neighbouring_True_SFC_nums_3_startlr_0.0005_n_epoches_100.txt
relative MSELoss saved to  /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_reMSELoss_Latent_128_nearest_neighbouring_True_SFC_nums_3_startlr_0.0005_n_epoches_100.txt
model saved to /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_128_Nearest_neighbouring_True_SFC_nums_3_startlr_0.0005_n_epoches_100.pth
model_dict saved to /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_128_Nearest_neighbouring_True_SFC_nums_3_startlr_0.0005_n_epoches_100_dict.pth
... Training slugflow data completed, Run finished Fri 13 Aug 22:33:43 BST 2021 ...
{'vtu_dir': '/rds/general/user/jy220/home/slugflow/', 'data_dir': '/rds/general/user/jy220/home/tensors/', 'data_type': 'tensors', 'vtu_fields': 'Component1::ComponentMassFractionPhase1, phase1::Velocity', 'variational': 'False', 'optimizer': 'Adamax', 'mode': 'train', 'structured': 'False', 'changing_lr': 'False', 'self_concat': '2', 'dimension': '3', 'sfc_nums': '3', 'components': '4', 'nearest_neighbouring': 'True', 'dims_latent': '128', 'activation': 'Tanh', 'tk_file': '/rds/general/user/jy220/home/slugflow_tk.pt', 'tb_file': '/rds/general/user/jy220/home/slugflow_tb.pt', 'sfc_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/sfcs.pt', 'inv_sfc_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/inv_sfcs.pt', 'coords_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/coords.pt', 'cells_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/cells.npy', 'state_load': '/rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_128_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_700_dict.pth', 'batch_size': '16', 'lr': '0.001', 'n_epoches': '10', 'seed': '15', 'visualize': 'False', 'reconstructed_path': 'None', 'reconstruct_start_index': '0', 'reconstruct_end_index': '100', 'save_path': '/rds/general/user/jy220/home/new_results/'}
[[ 2.50825269e-01 -1.67256990e-02 -1.22268730e-03]
 [ 2.38611959e-01 -5.50627777e-03 -5.06451913e-03]
 [ 2.39454874e-01 -1.59563257e-02  3.96778323e-03]
 ...
 [ 9.11361096e+00 -2.21908381e-02  2.22384902e-03]
 [ 9.11847310e+00 -2.84346931e-02 -2.34839590e-04]
 [ 9.11039661e+00 -2.51498573e-02 -8.70661044e-03]]
{'tetra': array([[      0,       1,       2,       3],
       [      4,       5,       6,       7],
       [      8,       9,      10,      11],
       ...,
       [1342744, 1342745, 1342746, 1342747],
       [1342748, 1342749, 1342750, 1342751],
       [1342752, 1342753, 1342754, 1342755]])}
vtu fields:  ['Component1::ComponentMassFractionPhase1', 'phase1::Velocity'] 
 structured  False 
 activation  Tanh() 
 self concat  2 
 sfc_nums  3 

dims_latent  128 
 components  4 
 nearest_neighbouring  True 

visualize  False 
 sample number  1706 

reading sfc nums......
[array([  41684,   41685,   41674, ..., 1235560, 1235564, 1235558]), array([954289, 954290, 954291, ..., 420685, 420687, 420688]), array([  40156,   40155,   40158, ..., 1123478, 1123475, 1123467])]
reading inverse sfc nums......
[array([   8146,    8145,    8144, ..., 1274444, 1274445, 1273166]), array([547280, 547283, 593037, ..., 933933, 933932, 864488]), array([ 66437,  66439, 122512, ..., 459416, 415130, 459418])]
tk:  tensor([0.7581, 0.0418, 0.1840, 0.1464], dtype=torch.float64) 

tb:  tensor([-0.3816, -0.3618, -0.0308,  0.1446], dtype=torch.float64) 

length of train set: 1505 

length of valid set: 100 

length of test set: 101 

... Training slugflow data completed, Run finished Sat 14 Aug 08:01:52 BST 2021 ...
{'vtu_dir': '/rds/general/user/jy220/home/slugflow/', 'data_dir': '/rds/general/user/jy220/home/tensors/', 'data_type': 'tensors', 'vtu_fields': 'Component1::ComponentMassFractionPhase1, phase1::Velocity', 'variational': 'False', 'optimizer': 'Adamax', 'mode': 'train', 'structured': 'False', 'changing_lr': 'False', 'self_concat': '2', 'dimension': '3', 'sfc_nums': '3', 'components': '4', 'nearest_neighbouring': 'True', 'dims_latent': '128', 'activation': 'Tanh', 'tk_file': '/rds/general/user/jy220/home/slugflow_tk.pt', 'tb_file': '/rds/general/user/jy220/home/slugflow_tb.pt', 'sfc_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/sfcs.pt', 'inv_sfc_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/inv_sfcs.pt', 'coords_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/coords.pt', 'cells_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/cells.npy', 'state_load': '/rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_128_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_700_dict.pth', 'batch_size': '16', 'lr': '0.001', 'n_epoches': '10', 'seed': '15', 'visualize': 'False', 'reconstructed_path': 'None', 'reconstruct_start_index': '0', 'reconstruct_end_index': '100', 'save_path': '/rds/general/user/jy220/home/new_results/'}
[[ 2.50825269e-01 -1.67256990e-02 -1.22268730e-03]
 [ 2.38611959e-01 -5.50627777e-03 -5.06451913e-03]
 [ 2.39454874e-01 -1.59563257e-02  3.96778323e-03]
 ...
 [ 9.11361096e+00 -2.21908381e-02  2.22384902e-03]
 [ 9.11847310e+00 -2.84346931e-02 -2.34839590e-04]
 [ 9.11039661e+00 -2.51498573e-02 -8.70661044e-03]]
{'tetra': array([[      0,       1,       2,       3],
       [      4,       5,       6,       7],
       [      8,       9,      10,      11],
       ...,
       [1342744, 1342745, 1342746, 1342747],
       [1342748, 1342749, 1342750, 1342751],
       [1342752, 1342753, 1342754, 1342755]])}
vtu fields:  ['Component1::ComponentMassFractionPhase1', 'phase1::Velocity'] 
 structured  False 
 activation  Tanh() 
 self concat  2 
 sfc_nums  3 

dims_latent  128 
 components  4 
 nearest_neighbouring  True 

visualize  False 
 sample number  1706 

reading sfc nums......
[array([  41684,   41685,   41674, ..., 1235560, 1235564, 1235558]), array([954289, 954290, 954291, ..., 420685, 420687, 420688]), array([  40156,   40155,   40158, ..., 1123478, 1123475, 1123467])]
reading inverse sfc nums......
[array([   8146,    8145,    8144, ..., 1274444, 1274445, 1273166]), array([547280, 547283, 593037, ..., 933933, 933932, 864488]), array([ 66437,  66439, 122512, ..., 459416, 415130, 459418])]
tk:  tensor([0.7581, 0.0418, 0.1840, 0.1464], dtype=torch.float64) 

tb:  tensor([-0.3816, -0.3618, -0.0308,  0.1446], dtype=torch.float64) 

length of train set: 1505 

length of valid set: 100 

length of test set: 101 

torch device num: 4 

Let's use 4 GPUs!
epoch 700 starting......
Epoch:  700 | train loss: 1.802110e-03 | valid loss: 1.687351e-03 
      	| train loss (relative): 3.539804e-02 | valid loss (relative): 3.318857e-02 
Epoch 700 use: 490.26 second.

epoch 701 starting......
Epoch:  701 | train loss: 1.581742e-03 | valid loss: 1.674081e-03 
      	| train loss (relative): 3.107321e-02 | valid loss (relative): 3.292063e-02 
Epoch 701 use: 391.62 second.

epoch 702 starting......
Epoch:  702 | train loss: 1.572431e-03 | valid loss: 1.672274e-03 
      	| train loss (relative): 3.087944e-02 | valid loss (relative): 3.288488e-02 
Epoch 702 use: 365.61 second.

epoch 703 starting......
Epoch:  703 | train loss: 1.569016e-03 | valid loss: 1.672230e-03 
      	| train loss (relative): 3.080916e-02 | valid loss (relative): 3.287357e-02 
Epoch 703 use: 376.35 second.

epoch 704 starting......
Epoch:  704 | train loss: 1.566937e-03 | valid loss: 1.671821e-03 
      	| train loss (relative): 3.076817e-02 | valid loss (relative): 3.286535e-02 
Epoch 704 use: 341.90 second.

epoch 705 starting......
Epoch:  705 | train loss: 1.565971e-03 | valid loss: 1.671381e-03 
      	| train loss (relative): 3.074342e-02 | valid loss (relative): 3.284524e-02 
Epoch 705 use: 370.45 second.

epoch 706 starting......
Epoch:  706 | train loss: 1.565394e-03 | valid loss: 1.672228e-03 
      	| train loss (relative): 3.073156e-02 | valid loss (relative): 3.292795e-02 
Epoch 706 use: 365.49 second.

epoch 707 starting......
Epoch:  707 | train loss: 1.565387e-03 | valid loss: 1.673605e-03 
      	| train loss (relative): 3.073676e-02 | valid loss (relative): 3.292467e-02 
Epoch 707 use: 418.44 second.

epoch 708 starting......
Epoch:  708 | train loss: 1.565190e-03 | valid loss: 1.677142e-03 
      	| train loss (relative): 3.073128e-02 | valid loss (relative): 3.301415e-02 
Epoch 708 use: 362.78 second.

epoch 709 starting......
Epoch:  709 | train loss: 1.567398e-03 | valid loss: 1.676296e-03 
      	| train loss (relative): 3.077140e-02 | valid loss (relative): 3.294749e-02 
Epoch 709 use: 380.01 second.

test MSE Error: 1.682422e-03 | relative MSE Error: 3.321648e-02 
 Total time used for training: 1.08 hour.
MESLoss saved to  /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_MSELoss_Latent_128_nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_710.txt
relative MSELoss saved to  /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_reMSELoss_Latent_128_nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_710.txt
model saved to /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_128_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_710.pth
model_dict saved to /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_128_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_710_dict.pth
... Training slugflow data completed, Run finished Sat 14 Aug 09:32:13 BST 2021 ...
{'vtu_dir': '/rds/general/user/jy220/home/slugflow/', 'data_dir': '/rds/general/user/jy220/home/tensors/', 'data_type': 'tensors', 'vtu_fields': 'Component1::ComponentMassFractionPhase1, phase1::Velocity', 'variational': 'False', 'optimizer': 'Adamax', 'mode': 'train', 'structured': 'False', 'changing_lr': 'False', 'self_concat': '2', 'dimension': '3', 'sfc_nums': '3', 'components': '4', 'nearest_neighbouring': 'True', 'dims_latent': '128', 'activation': 'Tanh', 'tk_file': '/rds/general/user/jy220/home/slugflow_tk.pt', 'tb_file': '/rds/general/user/jy220/home/slugflow_tb.pt', 'sfc_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/sfcs.pt', 'inv_sfc_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/inv_sfcs.pt', 'coords_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/coords.pt', 'cells_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/cells.npy', 'state_load': '/rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_128_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_700_dict.pth', 'batch_size': '16', 'lr': '0.001', 'n_epoches': '200', 'seed': '15', 'visualize': 'False', 'reconstructed_path': 'None', 'reconstruct_start_index': '0', 'reconstruct_end_index': '100', 'save_path': '/rds/general/user/jy220/home/new_results/'}
[[ 2.50825269e-01 -1.67256990e-02 -1.22268730e-03]
 [ 2.38611959e-01 -5.50627777e-03 -5.06451913e-03]
 [ 2.39454874e-01 -1.59563257e-02  3.96778323e-03]
 ...
 [ 9.11361096e+00 -2.21908381e-02  2.22384902e-03]
 [ 9.11847310e+00 -2.84346931e-02 -2.34839590e-04]
 [ 9.11039661e+00 -2.51498573e-02 -8.70661044e-03]]
{'tetra': array([[      0,       1,       2,       3],
       [      4,       5,       6,       7],
       [      8,       9,      10,      11],
       ...,
       [1342744, 1342745, 1342746, 1342747],
       [1342748, 1342749, 1342750, 1342751],
       [1342752, 1342753, 1342754, 1342755]])}
vtu fields:  ['Component1::ComponentMassFractionPhase1', 'phase1::Velocity'] 
 structured  False 
 activation  Tanh() 
 self concat  2 
 sfc_nums  3 

dims_latent  128 
 components  4 
 nearest_neighbouring  True 

visualize  False 
 sample number  1706 

reading sfc nums......
[array([  41684,   41685,   41674, ..., 1235560, 1235564, 1235558]), array([954289, 954290, 954291, ..., 420685, 420687, 420688]), array([  40156,   40155,   40158, ..., 1123478, 1123475, 1123467])]
reading inverse sfc nums......
[array([   8146,    8145,    8144, ..., 1274444, 1274445, 1273166]), array([547280, 547283, 593037, ..., 933933, 933932, 864488]), array([ 66437,  66439, 122512, ..., 459416, 415130, 459418])]
tk:  tensor([0.7581, 0.0418, 0.1840, 0.1464], dtype=torch.float64) 

tb:  tensor([-0.3816, -0.3618, -0.0308,  0.1446], dtype=torch.float64) 

length of train set: 1505 

length of valid set: 100 

length of test set: 101 

torch device num: 4 

Let's use 4 GPUs!
epoch 700 starting......
Epoch:  700 | train loss: 1.804287e-03 | valid loss: 1.593299e-03 
      	| train loss (relative): 3.549029e-02 | valid loss (relative): 3.135055e-02 
Epoch 700 use: 443.16 second.

epoch 701 starting......
Epoch:  701 | train loss: 1.600755e-03 | valid loss: 1.580981e-03 
      	| train loss (relative): 3.145672e-02 | valid loss (relative): 3.112648e-02 
Epoch 701 use: 389.09 second.

epoch 702 starting......
Epoch:  702 | train loss: 1.591536e-03 | valid loss: 1.579199e-03 
      	| train loss (relative): 3.126890e-02 | valid loss (relative): 3.104934e-02 
Epoch 702 use: 362.27 second.

epoch 703 starting......
Epoch:  703 | train loss: 1.588137e-03 | valid loss: 1.577580e-03 
      	| train loss (relative): 3.120137e-02 | valid loss (relative): 3.110289e-02 
Epoch 703 use: 360.26 second.

epoch 704 starting......
Epoch:  704 | train loss: 1.586759e-03 | valid loss: 1.578854e-03 
      	| train loss (relative): 3.117263e-02 | valid loss (relative): 3.107957e-02 
Epoch 704 use: 346.16 second.

epoch 705 starting......
Epoch:  705 | train loss: 1.585865e-03 | valid loss: 1.579224e-03 
      	| train loss (relative): 3.115219e-02 | valid loss (relative): 3.107776e-02 
Epoch 705 use: 349.79 second.

epoch 706 starting......
Epoch:  706 | train loss: 1.584752e-03 | valid loss: 1.578403e-03 
      	| train loss (relative): 3.112742e-02 | valid loss (relative): 3.107832e-02 
Epoch 706 use: 345.66 second.

epoch 707 starting......
Epoch:  707 | train loss: 1.585089e-03 | valid loss: 1.581132e-03 
      	| train loss (relative): 3.113764e-02 | valid loss (relative): 3.114798e-02 
Epoch 707 use: 337.88 second.

epoch 708 starting......
Epoch:  708 | train loss: 1.586299e-03 | valid loss: 1.585211e-03 
      	| train loss (relative): 3.115959e-02 | valid loss (relative): 3.117400e-02 
Epoch 708 use: 331.97 second.

epoch 709 starting......
Epoch:  709 | train loss: 1.588735e-03 | valid loss: 1.590648e-03 
      	| train loss (relative): 3.120553e-02 | valid loss (relative): 3.146707e-02 
Epoch 709 use: 338.52 second.

epoch 710 starting......
Epoch:  710 | train loss: 1.589269e-03 | valid loss: 1.588456e-03 
      	| train loss (relative): 3.121985e-02 | valid loss (relative): 3.134655e-02 
Epoch 710 use: 328.83 second.

epoch 711 starting......
Epoch:  711 | train loss: 1.590387e-03 | valid loss: 1.589626e-03 
      	| train loss (relative): 3.124552e-02 | valid loss (relative): 3.126878e-02 
Epoch 711 use: 329.16 second.

epoch 712 starting......
Epoch:  712 | train loss: 1.591840e-03 | valid loss: 1.590935e-03 
      	| train loss (relative): 3.127208e-02 | valid loss (relative): 3.132891e-02 
Epoch 712 use: 327.67 second.

epoch 713 starting......
Epoch:  713 | train loss: 1.595959e-03 | valid loss: 1.594957e-03 
      	| train loss (relative): 3.134838e-02 | valid loss (relative): 3.150963e-02 
Epoch 713 use: 331.49 second.

epoch 714 starting......
Epoch:  714 | train loss: 1.591016e-03 | valid loss: 1.587407e-03 
      	| train loss (relative): 3.126106e-02 | valid loss (relative): 3.123493e-02 
Epoch 714 use: 331.23 second.

epoch 715 starting......
Epoch:  715 | train loss: 1.586655e-03 | valid loss: 1.589134e-03 
      	| train loss (relative): 3.116297e-02 | valid loss (relative): 3.120160e-02 
Epoch 715 use: 454.03 second.

epoch 716 starting......
Epoch:  716 | train loss: 1.588616e-03 | valid loss: 1.587468e-03 
      	| train loss (relative): 3.120355e-02 | valid loss (relative): 3.126189e-02 
Epoch 716 use: 317.22 second.

epoch 717 starting......
Epoch:  717 | train loss: 1.589820e-03 | valid loss: 1.597068e-03 
      	| train loss (relative): 3.123068e-02 | valid loss (relative): 3.144960e-02 
Epoch 717 use: 337.30 second.

epoch 718 starting......
Epoch:  718 | train loss: 1.595813e-03 | valid loss: 1.593899e-03 
      	| train loss (relative): 3.134611e-02 | valid loss (relative): 3.155650e-02 
Epoch 718 use: 334.88 second.

epoch 719 starting......
Epoch:  719 | train loss: 1.588329e-03 | valid loss: 1.590091e-03 
      	| train loss (relative): 3.119562e-02 | valid loss (relative): 3.144415e-02 
Epoch 719 use: 324.53 second.

epoch 720 starting......
Epoch:  720 | train loss: 1.588635e-03 | valid loss: 1.593690e-03 
      	| train loss (relative): 3.120174e-02 | valid loss (relative): 3.150332e-02 
Epoch 720 use: 338.50 second.

epoch 721 starting......
Epoch:  721 | train loss: 1.588086e-03 | valid loss: 1.593248e-03 
      	| train loss (relative): 3.119021e-02 | valid loss (relative): 3.139009e-02 
Epoch 721 use: 337.52 second.

epoch 722 starting......
Epoch:  722 | train loss: 1.587097e-03 | valid loss: 1.590726e-03 
      	| train loss (relative): 3.116876e-02 | valid loss (relative): 3.129208e-02 
Epoch 722 use: 318.90 second.

epoch 723 starting......
Epoch:  723 | train loss: 1.584208e-03 | valid loss: 1.591672e-03 
      	| train loss (relative): 3.110723e-02 | valid loss (relative): 3.130610e-02 
Epoch 723 use: 353.87 second.

epoch 724 starting......
Epoch:  724 | train loss: 1.585126e-03 | valid loss: 1.594049e-03 
      	| train loss (relative): 3.112874e-02 | valid loss (relative): 3.146165e-02 
Epoch 724 use: 336.98 second.

epoch 725 starting......
Epoch:  725 | train loss: 1.589015e-03 | valid loss: 1.596082e-03 
      	| train loss (relative): 3.120285e-02 | valid loss (relative): 3.174969e-02 
Epoch 725 use: 343.10 second.

epoch 726 starting......
Epoch:  726 | train loss: 1.590233e-03 | valid loss: 1.604469e-03 
      	| train loss (relative): 3.123142e-02 | valid loss (relative): 3.161942e-02 
Epoch 726 use: 329.35 second.

epoch 727 starting......
Epoch:  727 | train loss: 1.590027e-03 | valid loss: 1.598866e-03 
      	| train loss (relative): 3.123250e-02 | valid loss (relative): 3.146055e-02 
Epoch 727 use: 349.56 second.

epoch 728 starting......
Epoch:  728 | train loss: 1.589677e-03 | valid loss: 1.593526e-03 
      	| train loss (relative): 3.122099e-02 | valid loss (relative): 3.119417e-02 
Epoch 728 use: 351.21 second.

epoch 729 starting......
Epoch:  729 | train loss: 1.590589e-03 | valid loss: 1.601912e-03 
      	| train loss (relative): 3.123914e-02 | valid loss (relative): 3.160510e-02 
Epoch 729 use: 351.11 second.

epoch 730 starting......
Epoch:  730 | train loss: 1.585636e-03 | valid loss: 1.588676e-03 
      	| train loss (relative): 3.114064e-02 | valid loss (relative): 3.120689e-02 
Epoch 730 use: 346.93 second.

epoch 731 starting......
Epoch:  731 | train loss: 1.588975e-03 | valid loss: 1.602518e-03 
      	| train loss (relative): 3.120713e-02 | valid loss (relative): 3.156987e-02 
Epoch 731 use: 344.24 second.

epoch 732 starting......
Epoch:  732 | train loss: 1.591228e-03 | valid loss: 1.595766e-03 
      	| train loss (relative): 3.125304e-02 | valid loss (relative): 3.137460e-02 
Epoch 732 use: 341.97 second.

epoch 733 starting......
Epoch:  733 | train loss: 1.583055e-03 | valid loss: 1.591659e-03 
      	| train loss (relative): 3.108478e-02 | valid loss (relative): 3.127735e-02 
Epoch 733 use: 342.88 second.

epoch 734 starting......
Epoch:  734 | train loss: 1.583456e-03 | valid loss: 1.594881e-03 
      	| train loss (relative): 3.109567e-02 | valid loss (relative): 3.135816e-02 
Epoch 734 use: 327.37 second.

epoch 735 starting......
Epoch:  735 | train loss: 1.588745e-03 | valid loss: 1.608492e-03 
      	| train loss (relative): 3.120164e-02 | valid loss (relative): 3.205186e-02 
Epoch 735 use: 348.13 second.

epoch 736 starting......
Epoch:  736 | train loss: 1.588111e-03 | valid loss: 1.587896e-03 
      	| train loss (relative): 3.119188e-02 | valid loss (relative): 3.116196e-02 
Epoch 736 use: 319.76 second.

epoch 737 starting......
Epoch:  737 | train loss: 1.578672e-03 | valid loss: 1.585936e-03 
      	| train loss (relative): 3.099807e-02 | valid loss (relative): 3.119878e-02 
Epoch 737 use: 341.89 second.

epoch 738 starting......
Epoch:  738 | train loss: 1.583664e-03 | valid loss: 1.601433e-03 
      	| train loss (relative): 3.110076e-02 | valid loss (relative): 3.161566e-02 
Epoch 738 use: 323.77 second.

epoch 739 starting......
Epoch:  739 | train loss: 1.587773e-03 | valid loss: 1.590194e-03 
      	| train loss (relative): 3.118214e-02 | valid loss (relative): 3.133900e-02 
Epoch 739 use: 326.15 second.

epoch 740 starting......
Epoch:  740 | train loss: 1.580307e-03 | valid loss: 1.598248e-03 
      	| train loss (relative): 3.103766e-02 | valid loss (relative): 3.136753e-02 
Epoch 740 use: 321.69 second.

epoch 741 starting......
Epoch:  741 | train loss: 1.587291e-03 | valid loss: 1.588014e-03 
      	| train loss (relative): 3.116920e-02 | valid loss (relative): 3.120861e-02 
Epoch 741 use: 333.25 second.

epoch 742 starting......
Epoch:  742 | train loss: 1.578193e-03 | valid loss: 1.592346e-03 
      	| train loss (relative): 3.098629e-02 | valid loss (relative): 3.140379e-02 
Epoch 742 use: 320.51 second.

epoch 743 starting......
Epoch:  743 | train loss: 1.583923e-03 | valid loss: 1.593446e-03 
      	| train loss (relative): 3.110603e-02 | valid loss (relative): 3.147942e-02 
Epoch 743 use: 325.17 second.

epoch 744 starting......
Epoch:  744 | train loss: 1.582537e-03 | valid loss: 1.589608e-03 
      	| train loss (relative): 3.107831e-02 | valid loss (relative): 3.144843e-02 
Epoch 744 use: 410.69 second.

epoch 745 starting......
Epoch:  745 | train loss: 1.579969e-03 | valid loss: 1.587860e-03 
      	| train loss (relative): 3.102636e-02 | valid loss (relative): 3.107222e-02 
Epoch 745 use: 358.87 second.

epoch 746 starting......
Epoch:  746 | train loss: 1.577677e-03 | valid loss: 1.584344e-03 
      	| train loss (relative): 3.097524e-02 | valid loss (relative): 3.123963e-02 
Epoch 746 use: 340.23 second.

epoch 747 starting......
Epoch:  747 | train loss: 1.575969e-03 | valid loss: 1.595443e-03 
      	| train loss (relative): 3.094747e-02 | valid loss (relative): 3.134245e-02 
Epoch 747 use: 331.79 second.

epoch 748 starting......
Epoch:  748 | train loss: 1.580974e-03 | valid loss: 1.585849e-03 
      	| train loss (relative): 3.104132e-02 | valid loss (relative): 3.116863e-02 
Epoch 748 use: 304.23 second.

epoch 749 starting......
Epoch:  749 | train loss: 1.573518e-03 | valid loss: 1.587761e-03 
      	| train loss (relative): 3.089774e-02 | valid loss (relative): 3.107655e-02 
Epoch 749 use: 304.49 second.

epoch 750 starting......
Epoch:  750 | train loss: 1.577842e-03 | valid loss: 1.585163e-03 
      	| train loss (relative): 3.097980e-02 | valid loss (relative): 3.125529e-02 
Epoch 750 use: 320.33 second.

epoch 751 starting......
Epoch:  751 | train loss: 1.575386e-03 | valid loss: 1.582693e-03 
      	| train loss (relative): 3.093173e-02 | valid loss (relative): 3.117838e-02 
Epoch 751 use: 323.08 second.

epoch 752 starting......
Epoch:  752 | train loss: 1.575255e-03 | valid loss: 1.591411e-03 
      	| train loss (relative): 3.092811e-02 | valid loss (relative): 3.134539e-02 
Epoch 752 use: 311.49 second.

epoch 753 starting......
Epoch:  753 | train loss: 1.577017e-03 | valid loss: 1.586662e-03 
      	| train loss (relative): 3.096481e-02 | valid loss (relative): 3.119137e-02 
Epoch 753 use: 322.26 second.

epoch 754 starting......
Epoch:  754 | train loss: 1.573427e-03 | valid loss: 1.584236e-03 
      	| train loss (relative): 3.089593e-02 | valid loss (relative): 3.118860e-02 
Epoch 754 use: 313.31 second.

epoch 755 starting......
Epoch:  755 | train loss: 1.571148e-03 | valid loss: 1.580020e-03 
      	| train loss (relative): 3.084475e-02 | valid loss (relative): 3.115693e-02 
Epoch 755 use: 316.03 second.

epoch 756 starting......
Epoch:  756 | train loss: 1.572932e-03 | valid loss: 1.596343e-03 
      	| train loss (relative): 3.087913e-02 | valid loss (relative): 3.158325e-02 
Epoch 756 use: 321.86 second.

epoch 757 starting......
Epoch:  757 | train loss: 1.574681e-03 | valid loss: 1.580975e-03 
      	| train loss (relative): 3.092171e-02 | valid loss (relative): 3.112105e-02 
Epoch 757 use: 332.31 second.

epoch 758 starting......
Epoch:  758 | train loss: 1.571788e-03 | valid loss: 1.577157e-03 
      	| train loss (relative): 3.086131e-02 | valid loss (relative): 3.093529e-02 
Epoch 758 use: 327.82 second.

epoch 759 starting......
Epoch:  759 | train loss: 1.567630e-03 | valid loss: 1.580023e-03 
      	| train loss (relative): 3.077584e-02 | valid loss (relative): 3.099042e-02 
Epoch 759 use: 321.40 second.

epoch 760 starting......
Epoch:  760 | train loss: 1.569244e-03 | valid loss: 1.583086e-03 
      	| train loss (relative): 3.080851e-02 | valid loss (relative): 3.116717e-02 
Epoch 760 use: 301.77 second.

epoch 761 starting......
Epoch:  761 | train loss: 1.571612e-03 | valid loss: 1.588439e-03 
      	| train loss (relative): 3.085535e-02 | valid loss (relative): 3.117121e-02 
Epoch 761 use: 320.45 second.

epoch 762 starting......
Epoch:  762 | train loss: 1.574534e-03 | valid loss: 1.577398e-03 
      	| train loss (relative): 3.091158e-02 | valid loss (relative): 3.097792e-02 
Epoch 762 use: 311.61 second.

epoch 763 starting......
Epoch:  763 | train loss: 1.569336e-03 | valid loss: 1.581970e-03 
      	| train loss (relative): 3.081051e-02 | valid loss (relative): 3.110258e-02 
Epoch 763 use: 315.60 second.

epoch 764 starting......
Epoch:  764 | train loss: 1.570079e-03 | valid loss: 1.582951e-03 
      	| train loss (relative): 3.082383e-02 | valid loss (relative): 3.107150e-02 
Epoch 764 use: 323.80 second.

epoch 765 starting......
Epoch:  765 | train loss: 1.568464e-03 | valid loss: 1.576541e-03 
      	| train loss (relative): 3.078813e-02 | valid loss (relative): 3.111018e-02 
Epoch 765 use: 314.71 second.

epoch 766 starting......
Epoch:  766 | train loss: 1.567924e-03 | valid loss: 1.587047e-03 
      	| train loss (relative): 3.078264e-02 | valid loss (relative): 3.121075e-02 
Epoch 766 use: 295.89 second.

epoch 767 starting......
Epoch:  767 | train loss: 1.573062e-03 | valid loss: 1.590335e-03 
      	| train loss (relative): 3.088708e-02 | valid loss (relative): 3.116265e-02 
Epoch 767 use: 297.64 second.

epoch 768 starting......
Epoch:  768 | train loss: 1.573960e-03 | valid loss: 1.576344e-03 
      	| train loss (relative): 3.089995e-02 | valid loss (relative): 3.096874e-02 
Epoch 768 use: 301.43 second.

epoch 769 starting......
Epoch:  769 | train loss: 1.566215e-03 | valid loss: 1.575292e-03 
      	| train loss (relative): 3.074723e-02 | valid loss (relative): 3.106386e-02 
Epoch 769 use: 306.64 second.

epoch 770 starting......
Epoch:  770 | train loss: 1.562610e-03 | valid loss: 1.573897e-03 
      	| train loss (relative): 3.067476e-02 | valid loss (relative): 3.103475e-02 
Epoch 770 use: 319.02 second.

epoch 771 starting......
Epoch:  771 | train loss: 1.566215e-03 | valid loss: 1.581426e-03 
      	| train loss (relative): 3.075176e-02 | valid loss (relative): 3.107162e-02 
Epoch 771 use: 312.88 second.

epoch 772 starting......
Epoch:  772 | train loss: 1.566900e-03 | valid loss: 1.575724e-03 
      	| train loss (relative): 3.076304e-02 | valid loss (relative): 3.100767e-02 
Epoch 772 use: 332.11 second.

epoch 773 starting......
Epoch:  773 | train loss: 1.563952e-03 | valid loss: 1.575341e-03 
      	| train loss (relative): 3.070004e-02 | valid loss (relative): 3.108283e-02 
Epoch 773 use: 311.35 second.

epoch 774 starting......
Epoch:  774 | train loss: 1.566965e-03 | valid loss: 1.572675e-03 
      	| train loss (relative): 3.076071e-02 | valid loss (relative): 3.094172e-02 
Epoch 774 use: 320.78 second.

epoch 775 starting......
Epoch:  775 | train loss: 1.560211e-03 | valid loss: 1.573058e-03 
      	| train loss (relative): 3.062588e-02 | valid loss (relative): 3.098141e-02 
Epoch 775 use: 299.63 second.

epoch 776 starting......
Epoch:  776 | train loss: 1.563012e-03 | valid loss: 1.576037e-03 
      	| train loss (relative): 3.068244e-02 | valid loss (relative): 3.099388e-02 
Epoch 776 use: 304.10 second.

epoch 777 starting......
Epoch:  777 | train loss: 1.563198e-03 | valid loss: 1.572629e-03 
      	| train loss (relative): 3.068647e-02 | valid loss (relative): 3.084543e-02 
Epoch 777 use: 304.40 second.

epoch 778 starting......
Epoch:  778 | train loss: 1.562262e-03 | valid loss: 1.581265e-03 
      	| train loss (relative): 3.066009e-02 | valid loss (relative): 3.124913e-02 
Epoch 778 use: 305.81 second.

epoch 779 starting......
Epoch:  779 | train loss: 1.568031e-03 | valid loss: 1.576108e-03 
      	| train loss (relative): 3.078258e-02 | valid loss (relative): 3.102105e-02 
Epoch 779 use: 305.76 second.

epoch 780 starting......
Epoch:  780 | train loss: 1.563405e-03 | valid loss: 1.581190e-03 
      	| train loss (relative): 3.069195e-02 | valid loss (relative): 3.127720e-02 
Epoch 780 use: 307.73 second.

epoch 781 starting......
Epoch:  781 | train loss: 1.567325e-03 | valid loss: 1.581352e-03 
      	| train loss (relative): 3.077212e-02 | valid loss (relative): 3.115195e-02 
Epoch 781 use: 312.62 second.

epoch 782 starting......
Epoch:  782 | train loss: 1.562061e-03 | valid loss: 1.574970e-03 
      	| train loss (relative): 3.066615e-02 | valid loss (relative): 3.095110e-02 
Epoch 782 use: 304.54 second.

epoch 783 starting......
Epoch:  783 | train loss: 1.563179e-03 | valid loss: 1.577173e-03 
      	| train loss (relative): 3.068562e-02 | valid loss (relative): 3.109556e-02 
Epoch 783 use: 316.51 second.

epoch 784 starting......
Epoch:  784 | train loss: 1.562775e-03 | valid loss: 1.582917e-03 
      	| train loss (relative): 3.067921e-02 | valid loss (relative): 3.101882e-02 
Epoch 784 use: 301.89 second.

epoch 785 starting......
Epoch:  785 | train loss: 1.567656e-03 | valid loss: 1.579243e-03 
      	| train loss (relative): 3.077525e-02 | valid loss (relative): 3.089237e-02 
Epoch 785 use: 306.21 second.

epoch 786 starting......
Epoch:  786 | train loss: 1.562992e-03 | valid loss: 1.571166e-03 
      	| train loss (relative): 3.067392e-02 | valid loss (relative): 3.089546e-02 
Epoch 786 use: 320.63 second.

epoch 787 starting......
Epoch:  787 | train loss: 1.560928e-03 | valid loss: 1.575556e-03 
      	| train loss (relative): 3.063994e-02 | valid loss (relative): 3.098601e-02 
Epoch 787 use: 310.14 second.

epoch 788 starting......
Epoch:  788 | train loss: 1.561847e-03 | valid loss: 1.587024e-03 
      	| train loss (relative): 3.065272e-02 | valid loss (relative): 3.140113e-02 
Epoch 788 use: 318.68 second.

epoch 789 starting......
Epoch:  789 | train loss: 1.565428e-03 | valid loss: 1.574963e-03 
      	| train loss (relative): 3.073066e-02 | valid loss (relative): 3.091731e-02 
Epoch 789 use: 298.73 second.

epoch 790 starting......
Epoch:  790 | train loss: 1.560402e-03 | valid loss: 1.571580e-03 
      	| train loss (relative): 3.062565e-02 | valid loss (relative): 3.102716e-02 
Epoch 790 use: 304.36 second.

epoch 791 starting......
Epoch:  791 | train loss: 1.558727e-03 | valid loss: 1.576597e-03 
      	| train loss (relative): 3.059088e-02 | valid loss (relative): 3.114038e-02 
Epoch 791 use: 299.83 second.

epoch 792 starting......
Epoch:  792 | train loss: 1.563817e-03 | valid loss: 1.565722e-03 
      	| train loss (relative): 3.069859e-02 | valid loss (relative): 3.083821e-02 
Epoch 792 use: 309.77 second.

epoch 793 starting......
Epoch:  793 | train loss: 1.553131e-03 | valid loss: 1.570228e-03 
      	| train loss (relative): 3.048504e-02 | valid loss (relative): 3.088822e-02 
Epoch 793 use: 311.19 second.

epoch 794 starting......
Epoch:  794 | train loss: 1.552805e-03 | valid loss: 1.567813e-03 
      	| train loss (relative): 3.047668e-02 | valid loss (relative): 3.076676e-02 
Epoch 794 use: 324.14 second.

epoch 795 starting......
Epoch:  795 | train loss: 1.549675e-03 | valid loss: 1.568382e-03 
      	| train loss (relative): 3.041007e-02 | valid loss (relative): 3.091876e-02 
Epoch 795 use: 305.19 second.

epoch 796 starting......
Epoch:  796 | train loss: 1.552854e-03 | valid loss: 1.567259e-03 
      	| train loss (relative): 3.047753e-02 | valid loss (relative): 3.085873e-02 
Epoch 796 use: 303.77 second.

epoch 797 starting......
Epoch:  797 | train loss: 1.549507e-03 | valid loss: 1.560607e-03 
      	| train loss (relative): 3.041239e-02 | valid loss (relative): 3.060942e-02 
Epoch 797 use: 303.69 second.

epoch 798 starting......
Epoch:  798 | train loss: 1.550465e-03 | valid loss: 1.562900e-03 
      	| train loss (relative): 3.042517e-02 | valid loss (relative): 3.081005e-02 
Epoch 798 use: 312.11 second.

epoch 799 starting......
Epoch:  799 | train loss: 1.549553e-03 | valid loss: 1.572509e-03 
      	| train loss (relative): 3.040716e-02 | valid loss (relative): 3.078682e-02 
Epoch 799 use: 325.07 second.

epoch 800 starting......
Epoch:  800 | train loss: 1.552947e-03 | valid loss: 1.577380e-03 
      	| train loss (relative): 3.047563e-02 | valid loss (relative): 3.105726e-02 
Epoch 800 use: 313.26 second.

epoch 801 starting......
Epoch:  801 | train loss: 1.555465e-03 | valid loss: 1.567780e-03 
      	| train loss (relative): 3.053072e-02 | valid loss (relative): 3.081260e-02 
Epoch 801 use: 342.00 second.

epoch 802 starting......
Epoch:  802 | train loss: 1.549797e-03 | valid loss: 1.562771e-03 
      	| train loss (relative): 3.041784e-02 | valid loss (relative): 3.090153e-02 
Epoch 802 use: 308.44 second.

epoch 803 starting......
Epoch:  803 | train loss: 1.545932e-03 | valid loss: 1.560952e-03 
      	| train loss (relative): 3.034028e-02 | valid loss (relative): 3.069898e-02 
Epoch 803 use: 306.38 second.

epoch 804 starting......
Epoch:  804 | train loss: 1.550268e-03 | valid loss: 1.571150e-03 
      	| train loss (relative): 3.042232e-02 | valid loss (relative): 3.079110e-02 
Epoch 804 use: 307.65 second.

epoch 805 starting......
Epoch:  805 | train loss: 1.553267e-03 | valid loss: 1.569543e-03 
      	| train loss (relative): 3.048019e-02 | valid loss (relative): 3.083164e-02 
Epoch 805 use: 316.04 second.

epoch 806 starting......
Epoch:  806 | train loss: 1.549612e-03 | valid loss: 1.561821e-03 
      	| train loss (relative): 3.040926e-02 | valid loss (relative): 3.077537e-02 
Epoch 806 use: 320.81 second.

epoch 807 starting......
Epoch:  807 | train loss: 1.546723e-03 | valid loss: 1.563133e-03 
      	| train loss (relative): 3.035443e-02 | valid loss (relative): 3.082026e-02 
Epoch 807 use: 310.18 second.

epoch 808 starting......
Epoch:  808 | train loss: 1.546047e-03 | valid loss: 1.565524e-03 
      	| train loss (relative): 3.033818e-02 | valid loss (relative): 3.069161e-02 
Epoch 808 use: 301.64 second.

epoch 809 starting......
Epoch:  809 | train loss: 1.551299e-03 | valid loss: 1.568816e-03 
      	| train loss (relative): 3.043897e-02 | valid loss (relative): 3.082765e-02 
Epoch 809 use: 325.96 second.

epoch 810 starting......
Epoch:  810 | train loss: 1.546535e-03 | valid loss: 1.559950e-03 
      	| train loss (relative): 3.034764e-02 | valid loss (relative): 3.063156e-02 
Epoch 810 use: 371.65 second.

epoch 811 starting......
Epoch:  811 | train loss: 1.544988e-03 | valid loss: 1.567057e-03 
      	| train loss (relative): 3.031638e-02 | valid loss (relative): 3.056836e-02 
Epoch 811 use: 365.41 second.

epoch 812 starting......
Epoch:  812 | train loss: 1.549544e-03 | valid loss: 1.568054e-03 
      	| train loss (relative): 3.040470e-02 | valid loss (relative): 3.077212e-02 
Epoch 812 use: 375.41 second.

epoch 813 starting......
Epoch:  813 | train loss: 1.550014e-03 | valid loss: 1.562919e-03 
      	| train loss (relative): 3.041667e-02 | valid loss (relative): 3.073353e-02 
Epoch 813 use: 380.41 second.

epoch 814 starting......
Epoch:  814 | train loss: 1.543440e-03 | valid loss: 1.560024e-03 
      	| train loss (relative): 3.028506e-02 | valid loss (relative): 3.065532e-02 
Epoch 814 use: 371.27 second.

epoch 815 starting......
Epoch:  815 | train loss: 1.543388e-03 | valid loss: 1.564836e-03 
      	| train loss (relative): 3.028899e-02 | valid loss (relative): 3.089507e-02 
Epoch 815 use: 361.89 second.

epoch 816 starting......
Epoch:  816 | train loss: 1.548303e-03 | valid loss: 1.560906e-03 
      	| train loss (relative): 3.038222e-02 | valid loss (relative): 3.062107e-02 
Epoch 816 use: 387.13 second.

epoch 817 starting......
Epoch:  817 | train loss: 1.543340e-03 | valid loss: 1.555585e-03 
      	| train loss (relative): 3.028264e-02 | valid loss (relative): 3.062748e-02 
Epoch 817 use: 397.78 second.

epoch 818 starting......
Epoch:  818 | train loss: 1.541506e-03 | valid loss: 1.556016e-03 
      	| train loss (relative): 3.024887e-02 | valid loss (relative): 3.056817e-02 
Epoch 818 use: 395.76 second.

epoch 819 starting......
Epoch:  819 | train loss: 1.542121e-03 | valid loss: 1.563188e-03 
      	| train loss (relative): 3.025968e-02 | valid loss (relative): 3.080849e-02 
Epoch 819 use: 377.87 second.

epoch 820 starting......
Epoch:  820 | train loss: 1.544688e-03 | valid loss: 1.565767e-03 
      	| train loss (relative): 3.031437e-02 | valid loss (relative): 3.077444e-02 
Epoch 820 use: 336.37 second.

epoch 821 starting......
Epoch:  821 | train loss: 1.543167e-03 | valid loss: 1.562186e-03 
      	| train loss (relative): 3.027843e-02 | valid loss (relative): 3.087166e-02 
Epoch 821 use: 347.84 second.

epoch 822 starting......
Epoch:  822 | train loss: 1.539498e-03 | valid loss: 1.564461e-03 
      	| train loss (relative): 3.020575e-02 | valid loss (relative): 3.065804e-02 
Epoch 822 use: 327.26 second.

epoch 823 starting......
Epoch:  823 | train loss: 1.543539e-03 | valid loss: 1.556801e-03 
      	| train loss (relative): 3.029029e-02 | valid loss (relative): 3.064257e-02 
Epoch 823 use: 333.22 second.

epoch 824 starting......
Epoch:  824 | train loss: 1.540147e-03 | valid loss: 1.566722e-03 
      	| train loss (relative): 3.022082e-02 | valid loss (relative): 3.073304e-02 
Epoch 824 use: 336.05 second.

epoch 825 starting......
Epoch:  825 | train loss: 1.543695e-03 | valid loss: 1.556621e-03 
      	| train loss (relative): 3.030003e-02 | valid loss (relative): 3.061185e-02 
Epoch 825 use: 330.04 second.

epoch 826 starting......
Epoch:  826 | train loss: 1.540208e-03 | valid loss: 1.555218e-03 
      	| train loss (relative): 3.022100e-02 | valid loss (relative): 3.068157e-02 
Epoch 826 use: 315.02 second.

epoch 827 starting......
Epoch:  827 | train loss: 1.538043e-03 | valid loss: 1.556395e-03 
      	| train loss (relative): 3.017744e-02 | valid loss (relative): 3.052931e-02 
Epoch 827 use: 322.56 second.

epoch 828 starting......
Epoch:  828 | train loss: 1.540412e-03 | valid loss: 1.554410e-03 
      	| train loss (relative): 3.022582e-02 | valid loss (relative): 3.052982e-02 
Epoch 828 use: 354.31 second.

epoch 829 starting......
Epoch:  829 | train loss: 1.543222e-03 | valid loss: 1.572985e-03 
      	| train loss (relative): 3.027847e-02 | valid loss (relative): 3.095317e-02 
Epoch 829 use: 345.34 second.

epoch 830 starting......
Epoch:  830 | train loss: 1.543533e-03 | valid loss: 1.557922e-03 
      	| train loss (relative): 3.028993e-02 | valid loss (relative): 3.066673e-02 
Epoch 830 use: 329.73 second.

epoch 831 starting......
Epoch:  831 | train loss: 1.542718e-03 | valid loss: 1.560510e-03 
      	| train loss (relative): 3.027305e-02 | valid loss (relative): 3.058912e-02 
Epoch 831 use: 382.04 second.

epoch 832 starting......
Epoch:  832 | train loss: 1.544659e-03 | valid loss: 1.566136e-03 
      	| train loss (relative): 3.030381e-02 | valid loss (relative): 3.082288e-02 
Epoch 832 use: 330.45 second.

epoch 833 starting......
Epoch:  833 | train loss: 1.544087e-03 | valid loss: 1.558085e-03 
      	| train loss (relative): 3.029215e-02 | valid loss (relative): 3.055782e-02 
Epoch 833 use: 340.17 second.

epoch 834 starting......
Epoch:  834 | train loss: 1.538967e-03 | valid loss: 1.554115e-03 
      	| train loss (relative): 3.020098e-02 | valid loss (relative): 3.058677e-02 
Epoch 834 use: 365.30 second.

epoch 835 starting......
Epoch:  835 | train loss: 1.538983e-03 | valid loss: 1.565946e-03 
      	| train loss (relative): 3.019519e-02 | valid loss (relative): 3.078178e-02 
Epoch 835 use: 336.27 second.

epoch 836 starting......
Epoch:  836 | train loss: 1.540093e-03 | valid loss: 1.557642e-03 
      	| train loss (relative): 3.022056e-02 | valid loss (relative): 3.065198e-02 
Epoch 836 use: 341.46 second.

epoch 837 starting......
Epoch:  837 | train loss: 1.540492e-03 | valid loss: 1.554076e-03 
      	| train loss (relative): 3.022385e-02 | valid loss (relative): 3.049908e-02 
Epoch 837 use: 351.86 second.

epoch 838 starting......
Epoch:  838 | train loss: 1.537059e-03 | valid loss: 1.558172e-03 
      	| train loss (relative): 3.015380e-02 | valid loss (relative): 3.046657e-02 
Epoch 838 use: 338.60 second.

epoch 839 starting......
Epoch:  839 | train loss: 1.538273e-03 | valid loss: 1.550395e-03 
      	| train loss (relative): 3.018144e-02 | valid loss (relative): 3.055526e-02 
Epoch 839 use: 353.64 second.

epoch 840 starting......
Epoch:  840 | train loss: 1.536267e-03 | valid loss: 1.555829e-03 
      	| train loss (relative): 3.014453e-02 | valid loss (relative): 3.059084e-02 
Epoch 840 use: 371.97 second.

epoch 841 starting......
Epoch:  841 | train loss: 1.535384e-03 | valid loss: 1.551260e-03 
      	| train loss (relative): 3.012069e-02 | valid loss (relative): 3.058279e-02 
Epoch 841 use: 339.19 second.

epoch 842 starting......
Epoch:  842 | train loss: 1.533028e-03 | valid loss: 1.551653e-03 
      	| train loss (relative): 3.007447e-02 | valid loss (relative): 3.038205e-02 
Epoch 842 use: 359.44 second.

epoch 843 starting......
Epoch:  843 | train loss: 1.531631e-03 | valid loss: 1.549320e-03 
      	| train loss (relative): 3.004526e-02 | valid loss (relative): 3.047271e-02 
Epoch 843 use: 349.04 second.

epoch 844 starting......
Epoch:  844 | train loss: 1.532586e-03 | valid loss: 1.555013e-03 
      	| train loss (relative): 3.006388e-02 | valid loss (relative): 3.082438e-02 
Epoch 844 use: 362.77 second.

epoch 845 starting......
Epoch:  845 | train loss: 1.536420e-03 | valid loss: 1.556084e-03 
      	| train loss (relative): 3.014257e-02 | valid loss (relative): 3.061817e-02 
Epoch 845 use: 349.11 second.

epoch 846 starting......
Epoch:  846 | train loss: 1.537962e-03 | valid loss: 1.558184e-03 
      	| train loss (relative): 3.017371e-02 | valid loss (relative): 3.062157e-02 
Epoch 846 use: 339.98 second.

epoch 847 starting......
Epoch:  847 | train loss: 1.536419e-03 | valid loss: 1.548292e-03 
      	| train loss (relative): 3.014463e-02 | valid loss (relative): 3.037398e-02 
Epoch 847 use: 356.59 second.

epoch 848 starting......
Epoch:  848 | train loss: 1.529439e-03 | valid loss: 1.545186e-03 
      	| train loss (relative): 3.000274e-02 | valid loss (relative): 3.035448e-02 
Epoch 848 use: 344.40 second.

epoch 849 starting......
Epoch:  849 | train loss: 1.529262e-03 | valid loss: 1.549593e-03 
      	| train loss (relative): 3.000467e-02 | valid loss (relative): 3.054890e-02 
Epoch 849 use: 350.62 second.

epoch 850 starting......
Epoch:  850 | train loss: 1.533488e-03 | valid loss: 1.550104e-03 
      	| train loss (relative): 3.008542e-02 | valid loss (relative): 3.056917e-02 
Epoch 850 use: 330.63 second.

epoch 851 starting......
Epoch:  851 | train loss: 1.532758e-03 | valid loss: 1.553310e-03 
      	| train loss (relative): 3.007226e-02 | valid loss (relative): 3.044350e-02 
Epoch 851 use: 371.66 second.

epoch 852 starting......
Epoch:  852 | train loss: 1.535498e-03 | valid loss: 1.554895e-03 
      	| train loss (relative): 3.012300e-02 | valid loss (relative): 3.065722e-02 
Epoch 852 use: 345.21 second.

epoch 853 starting......
Epoch:  853 | train loss: 1.531324e-03 | valid loss: 1.548529e-03 
      	| train loss (relative): 3.004032e-02 | valid loss (relative): 3.039995e-02 
Epoch 853 use: 337.17 second.

epoch 854 starting......
Epoch:  854 | train loss: 1.532429e-03 | valid loss: 1.546299e-03 
      	| train loss (relative): 3.006089e-02 | valid loss (relative): 3.034467e-02 
Epoch 854 use: 362.81 second.

epoch 855 starting......
Epoch:  855 | train loss: 1.529713e-03 | valid loss: 1.553677e-03 
      	| train loss (relative): 3.000743e-02 | valid loss (relative): 3.060042e-02 
Epoch 855 use: 340.23 second.

epoch 856 starting......
Epoch:  856 | train loss: 1.529287e-03 | valid loss: 1.548016e-03 
      	| train loss (relative): 2.999830e-02 | valid loss (relative): 3.060291e-02 
Epoch 856 use: 370.65 second.

epoch 857 starting......
Epoch:  857 | train loss: 1.527407e-03 | valid loss: 1.554451e-03 
      	| train loss (relative): 2.996570e-02 | valid loss (relative): 3.036372e-02 
Epoch 857 use: 344.22 second.

epoch 858 starting......
Epoch:  858 | train loss: 1.533865e-03 | valid loss: 1.550876e-03 
      	| train loss (relative): 3.009011e-02 | valid loss (relative): 3.037560e-02 
Epoch 858 use: 340.39 second.

epoch 859 starting......
Epoch:  859 | train loss: 1.530345e-03 | valid loss: 1.558562e-03 
      	| train loss (relative): 3.002093e-02 | valid loss (relative): 3.063790e-02 
Epoch 859 use: 365.36 second.

epoch 860 starting......
Epoch:  860 | train loss: 1.530858e-03 | valid loss: 1.547560e-03 
      	| train loss (relative): 3.002960e-02 | valid loss (relative): 3.036485e-02 
Epoch 860 use: 342.28 second.

epoch 861 starting......
Epoch:  861 | train loss: 1.526627e-03 | valid loss: 1.546926e-03 
      	| train loss (relative): 2.994423e-02 | valid loss (relative): 3.043262e-02 
Epoch 861 use: 348.62 second.

epoch 862 starting......
Epoch:  862 | train loss: 1.526638e-03 | valid loss: 1.549773e-03 
      	| train loss (relative): 2.994786e-02 | valid loss (relative): 3.038400e-02 
Epoch 862 use: 354.93 second.

epoch 863 starting......
Epoch:  863 | train loss: 1.526051e-03 | valid loss: 1.542514e-03 
      	| train loss (relative): 2.993326e-02 | valid loss (relative): 3.040522e-02 
Epoch 863 use: 343.20 second.

epoch 864 starting......
Epoch:  864 | train loss: 1.523161e-03 | valid loss: 1.541188e-03 
      	| train loss (relative): 2.987406e-02 | valid loss (relative): 3.019351e-02 
Epoch 864 use: 355.68 second.

epoch 865 starting......
Epoch:  865 | train loss: 1.529229e-03 | valid loss: 1.556003e-03 
      	| train loss (relative): 2.999349e-02 | valid loss (relative): 3.062283e-02 
Epoch 865 use: 338.43 second.

epoch 866 starting......
Epoch:  866 | train loss: 1.528520e-03 | valid loss: 1.545439e-03 
      	| train loss (relative): 2.998514e-02 | valid loss (relative): 3.030627e-02 
Epoch 866 use: 342.78 second.

epoch 867 starting......
Epoch:  867 | train loss: 1.524009e-03 | valid loss: 1.542271e-03 
      	| train loss (relative): 2.989245e-02 | valid loss (relative): 3.039267e-02 
Epoch 867 use: 356.70 second.

epoch 868 starting......
Epoch:  868 | train loss: 1.525583e-03 | valid loss: 1.546959e-03 
      	| train loss (relative): 2.992328e-02 | valid loss (relative): 3.051864e-02 
Epoch 868 use: 340.60 second.

epoch 869 starting......
Epoch:  869 | train loss: 1.528758e-03 | valid loss: 1.549764e-03 
      	| train loss (relative): 2.998669e-02 | valid loss (relative): 3.051873e-02 
Epoch 869 use: 360.69 second.

epoch 870 starting......
Epoch:  870 | train loss: 1.526148e-03 | valid loss: 1.538451e-03 
      	| train loss (relative): 2.993275e-02 | valid loss (relative): 3.025838e-02 
Epoch 870 use: 352.83 second.

epoch 871 starting......
Epoch:  871 | train loss: 1.520687e-03 | valid loss: 1.541167e-03 
      	| train loss (relative): 2.982659e-02 | valid loss (relative): 3.041874e-02 
Epoch 871 use: 339.72 second.

epoch 872 starting......
Epoch:  872 | train loss: 1.519529e-03 | valid loss: 1.541882e-03 
      	| train loss (relative): 2.980142e-02 | valid loss (relative): 3.016739e-02 
Epoch 872 use: 364.86 second.

epoch 873 starting......
Epoch:  873 | train loss: 1.524109e-03 | valid loss: 1.543612e-03 
      	| train loss (relative): 2.989287e-02 | valid loss (relative): 3.030552e-02 
Epoch 873 use: 353.69 second.

epoch 874 starting......
Epoch:  874 | train loss: 1.524432e-03 | valid loss: 1.543989e-03 
      	| train loss (relative): 2.990304e-02 | valid loss (relative): 3.048235e-02 
Epoch 874 use: 369.94 second.

epoch 875 starting......
Epoch:  875 | train loss: 1.521840e-03 | valid loss: 1.541776e-03 
      	| train loss (relative): 2.985272e-02 | valid loss (relative): 3.038712e-02 
Epoch 875 use: 340.99 second.

epoch 876 starting......
Epoch:  876 | train loss: 1.523826e-03 | valid loss: 1.539598e-03 
      	| train loss (relative): 2.988779e-02 | valid loss (relative): 3.029304e-02 
Epoch 876 use: 345.32 second.

epoch 877 starting......
Epoch:  877 | train loss: 1.517941e-03 | valid loss: 1.537960e-03 
      	| train loss (relative): 2.976887e-02 | valid loss (relative): 3.012204e-02 
Epoch 877 use: 355.71 second.

epoch 878 starting......
Epoch:  878 | train loss: 1.523732e-03 | valid loss: 1.547723e-03 
      	| train loss (relative): 2.988229e-02 | valid loss (relative): 3.052817e-02 
Epoch 878 use: 347.54 second.

epoch 879 starting......
Epoch:  879 | train loss: 1.521706e-03 | valid loss: 1.541590e-03 
      	| train loss (relative): 2.984539e-02 | valid loss (relative): 3.036151e-02 
Epoch 879 use: 368.13 second.

epoch 880 starting......
Epoch:  880 | train loss: 1.524967e-03 | valid loss: 1.540690e-03 
      	| train loss (relative): 2.991055e-02 | valid loss (relative): 3.033564e-02 
Epoch 880 use: 365.38 second.

epoch 881 starting......
Epoch:  881 | train loss: 1.525440e-03 | valid loss: 1.545587e-03 
      	| train loss (relative): 2.992020e-02 | valid loss (relative): 3.070752e-02 
Epoch 881 use: 349.84 second.

epoch 882 starting......
Epoch:  882 | train loss: 1.526643e-03 | valid loss: 1.535696e-03 
      	| train loss (relative): 2.994507e-02 | valid loss (relative): 3.027600e-02 
Epoch 882 use: 361.68 second.

epoch 883 starting......
Epoch:  883 | train loss: 1.513921e-03 | valid loss: 1.537274e-03 
      	| train loss (relative): 2.968960e-02 | valid loss (relative): 3.025986e-02 
Epoch 883 use: 371.37 second.

epoch 884 starting......
Epoch:  884 | train loss: 1.512596e-03 | valid loss: 1.532961e-03 
      	| train loss (relative): 2.966798e-02 | valid loss (relative): 3.006959e-02 
Epoch 884 use: 347.04 second.

epoch 885 starting......
Epoch:  885 | train loss: 1.513948e-03 | valid loss: 1.536770e-03 
      	| train loss (relative): 2.969139e-02 | valid loss (relative): 3.037271e-02 
Epoch 885 use: 372.64 second.

epoch 886 starting......
Epoch:  886 | train loss: 1.516561e-03 | valid loss: 1.534395e-03 
      	| train loss (relative): 2.974372e-02 | valid loss (relative): 3.004624e-02 
Epoch 886 use: 349.49 second.

epoch 887 starting......
Epoch:  887 | train loss: 1.515396e-03 | valid loss: 1.539631e-03 
      	| train loss (relative): 2.972378e-02 | valid loss (relative): 3.010202e-02 
Epoch 887 use: 378.02 second.

epoch 888 starting......
Epoch:  888 | train loss: 1.518876e-03 | valid loss: 1.537601e-03 
      	| train loss (relative): 2.978272e-02 | valid loss (relative): 3.025950e-02 
Epoch 888 use: 356.14 second.

epoch 889 starting......
Epoch:  889 | train loss: 1.517009e-03 | valid loss: 1.537898e-03 
      	| train loss (relative): 2.975246e-02 | valid loss (relative): 3.010343e-02 
Epoch 889 use: 359.20 second.

epoch 890 starting......
Epoch:  890 | train loss: 1.521211e-03 | valid loss: 1.539017e-03 
      	| train loss (relative): 2.983240e-02 | valid loss (relative): 3.029145e-02 
Epoch 890 use: 371.04 second.

epoch 891 starting......
Epoch:  891 | train loss: 1.515473e-03 | valid loss: 1.529354e-03 
      	| train loss (relative): 2.971759e-02 | valid loss (relative): 2.998843e-02 
Epoch 891 use: 361.97 second.

epoch 892 starting......
Epoch:  892 | train loss: 1.513625e-03 | valid loss: 1.537261e-03 
      	| train loss (relative): 2.968504e-02 | valid loss (relative): 3.026801e-02 
Epoch 892 use: 380.01 second.

epoch 893 starting......
Epoch:  893 | train loss: 1.516006e-03 | valid loss: 1.534485e-03 
      	| train loss (relative): 2.972800e-02 | valid loss (relative): 3.020025e-02 
Epoch 893 use: 364.86 second.

epoch 894 starting......
Epoch:  894 | train loss: 1.516223e-03 | valid loss: 1.543356e-03 
      	| train loss (relative): 2.973092e-02 | valid loss (relative): 3.045412e-02 
Epoch 894 use: 353.44 second.

epoch 895 starting......
Epoch:  895 | train loss: 1.519696e-03 | valid loss: 1.530929e-03 
      	| train loss (relative): 2.980413e-02 | valid loss (relative): 3.018180e-02 
Epoch 895 use: 363.48 second.

epoch 896 starting......
Epoch:  896 | train loss: 1.514424e-03 | valid loss: 1.531102e-03 
      	| train loss (relative): 2.969949e-02 | valid loss (relative): 2.999853e-02 
Epoch 896 use: 339.83 second.

epoch 897 starting......
Epoch:  897 | train loss: 1.512328e-03 | valid loss: 1.536834e-03 
      	| train loss (relative): 2.965772e-02 | valid loss (relative): 3.007719e-02 
Epoch 897 use: 355.39 second.

epoch 898 starting......
Epoch:  898 | train loss: 1.511274e-03 | valid loss: 1.532735e-03 
      	| train loss (relative): 2.963146e-02 | valid loss (relative): 3.002097e-02 
Epoch 898 use: 346.98 second.

epoch 899 starting......
Epoch:  899 | train loss: 1.511339e-03 | valid loss: 1.539814e-03 
      	| train loss (relative): 2.963513e-02 | valid loss (relative): 3.021785e-02 
Epoch 899 use: 343.81 second.

test MSE Error: 1.461735e-03 | relative MSE Error: 2.849825e-02 
 Total time used for training: 18.83 hour.
MESLoss saved to  /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_MSELoss_Latent_128_nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_900.txt
relative MSELoss saved to  /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_reMSELoss_Latent_128_nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_900.txt
model saved to /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_128_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_900.pth
model_dict saved to /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_128_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_900_dict.pth
... Training slugflow data completed, Run finished Sun 15 Aug 07:00:18 BST 2021 ...
{'vtu_dir': '/rds/general/user/jy220/home/slugflow/', 'data_dir': '/rds/general/user/jy220/home/tensors/', 'data_type': 'tensors', 'vtu_fields': 'Component1::ComponentMassFractionPhase1, phase1::Velocity', 'variational': 'False', 'optimizer': 'Adamax', 'mode': 'train', 'structured': 'False', 'changing_lr': 'False', 'self_concat': '2', 'dimension': '3', 'sfc_nums': '3', 'components': '4', 'nearest_neighbouring': 'True', 'dims_latent': '128', 'activation': 'Tanh', 'tk_file': '/rds/general/user/jy220/home/slugflow_tk.pt', 'tb_file': '/rds/general/user/jy220/home/slugflow_tb.pt', 'sfc_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/sfcs.pt', 'inv_sfc_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/inv_sfcs.pt', 'coords_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/coords.pt', 'cells_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/cells.npy', 'state_load': '/rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_128_Nearest_neighbouring_True_SFC_nums_3_startlr_0.0005_n_epoches_100_dict.pth', 'batch_size': '16', 'lr': '0.0005', 'n_epoches': '150', 'seed': '15', 'visualize': 'False', 'reconstructed_path': 'None', 'reconstruct_start_index': '0', 'reconstruct_end_index': '100', 'save_path': '/rds/general/user/jy220/home/new_results/'}
[[ 2.50825269e-01 -1.67256990e-02 -1.22268730e-03]
 [ 2.38611959e-01 -5.50627777e-03 -5.06451913e-03]
 [ 2.39454874e-01 -1.59563257e-02  3.96778323e-03]
 ...
 [ 9.11361096e+00 -2.21908381e-02  2.22384902e-03]
 [ 9.11847310e+00 -2.84346931e-02 -2.34839590e-04]
 [ 9.11039661e+00 -2.51498573e-02 -8.70661044e-03]]
{'tetra': array([[      0,       1,       2,       3],
       [      4,       5,       6,       7],
       [      8,       9,      10,      11],
       ...,
       [1342744, 1342745, 1342746, 1342747],
       [1342748, 1342749, 1342750, 1342751],
       [1342752, 1342753, 1342754, 1342755]])}
vtu fields:  ['Component1::ComponentMassFractionPhase1', 'phase1::Velocity'] 
 structured  False 
 activation  Tanh() 
 self concat  2 
 sfc_nums  3 

dims_latent  128 
 components  4 
 nearest_neighbouring  True 

visualize  False 
 sample number  1706 

reading sfc nums......
[array([  41684,   41685,   41674, ..., 1235560, 1235564, 1235558]), array([954289, 954290, 954291, ..., 420685, 420687, 420688]), array([  40156,   40155,   40158, ..., 1123478, 1123475, 1123467])]
reading inverse sfc nums......
[array([   8146,    8145,    8144, ..., 1274444, 1274445, 1273166]), array([547280, 547283, 593037, ..., 933933, 933932, 864488]), array([ 66437,  66439, 122512, ..., 459416, 415130, 459418])]
tk:  tensor([0.7581, 0.0418, 0.1840, 0.1464], dtype=torch.float64) 

tb:  tensor([-0.3816, -0.3618, -0.0308,  0.1446], dtype=torch.float64) 

length of train set: 1505 

length of valid set: 100 

length of test set: 101 

torch device num: 4 

Let's use 4 GPUs!
epoch 100 starting......
Epoch:  100 | train loss: 3.575584e-03 | valid loss: 2.954434e-03 
      	| train loss (relative): 7.311282e-02 | valid loss (relative): 5.957280e-02 
Epoch 100 use: 493.80 second.

epoch 101 starting......
Epoch:  101 | train loss: 2.844804e-03 | valid loss: 2.919174e-03 
      	| train loss (relative): 5.745338e-02 | valid loss (relative): 5.889622e-02 
Epoch 101 use: 436.94 second.

epoch 102 starting......
Epoch:  102 | train loss: 2.820823e-03 | valid loss: 2.907550e-03 
      	| train loss (relative): 5.696639e-02 | valid loss (relative): 5.867916e-02 
Epoch 102 use: 417.11 second.

epoch 103 starting......
Epoch:  103 | train loss: 2.808626e-03 | valid loss: 2.897390e-03 
      	| train loss (relative): 5.668952e-02 | valid loss (relative): 5.845774e-02 
Epoch 103 use: 410.82 second.

epoch 104 starting......
Epoch:  104 | train loss: 2.799634e-03 | valid loss: 2.889889e-03 
      	| train loss (relative): 5.649726e-02 | valid loss (relative): 5.819590e-02 
Epoch 104 use: 376.22 second.

epoch 105 starting......
Epoch:  105 | train loss: 2.792539e-03 | valid loss: 2.884543e-03 
      	| train loss (relative): 5.633654e-02 | valid loss (relative): 5.820740e-02 
Epoch 105 use: 388.02 second.

epoch 106 starting......
Epoch:  106 | train loss: 2.784866e-03 | valid loss: 2.879153e-03 
      	| train loss (relative): 5.618342e-02 | valid loss (relative): 5.804578e-02 
Epoch 106 use: 377.25 second.

epoch 107 starting......
Epoch:  107 | train loss: 2.778273e-03 | valid loss: 2.873430e-03 
      	| train loss (relative): 5.603154e-02 | valid loss (relative): 5.798594e-02 
Epoch 107 use: 383.20 second.

epoch 108 starting......
Epoch:  108 | train loss: 2.771723e-03 | valid loss: 2.868321e-03 
      	| train loss (relative): 5.590360e-02 | valid loss (relative): 5.772873e-02 
Epoch 108 use: 391.25 second.

epoch 109 starting......
Epoch:  109 | train loss: 2.765130e-03 | valid loss: 2.863932e-03 
      	| train loss (relative): 5.576032e-02 | valid loss (relative): 5.777690e-02 
Epoch 109 use: 403.69 second.

epoch 110 starting......
Epoch:  110 | train loss: 2.759281e-03 | valid loss: 2.858919e-03 
      	| train loss (relative): 5.565491e-02 | valid loss (relative): 5.759121e-02 
Epoch 110 use: 393.57 second.

epoch 111 starting......
Epoch:  111 | train loss: 2.753348e-03 | valid loss: 2.851281e-03 
      	| train loss (relative): 5.550783e-02 | valid loss (relative): 5.742146e-02 
Epoch 111 use: 385.04 second.

epoch 112 starting......
Epoch:  112 | train loss: 2.747210e-03 | valid loss: 2.846210e-03 
      	| train loss (relative): 5.537624e-02 | valid loss (relative): 5.726716e-02 
Epoch 112 use: 377.54 second.

epoch 113 starting......
Epoch:  113 | train loss: 2.740866e-03 | valid loss: 2.841297e-03 
      	| train loss (relative): 5.524182e-02 | valid loss (relative): 5.720025e-02 
Epoch 113 use: 366.96 second.

epoch 114 starting......
Epoch:  114 | train loss: 2.735177e-03 | valid loss: 2.835428e-03 
      	| train loss (relative): 5.512974e-02 | valid loss (relative): 5.711873e-02 
Epoch 114 use: 365.24 second.

epoch 115 starting......
Epoch:  115 | train loss: 2.729621e-03 | valid loss: 2.829820e-03 
      	| train loss (relative): 5.501959e-02 | valid loss (relative): 5.708704e-02 
Epoch 115 use: 382.10 second.

epoch 116 starting......
Epoch:  116 | train loss: 2.723058e-03 | valid loss: 2.824943e-03 
      	| train loss (relative): 5.487584e-02 | valid loss (relative): 5.682055e-02 
Epoch 116 use: 383.57 second.

epoch 117 starting......
Epoch:  117 | train loss: 2.716910e-03 | valid loss: 2.817570e-03 
      	| train loss (relative): 5.473373e-02 | valid loss (relative): 5.677262e-02 
Epoch 117 use: 376.03 second.

epoch 118 starting......
Epoch:  118 | train loss: 2.711096e-03 | valid loss: 2.815583e-03 
      	| train loss (relative): 5.462435e-02 | valid loss (relative): 5.667460e-02 
Epoch 118 use: 427.83 second.

epoch 119 starting......
Epoch:  119 | train loss: 2.707008e-03 | valid loss: 2.810348e-03 
      	| train loss (relative): 5.453425e-02 | valid loss (relative): 5.647549e-02 
Epoch 119 use: 521.61 second.

epoch 120 starting......
Epoch:  120 | train loss: 2.699731e-03 | valid loss: 2.804480e-03 
      	| train loss (relative): 5.436732e-02 | valid loss (relative): 5.637495e-02 
Epoch 120 use: 473.78 second.

epoch 121 starting......
Epoch:  121 | train loss: 2.696153e-03 | valid loss: 2.800160e-03 
      	| train loss (relative): 5.429555e-02 | valid loss (relative): 5.633440e-02 
Epoch 121 use: 368.34 second.

epoch 122 starting......
Epoch:  122 | train loss: 2.687839e-03 | valid loss: 2.793101e-03 
      	| train loss (relative): 5.411024e-02 | valid loss (relative): 5.626180e-02 
Epoch 122 use: 373.04 second.

epoch 123 starting......
Epoch:  123 | train loss: 2.683030e-03 | valid loss: 2.784993e-03 
      	| train loss (relative): 5.402184e-02 | valid loss (relative): 5.585710e-02 
Epoch 123 use: 364.32 second.

epoch 124 starting......
Epoch:  124 | train loss: 2.677660e-03 | valid loss: 2.783056e-03 
      	| train loss (relative): 5.390253e-02 | valid loss (relative): 5.595011e-02 
Epoch 124 use: 372.79 second.

epoch 125 starting......
Epoch:  125 | train loss: 2.672308e-03 | valid loss: 2.779096e-03 
      	| train loss (relative): 5.379736e-02 | valid loss (relative): 5.594617e-02 
Epoch 125 use: 380.68 second.

epoch 126 starting......
Epoch:  126 | train loss: 2.667037e-03 | valid loss: 2.773431e-03 
      	| train loss (relative): 5.369368e-02 | valid loss (relative): 5.592627e-02 
Epoch 126 use: 386.70 second.

epoch 127 starting......
Epoch:  127 | train loss: 2.665729e-03 | valid loss: 2.771936e-03 
      	| train loss (relative): 5.365956e-02 | valid loss (relative): 5.574237e-02 
Epoch 127 use: 391.18 second.

epoch 128 starting......
Epoch:  128 | train loss: 2.659155e-03 | valid loss: 2.761668e-03 
      	| train loss (relative): 5.351888e-02 | valid loss (relative): 5.564366e-02 
Epoch 128 use: 382.19 second.

epoch 129 starting......
Epoch:  129 | train loss: 2.649213e-03 | valid loss: 2.760671e-03 
      	| train loss (relative): 5.330989e-02 | valid loss (relative): 5.606386e-02 
Epoch 129 use: 384.35 second.

epoch 130 starting......
Epoch:  130 | train loss: 2.644941e-03 | valid loss: 2.748060e-03 
      	| train loss (relative): 5.321717e-02 | valid loss (relative): 5.558406e-02 
Epoch 130 use: 393.24 second.

epoch 131 starting......
Epoch:  131 | train loss: 2.639345e-03 | valid loss: 2.755310e-03 
      	| train loss (relative): 5.310863e-02 | valid loss (relative): 5.589980e-02 
Epoch 131 use: 380.29 second.

epoch 132 starting......
Epoch:  132 | train loss: 2.637558e-03 | valid loss: 2.747464e-03 
      	| train loss (relative): 5.307338e-02 | valid loss (relative): 5.566861e-02 
Epoch 132 use: 381.77 second.

epoch 133 starting......
Epoch:  133 | train loss: 2.630692e-03 | valid loss: 2.747470e-03 
      	| train loss (relative): 5.292107e-02 | valid loss (relative): 5.494528e-02 
Epoch 133 use: 378.57 second.

epoch 134 starting......
Epoch:  134 | train loss: 2.628588e-03 | valid loss: 2.738528e-03 
      	| train loss (relative): 5.286285e-02 | valid loss (relative): 5.496579e-02 
Epoch 134 use: 376.54 second.

epoch 135 starting......
Epoch:  135 | train loss: 2.619179e-03 | valid loss: 2.727366e-03 
      	| train loss (relative): 5.267304e-02 | valid loss (relative): 5.441929e-02 
Epoch 135 use: 371.84 second.

epoch 136 starting......
Epoch:  136 | train loss: 2.614053e-03 | valid loss: 2.729989e-03 
      	| train loss (relative): 5.256676e-02 | valid loss (relative): 5.458327e-02 
Epoch 136 use: 362.96 second.

epoch 137 starting......
Epoch:  137 | train loss: 2.614749e-03 | valid loss: 2.714306e-03 
      	| train loss (relative): 5.257673e-02 | valid loss (relative): 5.433076e-02 
Epoch 137 use: 368.84 second.

epoch 138 starting......
Epoch:  138 | train loss: 2.598753e-03 | valid loss: 2.704920e-03 
      	| train loss (relative): 5.223132e-02 | valid loss (relative): 5.445777e-02 
Epoch 138 use: 362.85 second.

epoch 139 starting......
Epoch:  139 | train loss: 2.594641e-03 | valid loss: 2.706540e-03 
      	| train loss (relative): 5.214405e-02 | valid loss (relative): 5.451069e-02 
Epoch 139 use: 376.86 second.

epoch 140 starting......
Epoch:  140 | train loss: 2.595779e-03 | valid loss: 2.706019e-03 
      	| train loss (relative): 5.216920e-02 | valid loss (relative): 5.448059e-02 
Epoch 140 use: 374.83 second.

epoch 141 starting......
Epoch:  141 | train loss: 2.593384e-03 | valid loss: 2.723061e-03 
      	| train loss (relative): 5.211602e-02 | valid loss (relative): 5.502961e-02 
Epoch 141 use: 361.82 second.

epoch 142 starting......
Epoch:  142 | train loss: 2.586173e-03 | valid loss: 2.690654e-03 
      	| train loss (relative): 5.197724e-02 | valid loss (relative): 5.440035e-02 
Epoch 142 use: 351.87 second.

epoch 143 starting......
Epoch:  143 | train loss: 2.572957e-03 | valid loss: 2.685958e-03 
      	| train loss (relative): 5.169738e-02 | valid loss (relative): 5.394188e-02 
Epoch 143 use: 362.99 second.

epoch 144 starting......
Epoch:  144 | train loss: 2.573690e-03 | valid loss: 2.689014e-03 
      	| train loss (relative): 5.170751e-02 | valid loss (relative): 5.409056e-02 
Epoch 144 use: 361.91 second.

epoch 145 starting......
Epoch:  145 | train loss: 2.568196e-03 | valid loss: 2.675923e-03 
      	| train loss (relative): 5.158803e-02 | valid loss (relative): 5.372151e-02 
Epoch 145 use: 354.65 second.

epoch 146 starting......
Epoch:  146 | train loss: 2.564312e-03 | valid loss: 2.678217e-03 
      	| train loss (relative): 5.151060e-02 | valid loss (relative): 5.397496e-02 
Epoch 146 use: 367.37 second.

epoch 147 starting......
Epoch:  147 | train loss: 2.558997e-03 | valid loss: 2.673547e-03 
      	| train loss (relative): 5.139178e-02 | valid loss (relative): 5.307390e-02 
Epoch 147 use: 364.87 second.

epoch 148 starting......
Epoch:  148 | train loss: 2.554023e-03 | valid loss: 2.670095e-03 
      	| train loss (relative): 5.128921e-02 | valid loss (relative): 5.366874e-02 
Epoch 148 use: 372.26 second.

epoch 149 starting......
Epoch:  149 | train loss: 2.546188e-03 | valid loss: 2.660150e-03 
      	| train loss (relative): 5.112513e-02 | valid loss (relative): 5.362574e-02 
Epoch 149 use: 359.27 second.

epoch 150 starting......
Epoch:  150 | train loss: 2.544717e-03 | valid loss: 2.654296e-03 
      	| train loss (relative): 5.109999e-02 | valid loss (relative): 5.297182e-02 
Epoch 150 use: 364.27 second.

epoch 151 starting......
Epoch:  151 | train loss: 2.534871e-03 | valid loss: 2.644262e-03 
      	| train loss (relative): 5.087989e-02 | valid loss (relative): 5.329646e-02 
Epoch 151 use: 385.79 second.

epoch 152 starting......
Epoch:  152 | train loss: 2.530659e-03 | valid loss: 2.639786e-03 
      	| train loss (relative): 5.079229e-02 | valid loss (relative): 5.325618e-02 
Epoch 152 use: 427.58 second.

epoch 153 starting......
Epoch:  153 | train loss: 2.527168e-03 | valid loss: 2.643357e-03 
      	| train loss (relative): 5.071848e-02 | valid loss (relative): 5.266984e-02 
Epoch 153 use: 457.10 second.

epoch 154 starting......
Epoch:  154 | train loss: 2.527415e-03 | valid loss: 2.633857e-03 
      	| train loss (relative): 5.072060e-02 | valid loss (relative): 5.312606e-02 
Epoch 154 use: 376.39 second.

epoch 155 starting......
Epoch:  155 | train loss: 2.516671e-03 | valid loss: 2.632565e-03 
      	| train loss (relative): 5.050585e-02 | valid loss (relative): 5.260637e-02 
Epoch 155 use: 358.52 second.

epoch 156 starting......
Epoch:  156 | train loss: 2.514576e-03 | valid loss: 2.628321e-03 
      	| train loss (relative): 5.045385e-02 | valid loss (relative): 5.287680e-02 
Epoch 156 use: 367.79 second.

epoch 157 starting......
Epoch:  157 | train loss: 2.509033e-03 | valid loss: 2.632728e-03 
      	| train loss (relative): 5.035325e-02 | valid loss (relative): 5.212861e-02 
Epoch 157 use: 360.29 second.

epoch 158 starting......
Epoch:  158 | train loss: 2.507853e-03 | valid loss: 2.619248e-03 
      	| train loss (relative): 5.030529e-02 | valid loss (relative): 5.244985e-02 
Epoch 158 use: 349.16 second.

epoch 159 starting......
Epoch:  159 | train loss: 2.513836e-03 | valid loss: 2.610163e-03 
      	| train loss (relative): 5.042386e-02 | valid loss (relative): 5.252665e-02 
Epoch 159 use: 356.50 second.

epoch 160 starting......
Epoch:  160 | train loss: 2.491254e-03 | valid loss: 2.599043e-03 
      	| train loss (relative): 4.997174e-02 | valid loss (relative): 5.225459e-02 
Epoch 160 use: 372.20 second.

epoch 161 starting......
Epoch:  161 | train loss: 2.492280e-03 | valid loss: 2.605821e-03 
      	| train loss (relative): 4.998629e-02 | valid loss (relative): 5.226815e-02 
Epoch 161 use: 364.21 second.

epoch 162 starting......
Epoch:  162 | train loss: 2.484353e-03 | valid loss: 2.600487e-03 
      	| train loss (relative): 4.981241e-02 | valid loss (relative): 5.194464e-02 
Epoch 162 use: 364.05 second.

epoch 163 starting......
Epoch:  163 | train loss: 2.484511e-03 | valid loss: 2.591068e-03 
      	| train loss (relative): 4.982495e-02 | valid loss (relative): 5.227367e-02 
Epoch 163 use: 372.74 second.

epoch 164 starting......
Epoch:  164 | train loss: 2.479567e-03 | valid loss: 2.593981e-03 
      	| train loss (relative): 4.972003e-02 | valid loss (relative): 5.198827e-02 
Epoch 164 use: 370.19 second.

epoch 165 starting......
Epoch:  165 | train loss: 2.474645e-03 | valid loss: 2.585619e-03 
      	| train loss (relative): 4.962529e-02 | valid loss (relative): 5.135069e-02 
Epoch 165 use: 380.15 second.

epoch 166 starting......
Epoch:  166 | train loss: 2.470751e-03 | valid loss: 2.582887e-03 
      	| train loss (relative): 4.952543e-02 | valid loss (relative): 5.185000e-02 
Epoch 166 use: 377.91 second.

epoch 167 starting......
Epoch:  167 | train loss: 2.468897e-03 | valid loss: 2.582682e-03 
      	| train loss (relative): 4.949361e-02 | valid loss (relative): 5.139162e-02 
Epoch 167 use: 388.85 second.

epoch 168 starting......
Epoch:  168 | train loss: 2.462853e-03 | valid loss: 2.571998e-03 
      	| train loss (relative): 4.936150e-02 | valid loss (relative): 5.202205e-02 
Epoch 168 use: 438.95 second.

epoch 169 starting......
Epoch:  169 | train loss: 2.460439e-03 | valid loss: 2.570583e-03 
      	| train loss (relative): 4.931874e-02 | valid loss (relative): 5.109023e-02 
Epoch 169 use: 362.72 second.

epoch 170 starting......
Epoch:  170 | train loss: 2.458716e-03 | valid loss: 2.582590e-03 
      	| train loss (relative): 4.928092e-02 | valid loss (relative): 5.158171e-02 
Epoch 170 use: 363.80 second.

epoch 171 starting......
Epoch:  171 | train loss: 2.459073e-03 | valid loss: 2.561510e-03 
      	| train loss (relative): 4.928721e-02 | valid loss (relative): 5.188350e-02 
Epoch 171 use: 369.97 second.

epoch 172 starting......
Epoch:  172 | train loss: 2.448392e-03 | valid loss: 2.551542e-03 
      	| train loss (relative): 4.906546e-02 | valid loss (relative): 5.126927e-02 
Epoch 172 use: 361.34 second.

epoch 173 starting......
Epoch:  173 | train loss: 2.443694e-03 | valid loss: 2.570353e-03 
      	| train loss (relative): 4.896703e-02 | valid loss (relative): 5.117791e-02 
Epoch 173 use: 353.06 second.

epoch 174 starting......
Epoch:  174 | train loss: 2.444538e-03 | valid loss: 2.551828e-03 
      	| train loss (relative): 4.895992e-02 | valid loss (relative): 5.153647e-02 
Epoch 174 use: 361.16 second.

epoch 175 starting......
Epoch:  175 | train loss: 2.436130e-03 | valid loss: 2.556744e-03 
      	| train loss (relative): 4.880619e-02 | valid loss (relative): 5.076154e-02 
Epoch 175 use: 353.13 second.

epoch 176 starting......
Epoch:  176 | train loss: 2.435727e-03 | valid loss: 2.548680e-03 
      	| train loss (relative): 4.878973e-02 | valid loss (relative): 5.106925e-02 
Epoch 176 use: 355.58 second.

epoch 177 starting......
Epoch:  177 | train loss: 2.433011e-03 | valid loss: 2.539059e-03 
      	| train loss (relative): 4.874075e-02 | valid loss (relative): 5.042363e-02 
Epoch 177 use: 371.03 second.

epoch 178 starting......
Epoch:  178 | train loss: 2.430206e-03 | valid loss: 2.536573e-03 
      	| train loss (relative): 4.866689e-02 | valid loss (relative): 5.140880e-02 
Epoch 178 use: 353.04 second.

epoch 179 starting......
Epoch:  179 | train loss: 2.422056e-03 | valid loss: 2.530266e-03 
      	| train loss (relative): 4.850814e-02 | valid loss (relative): 5.072251e-02 
Epoch 179 use: 374.42 second.

epoch 180 starting......
Epoch:  180 | train loss: 2.419466e-03 | valid loss: 2.525025e-03 
      	| train loss (relative): 4.845002e-02 | valid loss (relative): 5.061364e-02 
Epoch 180 use: 359.69 second.

epoch 181 starting......
Epoch:  181 | train loss: 2.413630e-03 | valid loss: 2.526196e-03 
      	| train loss (relative): 4.833351e-02 | valid loss (relative): 5.034691e-02 
Epoch 181 use: 357.79 second.

epoch 182 starting......
Epoch:  182 | train loss: 2.416805e-03 | valid loss: 2.543788e-03 
      	| train loss (relative): 4.839289e-02 | valid loss (relative): 5.063672e-02 
Epoch 182 use: 370.75 second.

epoch 183 starting......
Epoch:  183 | train loss: 2.417490e-03 | valid loss: 2.524758e-03 
      	| train loss (relative): 4.840959e-02 | valid loss (relative): 5.038838e-02 
Epoch 183 use: 377.13 second.

epoch 184 starting......
Epoch:  184 | train loss: 2.411886e-03 | valid loss: 2.511152e-03 
      	| train loss (relative): 4.829541e-02 | valid loss (relative): 5.034628e-02 
Epoch 184 use: 366.36 second.

epoch 185 starting......
Epoch:  185 | train loss: 2.401671e-03 | valid loss: 2.521609e-03 
      	| train loss (relative): 4.809271e-02 | valid loss (relative): 5.057308e-02 
Epoch 185 use: 370.99 second.

epoch 186 starting......
Epoch:  186 | train loss: 2.403283e-03 | valid loss: 2.509416e-03 
      	| train loss (relative): 4.811193e-02 | valid loss (relative): 5.053899e-02 
Epoch 186 use: 362.97 second.

epoch 187 starting......
Epoch:  187 | train loss: 2.402797e-03 | valid loss: 2.505282e-03 
      	| train loss (relative): 4.810204e-02 | valid loss (relative): 5.006700e-02 
Epoch 187 use: 375.56 second.

epoch 188 starting......
Epoch:  188 | train loss: 2.392029e-03 | valid loss: 2.500661e-03 
      	| train loss (relative): 4.787523e-02 | valid loss (relative): 5.035586e-02 
Epoch 188 use: 378.01 second.

epoch 189 starting......
Epoch:  189 | train loss: 2.393171e-03 | valid loss: 2.504929e-03 
      	| train loss (relative): 4.790038e-02 | valid loss (relative): 4.999484e-02 
Epoch 189 use: 360.36 second.

epoch 190 starting......
Epoch:  190 | train loss: 2.391578e-03 | valid loss: 2.500123e-03 
      	| train loss (relative): 4.787070e-02 | valid loss (relative): 4.995274e-02 
Epoch 190 use: 368.52 second.

epoch 191 starting......
Epoch:  191 | train loss: 2.387667e-03 | valid loss: 2.490559e-03 
      	| train loss (relative): 4.778677e-02 | valid loss (relative): 4.983113e-02 
Epoch 191 use: 356.26 second.

epoch 192 starting......
Epoch:  192 | train loss: 2.383464e-03 | valid loss: 2.484493e-03 
      	| train loss (relative): 4.768582e-02 | valid loss (relative): 4.949721e-02 
Epoch 192 use: 313.71 second.

epoch 193 starting......
Epoch:  193 | train loss: 2.376599e-03 | valid loss: 2.484176e-03 
      	| train loss (relative): 4.754649e-02 | valid loss (relative): 4.986591e-02 
Epoch 193 use: 326.46 second.

epoch 194 starting......
Epoch:  194 | train loss: 2.379405e-03 | valid loss: 2.492918e-03 
      	| train loss (relative): 4.760911e-02 | valid loss (relative): 4.958189e-02 
Epoch 194 use: 327.38 second.

epoch 195 starting......
Epoch:  195 | train loss: 2.376484e-03 | valid loss: 2.476315e-03 
      	| train loss (relative): 4.754186e-02 | valid loss (relative): 4.968550e-02 
Epoch 195 use: 309.45 second.

epoch 196 starting......
Epoch:  196 | train loss: 2.369962e-03 | valid loss: 2.491442e-03 
      	| train loss (relative): 4.740646e-02 | valid loss (relative): 5.033485e-02 
Epoch 196 use: 314.51 second.

epoch 197 starting......
Epoch:  197 | train loss: 2.370286e-03 | valid loss: 2.478082e-03 
      	| train loss (relative): 4.741538e-02 | valid loss (relative): 4.970927e-02 
Epoch 197 use: 336.56 second.

epoch 198 starting......
Epoch:  198 | train loss: 2.367785e-03 | valid loss: 2.469621e-03 
      	| train loss (relative): 4.736987e-02 | valid loss (relative): 4.928515e-02 
Epoch 198 use: 359.44 second.

epoch 199 starting......
Epoch:  199 | train loss: 2.360929e-03 | valid loss: 2.464501e-03 
      	| train loss (relative): 4.722174e-02 | valid loss (relative): 4.938319e-02 
Epoch 199 use: 380.91 second.

epoch 200 starting......
Epoch:  200 | train loss: 2.357079e-03 | valid loss: 2.466371e-03 
      	| train loss (relative): 4.714398e-02 | valid loss (relative): 4.915874e-02 
Epoch 200 use: 456.61 second.

epoch 201 starting......
Epoch:  201 | train loss: 2.359923e-03 | valid loss: 2.501432e-03 
      	| train loss (relative): 4.719117e-02 | valid loss (relative): 4.966880e-02 
Epoch 201 use: 521.27 second.

epoch 202 starting......
Epoch:  202 | train loss: 2.362753e-03 | valid loss: 2.456173e-03 
      	| train loss (relative): 4.725759e-02 | valid loss (relative): 4.912424e-02 
Epoch 202 use: 485.30 second.

epoch 203 starting......
Epoch:  203 | train loss: 2.342674e-03 | valid loss: 2.446388e-03 
      	| train loss (relative): 4.684329e-02 | valid loss (relative): 4.890918e-02 
Epoch 203 use: 492.95 second.

epoch 204 starting......
Epoch:  204 | train loss: 2.340447e-03 | valid loss: 2.450039e-03 
      	| train loss (relative): 4.679073e-02 | valid loss (relative): 4.904260e-02 
Epoch 204 use: 450.94 second.

epoch 205 starting......
Epoch:  205 | train loss: 2.342061e-03 | valid loss: 2.447698e-03 
      	| train loss (relative): 4.682394e-02 | valid loss (relative): 4.900081e-02 
Epoch 205 use: 450.21 second.

epoch 206 starting......
Epoch:  206 | train loss: 2.343243e-03 | valid loss: 2.469656e-03 
      	| train loss (relative): 4.685270e-02 | valid loss (relative): 4.913066e-02 
Epoch 206 use: 494.59 second.

epoch 207 starting......
Epoch:  207 | train loss: 2.345626e-03 | valid loss: 2.439356e-03 
      	| train loss (relative): 4.688329e-02 | valid loss (relative): 4.861148e-02 
Epoch 207 use: 467.03 second.

epoch 208 starting......
Epoch:  208 | train loss: 2.332016e-03 | valid loss: 2.439658e-03 
      	| train loss (relative): 4.661392e-02 | valid loss (relative): 4.860102e-02 
Epoch 208 use: 448.70 second.

epoch 209 starting......
Epoch:  209 | train loss: 2.333364e-03 | valid loss: 2.437556e-03 
      	| train loss (relative): 4.663830e-02 | valid loss (relative): 4.881857e-02 
Epoch 209 use: 478.82 second.

epoch 210 starting......
Epoch:  210 | train loss: 2.330485e-03 | valid loss: 2.435052e-03 
      	| train loss (relative): 4.658488e-02 | valid loss (relative): 4.890708e-02 
Epoch 210 use: 504.41 second.

epoch 211 starting......
Epoch:  211 | train loss: 2.326146e-03 | valid loss: 2.429700e-03 
      	| train loss (relative): 4.649459e-02 | valid loss (relative): 4.860597e-02 
Epoch 211 use: 387.95 second.

epoch 212 starting......
Epoch:  212 | train loss: 2.323596e-03 | valid loss: 2.442502e-03 
      	| train loss (relative): 4.643955e-02 | valid loss (relative): 4.884848e-02 
Epoch 212 use: 382.54 second.

epoch 213 starting......
Epoch:  213 | train loss: 2.325427e-03 | valid loss: 2.430769e-03 
      	| train loss (relative): 4.647194e-02 | valid loss (relative): 4.855670e-02 
Epoch 213 use: 369.82 second.

epoch 214 starting......
Epoch:  214 | train loss: 2.322030e-03 | valid loss: 2.424753e-03 
      	| train loss (relative): 4.640587e-02 | valid loss (relative): 4.856388e-02 
Epoch 214 use: 357.47 second.

epoch 215 starting......
Epoch:  215 | train loss: 2.315924e-03 | valid loss: 2.423552e-03 
      	| train loss (relative): 4.626426e-02 | valid loss (relative): 4.812016e-02 
Epoch 215 use: 370.71 second.

epoch 216 starting......
Epoch:  216 | train loss: 2.316898e-03 | valid loss: 2.423025e-03 
      	| train loss (relative): 4.629846e-02 | valid loss (relative): 4.844547e-02 
Epoch 216 use: 379.90 second.

epoch 217 starting......
Epoch:  217 | train loss: 2.316120e-03 | valid loss: 2.421857e-03 
      	| train loss (relative): 4.627224e-02 | valid loss (relative): 4.827413e-02 
Epoch 217 use: 520.57 second.

epoch 218 starting......
Epoch:  218 | train loss: 2.316094e-03 | valid loss: 2.412098e-03 
      	| train loss (relative): 4.627059e-02 | valid loss (relative): 4.849805e-02 
Epoch 218 use: 674.41 second.

epoch 219 starting......
Epoch:  219 | train loss: 2.308141e-03 | valid loss: 2.410510e-03 
      	| train loss (relative): 4.611411e-02 | valid loss (relative): 4.793888e-02 
Epoch 219 use: 607.51 second.

epoch 220 starting......
Epoch:  220 | train loss: 2.305113e-03 | valid loss: 2.411748e-03 
      	| train loss (relative): 4.604545e-02 | valid loss (relative): 4.826653e-02 
Epoch 220 use: 588.29 second.

epoch 221 starting......
Epoch:  221 | train loss: 2.307584e-03 | valid loss: 2.411213e-03 
      	| train loss (relative): 4.609935e-02 | valid loss (relative): 4.834908e-02 
Epoch 221 use: 660.70 second.

epoch 222 starting......
Epoch:  222 | train loss: 2.302116e-03 | valid loss: 2.405426e-03 
      	| train loss (relative): 4.598977e-02 | valid loss (relative): 4.807551e-02 
Epoch 222 use: 536.89 second.

epoch 223 starting......
Epoch:  223 | train loss: 2.299138e-03 | valid loss: 2.406708e-03 
      	| train loss (relative): 4.591522e-02 | valid loss (relative): 4.834289e-02 
Epoch 223 use: 543.79 second.

epoch 224 starting......
Epoch:  224 | train loss: 2.298332e-03 | valid loss: 2.402871e-03 
      	| train loss (relative): 4.591046e-02 | valid loss (relative): 4.788084e-02 
Epoch 224 use: 593.24 second.

epoch 225 starting......
Epoch:  225 | train loss: 2.293985e-03 | valid loss: 2.400061e-03 
      	| train loss (relative): 4.580799e-02 | valid loss (relative): 4.774325e-02 
Epoch 225 use: 597.45 second.

epoch 226 starting......
Epoch:  226 | train loss: 2.294166e-03 | valid loss: 2.411959e-03 
      	| train loss (relative): 4.581586e-02 | valid loss (relative): 4.850563e-02 
Epoch 226 use: 574.09 second.

epoch 227 starting......
Epoch:  227 | train loss: 2.288713e-03 | valid loss: 2.390522e-03 
      	| train loss (relative): 4.570531e-02 | valid loss (relative): 4.767231e-02 
Epoch 227 use: 475.32 second.

epoch 228 starting......
Epoch:  228 | train loss: 2.284322e-03 | valid loss: 2.381911e-03 
      	| train loss (relative): 4.561203e-02 | valid loss (relative): 4.756768e-02 
Epoch 228 use: 473.20 second.

epoch 229 starting......
Epoch:  229 | train loss: 2.277028e-03 | valid loss: 2.385148e-03 
      	| train loss (relative): 4.546844e-02 | valid loss (relative): 4.749659e-02 
Epoch 229 use: 492.76 second.

epoch 230 starting......
Epoch:  230 | train loss: 2.281321e-03 | valid loss: 2.387382e-03 
      	| train loss (relative): 4.554444e-02 | valid loss (relative): 4.751044e-02 
Epoch 230 use: 496.01 second.

epoch 231 starting......
Epoch:  231 | train loss: 2.278128e-03 | valid loss: 2.380313e-03 
      	| train loss (relative): 4.547720e-02 | valid loss (relative): 4.715133e-02 
Epoch 231 use: 477.75 second.

epoch 232 starting......
Epoch:  232 | train loss: 2.271572e-03 | valid loss: 2.373402e-03 
      	| train loss (relative): 4.534838e-02 | valid loss (relative): 4.738300e-02 
Epoch 232 use: 488.94 second.

epoch 233 starting......
Epoch:  233 | train loss: 2.271667e-03 | valid loss: 2.384279e-03 
      	| train loss (relative): 4.534023e-02 | valid loss (relative): 4.776107e-02 
Epoch 233 use: 450.76 second.

epoch 234 starting......
Epoch:  234 | train loss: 2.274079e-03 | valid loss: 2.389945e-03 
      	| train loss (relative): 4.539580e-02 | valid loss (relative): 4.802074e-02 
Epoch 234 use: 470.67 second.

epoch 235 starting......
Epoch:  235 | train loss: 2.273019e-03 | valid loss: 2.369236e-03 
      	| train loss (relative): 4.537608e-02 | valid loss (relative): 4.704631e-02 
Epoch 235 use: 441.32 second.

epoch 236 starting......
Epoch:  236 | train loss: 2.262390e-03 | valid loss: 2.363275e-03 
      	| train loss (relative): 4.514861e-02 | valid loss (relative): 4.712689e-02 
Epoch 236 use: 452.35 second.

epoch 237 starting......
Epoch:  237 | train loss: 2.262066e-03 | valid loss: 2.370340e-03 
      	| train loss (relative): 4.514014e-02 | valid loss (relative): 4.709442e-02 
Epoch 237 use: 442.80 second.

epoch 238 starting......
Epoch:  238 | train loss: 2.264935e-03 | valid loss: 2.365465e-03 
      	| train loss (relative): 4.519620e-02 | valid loss (relative): 4.713825e-02 
Epoch 238 use: 470.40 second.

epoch 239 starting......
Epoch:  239 | train loss: 2.259747e-03 | valid loss: 2.383644e-03 
      	| train loss (relative): 4.509304e-02 | valid loss (relative): 4.758034e-02 
Epoch 239 use: 451.08 second.

epoch 240 starting......
Epoch:  240 | train loss: 2.263316e-03 | valid loss: 2.361360e-03 
      	| train loss (relative): 4.516813e-02 | valid loss (relative): 4.688714e-02 
Epoch 240 use: 448.64 second.

epoch 241 starting......
Epoch:  241 | train loss: 2.254349e-03 | valid loss: 2.360739e-03 
      	| train loss (relative): 4.497612e-02 | valid loss (relative): 4.706974e-02 
Epoch 241 use: 467.83 second.

epoch 242 starting......
Epoch:  242 | train loss: 2.253731e-03 | valid loss: 2.360253e-03 
      	| train loss (relative): 4.496729e-02 | valid loss (relative): 4.694158e-02 
Epoch 242 use: 449.91 second.

epoch 243 starting......
Epoch:  243 | train loss: 2.252090e-03 | valid loss: 2.349607e-03 
      	| train loss (relative): 4.492885e-02 | valid loss (relative): 4.697940e-02 
Epoch 243 use: 485.05 second.

epoch 244 starting......
Epoch:  244 | train loss: 2.250963e-03 | valid loss: 2.360752e-03 
      	| train loss (relative): 4.490399e-02 | valid loss (relative): 4.712848e-02 
Epoch 244 use: 436.03 second.

epoch 245 starting......
Epoch:  245 | train loss: 2.250521e-03 | valid loss: 2.356030e-03 
      	| train loss (relative): 4.489930e-02 | valid loss (relative): 4.663017e-02 
Epoch 245 use: 432.59 second.

epoch 246 starting......
Epoch:  246 | train loss: 2.246537e-03 | valid loss: 2.358497e-03 
      	| train loss (relative): 4.481694e-02 | valid loss (relative): 4.723944e-02 
Epoch 246 use: 454.52 second.

epoch 247 starting......
Epoch:  247 | train loss: 2.251855e-03 | valid loss: 2.360271e-03 
      	| train loss (relative): 4.491290e-02 | valid loss (relative): 4.683620e-02 
Epoch 247 use: 437.01 second.

epoch 248 starting......
Epoch:  248 | train loss: 2.246191e-03 | valid loss: 2.344069e-03 
      	| train loss (relative): 4.480009e-02 | valid loss (relative): 4.663060e-02 
Epoch 248 use: 479.10 second.

epoch 249 starting......
Epoch:  249 | train loss: 2.237313e-03 | valid loss: 2.341991e-03 
      	| train loss (relative): 4.462869e-02 | valid loss (relative): 4.673694e-02 
Epoch 249 use: 433.23 second.

test MSE Error: 2.118563e-03 | relative MSE Error: 4.205241e-02 
 Total time used for training: 17.12 hour.
MESLoss saved to  /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_MSELoss_Latent_128_nearest_neighbouring_True_SFC_nums_3_startlr_0.0005_n_epoches_250.txt
relative MSELoss saved to  /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_reMSELoss_Latent_128_nearest_neighbouring_True_SFC_nums_3_startlr_0.0005_n_epoches_250.txt
model saved to /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_128_Nearest_neighbouring_True_SFC_nums_3_startlr_0.0005_n_epoches_250.pth
model_dict saved to /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_128_Nearest_neighbouring_True_SFC_nums_3_startlr_0.0005_n_epoches_250_dict.pth
... Training slugflow data completed, Run finished Mon 16 Aug 05:02:56 BST 2021 ...
{'vtu_dir': '/rds/general/user/jy220/home/slugflow/', 'data_dir': '/rds/general/user/jy220/home/tensors/', 'data_type': 'tensors', 'vtu_fields': 'Component1::ComponentMassFractionPhase1, phase1::Velocity', 'variational': 'False', 'optimizer': 'Adamax', 'mode': 'train', 'structured': 'False', 'changing_lr': 'False', 'self_concat': '2', 'dimension': '3', 'sfc_nums': '3', 'components': '4', 'nearest_neighbouring': 'True', 'dims_latent': '128', 'activation': 'Tanh', 'tk_file': '/rds/general/user/jy220/home/slugflow_tk.pt', 'tb_file': '/rds/general/user/jy220/home/slugflow_tb.pt', 'sfc_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/sfcs.pt', 'inv_sfc_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/inv_sfcs.pt', 'coords_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/coords.pt', 'cells_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/cells.npy', 'state_load': '/rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_128_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_900_dict.pth', 'batch_size': '16', 'lr': '0.001', 'n_epoches': '150', 'seed': '15', 'visualize': 'False', 'reconstructed_path': 'None', 'reconstruct_start_index': '0', 'reconstruct_end_index': '100', 'save_path': '/rds/general/user/jy220/home/new_results/'}
[[ 2.50825269e-01 -1.67256990e-02 -1.22268730e-03]
 [ 2.38611959e-01 -5.50627777e-03 -5.06451913e-03]
 [ 2.39454874e-01 -1.59563257e-02  3.96778323e-03]
 ...
 [ 9.11361096e+00 -2.21908381e-02  2.22384902e-03]
 [ 9.11847310e+00 -2.84346931e-02 -2.34839590e-04]
 [ 9.11039661e+00 -2.51498573e-02 -8.70661044e-03]]
{'tetra': array([[      0,       1,       2,       3],
       [      4,       5,       6,       7],
       [      8,       9,      10,      11],
       ...,
       [1342744, 1342745, 1342746, 1342747],
       [1342748, 1342749, 1342750, 1342751],
       [1342752, 1342753, 1342754, 1342755]])}
vtu fields:  ['Component1::ComponentMassFractionPhase1', 'phase1::Velocity'] 
 structured  False 
 activation  Tanh() 
 self concat  2 
 sfc_nums  3 

dims_latent  128 
 components  4 
 nearest_neighbouring  True 

visualize  False 
 sample number  1706 

reading sfc nums......
[array([  41684,   41685,   41674, ..., 1235560, 1235564, 1235558]), array([954289, 954290, 954291, ..., 420685, 420687, 420688]), array([  40156,   40155,   40158, ..., 1123478, 1123475, 1123467])]
reading inverse sfc nums......
[array([   8146,    8145,    8144, ..., 1274444, 1274445, 1273166]), array([547280, 547283, 593037, ..., 933933, 933932, 864488]), array([ 66437,  66439, 122512, ..., 459416, 415130, 459418])]
tk:  tensor([0.7581, 0.0418, 0.1840, 0.1464], dtype=torch.float64) 

tb:  tensor([-0.3816, -0.3618, -0.0308,  0.1446], dtype=torch.float64) 

length of train set: 1505 

length of valid set: 100 

length of test set: 101 

torch device num: 4 

Let's use 4 GPUs!
epoch 900 starting......
Epoch:  900 | train loss: 1.727218e-03 | valid loss: 1.417337e-03 
      	| train loss (relative): 3.387826e-02 | valid loss (relative): 2.773783e-02 
Epoch 900 use: 920.69 second.

epoch 901 starting......
Epoch:  901 | train loss: 1.510715e-03 | valid loss: 1.407315e-03 
      	| train loss (relative): 2.964263e-02 | valid loss (relative): 2.749592e-02 
Epoch 901 use: 689.31 second.

epoch 902 starting......
Epoch:  902 | train loss: 1.501667e-03 | valid loss: 1.405894e-03 
      	| train loss (relative): 2.945563e-02 | valid loss (relative): 2.748619e-02 
Epoch 902 use: 584.87 second.

epoch 903 starting......
Epoch:  903 | train loss: 1.498132e-03 | valid loss: 1.405449e-03 
      	| train loss (relative): 2.937999e-02 | valid loss (relative): 2.748002e-02 
Epoch 903 use: 612.01 second.

epoch 904 starting......
Epoch:  904 | train loss: 1.496851e-03 | valid loss: 1.403077e-03 
      	| train loss (relative): 2.935706e-02 | valid loss (relative): 2.741383e-02 
Epoch 904 use: 477.72 second.

epoch 905 starting......
Epoch:  905 | train loss: 1.495568e-03 | valid loss: 1.404654e-03 
      	| train loss (relative): 2.932964e-02 | valid loss (relative): 2.747914e-02 
Epoch 905 use: 444.22 second.

epoch 906 starting......
Epoch:  906 | train loss: 1.496915e-03 | valid loss: 1.405703e-03 
      	| train loss (relative): 2.935557e-02 | valid loss (relative): 2.747011e-02 
Epoch 906 use: 513.05 second.

epoch 907 starting......
Epoch:  907 | train loss: 1.495123e-03 | valid loss: 1.405566e-03 
      	| train loss (relative): 2.932277e-02 | valid loss (relative): 2.745579e-02 
Epoch 907 use: 496.66 second.

epoch 908 starting......
Epoch:  908 | train loss: 1.496382e-03 | valid loss: 1.405398e-03 
      	| train loss (relative): 2.934247e-02 | valid loss (relative): 2.747037e-02 
Epoch 908 use: 474.82 second.

epoch 909 starting......
Epoch:  909 | train loss: 1.495652e-03 | valid loss: 1.407655e-03 
      	| train loss (relative): 2.932834e-02 | valid loss (relative): 2.747081e-02 
Epoch 909 use: 612.82 second.

epoch 910 starting......
Epoch:  910 | train loss: 1.496570e-03 | valid loss: 1.409855e-03 
      	| train loss (relative): 2.934446e-02 | valid loss (relative): 2.754620e-02 
Epoch 910 use: 592.61 second.

epoch 911 starting......
Epoch:  911 | train loss: 1.500178e-03 | valid loss: 1.413361e-03 
      	| train loss (relative): 2.942088e-02 | valid loss (relative): 2.758168e-02 
Epoch 911 use: 499.13 second.

epoch 912 starting......
Epoch:  912 | train loss: 1.502342e-03 | valid loss: 1.414153e-03 
      	| train loss (relative): 2.946026e-02 | valid loss (relative): 2.759363e-02 
Epoch 912 use: 584.93 second.

epoch 913 starting......
Epoch:  913 | train loss: 1.500521e-03 | valid loss: 1.414822e-03 
      	| train loss (relative): 2.942758e-02 | valid loss (relative): 2.766572e-02 
Epoch 913 use: 645.21 second.

epoch 914 starting......
Epoch:  914 | train loss: 1.502390e-03 | valid loss: 1.416434e-03 
      	| train loss (relative): 2.946050e-02 | valid loss (relative): 2.768258e-02 
Epoch 914 use: 630.77 second.

epoch 915 starting......
Epoch:  915 | train loss: 1.505964e-03 | valid loss: 1.421430e-03 
      	| train loss (relative): 2.953422e-02 | valid loss (relative): 2.783915e-02 
Epoch 915 use: 680.53 second.

epoch 916 starting......
Epoch:  916 | train loss: 1.504623e-03 | valid loss: 1.421328e-03 
      	| train loss (relative): 2.950978e-02 | valid loss (relative): 2.779914e-02 
Epoch 916 use: 727.75 second.

epoch 917 starting......
Epoch:  917 | train loss: 1.505376e-03 | valid loss: 1.420880e-03 
      	| train loss (relative): 2.952275e-02 | valid loss (relative): 2.777005e-02 
Epoch 917 use: 906.39 second.

epoch 918 starting......
Epoch:  918 | train loss: 1.502966e-03 | valid loss: 1.412206e-03 
      	| train loss (relative): 2.947077e-02 | valid loss (relative): 2.765618e-02 
Epoch 918 use: 700.35 second.

epoch 919 starting......
Epoch:  919 | train loss: 1.499312e-03 | valid loss: 1.416213e-03 
      	| train loss (relative): 2.939891e-02 | valid loss (relative): 2.776877e-02 
Epoch 919 use: 544.76 second.

epoch 920 starting......
Epoch:  920 | train loss: 1.501517e-03 | valid loss: 1.419810e-03 
      	| train loss (relative): 2.944021e-02 | valid loss (relative): 2.773804e-02 
Epoch 920 use: 417.80 second.

epoch 921 starting......
Epoch:  921 | train loss: 1.501612e-03 | valid loss: 1.422847e-03 
      	| train loss (relative): 2.944444e-02 | valid loss (relative): 2.776108e-02 
Epoch 921 use: 409.91 second.

epoch 922 starting......
Epoch:  922 | train loss: 1.503141e-03 | valid loss: 1.423221e-03 
      	| train loss (relative): 2.946781e-02 | valid loss (relative): 2.786233e-02 
Epoch 922 use: 402.52 second.

epoch 923 starting......
Epoch:  923 | train loss: 1.502973e-03 | valid loss: 1.418414e-03 
      	| train loss (relative): 2.947193e-02 | valid loss (relative): 2.766539e-02 
Epoch 923 use: 397.09 second.

epoch 924 starting......
Epoch:  924 | train loss: 1.499618e-03 | valid loss: 1.416176e-03 
      	| train loss (relative): 2.940273e-02 | valid loss (relative): 2.760481e-02 
Epoch 924 use: 394.09 second.

epoch 925 starting......
Epoch:  925 | train loss: 1.500751e-03 | valid loss: 1.422444e-03 
      	| train loss (relative): 2.942511e-02 | valid loss (relative): 2.788239e-02 
Epoch 925 use: 385.92 second.

epoch 926 starting......
Epoch:  926 | train loss: 1.500270e-03 | valid loss: 1.427838e-03 
      	| train loss (relative): 2.941670e-02 | valid loss (relative): 2.798847e-02 
Epoch 926 use: 385.82 second.

epoch 927 starting......
Epoch:  927 | train loss: 1.505075e-03 | valid loss: 1.424917e-03 
      	| train loss (relative): 2.951598e-02 | valid loss (relative): 2.782481e-02 
Epoch 927 use: 395.75 second.

epoch 928 starting......
Epoch:  928 | train loss: 1.502644e-03 | valid loss: 1.424797e-03 
      	| train loss (relative): 2.946307e-02 | valid loss (relative): 2.790505e-02 
Epoch 928 use: 385.45 second.

epoch 929 starting......
Epoch:  929 | train loss: 1.499935e-03 | valid loss: 1.418026e-03 
      	| train loss (relative): 2.941068e-02 | valid loss (relative): 2.771059e-02 
Epoch 929 use: 390.32 second.

epoch 930 starting......
Epoch:  930 | train loss: 1.495930e-03 | valid loss: 1.418015e-03 
      	| train loss (relative): 2.933419e-02 | valid loss (relative): 2.780579e-02 
Epoch 930 use: 379.75 second.

epoch 931 starting......
Epoch:  931 | train loss: 1.499712e-03 | valid loss: 1.421910e-03 
      	| train loss (relative): 2.940782e-02 | valid loss (relative): 2.777714e-02 
Epoch 931 use: 417.11 second.

epoch 932 starting......
Epoch:  932 | train loss: 1.500365e-03 | valid loss: 1.420623e-03 
      	| train loss (relative): 2.941662e-02 | valid loss (relative): 2.779667e-02 
Epoch 932 use: 372.61 second.

epoch 933 starting......
Epoch:  933 | train loss: 1.501455e-03 | valid loss: 1.422808e-03 
      	| train loss (relative): 2.943744e-02 | valid loss (relative): 2.765613e-02 
Epoch 933 use: 394.60 second.

epoch 934 starting......
Epoch:  934 | train loss: 1.500087e-03 | valid loss: 1.421034e-03 
      	| train loss (relative): 2.941169e-02 | valid loss (relative): 2.786201e-02 
Epoch 934 use: 386.30 second.

epoch 935 starting......
Epoch:  935 | train loss: 1.497287e-03 | valid loss: 1.415881e-03 
      	| train loss (relative): 2.935934e-02 | valid loss (relative): 2.755331e-02 
Epoch 935 use: 381.67 second.

epoch 936 starting......
Epoch:  936 | train loss: 1.495298e-03 | valid loss: 1.422665e-03 
      	| train loss (relative): 2.931526e-02 | valid loss (relative): 2.773943e-02 
Epoch 936 use: 377.86 second.

epoch 937 starting......
Epoch:  937 | train loss: 1.498295e-03 | valid loss: 1.420926e-03 
      	| train loss (relative): 2.937394e-02 | valid loss (relative): 2.781091e-02 
Epoch 937 use: 382.36 second.

epoch 938 starting......
Epoch:  938 | train loss: 1.499925e-03 | valid loss: 1.420070e-03 
      	| train loss (relative): 2.940659e-02 | valid loss (relative): 2.771237e-02 
Epoch 938 use: 388.95 second.

epoch 939 starting......
Epoch:  939 | train loss: 1.497653e-03 | valid loss: 1.419085e-03 
      	| train loss (relative): 2.936765e-02 | valid loss (relative): 2.768768e-02 
Epoch 939 use: 376.47 second.

epoch 940 starting......
Epoch:  940 | train loss: 1.496603e-03 | valid loss: 1.422647e-03 
      	| train loss (relative): 2.933732e-02 | valid loss (relative): 2.785482e-02 
Epoch 940 use: 379.91 second.

epoch 941 starting......
Epoch:  941 | train loss: 1.497444e-03 | valid loss: 1.422210e-03 
      	| train loss (relative): 2.935838e-02 | valid loss (relative): 2.772032e-02 
Epoch 941 use: 375.02 second.

epoch 942 starting......
Epoch:  942 | train loss: 1.500531e-03 | valid loss: 1.429409e-03 
      	| train loss (relative): 2.941884e-02 | valid loss (relative): 2.792030e-02 
Epoch 942 use: 384.59 second.

epoch 943 starting......
Epoch:  943 | train loss: 1.501187e-03 | valid loss: 1.420500e-03 
      	| train loss (relative): 2.943387e-02 | valid loss (relative): 2.757678e-02 
Epoch 943 use: 394.26 second.

epoch 944 starting......
Epoch:  944 | train loss: 1.495873e-03 | valid loss: 1.420057e-03 
      	| train loss (relative): 2.933130e-02 | valid loss (relative): 2.762327e-02 
Epoch 944 use: 396.10 second.

epoch 945 starting......
Epoch:  945 | train loss: 1.493745e-03 | valid loss: 1.435373e-03 
      	| train loss (relative): 2.928250e-02 | valid loss (relative): 2.814775e-02 
Epoch 945 use: 377.35 second.

epoch 946 starting......
Epoch:  946 | train loss: 1.498985e-03 | valid loss: 1.418060e-03 
      	| train loss (relative): 2.939038e-02 | valid loss (relative): 2.771680e-02 
Epoch 946 use: 396.04 second.

epoch 947 starting......
Epoch:  947 | train loss: 1.495100e-03 | valid loss: 1.421733e-03 
      	| train loss (relative): 2.931310e-02 | valid loss (relative): 2.792763e-02 
Epoch 947 use: 382.36 second.

epoch 948 starting......
Epoch:  948 | train loss: 1.494586e-03 | valid loss: 1.413144e-03 
      	| train loss (relative): 2.930450e-02 | valid loss (relative): 2.766947e-02 
Epoch 948 use: 417.28 second.

epoch 949 starting......
Epoch:  949 | train loss: 1.489564e-03 | valid loss: 1.420312e-03 
      	| train loss (relative): 2.920275e-02 | valid loss (relative): 2.774669e-02 
Epoch 949 use: 403.33 second.

epoch 950 starting......
Epoch:  950 | train loss: 1.496972e-03 | valid loss: 1.415418e-03 
      	| train loss (relative): 2.934870e-02 | valid loss (relative): 2.756081e-02 
Epoch 950 use: 393.48 second.

epoch 951 starting......
Epoch:  951 | train loss: 1.489896e-03 | valid loss: 1.419778e-03 
      	| train loss (relative): 2.921039e-02 | valid loss (relative): 2.770687e-02 
Epoch 951 use: 396.49 second.

epoch 952 starting......
Epoch:  952 | train loss: 1.493548e-03 | valid loss: 1.424198e-03 
      	| train loss (relative): 2.928088e-02 | valid loss (relative): 2.785326e-02 
Epoch 952 use: 390.68 second.

epoch 953 starting......
Epoch:  953 | train loss: 1.496016e-03 | valid loss: 1.413610e-03 
      	| train loss (relative): 2.932829e-02 | valid loss (relative): 2.769998e-02 
Epoch 953 use: 393.09 second.

epoch 954 starting......
Epoch:  954 | train loss: 1.490435e-03 | valid loss: 1.415555e-03 
      	| train loss (relative): 2.921991e-02 | valid loss (relative): 2.776676e-02 
Epoch 954 use: 392.60 second.

epoch 955 starting......
Epoch:  955 | train loss: 1.487076e-03 | valid loss: 1.412393e-03 
      	| train loss (relative): 2.915081e-02 | valid loss (relative): 2.754718e-02 
Epoch 955 use: 392.40 second.

epoch 956 starting......
Epoch:  956 | train loss: 1.485864e-03 | valid loss: 1.416975e-03 
      	| train loss (relative): 2.913106e-02 | valid loss (relative): 2.779819e-02 
Epoch 956 use: 376.28 second.

epoch 957 starting......
Epoch:  957 | train loss: 1.488848e-03 | valid loss: 1.413207e-03 
      	| train loss (relative): 2.919009e-02 | valid loss (relative): 2.758995e-02 
Epoch 957 use: 393.21 second.

epoch 958 starting......
Epoch:  958 | train loss: 1.490358e-03 | valid loss: 1.426701e-03 
      	| train loss (relative): 2.921542e-02 | valid loss (relative): 2.770654e-02 
Epoch 958 use: 373.93 second.

epoch 959 starting......
Epoch:  959 | train loss: 1.493247e-03 | valid loss: 1.410236e-03 
      	| train loss (relative): 2.926820e-02 | valid loss (relative): 2.762461e-02 
Epoch 959 use: 384.22 second.

epoch 960 starting......
Epoch:  960 | train loss: 1.490237e-03 | valid loss: 1.415525e-03 
      	| train loss (relative): 2.921716e-02 | valid loss (relative): 2.744199e-02 
Epoch 960 use: 394.54 second.

epoch 961 starting......
Epoch:  961 | train loss: 1.486950e-03 | valid loss: 1.413487e-03 
      	| train loss (relative): 2.914510e-02 | valid loss (relative): 2.762768e-02 
Epoch 961 use: 370.42 second.

epoch 962 starting......
Epoch:  962 | train loss: 1.487808e-03 | valid loss: 1.415353e-03 
      	| train loss (relative): 2.916312e-02 | valid loss (relative): 2.763433e-02 
Epoch 962 use: 401.64 second.

epoch 963 starting......
Epoch:  963 | train loss: 1.488770e-03 | valid loss: 1.415238e-03 
      	| train loss (relative): 2.918169e-02 | valid loss (relative): 2.763500e-02 
Epoch 963 use: 383.57 second.

epoch 964 starting......
Epoch:  964 | train loss: 1.489913e-03 | valid loss: 1.410218e-03 
      	| train loss (relative): 2.920643e-02 | valid loss (relative): 2.754791e-02 
Epoch 964 use: 424.29 second.

epoch 965 starting......
Epoch:  965 | train loss: 1.486636e-03 | valid loss: 1.416197e-03 
      	| train loss (relative): 2.914413e-02 | valid loss (relative): 2.762452e-02 
Epoch 965 use: 400.82 second.

epoch 966 starting......
Epoch:  966 | train loss: 1.489728e-03 | valid loss: 1.414969e-03 
      	| train loss (relative): 2.920134e-02 | valid loss (relative): 2.771869e-02 
Epoch 966 use: 427.78 second.

epoch 967 starting......
Epoch:  967 | train loss: 1.492129e-03 | valid loss: 1.410563e-03 
      	| train loss (relative): 2.925365e-02 | valid loss (relative): 2.755172e-02 
Epoch 967 use: 453.51 second.

epoch 968 starting......
Epoch:  968 | train loss: 1.483308e-03 | valid loss: 1.406710e-03 
      	| train loss (relative): 2.907715e-02 | valid loss (relative): 2.752670e-02 
Epoch 968 use: 452.70 second.

epoch 969 starting......
Epoch:  969 | train loss: 1.483429e-03 | valid loss: 1.410820e-03 
      	| train loss (relative): 2.907669e-02 | valid loss (relative): 2.741105e-02 
Epoch 969 use: 411.64 second.

epoch 970 starting......
Epoch:  970 | train loss: 1.482346e-03 | valid loss: 1.411950e-03 
      	| train loss (relative): 2.905289e-02 | valid loss (relative): 2.763165e-02 
Epoch 970 use: 426.66 second.

epoch 971 starting......
Epoch:  971 | train loss: 1.487528e-03 | valid loss: 1.407923e-03 
      	| train loss (relative): 2.915848e-02 | valid loss (relative): 2.752706e-02 
Epoch 971 use: 376.60 second.

epoch 972 starting......
Epoch:  972 | train loss: 1.484630e-03 | valid loss: 1.409781e-03 
      	| train loss (relative): 2.910171e-02 | valid loss (relative): 2.752417e-02 
Epoch 972 use: 412.15 second.

epoch 973 starting......
Epoch:  973 | train loss: 1.486122e-03 | valid loss: 1.409146e-03 
      	| train loss (relative): 2.912946e-02 | valid loss (relative): 2.755388e-02 
Epoch 973 use: 385.97 second.

epoch 974 starting......
Epoch:  974 | train loss: 1.488242e-03 | valid loss: 1.415592e-03 
      	| train loss (relative): 2.917187e-02 | valid loss (relative): 2.763845e-02 
Epoch 974 use: 423.75 second.

epoch 975 starting......
Epoch:  975 | train loss: 1.487775e-03 | valid loss: 1.416297e-03 
      	| train loss (relative): 2.916118e-02 | valid loss (relative): 2.765164e-02 
Epoch 975 use: 392.23 second.

epoch 976 starting......
Epoch:  976 | train loss: 1.489528e-03 | valid loss: 1.422543e-03 
      	| train loss (relative): 2.919421e-02 | valid loss (relative): 2.775159e-02 
Epoch 976 use: 410.46 second.

epoch 977 starting......
Epoch:  977 | train loss: 1.489433e-03 | valid loss: 1.414127e-03 
      	| train loss (relative): 2.919452e-02 | valid loss (relative): 2.779476e-02 
Epoch 977 use: 385.20 second.

epoch 978 starting......
Epoch:  978 | train loss: 1.483288e-03 | valid loss: 1.412551e-03 
      	| train loss (relative): 2.907591e-02 | valid loss (relative): 2.765800e-02 
Epoch 978 use: 404.04 second.

epoch 979 starting......
Epoch:  979 | train loss: 1.484996e-03 | valid loss: 1.416359e-03 
      	| train loss (relative): 2.910499e-02 | valid loss (relative): 2.754601e-02 
Epoch 979 use: 411.07 second.

epoch 980 starting......
Epoch:  980 | train loss: 1.492382e-03 | valid loss: 1.420746e-03 
      	| train loss (relative): 2.925280e-02 | valid loss (relative): 2.760684e-02 
Epoch 980 use: 416.17 second.

epoch 981 starting......
Epoch:  981 | train loss: 1.485698e-03 | valid loss: 1.406927e-03 
      	| train loss (relative): 2.912132e-02 | valid loss (relative): 2.748560e-02 
Epoch 981 use: 391.76 second.

epoch 982 starting......
Epoch:  982 | train loss: 1.477723e-03 | valid loss: 1.405499e-03 
      	| train loss (relative): 2.896747e-02 | valid loss (relative): 2.746214e-02 
Epoch 982 use: 408.27 second.

epoch 983 starting......
Epoch:  983 | train loss: 1.476733e-03 | valid loss: 1.408462e-03 
      	| train loss (relative): 2.894304e-02 | valid loss (relative): 2.755290e-02 
Epoch 983 use: 394.87 second.

epoch 984 starting......
Epoch:  984 | train loss: 1.481480e-03 | valid loss: 1.406822e-03 
      	| train loss (relative): 2.904001e-02 | valid loss (relative): 2.750550e-02 
Epoch 984 use: 494.60 second.

epoch 985 starting......
Epoch:  985 | train loss: 1.481693e-03 | valid loss: 1.408186e-03 
      	| train loss (relative): 2.903949e-02 | valid loss (relative): 2.744003e-02 
Epoch 985 use: 401.80 second.

epoch 986 starting......
Epoch:  986 | train loss: 1.480568e-03 | valid loss: 1.410022e-03 
      	| train loss (relative): 2.902067e-02 | valid loss (relative): 2.744894e-02 
Epoch 986 use: 430.96 second.

epoch 987 starting......
Epoch:  987 | train loss: 1.481664e-03 | valid loss: 1.414926e-03 
      	| train loss (relative): 2.903949e-02 | valid loss (relative): 2.755671e-02 
Epoch 987 use: 427.14 second.

epoch 988 starting......
Epoch:  988 | train loss: 1.483813e-03 | valid loss: 1.410990e-03 
      	| train loss (relative): 2.907790e-02 | valid loss (relative): 2.755222e-02 
Epoch 988 use: 416.13 second.

epoch 989 starting......
Epoch:  989 | train loss: 1.483513e-03 | valid loss: 1.418613e-03 
      	| train loss (relative): 2.907800e-02 | valid loss (relative): 2.767439e-02 
Epoch 989 use: 429.20 second.

epoch 990 starting......
Epoch:  990 | train loss: 1.480607e-03 | valid loss: 1.408606e-03 
      	| train loss (relative): 2.902004e-02 | valid loss (relative): 2.748872e-02 
Epoch 990 use: 387.61 second.

epoch 991 starting......
Epoch:  991 | train loss: 1.481972e-03 | valid loss: 1.411086e-03 
      	| train loss (relative): 2.904049e-02 | valid loss (relative): 2.747189e-02 
Epoch 991 use: 416.47 second.

epoch 992 starting......
Epoch:  992 | train loss: 1.481577e-03 | valid loss: 1.405962e-03 
      	| train loss (relative): 2.903957e-02 | valid loss (relative): 2.752526e-02 
Epoch 992 use: 381.11 second.

epoch 993 starting......
Epoch:  993 | train loss: 1.478089e-03 | valid loss: 1.407107e-03 
      	| train loss (relative): 2.896663e-02 | valid loss (relative): 2.737283e-02 
Epoch 993 use: 419.49 second.

epoch 994 starting......
Epoch:  994 | train loss: 1.479294e-03 | valid loss: 1.409860e-03 
      	| train loss (relative): 2.899269e-02 | valid loss (relative): 2.743238e-02 
Epoch 994 use: 393.66 second.

epoch 995 starting......
Epoch:  995 | train loss: 1.478353e-03 | valid loss: 1.407049e-03 
      	| train loss (relative): 2.897054e-02 | valid loss (relative): 2.757668e-02 
Epoch 995 use: 391.49 second.

epoch 996 starting......
Epoch:  996 | train loss: 1.481612e-03 | valid loss: 1.413243e-03 
      	| train loss (relative): 2.904128e-02 | valid loss (relative): 2.750006e-02 
Epoch 996 use: 397.87 second.

epoch 997 starting......
Epoch:  997 | train loss: 1.482780e-03 | valid loss: 1.407014e-03 
      	| train loss (relative): 2.905948e-02 | valid loss (relative): 2.759919e-02 
Epoch 997 use: 389.96 second.

epoch 998 starting......
Epoch:  998 | train loss: 1.477018e-03 | valid loss: 1.407288e-03 
      	| train loss (relative): 2.894761e-02 | valid loss (relative): 2.742016e-02 
Epoch 998 use: 388.34 second.

epoch 999 starting......
Epoch:  999 | train loss: 1.472829e-03 | valid loss: 1.400200e-03 
      	| train loss (relative): 2.885691e-02 | valid loss (relative): 2.730289e-02 
Epoch 999 use: 407.87 second.

epoch 1000 starting......
Epoch:  1000 | train loss: 1.472598e-03 | valid loss: 1.402007e-03 
      	| train loss (relative): 2.885954e-02 | valid loss (relative): 2.730737e-02 
Epoch 1000 use: 383.05 second.

epoch 1001 starting......
Epoch:  1001 | train loss: 1.476599e-03 | valid loss: 1.415556e-03 
      	| train loss (relative): 2.893558e-02 | valid loss (relative): 2.773399e-02 
Epoch 1001 use: 400.62 second.

epoch 1002 starting......
Epoch:  1002 | train loss: 1.476748e-03 | valid loss: 1.402513e-03 
      	| train loss (relative): 2.893813e-02 | valid loss (relative): 2.746964e-02 
Epoch 1002 use: 377.07 second.

epoch 1003 starting......
Epoch:  1003 | train loss: 1.471662e-03 | valid loss: 1.401260e-03 
      	| train loss (relative): 2.884214e-02 | valid loss (relative): 2.735001e-02 
Epoch 1003 use: 394.57 second.

epoch 1004 starting......
Epoch:  1004 | train loss: 1.477213e-03 | valid loss: 1.400986e-03 
      	| train loss (relative): 2.895202e-02 | valid loss (relative): 2.752263e-02 
Epoch 1004 use: 382.04 second.

epoch 1005 starting......
Epoch:  1005 | train loss: 1.474650e-03 | valid loss: 1.403821e-03 
      	| train loss (relative): 2.889898e-02 | valid loss (relative): 2.746120e-02 
Epoch 1005 use: 370.63 second.

epoch 1006 starting......
Epoch:  1006 | train loss: 1.471322e-03 | valid loss: 1.399813e-03 
      	| train loss (relative): 2.883148e-02 | valid loss (relative): 2.748946e-02 
Epoch 1006 use: 405.37 second.

epoch 1007 starting......
Epoch:  1007 | train loss: 1.472576e-03 | valid loss: 1.405931e-03 
      	| train loss (relative): 2.885826e-02 | valid loss (relative): 2.748381e-02 
Epoch 1007 use: 378.65 second.

epoch 1008 starting......
Epoch:  1008 | train loss: 1.477549e-03 | valid loss: 1.411953e-03 
      	| train loss (relative): 2.895222e-02 | valid loss (relative): 2.758610e-02 
Epoch 1008 use: 400.47 second.

epoch 1009 starting......
Epoch:  1009 | train loss: 1.481568e-03 | valid loss: 1.404477e-03 
      	| train loss (relative): 2.903703e-02 | valid loss (relative): 2.738301e-02 
Epoch 1009 use: 392.85 second.

epoch 1010 starting......
Epoch:  1010 | train loss: 1.473005e-03 | valid loss: 1.400351e-03 
      	| train loss (relative): 2.886257e-02 | valid loss (relative): 2.735466e-02 
Epoch 1010 use: 381.46 second.

epoch 1011 starting......
Epoch:  1011 | train loss: 1.472948e-03 | valid loss: 1.411557e-03 
      	| train loss (relative): 2.885968e-02 | valid loss (relative): 2.772392e-02 
Epoch 1011 use: 381.51 second.

epoch 1012 starting......
Epoch:  1012 | train loss: 1.475467e-03 | valid loss: 1.403377e-03 
      	| train loss (relative): 2.891645e-02 | valid loss (relative): 2.733637e-02 
Epoch 1012 use: 376.86 second.

epoch 1013 starting......
Epoch:  1013 | train loss: 1.472933e-03 | valid loss: 1.410346e-03 
      	| train loss (relative): 2.887106e-02 | valid loss (relative): 2.736249e-02 
Epoch 1013 use: 390.09 second.

epoch 1014 starting......
Epoch:  1014 | train loss: 1.475819e-03 | valid loss: 1.405377e-03 
      	| train loss (relative): 2.891850e-02 | valid loss (relative): 2.739396e-02 
Epoch 1014 use: 365.83 second.

epoch 1015 starting......
Epoch:  1015 | train loss: 1.469452e-03 | valid loss: 1.407021e-03 
      	| train loss (relative): 2.879516e-02 | valid loss (relative): 2.758251e-02 
Epoch 1015 use: 386.61 second.

epoch 1016 starting......
Epoch:  1016 | train loss: 1.472408e-03 | valid loss: 1.405922e-03 
      	| train loss (relative): 2.885794e-02 | valid loss (relative): 2.747982e-02 
Epoch 1016 use: 375.12 second.

epoch 1017 starting......
Epoch:  1017 | train loss: 1.471713e-03 | valid loss: 1.409152e-03 
      	| train loss (relative): 2.884060e-02 | valid loss (relative): 2.754403e-02 
Epoch 1017 use: 385.27 second.

epoch 1018 starting......
Epoch:  1018 | train loss: 1.474457e-03 | valid loss: 1.400568e-03 
      	| train loss (relative): 2.889227e-02 | valid loss (relative): 2.739751e-02 
Epoch 1018 use: 385.21 second.

epoch 1019 starting......
Epoch:  1019 | train loss: 1.469645e-03 | valid loss: 1.411397e-03 
      	| train loss (relative): 2.880293e-02 | valid loss (relative): 2.764218e-02 
Epoch 1019 use: 387.77 second.

epoch 1020 starting......
Epoch:  1020 | train loss: 1.473286e-03 | valid loss: 1.406154e-03 
      	| train loss (relative): 2.887182e-02 | valid loss (relative): 2.738347e-02 
Epoch 1020 use: 389.49 second.

epoch 1021 starting......
Epoch:  1021 | train loss: 1.471977e-03 | valid loss: 1.400568e-03 
      	| train loss (relative): 2.884238e-02 | valid loss (relative): 2.745598e-02 
Epoch 1021 use: 392.47 second.

epoch 1022 starting......
Epoch:  1022 | train loss: 1.469321e-03 | valid loss: 1.401342e-03 
      	| train loss (relative): 2.879140e-02 | valid loss (relative): 2.730279e-02 
Epoch 1022 use: 397.53 second.

epoch 1023 starting......
Epoch:  1023 | train loss: 1.468434e-03 | valid loss: 1.401325e-03 
      	| train loss (relative): 2.877556e-02 | valid loss (relative): 2.740571e-02 
Epoch 1023 use: 380.23 second.

epoch 1024 starting......
Epoch:  1024 | train loss: 1.473111e-03 | valid loss: 1.402511e-03 
      	| train loss (relative): 2.886826e-02 | valid loss (relative): 2.722690e-02 
Epoch 1024 use: 388.42 second.

epoch 1025 starting......
Epoch:  1025 | train loss: 1.466424e-03 | valid loss: 1.396376e-03 
      	| train loss (relative): 2.873027e-02 | valid loss (relative): 2.727503e-02 
Epoch 1025 use: 369.12 second.

epoch 1026 starting......
Epoch:  1026 | train loss: 1.463141e-03 | valid loss: 1.399117e-03 
      	| train loss (relative): 2.867021e-02 | valid loss (relative): 2.733023e-02 
Epoch 1026 use: 375.09 second.

epoch 1027 starting......
Epoch:  1027 | train loss: 1.468223e-03 | valid loss: 1.398973e-03 
      	| train loss (relative): 2.877342e-02 | valid loss (relative): 2.732841e-02 
Epoch 1027 use: 400.87 second.

epoch 1028 starting......
Epoch:  1028 | train loss: 1.468489e-03 | valid loss: 1.398807e-03 
      	| train loss (relative): 2.877322e-02 | valid loss (relative): 2.722185e-02 
Epoch 1028 use: 385.24 second.

epoch 1029 starting......
Epoch:  1029 | train loss: 1.468621e-03 | valid loss: 1.418572e-03 
      	| train loss (relative): 2.877544e-02 | valid loss (relative): 2.792136e-02 
Epoch 1029 use: 376.01 second.

epoch 1030 starting......
Epoch:  1030 | train loss: 1.470918e-03 | valid loss: 1.400296e-03 
      	| train loss (relative): 2.882929e-02 | valid loss (relative): 2.737482e-02 
Epoch 1030 use: 398.76 second.

epoch 1031 starting......
Epoch:  1031 | train loss: 1.466859e-03 | valid loss: 1.398175e-03 
      	| train loss (relative): 2.874075e-02 | valid loss (relative): 2.725533e-02 
Epoch 1031 use: 1070.71 second.

epoch 1032 starting......
Epoch:  1032 | train loss: 1.466910e-03 | valid loss: 1.395060e-03 
      	| train loss (relative): 2.874308e-02 | valid loss (relative): 2.725624e-02 
Epoch 1032 use: 410.83 second.

epoch 1033 starting......
Epoch:  1033 | train loss: 1.466812e-03 | valid loss: 1.400081e-03 
      	| train loss (relative): 2.873864e-02 | valid loss (relative): 2.744269e-02 
Epoch 1033 use: 457.55 second.

epoch 1034 starting......
Epoch:  1034 | train loss: 1.471688e-03 | valid loss: 1.404832e-03 
      	| train loss (relative): 2.884288e-02 | valid loss (relative): 2.743620e-02 
Epoch 1034 use: 382.66 second.

epoch 1035 starting......
Epoch:  1035 | train loss: 1.467371e-03 | valid loss: 1.399881e-03 
      	| train loss (relative): 2.874958e-02 | valid loss (relative): 2.723834e-02 
Epoch 1035 use: 397.69 second.

epoch 1036 starting......
Epoch:  1036 | train loss: 1.466383e-03 | valid loss: 1.403831e-03 
      	| train loss (relative): 2.873207e-02 | valid loss (relative): 2.753493e-02 
Epoch 1036 use: 398.69 second.

epoch 1037 starting......
Epoch:  1037 | train loss: 1.469448e-03 | valid loss: 1.403618e-03 
      	| train loss (relative): 2.879528e-02 | valid loss (relative): 2.747927e-02 
Epoch 1037 use: 376.45 second.

epoch 1038 starting......
Epoch:  1038 | train loss: 1.465871e-03 | valid loss: 1.397553e-03 
      	| train loss (relative): 2.872195e-02 | valid loss (relative): 2.728004e-02 
Epoch 1038 use: 401.94 second.

epoch 1039 starting......
Epoch:  1039 | train loss: 1.466624e-03 | valid loss: 1.409447e-03 
      	| train loss (relative): 2.873407e-02 | valid loss (relative): 2.771076e-02 
Epoch 1039 use: 384.63 second.

epoch 1040 starting......
Epoch:  1040 | train loss: 1.471003e-03 | valid loss: 1.400106e-03 
      	| train loss (relative): 2.882370e-02 | valid loss (relative): 2.733371e-02 
Epoch 1040 use: 373.34 second.

epoch 1041 starting......
Epoch:  1041 | train loss: 1.466978e-03 | valid loss: 1.399005e-03 
      	| train loss (relative): 2.874092e-02 | valid loss (relative): 2.722012e-02 
Epoch 1041 use: 374.15 second.

epoch 1042 starting......
Epoch:  1042 | train loss: 1.462551e-03 | valid loss: 1.396428e-03 
      	| train loss (relative): 2.865811e-02 | valid loss (relative): 2.723937e-02 
Epoch 1042 use: 414.99 second.

epoch 1043 starting......
Epoch:  1043 | train loss: 1.460937e-03 | valid loss: 1.397678e-03 
      	| train loss (relative): 2.862263e-02 | valid loss (relative): 2.726500e-02 
Epoch 1043 use: 384.74 second.

epoch 1044 starting......
Epoch:  1044 | train loss: 1.461790e-03 | valid loss: 1.395954e-03 
      	| train loss (relative): 2.863425e-02 | valid loss (relative): 2.726537e-02 
Epoch 1044 use: 392.11 second.

epoch 1045 starting......
Epoch:  1045 | train loss: 1.462760e-03 | valid loss: 1.405034e-03 
      	| train loss (relative): 2.866008e-02 | valid loss (relative): 2.746419e-02 
Epoch 1045 use: 379.96 second.

epoch 1046 starting......
Epoch:  1046 | train loss: 1.466454e-03 | valid loss: 1.400163e-03 
      	| train loss (relative): 2.873245e-02 | valid loss (relative): 2.740060e-02 
Epoch 1046 use: 370.55 second.

epoch 1047 starting......
Epoch:  1047 | train loss: 1.463604e-03 | valid loss: 1.398018e-03 
      	| train loss (relative): 2.867652e-02 | valid loss (relative): 2.744863e-02 
Epoch 1047 use: 400.27 second.

epoch 1048 starting......
Epoch:  1048 | train loss: 1.462930e-03 | valid loss: 1.396777e-03 
      	| train loss (relative): 2.866480e-02 | valid loss (relative): 2.737528e-02 
Epoch 1048 use: 363.29 second.

epoch 1049 starting......
Epoch:  1049 | train loss: 1.461315e-03 | valid loss: 1.393934e-03 
      	| train loss (relative): 2.863314e-02 | valid loss (relative): 2.721827e-02 
Epoch 1049 use: 378.96 second.

test MSE Error: 1.508777e-03 | relative MSE Error: 2.954127e-02 
 Total time used for training: 17.89 hour.
MESLoss saved to  /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_MSELoss_Latent_128_nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_1050.txt
relative MSELoss saved to  /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_reMSELoss_Latent_128_nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_1050.txt
model saved to /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_128_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_1050.pth
model_dict saved to /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_128_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_1050_dict.pth
... Training slugflow data completed, Run finished Tue 17 Aug 12:04:39 BST 2021 ...
{'vtu_dir': '/rds/general/user/jy220/home/slugflow/', 'data_dir': '/rds/general/user/jy220/home/tensors/', 'data_type': 'tensors', 'vtu_fields': 'Component1::ComponentMassFractionPhase1, phase1::Velocity', 'variational': 'False', 'optimizer': 'Adamax', 'mode': 'train', 'structured': 'False', 'changing_lr': 'False', 'self_concat': '2', 'dimension': '3', 'sfc_nums': '3', 'components': '4', 'nearest_neighbouring': 'True', 'dims_latent': '128', 'activation': 'Tanh', 'tk_file': '/rds/general/user/jy220/home/slugflow_tk.pt', 'tb_file': '/rds/general/user/jy220/home/slugflow_tb.pt', 'sfc_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/sfcs.pt', 'inv_sfc_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/inv_sfcs.pt', 'coords_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/coords.pt', 'cells_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/cells.npy', 'state_load': '/rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_128_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_1050_dict.pth', 'batch_size': '16', 'lr': '0.001', 'n_epoches': '150', 'seed': '15', 'visualize': 'False', 'reconstructed_path': 'None', 'reconstruct_start_index': '0', 'reconstruct_end_index': '100', 'save_path': '/rds/general/user/jy220/home/new_results/'}
[[ 2.50825269e-01 -1.67256990e-02 -1.22268730e-03]
 [ 2.38611959e-01 -5.50627777e-03 -5.06451913e-03]
 [ 2.39454874e-01 -1.59563257e-02  3.96778323e-03]
 ...
 [ 9.11361096e+00 -2.21908381e-02  2.22384902e-03]
 [ 9.11847310e+00 -2.84346931e-02 -2.34839590e-04]
 [ 9.11039661e+00 -2.51498573e-02 -8.70661044e-03]]
{'tetra': array([[      0,       1,       2,       3],
       [      4,       5,       6,       7],
       [      8,       9,      10,      11],
       ...,
       [1342744, 1342745, 1342746, 1342747],
       [1342748, 1342749, 1342750, 1342751],
       [1342752, 1342753, 1342754, 1342755]])}
vtu fields:  ['Component1::ComponentMassFractionPhase1', 'phase1::Velocity'] 
 structured  False 
 activation  Tanh() 
 self concat  2 
 sfc_nums  3 

dims_latent  128 
 components  4 
 nearest_neighbouring  True 

visualize  False 
 sample number  1706 

reading sfc nums......
[array([  41684,   41685,   41674, ..., 1235560, 1235564, 1235558]), array([954289, 954290, 954291, ..., 420685, 420687, 420688]), array([  40156,   40155,   40158, ..., 1123478, 1123475, 1123467])]
reading inverse sfc nums......
[array([   8146,    8145,    8144, ..., 1274444, 1274445, 1273166]), array([547280, 547283, 593037, ..., 933933, 933932, 864488]), array([ 66437,  66439, 122512, ..., 459416, 415130, 459418])]
tk:  tensor([0.7581, 0.0418, 0.1840, 0.1464], dtype=torch.float64) 

tb:  tensor([-0.3816, -0.3618, -0.0308,  0.1446], dtype=torch.float64) 

length of train set: 1505 

length of valid set: 100 

length of test set: 101 

torch device num: 4 

Let's use 4 GPUs!
epoch 1050 starting......
Epoch:  1050 | train loss: 1.660006e-03 | valid loss: 1.530635e-03 
      	| train loss (relative): 3.250325e-02 | valid loss (relative): 3.007126e-02 
Epoch 1050 use: 512.56 second.

epoch 1051 starting......
Epoch:  1051 | train loss: 1.456732e-03 | valid loss: 1.515109e-03 
      	| train loss (relative): 2.854464e-02 | valid loss (relative): 2.977103e-02 
Epoch 1051 use: 459.84 second.

epoch 1052 starting......
Epoch:  1052 | train loss: 1.446735e-03 | valid loss: 1.511623e-03 
      	| train loss (relative): 2.834379e-02 | valid loss (relative): 2.967239e-02 
Epoch 1052 use: 398.94 second.

epoch 1053 starting......
Epoch:  1053 | train loss: 1.443343e-03 | valid loss: 1.512629e-03 
      	| train loss (relative): 2.827385e-02 | valid loss (relative): 2.972916e-02 
Epoch 1053 use: 416.68 second.

epoch 1054 starting......
Epoch:  1054 | train loss: 1.441912e-03 | valid loss: 1.511520e-03 
      	| train loss (relative): 2.824549e-02 | valid loss (relative): 2.969531e-02 
Epoch 1054 use: 420.89 second.

epoch 1055 starting......
Epoch:  1055 | train loss: 1.440499e-03 | valid loss: 1.512020e-03 
      	| train loss (relative): 2.822129e-02 | valid loss (relative): 2.969151e-02 
Epoch 1055 use: 384.20 second.

epoch 1056 starting......
Epoch:  1056 | train loss: 1.439571e-03 | valid loss: 1.512951e-03 
      	| train loss (relative): 2.819774e-02 | valid loss (relative): 2.970239e-02 
Epoch 1056 use: 387.93 second.

epoch 1057 starting......
Epoch:  1057 | train loss: 1.439666e-03 | valid loss: 1.514669e-03 
      	| train loss (relative): 2.819985e-02 | valid loss (relative): 2.976151e-02 
Epoch 1057 use: 417.21 second.

epoch 1058 starting......
Epoch:  1058 | train loss: 1.440908e-03 | valid loss: 1.515571e-03 
      	| train loss (relative): 2.822102e-02 | valid loss (relative): 2.979932e-02 
Epoch 1058 use: 401.51 second.

epoch 1059 starting......
Epoch:  1059 | train loss: 1.442206e-03 | valid loss: 1.518128e-03 
      	| train loss (relative): 2.825251e-02 | valid loss (relative): 2.980916e-02 
Epoch 1059 use: 409.64 second.

epoch 1060 starting......
Epoch:  1060 | train loss: 1.443749e-03 | valid loss: 1.522261e-03 
      	| train loss (relative): 2.827868e-02 | valid loss (relative): 2.987296e-02 
Epoch 1060 use: 400.65 second.

epoch 1061 starting......
Epoch:  1061 | train loss: 1.444998e-03 | valid loss: 1.521998e-03 
      	| train loss (relative): 2.830580e-02 | valid loss (relative): 2.988437e-02 
Epoch 1061 use: 408.87 second.

epoch 1062 starting......
Epoch:  1062 | train loss: 1.446727e-03 | valid loss: 1.530924e-03 
      	| train loss (relative): 2.833824e-02 | valid loss (relative): 3.000253e-02 
Epoch 1062 use: 404.79 second.

epoch 1063 starting......
Epoch:  1063 | train loss: 1.451949e-03 | valid loss: 1.525350e-03 
      	| train loss (relative): 2.844243e-02 | valid loss (relative): 2.994562e-02 
Epoch 1063 use: 420.26 second.

epoch 1064 starting......
Epoch:  1064 | train loss: 1.446151e-03 | valid loss: 1.524822e-03 
      	| train loss (relative): 2.832679e-02 | valid loss (relative): 2.991186e-02 
Epoch 1064 use: 409.77 second.

epoch 1065 starting......
Epoch:  1065 | train loss: 1.450018e-03 | valid loss: 1.527408e-03 
      	| train loss (relative): 2.840445e-02 | valid loss (relative): 2.997539e-02 
Epoch 1065 use: 403.99 second.

epoch 1066 starting......
Epoch:  1066 | train loss: 1.447122e-03 | valid loss: 1.526129e-03 
      	| train loss (relative): 2.834955e-02 | valid loss (relative): 2.992406e-02 
Epoch 1066 use: 403.41 second.

epoch 1067 starting......
Epoch:  1067 | train loss: 1.446147e-03 | valid loss: 1.530277e-03 
      	| train loss (relative): 2.832345e-02 | valid loss (relative): 3.004845e-02 
Epoch 1067 use: 407.48 second.

epoch 1068 starting......
Epoch:  1068 | train loss: 1.450711e-03 | valid loss: 1.528703e-03 
      	| train loss (relative): 2.841417e-02 | valid loss (relative): 2.987356e-02 
Epoch 1068 use: 398.83 second.

epoch 1069 starting......
Epoch:  1069 | train loss: 1.444537e-03 | valid loss: 1.523892e-03 
      	| train loss (relative): 2.829278e-02 | valid loss (relative): 2.996188e-02 
Epoch 1069 use: 444.47 second.

epoch 1070 starting......
Epoch:  1070 | train loss: 1.445040e-03 | valid loss: 1.528994e-03 
      	| train loss (relative): 2.830620e-02 | valid loss (relative): 3.010341e-02 
Epoch 1070 use: 440.46 second.

epoch 1071 starting......
Epoch:  1071 | train loss: 1.445874e-03 | valid loss: 1.525893e-03 
      	| train loss (relative): 2.832200e-02 | valid loss (relative): 2.993800e-02 
Epoch 1071 use: 459.43 second.

epoch 1072 starting......
Epoch:  1072 | train loss: 1.443837e-03 | valid loss: 1.526043e-03 
      	| train loss (relative): 2.827871e-02 | valid loss (relative): 3.002467e-02 
Epoch 1072 use: 416.89 second.

epoch 1073 starting......
Epoch:  1073 | train loss: 1.443972e-03 | valid loss: 1.525298e-03 
      	| train loss (relative): 2.828235e-02 | valid loss (relative): 2.993193e-02 
Epoch 1073 use: 401.22 second.

epoch 1074 starting......
Epoch:  1074 | train loss: 1.442626e-03 | valid loss: 1.527476e-03 
      	| train loss (relative): 2.825303e-02 | valid loss (relative): 3.004925e-02 
Epoch 1074 use: 404.36 second.

epoch 1075 starting......
Epoch:  1075 | train loss: 1.447066e-03 | valid loss: 1.535414e-03 
      	| train loss (relative): 2.834216e-02 | valid loss (relative): 3.003401e-02 
Epoch 1075 use: 403.80 second.

epoch 1076 starting......
Epoch:  1076 | train loss: 1.452425e-03 | valid loss: 1.528892e-03 
      	| train loss (relative): 2.844896e-02 | valid loss (relative): 3.001118e-02 
Epoch 1076 use: 404.18 second.

epoch 1077 starting......
Epoch:  1077 | train loss: 1.443995e-03 | valid loss: 1.526320e-03 
      	| train loss (relative): 2.828179e-02 | valid loss (relative): 2.994589e-02 
Epoch 1077 use: 403.73 second.

epoch 1078 starting......
Epoch:  1078 | train loss: 1.446312e-03 | valid loss: 1.532791e-03 
      	| train loss (relative): 2.832681e-02 | valid loss (relative): 2.993767e-02 
Epoch 1078 use: 401.16 second.

epoch 1079 starting......
Epoch:  1079 | train loss: 1.445852e-03 | valid loss: 1.533747e-03 
      	| train loss (relative): 2.832003e-02 | valid loss (relative): 2.998143e-02 
Epoch 1079 use: 401.86 second.

epoch 1080 starting......
Epoch:  1080 | train loss: 1.444759e-03 | valid loss: 1.536327e-03 
      	| train loss (relative): 2.829673e-02 | valid loss (relative): 3.024842e-02 
Epoch 1080 use: 404.32 second.

epoch 1081 starting......
Epoch:  1081 | train loss: 1.448583e-03 | valid loss: 1.536443e-03 
      	| train loss (relative): 2.837062e-02 | valid loss (relative): 3.025743e-02 
Epoch 1081 use: 399.65 second.

epoch 1082 starting......
Epoch:  1082 | train loss: 1.447011e-03 | valid loss: 1.526984e-03 
      	| train loss (relative): 2.834594e-02 | valid loss (relative): 3.007629e-02 
Epoch 1082 use: 392.78 second.

epoch 1083 starting......
Epoch:  1083 | train loss: 1.442296e-03 | valid loss: 1.533251e-03 
      	| train loss (relative): 2.824861e-02 | valid loss (relative): 3.025142e-02 
Epoch 1083 use: 406.34 second.

epoch 1084 starting......
Epoch:  1084 | train loss: 1.444369e-03 | valid loss: 1.530406e-03 
      	| train loss (relative): 2.828815e-02 | valid loss (relative): 3.019129e-02 
Epoch 1084 use: 413.91 second.

epoch 1085 starting......
Epoch:  1085 | train loss: 1.442253e-03 | valid loss: 1.530691e-03 
      	| train loss (relative): 2.824911e-02 | valid loss (relative): 3.006944e-02 
Epoch 1085 use: 402.72 second.

epoch 1086 starting......
Epoch:  1086 | train loss: 1.445078e-03 | valid loss: 1.544373e-03 
      	| train loss (relative): 2.830455e-02 | valid loss (relative): 3.023233e-02 
Epoch 1086 use: 398.01 second.

epoch 1087 starting......
Epoch:  1087 | train loss: 1.448322e-03 | valid loss: 1.534014e-03 
      	| train loss (relative): 2.837062e-02 | valid loss (relative): 3.014391e-02 
Epoch 1087 use: 402.70 second.

epoch 1088 starting......
Epoch:  1088 | train loss: 1.444992e-03 | valid loss: 1.528443e-03 
      	| train loss (relative): 2.830017e-02 | valid loss (relative): 3.003399e-02 
Epoch 1088 use: 401.29 second.

epoch 1089 starting......
Epoch:  1089 | train loss: 1.442228e-03 | valid loss: 1.539645e-03 
      	| train loss (relative): 2.824836e-02 | valid loss (relative): 3.020975e-02 
Epoch 1089 use: 404.90 second.

epoch 1090 starting......
Epoch:  1090 | train loss: 1.444327e-03 | valid loss: 1.527681e-03 
      	| train loss (relative): 2.828579e-02 | valid loss (relative): 2.999882e-02 
Epoch 1090 use: 397.96 second.

epoch 1091 starting......
Epoch:  1091 | train loss: 1.444217e-03 | valid loss: 1.540334e-03 
      	| train loss (relative): 2.828681e-02 | valid loss (relative): 3.016347e-02 
Epoch 1091 use: 420.30 second.

epoch 1092 starting......
Epoch:  1092 | train loss: 1.445303e-03 | valid loss: 1.530164e-03 
      	| train loss (relative): 2.830655e-02 | valid loss (relative): 3.002444e-02 
Epoch 1092 use: 408.95 second.

epoch 1093 starting......
Epoch:  1093 | train loss: 1.440845e-03 | valid loss: 1.536009e-03 
      	| train loss (relative): 2.821357e-02 | valid loss (relative): 3.016982e-02 
Epoch 1093 use: 402.93 second.

epoch 1094 starting......
Epoch:  1094 | train loss: 1.444975e-03 | valid loss: 1.536196e-03 
      	| train loss (relative): 2.829902e-02 | valid loss (relative): 3.022864e-02 
Epoch 1094 use: 415.46 second.

epoch 1095 starting......
Epoch:  1095 | train loss: 1.442251e-03 | valid loss: 1.534324e-03 
      	| train loss (relative): 2.824271e-02 | valid loss (relative): 3.007295e-02 
Epoch 1095 use: 408.83 second.

epoch 1096 starting......
Epoch:  1096 | train loss: 1.445025e-03 | valid loss: 1.535265e-03 
      	| train loss (relative): 2.830164e-02 | valid loss (relative): 3.003121e-02 
Epoch 1096 use: 400.20 second.

epoch 1097 starting......
Epoch:  1097 | train loss: 1.447060e-03 | valid loss: 1.530857e-03 
      	| train loss (relative): 2.833656e-02 | valid loss (relative): 2.995226e-02 
Epoch 1097 use: 398.74 second.

epoch 1098 starting......
Epoch:  1098 | train loss: 1.441203e-03 | valid loss: 1.527213e-03 
      	| train loss (relative): 2.822246e-02 | valid loss (relative): 3.000701e-02 
Epoch 1098 use: 419.56 second.

epoch 1099 starting......
Epoch:  1099 | train loss: 1.439623e-03 | valid loss: 1.525908e-03 
      	| train loss (relative): 2.819316e-02 | valid loss (relative): 2.992738e-02 
Epoch 1099 use: 419.67 second.

epoch 1100 starting......
Epoch:  1100 | train loss: 1.440037e-03 | valid loss: 1.531876e-03 
      	| train loss (relative): 2.819916e-02 | valid loss (relative): 3.014533e-02 
Epoch 1100 use: 487.95 second.

epoch 1101 starting......
Epoch:  1101 | train loss: 1.441820e-03 | valid loss: 1.536873e-03 
      	| train loss (relative): 2.822983e-02 | valid loss (relative): 3.024665e-02 
Epoch 1101 use: 422.73 second.

epoch 1102 starting......
Epoch:  1102 | train loss: 1.445048e-03 | valid loss: 1.526425e-03 
      	| train loss (relative): 2.829894e-02 | valid loss (relative): 2.993855e-02 
Epoch 1102 use: 401.97 second.

epoch 1103 starting......
Epoch:  1103 | train loss: 1.437796e-03 | valid loss: 1.525736e-03 
      	| train loss (relative): 2.815503e-02 | valid loss (relative): 2.988050e-02 
Epoch 1103 use: 420.23 second.

epoch 1104 starting......
Epoch:  1104 | train loss: 1.439748e-03 | valid loss: 1.535376e-03 
      	| train loss (relative): 2.820018e-02 | valid loss (relative): 2.993732e-02 
Epoch 1104 use: 416.26 second.

epoch 1105 starting......
Epoch:  1105 | train loss: 1.441568e-03 | valid loss: 1.531013e-03 
      	| train loss (relative): 2.822625e-02 | valid loss (relative): 2.998800e-02 
Epoch 1105 use: 403.81 second.

epoch 1106 starting......
Epoch:  1106 | train loss: 1.439965e-03 | valid loss: 1.528204e-03 
      	| train loss (relative): 2.819936e-02 | valid loss (relative): 2.999504e-02 
Epoch 1106 use: 407.89 second.

epoch 1107 starting......
Epoch:  1107 | train loss: 1.438337e-03 | valid loss: 1.529149e-03 
      	| train loss (relative): 2.816570e-02 | valid loss (relative): 3.006097e-02 
Epoch 1107 use: 407.94 second.

epoch 1108 starting......
Epoch:  1108 | train loss: 1.441922e-03 | valid loss: 1.531849e-03 
      	| train loss (relative): 2.824086e-02 | valid loss (relative): 3.003730e-02 
Epoch 1108 use: 406.10 second.

epoch 1109 starting......
Epoch:  1109 | train loss: 1.440334e-03 | valid loss: 1.530856e-03 
      	| train loss (relative): 2.821072e-02 | valid loss (relative): 3.008088e-02 
Epoch 1109 use: 425.07 second.

epoch 1110 starting......
Epoch:  1110 | train loss: 1.439737e-03 | valid loss: 1.530389e-03 
      	| train loss (relative): 2.819447e-02 | valid loss (relative): 3.001360e-02 
Epoch 1110 use: 387.56 second.

epoch 1111 starting......
Epoch:  1111 | train loss: 1.436479e-03 | valid loss: 1.528776e-03 
      	| train loss (relative): 2.812849e-02 | valid loss (relative): 2.984742e-02 
Epoch 1111 use: 419.94 second.

epoch 1112 starting......
Epoch:  1112 | train loss: 1.437411e-03 | valid loss: 1.527139e-03 
      	| train loss (relative): 2.814513e-02 | valid loss (relative): 3.004038e-02 
Epoch 1112 use: 401.10 second.

epoch 1113 starting......
Epoch:  1113 | train loss: 1.439338e-03 | valid loss: 1.536565e-03 
      	| train loss (relative): 2.818669e-02 | valid loss (relative): 3.031400e-02 
Epoch 1113 use: 405.29 second.

epoch 1114 starting......
Epoch:  1114 | train loss: 1.440088e-03 | valid loss: 1.529253e-03 
      	| train loss (relative): 2.820134e-02 | valid loss (relative): 2.993936e-02 
Epoch 1114 use: 406.54 second.

epoch 1115 starting......
Epoch:  1115 | train loss: 1.439871e-03 | valid loss: 1.534528e-03 
      	| train loss (relative): 2.819593e-02 | valid loss (relative): 3.018939e-02 
Epoch 1115 use: 407.65 second.

epoch 1116 starting......
Epoch:  1116 | train loss: 1.437187e-03 | valid loss: 1.527698e-03 
      	| train loss (relative): 2.814548e-02 | valid loss (relative): 2.997605e-02 
Epoch 1116 use: 409.34 second.

epoch 1117 starting......
Epoch:  1117 | train loss: 1.435740e-03 | valid loss: 1.533379e-03 
      	| train loss (relative): 2.811738e-02 | valid loss (relative): 3.013174e-02 
Epoch 1117 use: 412.74 second.

epoch 1118 starting......
Epoch:  1118 | train loss: 1.437901e-03 | valid loss: 1.527787e-03 
      	| train loss (relative): 2.815666e-02 | valid loss (relative): 3.010395e-02 
Epoch 1118 use: 401.92 second.

epoch 1119 starting......
Epoch:  1119 | train loss: 1.434581e-03 | valid loss: 1.529400e-03 
      	| train loss (relative): 2.809114e-02 | valid loss (relative): 3.000285e-02 
Epoch 1119 use: 412.09 second.

epoch 1120 starting......
Epoch:  1120 | train loss: 1.436313e-03 | valid loss: 1.529870e-03 
      	| train loss (relative): 2.812701e-02 | valid loss (relative): 3.016380e-02 
Epoch 1120 use: 397.54 second.

epoch 1121 starting......
Epoch:  1121 | train loss: 1.436876e-03 | valid loss: 1.524978e-03 
      	| train loss (relative): 2.814004e-02 | valid loss (relative): 2.989845e-02 
Epoch 1121 use: 388.02 second.

epoch 1122 starting......
Epoch:  1122 | train loss: 1.437348e-03 | valid loss: 1.534041e-03 
      	| train loss (relative): 2.814736e-02 | valid loss (relative): 3.007030e-02 
Epoch 1122 use: 409.91 second.

epoch 1123 starting......
Epoch:  1123 | train loss: 1.439943e-03 | valid loss: 1.536749e-03 
      	| train loss (relative): 2.819419e-02 | valid loss (relative): 3.020017e-02 
Epoch 1123 use: 408.43 second.

epoch 1124 starting......
Epoch:  1124 | train loss: 1.437114e-03 | valid loss: 1.526216e-03 
      	| train loss (relative): 2.814193e-02 | valid loss (relative): 2.994642e-02 
Epoch 1124 use: 408.69 second.

epoch 1125 starting......
Epoch:  1125 | train loss: 1.431690e-03 | valid loss: 1.527362e-03 
      	| train loss (relative): 2.803555e-02 | valid loss (relative): 3.002736e-02 
Epoch 1125 use: 401.52 second.

epoch 1126 starting......
Epoch:  1126 | train loss: 1.435460e-03 | valid loss: 1.533737e-03 
      	| train loss (relative): 2.811182e-02 | valid loss (relative): 3.011768e-02 
Epoch 1126 use: 420.80 second.

epoch 1127 starting......
Epoch:  1127 | train loss: 1.437977e-03 | valid loss: 1.522509e-03 
      	| train loss (relative): 2.815956e-02 | valid loss (relative): 2.981194e-02 
Epoch 1127 use: 410.90 second.

epoch 1128 starting......
Epoch:  1128 | train loss: 1.429154e-03 | valid loss: 1.521321e-03 
      	| train loss (relative): 2.798168e-02 | valid loss (relative): 2.990949e-02 
Epoch 1128 use: 410.82 second.

epoch 1129 starting......
Epoch:  1129 | train loss: 1.433359e-03 | valid loss: 1.524311e-03 
      	| train loss (relative): 2.807010e-02 | valid loss (relative): 2.995407e-02 
Epoch 1129 use: 487.33 second.

epoch 1130 starting......
Epoch:  1130 | train loss: 1.426851e-03 | valid loss: 1.519276e-03 
      	| train loss (relative): 2.793639e-02 | valid loss (relative): 2.972119e-02 
Epoch 1130 use: 433.86 second.

epoch 1131 starting......
Epoch:  1131 | train loss: 1.429877e-03 | valid loss: 1.527384e-03 
      	| train loss (relative): 2.799924e-02 | valid loss (relative): 3.007719e-02 
Epoch 1131 use: 417.24 second.

epoch 1132 starting......
Epoch:  1132 | train loss: 1.433288e-03 | valid loss: 1.524686e-03 
      	| train loss (relative): 2.806413e-02 | valid loss (relative): 2.996850e-02 
Epoch 1132 use: 409.95 second.

epoch 1133 starting......
Epoch:  1133 | train loss: 1.431391e-03 | valid loss: 1.525192e-03 
      	| train loss (relative): 2.802357e-02 | valid loss (relative): 3.009623e-02 
Epoch 1133 use: 414.38 second.

epoch 1134 starting......
Epoch:  1134 | train loss: 1.431722e-03 | valid loss: 1.530169e-03 
      	| train loss (relative): 2.803372e-02 | valid loss (relative): 2.995354e-02 
Epoch 1134 use: 412.46 second.

epoch 1135 starting......
Epoch:  1135 | train loss: 1.434509e-03 | valid loss: 1.533008e-03 
      	| train loss (relative): 2.809086e-02 | valid loss (relative): 3.006916e-02 
Epoch 1135 use: 420.38 second.

epoch 1136 starting......
Epoch:  1136 | train loss: 1.433197e-03 | valid loss: 1.527478e-03 
      	| train loss (relative): 2.806389e-02 | valid loss (relative): 2.994924e-02 
Epoch 1136 use: 403.12 second.

epoch 1137 starting......
Epoch:  1137 | train loss: 1.430450e-03 | valid loss: 1.529497e-03 
      	| train loss (relative): 2.800954e-02 | valid loss (relative): 3.001834e-02 
Epoch 1137 use: 411.06 second.

epoch 1138 starting......
Epoch:  1138 | train loss: 1.433169e-03 | valid loss: 1.523044e-03 
      	| train loss (relative): 2.806290e-02 | valid loss (relative): 2.992873e-02 
Epoch 1138 use: 411.00 second.

epoch 1139 starting......
Epoch:  1139 | train loss: 1.431422e-03 | valid loss: 1.525628e-03 
      	| train loss (relative): 2.802488e-02 | valid loss (relative): 3.000460e-02 
Epoch 1139 use: 408.62 second.

epoch 1140 starting......
Epoch:  1140 | train loss: 1.431496e-03 | valid loss: 1.518608e-03 
      	| train loss (relative): 2.802770e-02 | valid loss (relative): 2.971939e-02 
Epoch 1140 use: 437.59 second.

epoch 1141 starting......
Epoch:  1141 | train loss: 1.425000e-03 | valid loss: 1.520175e-03 
      	| train loss (relative): 2.789942e-02 | valid loss (relative): 2.978155e-02 
Epoch 1141 use: 466.44 second.

epoch 1142 starting......
Epoch:  1142 | train loss: 1.427480e-03 | valid loss: 1.520069e-03 
      	| train loss (relative): 2.794425e-02 | valid loss (relative): 2.996454e-02 
Epoch 1142 use: 437.19 second.

epoch 1143 starting......
Epoch:  1143 | train loss: 1.428890e-03 | valid loss: 1.525964e-03 
      	| train loss (relative): 2.797950e-02 | valid loss (relative): 3.010503e-02 
Epoch 1143 use: 479.85 second.

epoch 1144 starting......
Epoch:  1144 | train loss: 1.428514e-03 | valid loss: 1.520854e-03 
      	| train loss (relative): 2.797130e-02 | valid loss (relative): 2.987059e-02 
Epoch 1144 use: 410.80 second.

epoch 1145 starting......
Epoch:  1145 | train loss: 1.431147e-03 | valid loss: 1.529433e-03 
      	| train loss (relative): 2.801962e-02 | valid loss (relative): 3.006276e-02 
Epoch 1145 use: 416.69 second.

epoch 1146 starting......
Epoch:  1146 | train loss: 1.431905e-03 | valid loss: 1.521875e-03 
      	| train loss (relative): 2.803586e-02 | valid loss (relative): 2.979551e-02 
Epoch 1146 use: 395.31 second.

epoch 1147 starting......
Epoch:  1147 | train loss: 1.429554e-03 | valid loss: 1.522636e-03 
      	| train loss (relative): 2.799079e-02 | valid loss (relative): 2.981626e-02 
Epoch 1147 use: 414.31 second.

epoch 1148 starting......
Epoch:  1148 | train loss: 1.425396e-03 | valid loss: 1.520733e-03 
      	| train loss (relative): 2.790602e-02 | valid loss (relative): 3.001647e-02 
Epoch 1148 use: 406.04 second.

epoch 1149 starting......
Epoch:  1149 | train loss: 1.432327e-03 | valid loss: 1.531897e-03 
      	| train loss (relative): 2.804404e-02 | valid loss (relative): 3.015289e-02 
Epoch 1149 use: 415.48 second.

epoch 1150 starting......
Epoch:  1150 | train loss: 1.428129e-03 | valid loss: 1.516271e-03 
      	| train loss (relative): 2.796108e-02 | valid loss (relative): 2.965386e-02 
Epoch 1150 use: 400.36 second.

epoch 1151 starting......
Epoch:  1151 | train loss: 1.421033e-03 | valid loss: 1.521534e-03 
      	| train loss (relative): 2.782440e-02 | valid loss (relative): 2.979756e-02 
Epoch 1151 use: 405.67 second.

epoch 1152 starting......
Epoch:  1152 | train loss: 1.426806e-03 | valid loss: 1.518320e-03 
      	| train loss (relative): 2.793347e-02 | valid loss (relative): 2.981803e-02 
Epoch 1152 use: 413.89 second.

epoch 1153 starting......
Epoch:  1153 | train loss: 1.424897e-03 | valid loss: 1.518328e-03 
      	| train loss (relative): 2.790330e-02 | valid loss (relative): 2.975013e-02 
Epoch 1153 use: 407.03 second.

epoch 1154 starting......
Epoch:  1154 | train loss: 1.421818e-03 | valid loss: 1.517170e-03 
      	| train loss (relative): 2.783582e-02 | valid loss (relative): 2.980448e-02 
Epoch 1154 use: 404.69 second.

epoch 1155 starting......
Epoch:  1155 | train loss: 1.424201e-03 | valid loss: 1.525614e-03 
      	| train loss (relative): 2.788062e-02 | valid loss (relative): 3.000486e-02 
Epoch 1155 use: 398.73 second.

epoch 1156 starting......
Epoch:  1156 | train loss: 1.428729e-03 | valid loss: 1.517642e-03 
      	| train loss (relative): 2.797415e-02 | valid loss (relative): 2.975664e-02 
Epoch 1156 use: 406.17 second.

epoch 1157 starting......
Epoch:  1157 | train loss: 1.422729e-03 | valid loss: 1.516838e-03 
      	| train loss (relative): 2.785246e-02 | valid loss (relative): 2.970659e-02 
Epoch 1157 use: 419.66 second.

epoch 1158 starting......
Epoch:  1158 | train loss: 1.422800e-03 | valid loss: 1.521616e-03 
      	| train loss (relative): 2.785265e-02 | valid loss (relative): 2.991639e-02 
Epoch 1158 use: 390.85 second.

epoch 1159 starting......
Epoch:  1159 | train loss: 1.423741e-03 | valid loss: 1.522305e-03 
      	| train loss (relative): 2.787288e-02 | valid loss (relative): 2.985309e-02 
Epoch 1159 use: 413.21 second.

epoch 1160 starting......
Epoch:  1160 | train loss: 1.425691e-03 | valid loss: 1.524818e-03 
      	| train loss (relative): 2.790807e-02 | valid loss (relative): 2.992068e-02 
Epoch 1160 use: 415.87 second.

epoch 1161 starting......
Epoch:  1161 | train loss: 1.423448e-03 | valid loss: 1.514925e-03 
      	| train loss (relative): 2.786721e-02 | valid loss (relative): 2.967078e-02 
Epoch 1161 use: 406.03 second.

epoch 1162 starting......
Epoch:  1162 | train loss: 1.421783e-03 | valid loss: 1.523115e-03 
      	| train loss (relative): 2.783337e-02 | valid loss (relative): 3.000152e-02 
Epoch 1162 use: 396.10 second.

epoch 1163 starting......
Epoch:  1163 | train loss: 1.426019e-03 | valid loss: 1.523728e-03 
      	| train loss (relative): 2.791485e-02 | valid loss (relative): 2.980022e-02 
Epoch 1163 use: 441.48 second.

epoch 1164 starting......
Epoch:  1164 | train loss: 1.425396e-03 | valid loss: 1.517754e-03 
      	| train loss (relative): 2.790567e-02 | valid loss (relative): 2.966293e-02 
Epoch 1164 use: 455.34 second.

epoch 1165 starting......
Epoch:  1165 | train loss: 1.424577e-03 | valid loss: 1.525121e-03 
      	| train loss (relative): 2.789023e-02 | valid loss (relative): 2.995250e-02 
Epoch 1165 use: 421.16 second.

epoch 1166 starting......
Epoch:  1166 | train loss: 1.425493e-03 | valid loss: 1.518420e-03 
      	| train loss (relative): 2.790975e-02 | valid loss (relative): 2.973950e-02 
Epoch 1166 use: 443.15 second.

epoch 1167 starting......
Epoch:  1167 | train loss: 1.421788e-03 | valid loss: 1.518189e-03 
      	| train loss (relative): 2.783651e-02 | valid loss (relative): 2.985637e-02 
Epoch 1167 use: 439.77 second.

epoch 1168 starting......
Epoch:  1168 | train loss: 1.423473e-03 | valid loss: 1.518523e-03 
      	| train loss (relative): 2.786655e-02 | valid loss (relative): 2.991610e-02 
Epoch 1168 use: 413.36 second.

epoch 1169 starting......
Epoch:  1169 | train loss: 1.422779e-03 | valid loss: 1.528122e-03 
      	| train loss (relative): 2.785812e-02 | valid loss (relative): 3.004979e-02 
Epoch 1169 use: 397.64 second.

epoch 1170 starting......
Epoch:  1170 | train loss: 1.425522e-03 | valid loss: 1.519709e-03 
      	| train loss (relative): 2.790653e-02 | valid loss (relative): 2.981026e-02 
Epoch 1170 use: 411.56 second.

epoch 1171 starting......
Epoch:  1171 | train loss: 1.420947e-03 | valid loss: 1.524128e-03 
      	| train loss (relative): 2.781884e-02 | valid loss (relative): 2.994003e-02 
Epoch 1171 use: 397.12 second.

epoch 1172 starting......
Epoch:  1172 | train loss: 1.426024e-03 | valid loss: 1.522281e-03 
      	| train loss (relative): 2.791920e-02 | valid loss (relative): 2.976258e-02 
Epoch 1172 use: 420.89 second.

epoch 1173 starting......
Epoch:  1173 | train loss: 1.424665e-03 | valid loss: 1.522088e-03 
      	| train loss (relative): 2.788833e-02 | valid loss (relative): 2.985370e-02 
Epoch 1173 use: 479.22 second.

epoch 1174 starting......
Epoch:  1174 | train loss: 1.423039e-03 | valid loss: 1.519487e-03 
      	| train loss (relative): 2.785846e-02 | valid loss (relative): 2.986437e-02 
Epoch 1174 use: 462.54 second.

epoch 1175 starting......
Epoch:  1175 | train loss: 1.424390e-03 | valid loss: 1.537928e-03 
      	| train loss (relative): 2.788221e-02 | valid loss (relative): 3.048145e-02 
Epoch 1175 use: 415.72 second.

epoch 1176 starting......
Epoch:  1176 | train loss: 1.425385e-03 | valid loss: 1.517085e-03 
      	| train loss (relative): 2.790314e-02 | valid loss (relative): 2.970219e-02 
Epoch 1176 use: 438.16 second.

epoch 1177 starting......
Epoch:  1177 | train loss: 1.421922e-03 | valid loss: 1.518678e-03 
      	| train loss (relative): 2.783761e-02 | valid loss (relative): 2.968746e-02 
Epoch 1177 use: 413.90 second.

epoch 1178 starting......
Epoch:  1178 | train loss: 1.423384e-03 | valid loss: 1.524489e-03 
      	| train loss (relative): 2.786155e-02 | valid loss (relative): 2.991775e-02 
Epoch 1178 use: 421.16 second.

epoch 1179 starting......
Epoch:  1179 | train loss: 1.424285e-03 | valid loss: 1.521409e-03 
      	| train loss (relative): 2.788096e-02 | valid loss (relative): 2.981787e-02 
Epoch 1179 use: 420.58 second.

epoch 1180 starting......
Epoch:  1180 | train loss: 1.422834e-03 | valid loss: 1.519788e-03 
      	| train loss (relative): 2.785876e-02 | valid loss (relative): 2.982003e-02 
Epoch 1180 use: 418.48 second.

epoch 1181 starting......
Epoch:  1181 | train loss: 1.424430e-03 | valid loss: 1.521919e-03 
      	| train loss (relative): 2.788675e-02 | valid loss (relative): 2.986337e-02 
Epoch 1181 use: 423.11 second.

epoch 1182 starting......
Epoch:  1182 | train loss: 1.421871e-03 | valid loss: 1.517198e-03 
      	| train loss (relative): 2.783799e-02 | valid loss (relative): 2.983722e-02 
Epoch 1182 use: 415.58 second.

epoch 1183 starting......
Epoch:  1183 | train loss: 1.419500e-03 | valid loss: 1.521515e-03 
      	| train loss (relative): 2.778647e-02 | valid loss (relative): 2.992033e-02 
Epoch 1183 use: 464.21 second.

epoch 1184 starting......
Epoch:  1184 | train loss: 1.423696e-03 | valid loss: 1.513201e-03 
      	| train loss (relative): 2.787053e-02 | valid loss (relative): 2.971250e-02 
Epoch 1184 use: 426.69 second.

epoch 1185 starting......
Epoch:  1185 | train loss: 1.414853e-03 | valid loss: 1.512117e-03 
      	| train loss (relative): 2.769838e-02 | valid loss (relative): 2.955325e-02 
Epoch 1185 use: 431.77 second.

epoch 1186 starting......
Epoch:  1186 | train loss: 1.414592e-03 | valid loss: 1.510695e-03 
      	| train loss (relative): 2.769119e-02 | valid loss (relative): 2.956404e-02 
Epoch 1186 use: 428.38 second.

epoch 1187 starting......
Epoch:  1187 | train loss: 1.416465e-03 | valid loss: 1.512284e-03 
      	| train loss (relative): 2.772649e-02 | valid loss (relative): 2.965428e-02 
Epoch 1187 use: 424.19 second.

epoch 1188 starting......
Epoch:  1188 | train loss: 1.417222e-03 | valid loss: 1.518219e-03 
      	| train loss (relative): 2.774592e-02 | valid loss (relative): 2.992626e-02 
Epoch 1188 use: 430.75 second.

epoch 1189 starting......
Epoch:  1189 | train loss: 1.417950e-03 | valid loss: 1.513316e-03 
      	| train loss (relative): 2.776168e-02 | valid loss (relative): 2.975338e-02 
Epoch 1189 use: 434.89 second.

epoch 1190 starting......
Epoch:  1190 | train loss: 1.415394e-03 | valid loss: 1.513618e-03 
      	| train loss (relative): 2.770476e-02 | valid loss (relative): 2.978232e-02 
Epoch 1190 use: 412.54 second.

epoch 1191 starting......
Epoch:  1191 | train loss: 1.419668e-03 | valid loss: 1.520326e-03 
      	| train loss (relative): 2.778671e-02 | valid loss (relative): 2.998730e-02 
Epoch 1191 use: 415.38 second.

epoch 1192 starting......
Epoch:  1192 | train loss: 1.421288e-03 | valid loss: 1.520948e-03 
      	| train loss (relative): 2.782112e-02 | valid loss (relative): 2.998302e-02 
Epoch 1192 use: 459.22 second.

epoch 1193 starting......
Epoch:  1193 | train loss: 1.417258e-03 | valid loss: 1.515287e-03 
      	| train loss (relative): 2.774474e-02 | valid loss (relative): 2.981768e-02 
Epoch 1193 use: 429.57 second.

epoch 1194 starting......
Epoch:  1194 | train loss: 1.414687e-03 | valid loss: 1.510085e-03 
      	| train loss (relative): 2.769417e-02 | valid loss (relative): 2.969002e-02 
Epoch 1194 use: 421.18 second.

epoch 1195 starting......
Epoch:  1195 | train loss: 1.413812e-03 | valid loss: 1.510440e-03 
      	| train loss (relative): 2.767575e-02 | valid loss (relative): 2.963597e-02 
Epoch 1195 use: 435.75 second.

epoch 1196 starting......
Epoch:  1196 | train loss: 1.414777e-03 | valid loss: 1.513761e-03 
      	| train loss (relative): 2.769125e-02 | valid loss (relative): 2.969500e-02 
Epoch 1196 use: 414.54 second.

epoch 1197 starting......
Epoch:  1197 | train loss: 1.418120e-03 | valid loss: 1.520294e-03 
      	| train loss (relative): 2.776193e-02 | valid loss (relative): 2.978612e-02 
Epoch 1197 use: 430.48 second.

epoch 1198 starting......
Epoch:  1198 | train loss: 1.420611e-03 | valid loss: 1.518145e-03 
      	| train loss (relative): 2.780725e-02 | valid loss (relative): 2.981949e-02 
Epoch 1198 use: 424.82 second.

epoch 1199 starting......
Epoch:  1199 | train loss: 1.415671e-03 | valid loss: 1.515694e-03 
      	| train loss (relative): 2.771263e-02 | valid loss (relative): 2.972735e-02 
Epoch 1199 use: 436.63 second.

test MSE Error: 1.435800e-03 | relative MSE Error: 2.806803e-02 
 Total time used for training: 17.38 hour.
MESLoss saved to  /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_MSELoss_Latent_128_nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_1200.txt
relative MSELoss saved to  /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_reMSELoss_Latent_128_nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_1200.txt
model saved to /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_128_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_1200.pth
model_dict saved to /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_128_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_1200_dict.pth
... Training slugflow data completed, Run finished Sat 21 Aug 07:58:03 BST 2021 ...
{'vtu_dir': '/rds/general/user/jy220/home/slugflow/', 'data_dir': '/rds/general/user/jy220/home/tensors/', 'data_type': 'tensors', 'vtu_fields': 'Component1::ComponentMassFractionPhase1, phase1::Velocity', 'variational': 'False', 'optimizer': 'Adamax', 'mode': 'train', 'structured': 'False', 'changing_lr': 'False', 'self_concat': '2', 'dimension': '3', 'sfc_nums': '3', 'components': '4', 'nearest_neighbouring': 'True', 'dims_latent': '128', 'activation': 'Tanh', 'tk_file': '/rds/general/user/jy220/home/slugflow_tk.pt', 'tb_file': '/rds/general/user/jy220/home/slugflow_tb.pt', 'sfc_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/sfcs.pt', 'inv_sfc_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/inv_sfcs.pt', 'coords_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/coords.pt', 'cells_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/cells.npy', 'state_load': '/rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_128_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_1200_dict.pth', 'batch_size': '16', 'lr': '0.001', 'n_epoches': '150', 'seed': '15', 'visualize': 'False', 'reconstructed_path': 'None', 'reconstruct_start_index': '0', 'reconstruct_end_index': '100', 'save_path': '/rds/general/user/jy220/home/new_results/'}
[[ 2.50825269e-01 -1.67256990e-02 -1.22268730e-03]
 [ 2.38611959e-01 -5.50627777e-03 -5.06451913e-03]
 [ 2.39454874e-01 -1.59563257e-02  3.96778323e-03]
 ...
 [ 9.11361096e+00 -2.21908381e-02  2.22384902e-03]
 [ 9.11847310e+00 -2.84346931e-02 -2.34839590e-04]
 [ 9.11039661e+00 -2.51498573e-02 -8.70661044e-03]]
{'tetra': array([[      0,       1,       2,       3],
       [      4,       5,       6,       7],
       [      8,       9,      10,      11],
       ...,
       [1342744, 1342745, 1342746, 1342747],
       [1342748, 1342749, 1342750, 1342751],
       [1342752, 1342753, 1342754, 1342755]])}
vtu fields:  ['Component1::ComponentMassFractionPhase1', 'phase1::Velocity'] 
 structured  False 
 activation  Tanh() 
 self concat  2 
 sfc_nums  3 

dims_latent  128 
 components  4 
 nearest_neighbouring  True 

visualize  False 
 sample number  1706 

reading sfc nums......
[array([  41684,   41685,   41674, ..., 1235560, 1235564, 1235558]), array([954289, 954290, 954291, ..., 420685, 420687, 420688]), array([  40156,   40155,   40158, ..., 1123478, 1123475, 1123467])]
reading inverse sfc nums......
[array([   8146,    8145,    8144, ..., 1274444, 1274445, 1273166]), array([547280, 547283, 593037, ..., 933933, 933932, 864488]), array([ 66437,  66439, 122512, ..., 459416, 415130, 459418])]
tk:  tensor([0.7581, 0.0418, 0.1840, 0.1464], dtype=torch.float64) 

tb:  tensor([-0.3816, -0.3618, -0.0308,  0.1446], dtype=torch.float64) 

length of train set: 1505 

length of valid set: 100 

length of test set: 101 

torch device num: 4 

Let's use 4 GPUs!
epoch 1200 starting......
Epoch:  1200 | train loss: 1.635246e-03 | valid loss: 1.435891e-03 
      	| train loss (relative): 3.209489e-02 | valid loss (relative): 2.811071e-02 
Epoch 1200 use: 517.18 second.

epoch 1201 starting......
Epoch:  1201 | train loss: 1.419742e-03 | valid loss: 1.420621e-03 
      	| train loss (relative): 2.780114e-02 | valid loss (relative): 2.779248e-02 
Epoch 1201 use: 505.04 second.

epoch 1202 starting......
Epoch:  1202 | train loss: 1.410736e-03 | valid loss: 1.417912e-03 
      	| train loss (relative): 2.762254e-02 | valid loss (relative): 2.775584e-02 
Epoch 1202 use: 475.86 second.

epoch 1203 starting......
Epoch:  1203 | train loss: 1.408068e-03 | valid loss: 1.418402e-03 
      	| train loss (relative): 2.757037e-02 | valid loss (relative): 2.776440e-02 
Epoch 1203 use: 487.86 second.

epoch 1204 starting......
Epoch:  1204 | train loss: 1.407218e-03 | valid loss: 1.417221e-03 
      	| train loss (relative): 2.755338e-02 | valid loss (relative): 2.772414e-02 
Epoch 1204 use: 481.15 second.

epoch 1205 starting......
Epoch:  1205 | train loss: 1.406681e-03 | valid loss: 1.418129e-03 
      	| train loss (relative): 2.753812e-02 | valid loss (relative): 2.774317e-02 
Epoch 1205 use: 500.65 second.

epoch 1206 starting......
Epoch:  1206 | train loss: 1.406743e-03 | valid loss: 1.419402e-03 
      	| train loss (relative): 2.754075e-02 | valid loss (relative): 2.774884e-02 
Epoch 1206 use: 497.47 second.

epoch 1207 starting......
Epoch:  1207 | train loss: 1.405208e-03 | valid loss: 1.419924e-03 
      	| train loss (relative): 2.750683e-02 | valid loss (relative): 2.778395e-02 
Epoch 1207 use: 497.98 second.

epoch 1208 starting......
Epoch:  1208 | train loss: 1.406481e-03 | valid loss: 1.422234e-03 
      	| train loss (relative): 2.753501e-02 | valid loss (relative): 2.779663e-02 
Epoch 1208 use: 493.00 second.

epoch 1209 starting......
Epoch:  1209 | train loss: 1.406591e-03 | valid loss: 1.423508e-03 
      	| train loss (relative): 2.753527e-02 | valid loss (relative): 2.785304e-02 
Epoch 1209 use: 488.76 second.

epoch 1210 starting......
Epoch:  1210 | train loss: 1.408849e-03 | valid loss: 1.423213e-03 
      	| train loss (relative): 2.757960e-02 | valid loss (relative): 2.784720e-02 
Epoch 1210 use: 501.18 second.

epoch 1211 starting......
Epoch:  1211 | train loss: 1.409513e-03 | valid loss: 1.428385e-03 
      	| train loss (relative): 2.759115e-02 | valid loss (relative): 2.794817e-02 
Epoch 1211 use: 486.59 second.

epoch 1212 starting......
Epoch:  1212 | train loss: 1.412997e-03 | valid loss: 1.428965e-03 
      	| train loss (relative): 2.766731e-02 | valid loss (relative): 2.797399e-02 
Epoch 1212 use: 500.32 second.

epoch 1213 starting......
Epoch:  1213 | train loss: 1.411971e-03 | valid loss: 1.428934e-03 
      	| train loss (relative): 2.764460e-02 | valid loss (relative): 2.793727e-02 
Epoch 1213 use: 484.90 second.

epoch 1214 starting......
Epoch:  1214 | train loss: 1.410936e-03 | valid loss: 1.427821e-03 
      	| train loss (relative): 2.761942e-02 | valid loss (relative): 2.784013e-02 
Epoch 1214 use: 483.29 second.

epoch 1215 starting......
Epoch:  1215 | train loss: 1.413011e-03 | valid loss: 1.436124e-03 
      	| train loss (relative): 2.766436e-02 | valid loss (relative): 2.807886e-02 
Epoch 1215 use: 507.01 second.

epoch 1216 starting......
Epoch:  1216 | train loss: 1.413998e-03 | valid loss: 1.433748e-03 
      	| train loss (relative): 2.768172e-02 | valid loss (relative): 2.808286e-02 
Epoch 1216 use: 478.02 second.

epoch 1217 starting......
Epoch:  1217 | train loss: 1.410935e-03 | valid loss: 1.430738e-03 
      	| train loss (relative): 2.762128e-02 | valid loss (relative): 2.797754e-02 
Epoch 1217 use: 495.16 second.

epoch 1218 starting......
Epoch:  1218 | train loss: 1.414743e-03 | valid loss: 1.439470e-03 
      	| train loss (relative): 2.769802e-02 | valid loss (relative): 2.813768e-02 
Epoch 1218 use: 480.47 second.

epoch 1219 starting......
Epoch:  1219 | train loss: 1.415656e-03 | valid loss: 1.436879e-03 
      	| train loss (relative): 2.771575e-02 | valid loss (relative): 2.810540e-02 
Epoch 1219 use: 494.11 second.

epoch 1220 starting......
Epoch:  1220 | train loss: 1.419274e-03 | valid loss: 1.441967e-03 
      	| train loss (relative): 2.778759e-02 | valid loss (relative): 2.823001e-02 
Epoch 1220 use: 471.53 second.

epoch 1221 starting......
Epoch:  1221 | train loss: 1.414644e-03 | valid loss: 1.433768e-03 
      	| train loss (relative): 2.769459e-02 | valid loss (relative): 2.804495e-02 
Epoch 1221 use: 487.67 second.

epoch 1222 starting......
Epoch:  1222 | train loss: 1.412365e-03 | valid loss: 1.433293e-03 
      	| train loss (relative): 2.764719e-02 | valid loss (relative): 2.802816e-02 
Epoch 1222 use: 475.11 second.

epoch 1223 starting......
Epoch:  1223 | train loss: 1.414385e-03 | valid loss: 1.440416e-03 
      	| train loss (relative): 2.768894e-02 | valid loss (relative): 2.814016e-02 
Epoch 1223 use: 496.09 second.

epoch 1224 starting......
Epoch:  1224 | train loss: 1.413676e-03 | valid loss: 1.436421e-03 
      	| train loss (relative): 2.767728e-02 | valid loss (relative): 2.817180e-02 
Epoch 1224 use: 473.62 second.

epoch 1225 starting......
Epoch:  1225 | train loss: 1.413411e-03 | valid loss: 1.433518e-03 
      	| train loss (relative): 2.766685e-02 | valid loss (relative): 2.806583e-02 
Epoch 1225 use: 494.64 second.

epoch 1226 starting......
Epoch:  1226 | train loss: 1.414407e-03 | valid loss: 1.439049e-03 
      	| train loss (relative): 2.769013e-02 | valid loss (relative): 2.819639e-02 
Epoch 1226 use: 468.39 second.

epoch 1227 starting......
Epoch:  1227 | train loss: 1.413827e-03 | valid loss: 1.438806e-03 
      	| train loss (relative): 2.767993e-02 | valid loss (relative): 2.823243e-02 
Epoch 1227 use: 494.79 second.

epoch 1228 starting......
Epoch:  1228 | train loss: 1.415562e-03 | valid loss: 1.438670e-03 
      	| train loss (relative): 2.771601e-02 | valid loss (relative): 2.811058e-02 
Epoch 1228 use: 469.34 second.

epoch 1229 starting......
Epoch:  1229 | train loss: 1.414923e-03 | valid loss: 1.436751e-03 
      	| train loss (relative): 2.769810e-02 | valid loss (relative): 2.799635e-02 
Epoch 1229 use: 485.09 second.

epoch 1230 starting......
Epoch:  1230 | train loss: 1.411144e-03 | valid loss: 1.435782e-03 
      	| train loss (relative): 2.762542e-02 | valid loss (relative): 2.810338e-02 
Epoch 1230 use: 504.50 second.

epoch 1231 starting......
Epoch:  1231 | train loss: 1.412249e-03 | valid loss: 1.442278e-03 
      	| train loss (relative): 2.764399e-02 | valid loss (relative): 2.830422e-02 
Epoch 1231 use: 483.27 second.

epoch 1232 starting......
Epoch:  1232 | train loss: 1.414493e-03 | valid loss: 1.446379e-03 
      	| train loss (relative): 2.769038e-02 | valid loss (relative): 2.829928e-02 
Epoch 1232 use: 488.22 second.

epoch 1233 starting......
Epoch:  1233 | train loss: 1.418074e-03 | valid loss: 1.437376e-03 
      	| train loss (relative): 2.775977e-02 | valid loss (relative): 2.822215e-02 
Epoch 1233 use: 481.66 second.

epoch 1234 starting......
Epoch:  1234 | train loss: 1.410769e-03 | valid loss: 1.433884e-03 
      	| train loss (relative): 2.761859e-02 | valid loss (relative): 2.799478e-02 
Epoch 1234 use: 502.67 second.

epoch 1235 starting......
Epoch:  1235 | train loss: 1.408948e-03 | valid loss: 1.431962e-03 
      	| train loss (relative): 2.758515e-02 | valid loss (relative): 2.805395e-02 
Epoch 1235 use: 491.18 second.

epoch 1236 starting......
Epoch:  1236 | train loss: 1.407336e-03 | valid loss: 1.437738e-03 
      	| train loss (relative): 2.754847e-02 | valid loss (relative): 2.812523e-02 
Epoch 1236 use: 501.50 second.

epoch 1237 starting......
Epoch:  1237 | train loss: 1.409633e-03 | valid loss: 1.436785e-03 
      	| train loss (relative): 2.759420e-02 | valid loss (relative): 2.814183e-02 
Epoch 1237 use: 489.15 second.

epoch 1238 starting......
Epoch:  1238 | train loss: 1.411360e-03 | valid loss: 1.440818e-03 
      	| train loss (relative): 2.762336e-02 | valid loss (relative): 2.816583e-02 
Epoch 1238 use: 477.49 second.

epoch 1239 starting......
Epoch:  1239 | train loss: 1.411895e-03 | valid loss: 1.436090e-03 
      	| train loss (relative): 2.763857e-02 | valid loss (relative): 2.812545e-02 
Epoch 1239 use: 497.18 second.

epoch 1240 starting......
Epoch:  1240 | train loss: 1.409394e-03 | valid loss: 1.441735e-03 
      	| train loss (relative): 2.758756e-02 | valid loss (relative): 2.825550e-02 
Epoch 1240 use: 480.10 second.

epoch 1241 starting......
Epoch:  1241 | train loss: 1.411178e-03 | valid loss: 1.438770e-03 
      	| train loss (relative): 2.762385e-02 | valid loss (relative): 2.804861e-02 
Epoch 1241 use: 481.44 second.

epoch 1242 starting......
Epoch:  1242 | train loss: 1.412008e-03 | valid loss: 1.440068e-03 
      	| train loss (relative): 2.763983e-02 | valid loss (relative): 2.823075e-02 
Epoch 1242 use: 505.14 second.

epoch 1243 starting......
Epoch:  1243 | train loss: 1.411016e-03 | valid loss: 1.439256e-03 
      	| train loss (relative): 2.761989e-02 | valid loss (relative): 2.819969e-02 
Epoch 1243 use: 485.42 second.

epoch 1244 starting......
Epoch:  1244 | train loss: 1.406829e-03 | valid loss: 1.432913e-03 
      	| train loss (relative): 2.753536e-02 | valid loss (relative): 2.804655e-02 
Epoch 1244 use: 486.18 second.

epoch 1245 starting......
Epoch:  1245 | train loss: 1.405815e-03 | valid loss: 1.438887e-03 
      	| train loss (relative): 2.751654e-02 | valid loss (relative): 2.807387e-02 
Epoch 1245 use: 484.11 second.

epoch 1246 starting......
Epoch:  1246 | train loss: 1.409747e-03 | valid loss: 1.438501e-03 
      	| train loss (relative): 2.759394e-02 | valid loss (relative): 2.815611e-02 
Epoch 1246 use: 477.95 second.

epoch 1247 starting......
Epoch:  1247 | train loss: 1.409267e-03 | valid loss: 1.437342e-03 
      	| train loss (relative): 2.758735e-02 | valid loss (relative): 2.800074e-02 
Epoch 1247 use: 477.64 second.

epoch 1248 starting......
Epoch:  1248 | train loss: 1.407726e-03 | valid loss: 1.439386e-03 
      	| train loss (relative): 2.755309e-02 | valid loss (relative): 2.809254e-02 
Epoch 1248 use: 459.01 second.

epoch 1249 starting......
Epoch:  1249 | train loss: 1.409126e-03 | valid loss: 1.437578e-03 
      	| train loss (relative): 2.758130e-02 | valid loss (relative): 2.812439e-02 
Epoch 1249 use: 480.68 second.

epoch 1250 starting......
Epoch:  1250 | train loss: 1.406967e-03 | valid loss: 1.429049e-03 
      	| train loss (relative): 2.753620e-02 | valid loss (relative): 2.803313e-02 
Epoch 1250 use: 467.91 second.

epoch 1251 starting......
Epoch:  1251 | train loss: 1.405696e-03 | valid loss: 1.448117e-03 
      	| train loss (relative): 2.751840e-02 | valid loss (relative): 2.839910e-02 
Epoch 1251 use: 508.67 second.

epoch 1252 starting......
Epoch:  1252 | train loss: 1.413233e-03 | valid loss: 1.436262e-03 
      	| train loss (relative): 2.766862e-02 | valid loss (relative): 2.800251e-02 
Epoch 1252 use: 497.61 second.

epoch 1253 starting......
Epoch:  1253 | train loss: 1.408391e-03 | valid loss: 1.437681e-03 
      	| train loss (relative): 2.756833e-02 | valid loss (relative): 2.817862e-02 
Epoch 1253 use: 500.61 second.

epoch 1254 starting......
Epoch:  1254 | train loss: 1.408052e-03 | valid loss: 1.437267e-03 
      	| train loss (relative): 2.756314e-02 | valid loss (relative): 2.807361e-02 
Epoch 1254 use: 488.53 second.

epoch 1255 starting......
Epoch:  1255 | train loss: 1.409634e-03 | valid loss: 1.433672e-03 
      	| train loss (relative): 2.759381e-02 | valid loss (relative): 2.799687e-02 
Epoch 1255 use: 493.74 second.

epoch 1256 starting......
Epoch:  1256 | train loss: 1.401940e-03 | valid loss: 1.430540e-03 
      	| train loss (relative): 2.744071e-02 | valid loss (relative): 2.801603e-02 
Epoch 1256 use: 478.69 second.

epoch 1257 starting......
Epoch:  1257 | train loss: 1.402372e-03 | valid loss: 1.437530e-03 
      	| train loss (relative): 2.745128e-02 | valid loss (relative): 2.813952e-02 
Epoch 1257 use: 495.14 second.

epoch 1258 starting......
Epoch:  1258 | train loss: 1.409636e-03 | valid loss: 1.446708e-03 
      	| train loss (relative): 2.759228e-02 | valid loss (relative): 2.829049e-02 
Epoch 1258 use: 483.13 second.

epoch 1259 starting......
Epoch:  1259 | train loss: 1.411383e-03 | valid loss: 1.436419e-03 
      	| train loss (relative): 2.762774e-02 | valid loss (relative): 2.821731e-02 
Epoch 1259 use: 508.98 second.

epoch 1260 starting......
Epoch:  1260 | train loss: 1.405902e-03 | valid loss: 1.435863e-03 
      	| train loss (relative): 2.752334e-02 | valid loss (relative): 2.811979e-02 
Epoch 1260 use: 486.04 second.

epoch 1261 starting......
Epoch:  1261 | train loss: 1.404036e-03 | valid loss: 1.433478e-03 
      	| train loss (relative): 2.748753e-02 | valid loss (relative): 2.794206e-02 
Epoch 1261 use: 508.29 second.

epoch 1262 starting......
Epoch:  1262 | train loss: 1.405365e-03 | valid loss: 1.435596e-03 
      	| train loss (relative): 2.750596e-02 | valid loss (relative): 2.799322e-02 
Epoch 1262 use: 488.66 second.

epoch 1263 starting......
Epoch:  1263 | train loss: 1.407480e-03 | valid loss: 1.437593e-03 
      	| train loss (relative): 2.754790e-02 | valid loss (relative): 2.803196e-02 
Epoch 1263 use: 503.43 second.

epoch 1264 starting......
Epoch:  1264 | train loss: 1.405894e-03 | valid loss: 1.439614e-03 
      	| train loss (relative): 2.751652e-02 | valid loss (relative): 2.814840e-02 
Epoch 1264 use: 493.52 second.

epoch 1265 starting......
Epoch:  1265 | train loss: 1.406475e-03 | valid loss: 1.437964e-03 
      	| train loss (relative): 2.753115e-02 | valid loss (relative): 2.810019e-02 
Epoch 1265 use: 504.10 second.

epoch 1266 starting......
Epoch:  1266 | train loss: 1.406424e-03 | valid loss: 1.435045e-03 
      	| train loss (relative): 2.752049e-02 | valid loss (relative): 2.798698e-02 
Epoch 1266 use: 482.86 second.

epoch 1267 starting......
Epoch:  1267 | train loss: 1.402956e-03 | valid loss: 1.440993e-03 
      	| train loss (relative): 2.745826e-02 | valid loss (relative): 2.823797e-02 
Epoch 1267 use: 492.21 second.

epoch 1268 starting......
Epoch:  1268 | train loss: 1.403990e-03 | valid loss: 1.431442e-03 
      	| train loss (relative): 2.747895e-02 | valid loss (relative): 2.792161e-02 
Epoch 1268 use: 491.32 second.

epoch 1269 starting......
Epoch:  1269 | train loss: 1.404083e-03 | valid loss: 1.440906e-03 
      	| train loss (relative): 2.748619e-02 | valid loss (relative): 2.826248e-02 
Epoch 1269 use: 498.49 second.

epoch 1270 starting......
Epoch:  1270 | train loss: 1.407220e-03 | valid loss: 1.434343e-03 
      	| train loss (relative): 2.754420e-02 | valid loss (relative): 2.800576e-02 
Epoch 1270 use: 503.77 second.

epoch 1271 starting......
Epoch:  1271 | train loss: 1.404932e-03 | valid loss: 1.428909e-03 
      	| train loss (relative): 2.749973e-02 | valid loss (relative): 2.800302e-02 
Epoch 1271 use: 490.10 second.

epoch 1272 starting......
Epoch:  1272 | train loss: 1.399177e-03 | valid loss: 1.431740e-03 
      	| train loss (relative): 2.738848e-02 | valid loss (relative): 2.800009e-02 
Epoch 1272 use: 513.71 second.

epoch 1273 starting......
Epoch:  1273 | train loss: 1.399895e-03 | valid loss: 1.428869e-03 
      	| train loss (relative): 2.740073e-02 | valid loss (relative): 2.805329e-02 
Epoch 1273 use: 483.03 second.

epoch 1274 starting......
Epoch:  1274 | train loss: 1.397624e-03 | valid loss: 1.430201e-03 
      	| train loss (relative): 2.735372e-02 | valid loss (relative): 2.794891e-02 
Epoch 1274 use: 497.36 second.

epoch 1275 starting......
Epoch:  1275 | train loss: 1.399782e-03 | valid loss: 1.429525e-03 
      	| train loss (relative): 2.739359e-02 | valid loss (relative): 2.790761e-02 
Epoch 1275 use: 489.87 second.

epoch 1276 starting......
Epoch:  1276 | train loss: 1.403136e-03 | valid loss: 1.429793e-03 
      	| train loss (relative): 2.746020e-02 | valid loss (relative): 2.804639e-02 
Epoch 1276 use: 495.05 second.

epoch 1277 starting......
Epoch:  1277 | train loss: 1.397371e-03 | valid loss: 1.429218e-03 
      	| train loss (relative): 2.734781e-02 | valid loss (relative): 2.801216e-02 
Epoch 1277 use: 483.84 second.

epoch 1278 starting......
Epoch:  1278 | train loss: 1.397923e-03 | valid loss: 1.431358e-03 
      	| train loss (relative): 2.735555e-02 | valid loss (relative): 2.801563e-02 
Epoch 1278 use: 511.12 second.

epoch 1279 starting......
Epoch:  1279 | train loss: 1.400348e-03 | valid loss: 1.438070e-03 
      	| train loss (relative): 2.740726e-02 | valid loss (relative): 2.818063e-02 
Epoch 1279 use: 486.90 second.

epoch 1280 starting......
Epoch:  1280 | train loss: 1.403795e-03 | valid loss: 1.437215e-03 
      	| train loss (relative): 2.747535e-02 | valid loss (relative): 2.812701e-02 
Epoch 1280 use: 514.20 second.

epoch 1281 starting......
Epoch:  1281 | train loss: 1.404665e-03 | valid loss: 1.435043e-03 
      	| train loss (relative): 2.749376e-02 | valid loss (relative): 2.797866e-02 
Epoch 1281 use: 492.79 second.

epoch 1282 starting......
Epoch:  1282 | train loss: 1.402325e-03 | valid loss: 1.434395e-03 
      	| train loss (relative): 2.744408e-02 | valid loss (relative): 2.812389e-02 
Epoch 1282 use: 490.98 second.

epoch 1283 starting......
Epoch:  1283 | train loss: 1.401190e-03 | valid loss: 1.429964e-03 
      	| train loss (relative): 2.742483e-02 | valid loss (relative): 2.797378e-02 
Epoch 1283 use: 504.01 second.

epoch 1284 starting......
Epoch:  1284 | train loss: 1.396603e-03 | valid loss: 1.431543e-03 
      	| train loss (relative): 2.733758e-02 | valid loss (relative): 2.797205e-02 
Epoch 1284 use: 507.06 second.

epoch 1285 starting......
Epoch:  1285 | train loss: 1.399316e-03 | valid loss: 1.431394e-03 
      	| train loss (relative): 2.738466e-02 | valid loss (relative): 2.800262e-02 
Epoch 1285 use: 515.94 second.

epoch 1286 starting......
Epoch:  1286 | train loss: 1.399375e-03 | valid loss: 1.428258e-03 
      	| train loss (relative): 2.738740e-02 | valid loss (relative): 2.790357e-02 
Epoch 1286 use: 495.53 second.

epoch 1287 starting......
Epoch:  1287 | train loss: 1.398606e-03 | valid loss: 1.432083e-03 
      	| train loss (relative): 2.737376e-02 | valid loss (relative): 2.802163e-02 
Epoch 1287 use: 516.34 second.

epoch 1288 starting......
Epoch:  1288 | train loss: 1.398048e-03 | valid loss: 1.433106e-03 
      	| train loss (relative): 2.736223e-02 | valid loss (relative): 2.808286e-02 
Epoch 1288 use: 514.79 second.

epoch 1289 starting......
Epoch:  1289 | train loss: 1.400886e-03 | valid loss: 1.438518e-03 
      	| train loss (relative): 2.742003e-02 | valid loss (relative): 2.807047e-02 
Epoch 1289 use: 505.82 second.

epoch 1290 starting......
Epoch:  1290 | train loss: 1.399676e-03 | valid loss: 1.433910e-03 
      	| train loss (relative): 2.739421e-02 | valid loss (relative): 2.805948e-02 
Epoch 1290 use: 474.04 second.

epoch 1291 starting......
Epoch:  1291 | train loss: 1.400900e-03 | valid loss: 1.438653e-03 
      	| train loss (relative): 2.741913e-02 | valid loss (relative): 2.804404e-02 
Epoch 1291 use: 467.41 second.

epoch 1292 starting......
Epoch:  1292 | train loss: 1.404882e-03 | valid loss: 1.438504e-03 
      	| train loss (relative): 2.749349e-02 | valid loss (relative): 2.814182e-02 
Epoch 1292 use: 495.54 second.

epoch 1293 starting......
Epoch:  1293 | train loss: 1.399720e-03 | valid loss: 1.433512e-03 
      	| train loss (relative): 2.739760e-02 | valid loss (relative): 2.793001e-02 
Epoch 1293 use: 489.39 second.

epoch 1294 starting......
Epoch:  1294 | train loss: 1.396293e-03 | valid loss: 1.431265e-03 
      	| train loss (relative): 2.732484e-02 | valid loss (relative): 2.799718e-02 
Epoch 1294 use: 489.78 second.

epoch 1295 starting......
Epoch:  1295 | train loss: 1.395260e-03 | valid loss: 1.431749e-03 
      	| train loss (relative): 2.730653e-02 | valid loss (relative): 2.795329e-02 
Epoch 1295 use: 486.45 second.

epoch 1296 starting......
Epoch:  1296 | train loss: 1.397377e-03 | valid loss: 1.434048e-03 
      	| train loss (relative): 2.734311e-02 | valid loss (relative): 2.811294e-02 
Epoch 1296 use: 503.40 second.

epoch 1297 starting......
Epoch:  1297 | train loss: 1.399185e-03 | valid loss: 1.437957e-03 
      	| train loss (relative): 2.738467e-02 | valid loss (relative): 2.809781e-02 
Epoch 1297 use: 480.74 second.

epoch 1298 starting......
Epoch:  1298 | train loss: 1.400815e-03 | valid loss: 1.425513e-03 
      	| train loss (relative): 2.741317e-02 | valid loss (relative): 2.793043e-02 
Epoch 1298 use: 496.37 second.

epoch 1299 starting......
Epoch:  1299 | train loss: 1.389761e-03 | valid loss: 1.423692e-03 
      	| train loss (relative): 2.719764e-02 | valid loss (relative): 2.781872e-02 
Epoch 1299 use: 477.05 second.

epoch 1300 starting......
Epoch:  1300 | train loss: 1.390225e-03 | valid loss: 1.424183e-03 
      	| train loss (relative): 2.720169e-02 | valid loss (relative): 2.788678e-02 
Epoch 1300 use: 496.48 second.

epoch 1301 starting......
Epoch:  1301 | train loss: 1.391158e-03 | valid loss: 1.428342e-03 
      	| train loss (relative): 2.722342e-02 | valid loss (relative): 2.795523e-02 
Epoch 1301 use: 495.74 second.

epoch 1302 starting......
Epoch:  1302 | train loss: 1.393994e-03 | valid loss: 1.426234e-03 
      	| train loss (relative): 2.728265e-02 | valid loss (relative): 2.785883e-02 
Epoch 1302 use: 486.83 second.

epoch 1303 starting......
Epoch:  1303 | train loss: 1.394338e-03 | valid loss: 1.437398e-03 
      	| train loss (relative): 2.728614e-02 | valid loss (relative): 2.814617e-02 
Epoch 1303 use: 500.75 second.

epoch 1304 starting......
Epoch:  1304 | train loss: 1.398609e-03 | valid loss: 1.432172e-03 
      	| train loss (relative): 2.737445e-02 | valid loss (relative): 2.806099e-02 
Epoch 1304 use: 530.48 second.

epoch 1305 starting......
Epoch:  1305 | train loss: 1.398469e-03 | valid loss: 1.429146e-03 
      	| train loss (relative): 2.736706e-02 | valid loss (relative): 2.791431e-02 
Epoch 1305 use: 494.80 second.

epoch 1306 starting......
Epoch:  1306 | train loss: 1.392383e-03 | valid loss: 1.427099e-03 
      	| train loss (relative): 2.724786e-02 | valid loss (relative): 2.793942e-02 
Epoch 1306 use: 483.53 second.

epoch 1307 starting......
Epoch:  1307 | train loss: 1.394307e-03 | valid loss: 1.427931e-03 
      	| train loss (relative): 2.728428e-02 | valid loss (relative): 2.791771e-02 
Epoch 1307 use: 488.67 second.

epoch 1308 starting......
Epoch:  1308 | train loss: 1.390787e-03 | valid loss: 1.428640e-03 
      	| train loss (relative): 2.721586e-02 | valid loss (relative): 2.792258e-02 
Epoch 1308 use: 493.11 second.

epoch 1309 starting......
Epoch:  1309 | train loss: 1.396690e-03 | valid loss: 1.435904e-03 
      	| train loss (relative): 2.733252e-02 | valid loss (relative): 2.807484e-02 
Epoch 1309 use: 488.28 second.

epoch 1310 starting......
Epoch:  1310 | train loss: 1.393921e-03 | valid loss: 1.427027e-03 
      	| train loss (relative): 2.727612e-02 | valid loss (relative): 2.801613e-02 
Epoch 1310 use: 495.48 second.

epoch 1311 starting......
Epoch:  1311 | train loss: 1.389595e-03 | valid loss: 1.430303e-03 
      	| train loss (relative): 2.719332e-02 | valid loss (relative): 2.799455e-02 
Epoch 1311 use: 469.47 second.

epoch 1312 starting......
Epoch:  1312 | train loss: 1.393536e-03 | valid loss: 1.428880e-03 
      	| train loss (relative): 2.727243e-02 | valid loss (relative): 2.800367e-02 
Epoch 1312 use: 503.49 second.

epoch 1313 starting......
Epoch:  1313 | train loss: 1.392046e-03 | valid loss: 1.430528e-03 
      	| train loss (relative): 2.724331e-02 | valid loss (relative): 2.806080e-02 
Epoch 1313 use: 473.25 second.

epoch 1314 starting......
Epoch:  1314 | train loss: 1.394729e-03 | valid loss: 1.428875e-03 
      	| train loss (relative): 2.729276e-02 | valid loss (relative): 2.796885e-02 
Epoch 1314 use: 494.91 second.

epoch 1315 starting......
Epoch:  1315 | train loss: 1.394403e-03 | valid loss: 1.430926e-03 
      	| train loss (relative): 2.728852e-02 | valid loss (relative): 2.799722e-02 
Epoch 1315 use: 471.54 second.

epoch 1316 starting......
Epoch:  1316 | train loss: 1.396305e-03 | valid loss: 1.436417e-03 
      	| train loss (relative): 2.732358e-02 | valid loss (relative): 2.818097e-02 
Epoch 1316 use: 515.35 second.

epoch 1317 starting......
Epoch:  1317 | train loss: 1.398661e-03 | valid loss: 1.430882e-03 
      	| train loss (relative): 2.737543e-02 | valid loss (relative): 2.793741e-02 
Epoch 1317 use: 490.31 second.

epoch 1318 starting......
Epoch:  1318 | train loss: 1.394752e-03 | valid loss: 1.437241e-03 
      	| train loss (relative): 2.729154e-02 | valid loss (relative): 2.811555e-02 
Epoch 1318 use: 543.94 second.

epoch 1319 starting......
Epoch:  1319 | train loss: 1.394398e-03 | valid loss: 1.431961e-03 
      	| train loss (relative): 2.728657e-02 | valid loss (relative): 2.807430e-02 
Epoch 1319 use: 527.41 second.

epoch 1320 starting......
Epoch:  1320 | train loss: 1.395430e-03 | valid loss: 1.435533e-03 
      	| train loss (relative): 2.730905e-02 | valid loss (relative): 2.815310e-02 
Epoch 1320 use: 536.27 second.

epoch 1321 starting......
Epoch:  1321 | train loss: 1.392868e-03 | valid loss: 1.426513e-03 
      	| train loss (relative): 2.725802e-02 | valid loss (relative): 2.791420e-02 
Epoch 1321 use: 604.09 second.

epoch 1322 starting......
Epoch:  1322 | train loss: 1.391057e-03 | valid loss: 1.428258e-03 
      	| train loss (relative): 2.721830e-02 | valid loss (relative): 2.796241e-02 
Epoch 1322 use: 568.96 second.

epoch 1323 starting......
Epoch:  1323 | train loss: 1.393331e-03 | valid loss: 1.429916e-03 
      	| train loss (relative): 2.726881e-02 | valid loss (relative): 2.801580e-02 
Epoch 1323 use: 523.99 second.

epoch 1324 starting......
Epoch:  1324 | train loss: 1.390175e-03 | valid loss: 1.427215e-03 
      	| train loss (relative): 2.720598e-02 | valid loss (relative): 2.783277e-02 
Epoch 1324 use: 636.89 second.

epoch 1325 starting......
Epoch:  1325 | train loss: 1.393417e-03 | valid loss: 1.426746e-03 
      	| train loss (relative): 2.726345e-02 | valid loss (relative): 2.799369e-02 
Epoch 1325 use: 489.13 second.

epoch 1326 starting......
Epoch:  1326 | train loss: 1.391477e-03 | valid loss: 1.429043e-03 
      	| train loss (relative): 2.722718e-02 | valid loss (relative): 2.801351e-02 
Epoch 1326 use: 475.26 second.

epoch 1327 starting......
Epoch:  1327 | train loss: 1.391122e-03 | valid loss: 1.431874e-03 
      	| train loss (relative): 2.722160e-02 | valid loss (relative): 2.812449e-02 
Epoch 1327 use: 437.53 second.

epoch 1328 starting......
Epoch:  1328 | train loss: 1.393885e-03 | valid loss: 1.429350e-03 
      	| train loss (relative): 2.727372e-02 | valid loss (relative): 2.797854e-02 
Epoch 1328 use: 443.36 second.

epoch 1329 starting......
Epoch:  1329 | train loss: 1.390396e-03 | valid loss: 1.425540e-03 
      	| train loss (relative): 2.720817e-02 | valid loss (relative): 2.779596e-02 
Epoch 1329 use: 435.14 second.

epoch 1330 starting......
Epoch:  1330 | train loss: 1.392557e-03 | valid loss: 1.422519e-03 
      	| train loss (relative): 2.725244e-02 | valid loss (relative): 2.781758e-02 
Epoch 1330 use: 427.84 second.

epoch 1331 starting......
Epoch:  1331 | train loss: 1.388437e-03 | valid loss: 1.425248e-03 
      	| train loss (relative): 2.716967e-02 | valid loss (relative): 2.796673e-02 
Epoch 1331 use: 411.73 second.

epoch 1332 starting......
Epoch:  1332 | train loss: 1.388930e-03 | valid loss: 1.426381e-03 
      	| train loss (relative): 2.717828e-02 | valid loss (relative): 2.783124e-02 
Epoch 1332 use: 423.42 second.

epoch 1333 starting......
Epoch:  1333 | train loss: 1.388754e-03 | valid loss: 1.428074e-03 
      	| train loss (relative): 2.717466e-02 | valid loss (relative): 2.793554e-02 
Epoch 1333 use: 421.68 second.

epoch 1334 starting......
Epoch:  1334 | train loss: 1.390312e-03 | valid loss: 1.432292e-03 
      	| train loss (relative): 2.720322e-02 | valid loss (relative): 2.788478e-02 
Epoch 1334 use: 409.70 second.

epoch 1335 starting......
Epoch:  1335 | train loss: 1.392933e-03 | valid loss: 1.431923e-03 
      	| train loss (relative): 2.725843e-02 | valid loss (relative): 2.791686e-02 
Epoch 1335 use: 432.00 second.

epoch 1336 starting......
Epoch:  1336 | train loss: 1.393301e-03 | valid loss: 1.444060e-03 
      	| train loss (relative): 2.726751e-02 | valid loss (relative): 2.813930e-02 
Epoch 1336 use: 411.22 second.

epoch 1337 starting......
Epoch:  1337 | train loss: 1.394413e-03 | valid loss: 1.426262e-03 
      	| train loss (relative): 2.728052e-02 | valid loss (relative): 2.782127e-02 
Epoch 1337 use: 419.61 second.

epoch 1338 starting......
Epoch:  1338 | train loss: 1.389124e-03 | valid loss: 1.428448e-03 
      	| train loss (relative): 2.717916e-02 | valid loss (relative): 2.803856e-02 
Epoch 1338 use: 418.71 second.

epoch 1339 starting......
Epoch:  1339 | train loss: 1.393556e-03 | valid loss: 1.435389e-03 
      	| train loss (relative): 2.726893e-02 | valid loss (relative): 2.820278e-02 
Epoch 1339 use: 405.91 second.

epoch 1340 starting......
Epoch:  1340 | train loss: 1.391903e-03 | valid loss: 1.429699e-03 
      	| train loss (relative): 2.723607e-02 | valid loss (relative): 2.807841e-02 
Epoch 1340 use: 419.86 second.

epoch 1341 starting......
Epoch:  1341 | train loss: 1.391408e-03 | valid loss: 1.426475e-03 
      	| train loss (relative): 2.722572e-02 | valid loss (relative): 2.785999e-02 
Epoch 1341 use: 411.52 second.

epoch 1342 starting......
Epoch:  1342 | train loss: 1.389544e-03 | valid loss: 1.433564e-03 
      	| train loss (relative): 2.718980e-02 | valid loss (relative): 2.789441e-02 
Epoch 1342 use: 413.94 second.

epoch 1343 starting......
Epoch:  1343 | train loss: 1.390360e-03 | valid loss: 1.429475e-03 
      	| train loss (relative): 2.720511e-02 | valid loss (relative): 2.792595e-02 
Epoch 1343 use: 394.32 second.

epoch 1344 starting......
Epoch:  1344 | train loss: 1.391448e-03 | valid loss: 1.431038e-03 
      	| train loss (relative): 2.722957e-02 | valid loss (relative): 2.791883e-02 
Epoch 1344 use: 412.76 second.

epoch 1345 starting......
Epoch:  1345 | train loss: 1.389268e-03 | valid loss: 1.440065e-03 
      	| train loss (relative): 2.718539e-02 | valid loss (relative): 2.799338e-02 
Epoch 1345 use: 399.70 second.

epoch 1346 starting......
Epoch:  1346 | train loss: 1.395184e-03 | valid loss: 1.430504e-03 
      	| train loss (relative): 2.730011e-02 | valid loss (relative): 2.792899e-02 
Epoch 1346 use: 413.36 second.

epoch 1347 starting......
Epoch:  1347 | train loss: 1.390383e-03 | valid loss: 1.423799e-03 
      	| train loss (relative): 2.720721e-02 | valid loss (relative): 2.784374e-02 
Epoch 1347 use: 412.32 second.

epoch 1348 starting......
Epoch:  1348 | train loss: 1.386019e-03 | valid loss: 1.421833e-03 
      	| train loss (relative): 2.712105e-02 | valid loss (relative): 2.784490e-02 
Epoch 1348 use: 404.85 second.

epoch 1349 starting......
Epoch:  1349 | train loss: 1.382431e-03 | valid loss: 1.421625e-03 
      	| train loss (relative): 2.704510e-02 | valid loss (relative): 2.783531e-02 
Epoch 1349 use: 404.39 second.

test MSE Error: 1.424237e-03 | relative MSE Error: 2.780312e-02 
 Total time used for training: 20.15 hour.
MESLoss saved to  /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_MSELoss_Latent_128_nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_1350.txt
relative MSELoss saved to  /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_reMSELoss_Latent_128_nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_1350.txt
model saved to /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_128_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_1350.pth
model_dict saved to /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_128_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_1350_dict.pth
... Training slugflow data completed, Run finished Sun 22 Aug 13:07:22 BST 2021 ...
{'vtu_dir': '/rds/general/user/jy220/home/slugflow/', 'data_dir': '/rds/general/user/jy220/home/tensors/', 'data_type': 'tensors', 'vtu_fields': 'Component1::ComponentMassFractionPhase1, phase1::Velocity', 'variational': 'False', 'optimizer': 'Adamax', 'mode': 'train', 'structured': 'False', 'changing_lr': 'False', 'self_concat': '2', 'dimension': '3', 'sfc_nums': '3', 'components': '4', 'nearest_neighbouring': 'True', 'dims_latent': '128', 'activation': 'Tanh', 'tk_file': '/rds/general/user/jy220/home/slugflow_tk.pt', 'tb_file': '/rds/general/user/jy220/home/slugflow_tb.pt', 'sfc_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/sfcs.pt', 'inv_sfc_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/inv_sfcs.pt', 'coords_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/coords.pt', 'cells_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/cells.npy', 'state_load': '/rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_128_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_1350_dict.pth', 'batch_size': '16', 'lr': '0.001', 'n_epoches': '150', 'seed': '15', 'visualize': 'False', 'reconstructed_path': 'None', 'reconstruct_start_index': '0', 'reconstruct_end_index': '100', 'save_path': '/rds/general/user/jy220/home/new_results/'}
[[ 2.50825269e-01 -1.67256990e-02 -1.22268730e-03]
 [ 2.38611959e-01 -5.50627777e-03 -5.06451913e-03]
 [ 2.39454874e-01 -1.59563257e-02  3.96778323e-03]
 ...
 [ 9.11361096e+00 -2.21908381e-02  2.22384902e-03]
 [ 9.11847310e+00 -2.84346931e-02 -2.34839590e-04]
 [ 9.11039661e+00 -2.51498573e-02 -8.70661044e-03]]
{'tetra': array([[      0,       1,       2,       3],
       [      4,       5,       6,       7],
       [      8,       9,      10,      11],
       ...,
       [1342744, 1342745, 1342746, 1342747],
       [1342748, 1342749, 1342750, 1342751],
       [1342752, 1342753, 1342754, 1342755]])}
vtu fields:  ['Component1::ComponentMassFractionPhase1', 'phase1::Velocity'] 
 structured  False 
 activation  Tanh() 
 self concat  2 
 sfc_nums  3 

dims_latent  128 
 components  4 
 nearest_neighbouring  True 

visualize  False 
 sample number  1706 

reading sfc nums......
[array([  41684,   41685,   41674, ..., 1235560, 1235564, 1235558]), array([954289, 954290, 954291, ..., 420685, 420687, 420688]), array([  40156,   40155,   40158, ..., 1123478, 1123475, 1123467])]
reading inverse sfc nums......
[array([   8146,    8145,    8144, ..., 1274444, 1274445, 1273166]), array([547280, 547283, 593037, ..., 933933, 933932, 864488]), array([ 66437,  66439, 122512, ..., 459416, 415130, 459418])]
tk:  tensor([0.7581, 0.0418, 0.1840, 0.1464], dtype=torch.float64) 

tb:  tensor([-0.3816, -0.3618, -0.0308,  0.1446], dtype=torch.float64) 

length of train set: 1505 

length of valid set: 100 

length of test set: 101 

torch device num: 4 

Let's use 4 GPUs!
epoch 1350 starting......
Epoch:  1350 | train loss: 1.552550e-03 | valid loss: 1.414532e-03 
      	| train loss (relative): 3.033978e-02 | valid loss (relative): 2.774495e-02 
Epoch 1350 use: 514.75 second.

epoch 1351 starting......
Epoch:  1351 | train loss: 1.386588e-03 | valid loss: 1.403691e-03 
      	| train loss (relative): 2.713064e-02 | valid loss (relative): 2.751582e-02 
Epoch 1351 use: 426.36 second.

epoch 1352 starting......
Epoch:  1352 | train loss: 1.379502e-03 | valid loss: 1.403395e-03 
      	| train loss (relative): 2.699115e-02 | valid loss (relative): 2.751567e-02 
Epoch 1352 use: 419.89 second.

epoch 1353 starting......
Epoch:  1353 | train loss: 1.377465e-03 | valid loss: 1.404309e-03 
      	| train loss (relative): 2.694838e-02 | valid loss (relative): 2.754348e-02 
Epoch 1353 use: 424.54 second.

epoch 1354 starting......
Epoch:  1354 | train loss: 1.376636e-03 | valid loss: 1.405336e-03 
      	| train loss (relative): 2.692924e-02 | valid loss (relative): 2.752817e-02 
Epoch 1354 use: 422.12 second.

epoch 1355 starting......
Epoch:  1355 | train loss: 1.376758e-03 | valid loss: 1.408538e-03 
      	| train loss (relative): 2.693144e-02 | valid loss (relative): 2.761061e-02 
Epoch 1355 use: 419.40 second.

epoch 1356 starting......
Epoch:  1356 | train loss: 1.377285e-03 | valid loss: 1.408845e-03 
      	| train loss (relative): 2.694192e-02 | valid loss (relative): 2.761923e-02 
Epoch 1356 use: 421.13 second.

epoch 1357 starting......
Epoch:  1357 | train loss: 1.378211e-03 | valid loss: 1.412116e-03 
      	| train loss (relative): 2.696002e-02 | valid loss (relative): 2.764327e-02 
Epoch 1357 use: 417.97 second.

epoch 1358 starting......
Epoch:  1358 | train loss: 1.379921e-03 | valid loss: 1.414713e-03 
      	| train loss (relative): 2.699743e-02 | valid loss (relative): 2.776546e-02 
Epoch 1358 use: 431.63 second.

epoch 1359 starting......
Epoch:  1359 | train loss: 1.383216e-03 | valid loss: 1.417217e-03 
      	| train loss (relative): 2.705791e-02 | valid loss (relative): 2.775078e-02 
Epoch 1359 use: 470.52 second.

epoch 1360 starting......
Epoch:  1360 | train loss: 1.383348e-03 | valid loss: 1.420998e-03 
      	| train loss (relative): 2.705790e-02 | valid loss (relative): 2.782242e-02 
Epoch 1360 use: 419.09 second.

epoch 1361 starting......
Epoch:  1361 | train loss: 1.388004e-03 | valid loss: 1.420706e-03 
      	| train loss (relative): 2.715752e-02 | valid loss (relative): 2.780811e-02 
Epoch 1361 use: 417.93 second.

epoch 1362 starting......
Epoch:  1362 | train loss: 1.385097e-03 | valid loss: 1.418030e-03 
      	| train loss (relative): 2.709484e-02 | valid loss (relative): 2.786066e-02 
Epoch 1362 use: 420.33 second.

epoch 1363 starting......
Epoch:  1363 | train loss: 1.382441e-03 | valid loss: 1.419812e-03 
      	| train loss (relative): 2.704404e-02 | valid loss (relative): 2.783294e-02 
Epoch 1363 use: 408.42 second.

epoch 1364 starting......
Epoch:  1364 | train loss: 1.383021e-03 | valid loss: 1.418915e-03 
      	| train loss (relative): 2.705196e-02 | valid loss (relative): 2.778093e-02 
Epoch 1364 use: 449.77 second.

epoch 1365 starting......
Epoch:  1365 | train loss: 1.383960e-03 | valid loss: 1.424363e-03 
      	| train loss (relative): 2.706976e-02 | valid loss (relative): 2.796007e-02 
Epoch 1365 use: 434.39 second.

epoch 1366 starting......
Epoch:  1366 | train loss: 1.386987e-03 | valid loss: 1.428067e-03 
      	| train loss (relative): 2.713021e-02 | valid loss (relative): 2.799944e-02 
Epoch 1366 use: 412.69 second.

epoch 1367 starting......
Epoch:  1367 | train loss: 1.386664e-03 | valid loss: 1.425840e-03 
      	| train loss (relative): 2.712264e-02 | valid loss (relative): 2.795176e-02 
Epoch 1367 use: 417.30 second.

epoch 1368 starting......
Epoch:  1368 | train loss: 1.389466e-03 | valid loss: 1.429328e-03 
      	| train loss (relative): 2.717949e-02 | valid loss (relative): 2.800338e-02 
Epoch 1368 use: 422.29 second.

epoch 1369 starting......
Epoch:  1369 | train loss: 1.389525e-03 | valid loss: 1.427441e-03 
      	| train loss (relative): 2.718133e-02 | valid loss (relative): 2.789988e-02 
Epoch 1369 use: 411.73 second.

epoch 1370 starting......
Epoch:  1370 | train loss: 1.384510e-03 | valid loss: 1.425310e-03 
      	| train loss (relative): 2.708441e-02 | valid loss (relative): 2.792394e-02 
Epoch 1370 use: 422.49 second.

epoch 1371 starting......
Epoch:  1371 | train loss: 1.384539e-03 | valid loss: 1.426975e-03 
      	| train loss (relative): 2.708665e-02 | valid loss (relative): 2.802443e-02 
Epoch 1371 use: 416.33 second.

epoch 1372 starting......
Epoch:  1372 | train loss: 1.385380e-03 | valid loss: 1.429829e-03 
      	| train loss (relative): 2.710043e-02 | valid loss (relative): 2.799553e-02 
Epoch 1372 use: 412.27 second.

epoch 1373 starting......
Epoch:  1373 | train loss: 1.387484e-03 | valid loss: 1.430035e-03 
      	| train loss (relative): 2.714138e-02 | valid loss (relative): 2.809285e-02 
Epoch 1373 use: 417.04 second.

epoch 1374 starting......
Epoch:  1374 | train loss: 1.383828e-03 | valid loss: 1.434192e-03 
      	| train loss (relative): 2.706547e-02 | valid loss (relative): 2.825645e-02 
Epoch 1374 use: 452.30 second.

epoch 1375 starting......
Epoch:  1375 | train loss: 1.385141e-03 | valid loss: 1.423595e-03 
      	| train loss (relative): 2.709479e-02 | valid loss (relative): 2.785899e-02 
Epoch 1375 use: 510.36 second.

epoch 1376 starting......
Epoch:  1376 | train loss: 1.385155e-03 | valid loss: 1.435016e-03 
      	| train loss (relative): 2.709597e-02 | valid loss (relative): 2.806511e-02 
Epoch 1376 use: 450.25 second.

epoch 1377 starting......
Epoch:  1377 | train loss: 1.389252e-03 | valid loss: 1.430886e-03 
      	| train loss (relative): 2.717193e-02 | valid loss (relative): 2.803740e-02 
Epoch 1377 use: 425.68 second.

epoch 1378 starting......
Epoch:  1378 | train loss: 1.386499e-03 | valid loss: 1.427711e-03 
      	| train loss (relative): 2.712153e-02 | valid loss (relative): 2.805704e-02 
Epoch 1378 use: 425.14 second.

epoch 1379 starting......
Epoch:  1379 | train loss: 1.385622e-03 | valid loss: 1.434375e-03 
      	| train loss (relative): 2.710073e-02 | valid loss (relative): 2.815185e-02 
Epoch 1379 use: 420.46 second.

epoch 1380 starting......
Epoch:  1380 | train loss: 1.385843e-03 | valid loss: 1.427522e-03 
      	| train loss (relative): 2.710800e-02 | valid loss (relative): 2.793544e-02 
Epoch 1380 use: 423.76 second.

epoch 1381 starting......
Epoch:  1381 | train loss: 1.387371e-03 | valid loss: 1.425877e-03 
      	| train loss (relative): 2.713918e-02 | valid loss (relative): 2.801977e-02 
Epoch 1381 use: 416.78 second.

epoch 1382 starting......
Epoch:  1382 | train loss: 1.383477e-03 | valid loss: 1.428375e-03 
      	| train loss (relative): 2.706386e-02 | valid loss (relative): 2.795525e-02 
Epoch 1382 use: 419.47 second.

epoch 1383 starting......
Epoch:  1383 | train loss: 1.387389e-03 | valid loss: 1.435657e-03 
      	| train loss (relative): 2.713882e-02 | valid loss (relative): 2.805772e-02 
Epoch 1383 use: 418.05 second.

epoch 1384 starting......
Epoch:  1384 | train loss: 1.389559e-03 | valid loss: 1.428998e-03 
      	| train loss (relative): 2.718266e-02 | valid loss (relative): 2.796727e-02 
Epoch 1384 use: 417.97 second.

epoch 1385 starting......
Epoch:  1385 | train loss: 1.385204e-03 | valid loss: 1.425289e-03 
      	| train loss (relative): 2.709192e-02 | valid loss (relative): 2.793669e-02 
Epoch 1385 use: 421.18 second.

epoch 1386 starting......
Epoch:  1386 | train loss: 1.382324e-03 | valid loss: 1.431166e-03 
      	| train loss (relative): 2.704219e-02 | valid loss (relative): 2.809384e-02 
Epoch 1386 use: 417.32 second.

epoch 1387 starting......
Epoch:  1387 | train loss: 1.386151e-03 | valid loss: 1.438126e-03 
      	| train loss (relative): 2.711330e-02 | valid loss (relative): 2.817765e-02 
Epoch 1387 use: 422.15 second.

epoch 1388 starting......
Epoch:  1388 | train loss: 1.387488e-03 | valid loss: 1.431625e-03 
      	| train loss (relative): 2.714045e-02 | valid loss (relative): 2.809310e-02 
Epoch 1388 use: 416.46 second.

epoch 1389 starting......
Epoch:  1389 | train loss: 1.385147e-03 | valid loss: 1.431523e-03 
      	| train loss (relative): 2.709757e-02 | valid loss (relative): 2.803568e-02 
Epoch 1389 use: 413.84 second.

epoch 1390 starting......
Epoch:  1390 | train loss: 1.384254e-03 | valid loss: 1.428433e-03 
      	| train loss (relative): 2.707786e-02 | valid loss (relative): 2.801587e-02 
Epoch 1390 use: 413.28 second.

epoch 1391 starting......
Epoch:  1391 | train loss: 1.385523e-03 | valid loss: 1.436315e-03 
      	| train loss (relative): 2.709849e-02 | valid loss (relative): 2.820190e-02 
Epoch 1391 use: 423.75 second.

epoch 1392 starting......
Epoch:  1392 | train loss: 1.386445e-03 | valid loss: 1.426873e-03 
      	| train loss (relative): 2.711822e-02 | valid loss (relative): 2.791601e-02 
Epoch 1392 use: 412.45 second.

epoch 1393 starting......
Epoch:  1393 | train loss: 1.381612e-03 | valid loss: 1.426213e-03 
      	| train loss (relative): 2.702335e-02 | valid loss (relative): 2.788069e-02 
Epoch 1393 use: 421.21 second.

epoch 1394 starting......
Epoch:  1394 | train loss: 1.380786e-03 | valid loss: 1.428517e-03 
      	| train loss (relative): 2.700316e-02 | valid loss (relative): 2.797192e-02 
Epoch 1394 use: 426.46 second.

epoch 1395 starting......
Epoch:  1395 | train loss: 1.386613e-03 | valid loss: 1.426505e-03 
      	| train loss (relative): 2.712207e-02 | valid loss (relative): 2.785164e-02 
Epoch 1395 use: 421.46 second.

epoch 1396 starting......
Epoch:  1396 | train loss: 1.380923e-03 | valid loss: 1.429461e-03 
      	| train loss (relative): 2.701079e-02 | valid loss (relative): 2.796953e-02 
Epoch 1396 use: 424.86 second.

epoch 1397 starting......
Epoch:  1397 | train loss: 1.387811e-03 | valid loss: 1.432731e-03 
      	| train loss (relative): 2.714243e-02 | valid loss (relative): 2.806824e-02 
Epoch 1397 use: 411.04 second.

epoch 1398 starting......
Epoch:  1398 | train loss: 1.382171e-03 | valid loss: 1.424643e-03 
      	| train loss (relative): 2.703338e-02 | valid loss (relative): 2.799677e-02 
Epoch 1398 use: 418.96 second.

epoch 1399 starting......
Epoch:  1399 | train loss: 1.379844e-03 | valid loss: 1.428169e-03 
      	| train loss (relative): 2.699012e-02 | valid loss (relative): 2.788915e-02 
Epoch 1399 use: 409.14 second.

epoch 1400 starting......
Epoch:  1400 | train loss: 1.381303e-03 | valid loss: 1.423347e-03 
      	| train loss (relative): 2.701696e-02 | valid loss (relative): 2.787017e-02 
Epoch 1400 use: 425.04 second.

epoch 1401 starting......
Epoch:  1401 | train loss: 1.381652e-03 | valid loss: 1.436165e-03 
      	| train loss (relative): 2.702312e-02 | valid loss (relative): 2.808155e-02 
Epoch 1401 use: 407.26 second.

epoch 1402 starting......
Epoch:  1402 | train loss: 1.386338e-03 | valid loss: 1.429558e-03 
      	| train loss (relative): 2.711443e-02 | valid loss (relative): 2.808207e-02 
Epoch 1402 use: 408.28 second.

epoch 1403 starting......
Epoch:  1403 | train loss: 1.382284e-03 | valid loss: 1.425717e-03 
      	| train loss (relative): 2.703538e-02 | valid loss (relative): 2.786950e-02 
Epoch 1403 use: 411.09 second.

epoch 1404 starting......
Epoch:  1404 | train loss: 1.382165e-03 | valid loss: 1.429879e-03 
      	| train loss (relative): 2.703336e-02 | valid loss (relative): 2.801851e-02 
Epoch 1404 use: 404.77 second.

epoch 1405 starting......
Epoch:  1405 | train loss: 1.384185e-03 | valid loss: 1.426563e-03 
      	| train loss (relative): 2.707150e-02 | valid loss (relative): 2.792780e-02 
Epoch 1405 use: 409.47 second.

epoch 1406 starting......
Epoch:  1406 | train loss: 1.381310e-03 | valid loss: 1.431452e-03 
      	| train loss (relative): 2.701889e-02 | valid loss (relative): 2.793627e-02 
Epoch 1406 use: 408.00 second.

epoch 1407 starting......
Epoch:  1407 | train loss: 1.382549e-03 | valid loss: 1.435519e-03 
      	| train loss (relative): 2.703949e-02 | valid loss (relative): 2.810531e-02 
Epoch 1407 use: 405.45 second.

epoch 1408 starting......
Epoch:  1408 | train loss: 1.383831e-03 | valid loss: 1.431866e-03 
      	| train loss (relative): 2.706553e-02 | valid loss (relative): 2.805314e-02 
Epoch 1408 use: 411.74 second.

epoch 1409 starting......
Epoch:  1409 | train loss: 1.381802e-03 | valid loss: 1.429613e-03 
      	| train loss (relative): 2.702540e-02 | valid loss (relative): 2.801997e-02 
Epoch 1409 use: 402.47 second.

epoch 1410 starting......
Epoch:  1410 | train loss: 1.381439e-03 | valid loss: 1.432193e-03 
      	| train loss (relative): 2.702269e-02 | valid loss (relative): 2.788249e-02 
Epoch 1410 use: 414.04 second.

epoch 1411 starting......
Epoch:  1411 | train loss: 1.379732e-03 | valid loss: 1.427024e-03 
      	| train loss (relative): 2.698533e-02 | valid loss (relative): 2.794651e-02 
Epoch 1411 use: 402.53 second.

epoch 1412 starting......
Epoch:  1412 | train loss: 1.379048e-03 | valid loss: 1.426700e-03 
      	| train loss (relative): 2.697040e-02 | valid loss (relative): 2.796257e-02 
Epoch 1412 use: 411.70 second.

epoch 1413 starting......
Epoch:  1413 | train loss: 1.381975e-03 | valid loss: 1.437469e-03 
      	| train loss (relative): 2.703023e-02 | valid loss (relative): 2.829241e-02 
Epoch 1413 use: 407.03 second.

epoch 1414 starting......
Epoch:  1414 | train loss: 1.381148e-03 | valid loss: 1.431852e-03 
      	| train loss (relative): 2.701097e-02 | valid loss (relative): 2.816166e-02 
Epoch 1414 use: 408.61 second.

epoch 1415 starting......
Epoch:  1415 | train loss: 1.380512e-03 | valid loss: 1.424225e-03 
      	| train loss (relative): 2.700018e-02 | valid loss (relative): 2.785691e-02 
Epoch 1415 use: 414.70 second.

epoch 1416 starting......
Epoch:  1416 | train loss: 1.374975e-03 | valid loss: 1.425979e-03 
      	| train loss (relative): 2.688659e-02 | valid loss (relative): 2.792605e-02 
Epoch 1416 use: 406.06 second.

epoch 1417 starting......
Epoch:  1417 | train loss: 1.379456e-03 | valid loss: 1.426263e-03 
      	| train loss (relative): 2.697828e-02 | valid loss (relative): 2.793748e-02 
Epoch 1417 use: 423.07 second.

epoch 1418 starting......
Epoch:  1418 | train loss: 1.379719e-03 | valid loss: 1.431936e-03 
      	| train loss (relative): 2.698339e-02 | valid loss (relative): 2.792209e-02 
Epoch 1418 use: 401.37 second.

epoch 1419 starting......
Epoch:  1419 | train loss: 1.380930e-03 | valid loss: 1.426381e-03 
      	| train loss (relative): 2.700573e-02 | valid loss (relative): 2.796189e-02 
Epoch 1419 use: 416.47 second.

epoch 1420 starting......
Epoch:  1420 | train loss: 1.377287e-03 | valid loss: 1.431327e-03 
      	| train loss (relative): 2.693350e-02 | valid loss (relative): 2.809084e-02 
Epoch 1420 use: 407.69 second.

epoch 1421 starting......
Epoch:  1421 | train loss: 1.379333e-03 | valid loss: 1.427797e-03 
      	| train loss (relative): 2.698136e-02 | valid loss (relative): 2.783892e-02 
Epoch 1421 use: 409.51 second.

epoch 1422 starting......
Epoch:  1422 | train loss: 1.378570e-03 | valid loss: 1.426001e-03 
      	| train loss (relative): 2.696133e-02 | valid loss (relative): 2.792900e-02 
Epoch 1422 use: 411.56 second.

epoch 1423 starting......
Epoch:  1423 | train loss: 1.380322e-03 | valid loss: 1.437844e-03 
      	| train loss (relative): 2.699558e-02 | valid loss (relative): 2.823127e-02 
Epoch 1423 use: 406.59 second.

epoch 1424 starting......
Epoch:  1424 | train loss: 1.380660e-03 | valid loss: 1.431410e-03 
      	| train loss (relative): 2.700465e-02 | valid loss (relative): 2.798063e-02 
Epoch 1424 use: 426.78 second.

epoch 1425 starting......
Epoch:  1425 | train loss: 1.376173e-03 | valid loss: 1.429834e-03 
      	| train loss (relative): 2.691669e-02 | valid loss (relative): 2.774568e-02 
Epoch 1425 use: 407.74 second.

epoch 1426 starting......
Epoch:  1426 | train loss: 1.378944e-03 | valid loss: 1.426443e-03 
      	| train loss (relative): 2.696722e-02 | valid loss (relative): 2.799626e-02 
Epoch 1426 use: 409.99 second.

epoch 1427 starting......
Epoch:  1427 | train loss: 1.375255e-03 | valid loss: 1.430376e-03 
      	| train loss (relative): 2.689580e-02 | valid loss (relative): 2.801678e-02 
Epoch 1427 use: 410.89 second.

epoch 1428 starting......
Epoch:  1428 | train loss: 1.377043e-03 | valid loss: 1.430784e-03 
      	| train loss (relative): 2.692967e-02 | valid loss (relative): 2.804777e-02 
Epoch 1428 use: 409.14 second.

epoch 1429 starting......
Epoch:  1429 | train loss: 1.376883e-03 | valid loss: 1.426580e-03 
      	| train loss (relative): 2.692732e-02 | valid loss (relative): 2.794212e-02 
Epoch 1429 use: 407.26 second.

epoch 1430 starting......
Epoch:  1430 | train loss: 1.376512e-03 | valid loss: 1.426479e-03 
      	| train loss (relative): 2.691639e-02 | valid loss (relative): 2.794085e-02 
Epoch 1430 use: 408.87 second.

epoch 1431 starting......
Epoch:  1431 | train loss: 1.375817e-03 | valid loss: 1.424030e-03 
      	| train loss (relative): 2.690799e-02 | valid loss (relative): 2.782437e-02 
Epoch 1431 use: 416.32 second.

epoch 1432 starting......
Epoch:  1432 | train loss: 1.376785e-03 | valid loss: 1.430125e-03 
      	| train loss (relative): 2.692656e-02 | valid loss (relative): 2.799658e-02 
Epoch 1432 use: 406.77 second.

epoch 1433 starting......
Epoch:  1433 | train loss: 1.376513e-03 | valid loss: 1.423423e-03 
      	| train loss (relative): 2.691895e-02 | valid loss (relative): 2.797199e-02 
Epoch 1433 use: 413.87 second.

epoch 1434 starting......
Epoch:  1434 | train loss: 1.372337e-03 | valid loss: 1.425486e-03 
      	| train loss (relative): 2.683854e-02 | valid loss (relative): 2.800271e-02 
Epoch 1434 use: 408.23 second.

epoch 1435 starting......
Epoch:  1435 | train loss: 1.375893e-03 | valid loss: 1.425400e-03 
      	| train loss (relative): 2.691206e-02 | valid loss (relative): 2.793716e-02 
Epoch 1435 use: 410.66 second.

epoch 1436 starting......
Epoch:  1436 | train loss: 1.376596e-03 | valid loss: 1.428917e-03 
      	| train loss (relative): 2.692069e-02 | valid loss (relative): 2.794370e-02 
Epoch 1436 use: 419.27 second.

epoch 1437 starting......
Epoch:  1437 | train loss: 1.376983e-03 | valid loss: 1.424013e-03 
      	| train loss (relative): 2.692773e-02 | valid loss (relative): 2.787219e-02 
Epoch 1437 use: 415.61 second.

epoch 1438 starting......
Epoch:  1438 | train loss: 1.375228e-03 | valid loss: 1.423351e-03 
      	| train loss (relative): 2.689386e-02 | valid loss (relative): 2.787168e-02 
Epoch 1438 use: 424.73 second.

epoch 1439 starting......
Epoch:  1439 | train loss: 1.379407e-03 | valid loss: 1.431952e-03 
      	| train loss (relative): 2.697933e-02 | valid loss (relative): 2.801191e-02 
Epoch 1439 use: 413.35 second.

epoch 1440 starting......
Epoch:  1440 | train loss: 1.377386e-03 | valid loss: 1.432878e-03 
      	| train loss (relative): 2.693853e-02 | valid loss (relative): 2.794758e-02 
Epoch 1440 use: 422.38 second.

epoch 1441 starting......
Epoch:  1441 | train loss: 1.382594e-03 | valid loss: 1.431457e-03 
      	| train loss (relative): 2.703837e-02 | valid loss (relative): 2.804672e-02 
Epoch 1441 use: 421.22 second.

epoch 1442 starting......
Epoch:  1442 | train loss: 1.376438e-03 | valid loss: 1.429303e-03 
      	| train loss (relative): 2.692050e-02 | valid loss (relative): 2.807295e-02 
Epoch 1442 use: 430.81 second.

epoch 1443 starting......
Epoch:  1443 | train loss: 1.373794e-03 | valid loss: 1.428797e-03 
      	| train loss (relative): 2.686752e-02 | valid loss (relative): 2.803646e-02 
Epoch 1443 use: 423.84 second.

epoch 1444 starting......
Epoch:  1444 | train loss: 1.371365e-03 | valid loss: 1.423187e-03 
      	| train loss (relative): 2.681859e-02 | valid loss (relative): 2.793901e-02 
Epoch 1444 use: 422.76 second.

epoch 1445 starting......
Epoch:  1445 | train loss: 1.374711e-03 | valid loss: 1.435157e-03 
      	| train loss (relative): 2.688493e-02 | valid loss (relative): 2.810494e-02 
Epoch 1445 use: 419.15 second.

epoch 1446 starting......
Epoch:  1446 | train loss: 1.378493e-03 | valid loss: 1.428151e-03 
      	| train loss (relative): 2.695658e-02 | valid loss (relative): 2.797289e-02 
Epoch 1446 use: 432.08 second.

epoch 1447 starting......
Epoch:  1447 | train loss: 1.373796e-03 | valid loss: 1.420906e-03 
      	| train loss (relative): 2.686959e-02 | valid loss (relative): 2.788919e-02 
Epoch 1447 use: 421.13 second.

epoch 1448 starting......
Epoch:  1448 | train loss: 1.372938e-03 | valid loss: 1.420602e-03 
      	| train loss (relative): 2.684746e-02 | valid loss (relative): 2.775593e-02 
Epoch 1448 use: 426.91 second.

epoch 1449 starting......
Epoch:  1449 | train loss: 1.370632e-03 | valid loss: 1.422826e-03 
      	| train loss (relative): 2.680521e-02 | valid loss (relative): 2.790851e-02 
Epoch 1449 use: 425.45 second.

epoch 1450 starting......
Epoch:  1450 | train loss: 1.373758e-03 | valid loss: 1.426937e-03 
      	| train loss (relative): 2.686552e-02 | valid loss (relative): 2.799544e-02 
Epoch 1450 use: 430.96 second.

epoch 1451 starting......
Epoch:  1451 | train loss: 1.374382e-03 | valid loss: 1.428389e-03 
      	| train loss (relative): 2.687662e-02 | valid loss (relative): 2.799395e-02 
Epoch 1451 use: 426.44 second.

epoch 1452 starting......
Epoch:  1452 | train loss: 1.373749e-03 | valid loss: 1.423686e-03 
      	| train loss (relative): 2.686278e-02 | valid loss (relative): 2.782278e-02 
Epoch 1452 use: 426.84 second.

epoch 1453 starting......
Epoch:  1453 | train loss: 1.372655e-03 | valid loss: 1.425387e-03 
      	| train loss (relative): 2.684183e-02 | valid loss (relative): 2.774533e-02 
Epoch 1453 use: 426.17 second.

epoch 1454 starting......
Epoch:  1454 | train loss: 1.373770e-03 | valid loss: 1.427032e-03 
      	| train loss (relative): 2.686405e-02 | valid loss (relative): 2.799018e-02 
Epoch 1454 use: 419.48 second.

epoch 1455 starting......
Epoch:  1455 | train loss: 1.374988e-03 | valid loss: 1.423787e-03 
      	| train loss (relative): 2.688643e-02 | valid loss (relative): 2.799259e-02 
Epoch 1455 use: 419.67 second.

epoch 1456 starting......
Epoch:  1456 | train loss: 1.372334e-03 | valid loss: 1.422185e-03 
      	| train loss (relative): 2.683955e-02 | valid loss (relative): 2.787247e-02 
Epoch 1456 use: 420.74 second.

epoch 1457 starting......
Epoch:  1457 | train loss: 1.369745e-03 | valid loss: 1.423984e-03 
      	| train loss (relative): 2.678392e-02 | valid loss (relative): 2.798912e-02 
Epoch 1457 use: 426.92 second.

epoch 1458 starting......
Epoch:  1458 | train loss: 1.370817e-03 | valid loss: 1.428582e-03 
      	| train loss (relative): 2.681012e-02 | valid loss (relative): 2.791872e-02 
Epoch 1458 use: 424.22 second.

epoch 1459 starting......
Epoch:  1459 | train loss: 1.370870e-03 | valid loss: 1.419582e-03 
      	| train loss (relative): 2.680477e-02 | valid loss (relative): 2.779923e-02 
Epoch 1459 use: 423.85 second.

epoch 1460 starting......
Epoch:  1460 | train loss: 1.370645e-03 | valid loss: 1.431152e-03 
      	| train loss (relative): 2.680278e-02 | valid loss (relative): 2.814439e-02 
Epoch 1460 use: 427.08 second.

epoch 1461 starting......
Epoch:  1461 | train loss: 1.374477e-03 | valid loss: 1.421180e-03 
      	| train loss (relative): 2.687904e-02 | valid loss (relative): 2.787877e-02 
Epoch 1461 use: 427.73 second.

epoch 1462 starting......
Epoch:  1462 | train loss: 1.369021e-03 | valid loss: 1.421087e-03 
      	| train loss (relative): 2.677048e-02 | valid loss (relative): 2.779141e-02 
Epoch 1462 use: 413.84 second.

epoch 1463 starting......
Epoch:  1463 | train loss: 1.364613e-03 | valid loss: 1.434732e-03 
      	| train loss (relative): 2.668573e-02 | valid loss (relative): 2.805224e-02 
Epoch 1463 use: 420.66 second.

epoch 1464 starting......
Epoch:  1464 | train loss: 1.371724e-03 | valid loss: 1.433369e-03 
      	| train loss (relative): 2.682217e-02 | valid loss (relative): 2.800511e-02 
Epoch 1464 use: 416.49 second.

epoch 1465 starting......
Epoch:  1465 | train loss: 1.371264e-03 | valid loss: 1.417487e-03 
      	| train loss (relative): 2.681364e-02 | valid loss (relative): 2.769467e-02 
Epoch 1465 use: 431.17 second.

epoch 1466 starting......
Epoch:  1466 | train loss: 1.364397e-03 | valid loss: 1.422717e-03 
      	| train loss (relative): 2.668140e-02 | valid loss (relative): 2.802137e-02 
Epoch 1466 use: 418.31 second.

epoch 1467 starting......
Epoch:  1467 | train loss: 1.366924e-03 | valid loss: 1.420483e-03 
      	| train loss (relative): 2.673126e-02 | valid loss (relative): 2.778837e-02 
Epoch 1467 use: 425.68 second.

epoch 1468 starting......
Epoch:  1468 | train loss: 1.368308e-03 | valid loss: 1.424984e-03 
      	| train loss (relative): 2.675478e-02 | valid loss (relative): 2.799535e-02 
Epoch 1468 use: 429.24 second.

epoch 1469 starting......
Epoch:  1469 | train loss: 1.370191e-03 | valid loss: 1.422377e-03 
      	| train loss (relative): 2.679019e-02 | valid loss (relative): 2.780267e-02 
Epoch 1469 use: 435.32 second.

epoch 1470 starting......
Epoch:  1470 | train loss: 1.368347e-03 | valid loss: 1.419949e-03 
      	| train loss (relative): 2.675604e-02 | valid loss (relative): 2.777811e-02 
Epoch 1470 use: 425.27 second.

epoch 1471 starting......
Epoch:  1471 | train loss: 1.366294e-03 | valid loss: 1.425495e-03 
      	| train loss (relative): 2.671853e-02 | valid loss (relative): 2.794451e-02 
Epoch 1471 use: 431.58 second.

epoch 1472 starting......
Epoch:  1472 | train loss: 1.367786e-03 | valid loss: 1.423273e-03 
      	| train loss (relative): 2.674473e-02 | valid loss (relative): 2.793191e-02 
Epoch 1472 use: 423.75 second.

epoch 1473 starting......
Epoch:  1473 | train loss: 1.368344e-03 | valid loss: 1.419027e-03 
      	| train loss (relative): 2.675899e-02 | valid loss (relative): 2.781143e-02 
Epoch 1473 use: 430.78 second.

epoch 1474 starting......
Epoch:  1474 | train loss: 1.366238e-03 | valid loss: 1.421886e-03 
      	| train loss (relative): 2.671406e-02 | valid loss (relative): 2.785829e-02 
Epoch 1474 use: 425.42 second.

epoch 1475 starting......
Epoch:  1475 | train loss: 1.369348e-03 | valid loss: 1.427960e-03 
      	| train loss (relative): 2.677511e-02 | valid loss (relative): 2.793695e-02 
Epoch 1475 use: 440.42 second.

epoch 1476 starting......
Epoch:  1476 | train loss: 1.370610e-03 | valid loss: 1.429141e-03 
      	| train loss (relative): 2.679669e-02 | valid loss (relative): 2.803143e-02 
Epoch 1476 use: 425.17 second.

epoch 1477 starting......
Epoch:  1477 | train loss: 1.377225e-03 | valid loss: 1.412831e-03 
      	| train loss (relative): 2.693551e-02 | valid loss (relative): 2.765743e-02 
Epoch 1477 use: 433.11 second.

epoch 1478 starting......
Epoch:  1478 | train loss: 1.361545e-03 | valid loss: 1.414105e-03 
      	| train loss (relative): 2.662346e-02 | valid loss (relative): 2.775370e-02 
Epoch 1478 use: 425.94 second.

epoch 1479 starting......
Epoch:  1479 | train loss: 1.361652e-03 | valid loss: 1.417187e-03 
      	| train loss (relative): 2.662549e-02 | valid loss (relative): 2.782336e-02 
Epoch 1479 use: 431.11 second.

epoch 1480 starting......
Epoch:  1480 | train loss: 1.362465e-03 | valid loss: 1.417792e-03 
      	| train loss (relative): 2.664251e-02 | valid loss (relative): 2.779929e-02 
Epoch 1480 use: 419.69 second.

epoch 1481 starting......
Epoch:  1481 | train loss: 1.362395e-03 | valid loss: 1.414303e-03 
      	| train loss (relative): 2.664061e-02 | valid loss (relative): 2.769116e-02 
Epoch 1481 use: 437.76 second.

epoch 1482 starting......
Epoch:  1482 | train loss: 1.362862e-03 | valid loss: 1.415006e-03 
      	| train loss (relative): 2.664423e-02 | valid loss (relative): 2.776533e-02 
Epoch 1482 use: 423.37 second.

epoch 1483 starting......
Epoch:  1483 | train loss: 1.362801e-03 | valid loss: 1.415499e-03 
      	| train loss (relative): 2.664722e-02 | valid loss (relative): 2.779101e-02 
Epoch 1483 use: 421.66 second.

epoch 1484 starting......
Epoch:  1484 | train loss: 1.364404e-03 | valid loss: 1.421851e-03 
      	| train loss (relative): 2.667600e-02 | valid loss (relative): 2.783487e-02 
Epoch 1484 use: 426.49 second.

epoch 1485 starting......
Epoch:  1485 | train loss: 1.365046e-03 | valid loss: 1.422691e-03 
      	| train loss (relative): 2.669013e-02 | valid loss (relative): 2.779438e-02 
Epoch 1485 use: 424.34 second.

epoch 1486 starting......
Epoch:  1486 | train loss: 1.364496e-03 | valid loss: 1.424250e-03 
      	| train loss (relative): 2.668132e-02 | valid loss (relative): 2.797722e-02 
Epoch 1486 use: 419.53 second.

epoch 1487 starting......
Epoch:  1487 | train loss: 1.366730e-03 | valid loss: 1.419151e-03 
      	| train loss (relative): 2.672296e-02 | valid loss (relative): 2.789583e-02 
Epoch 1487 use: 430.64 second.

epoch 1488 starting......
Epoch:  1488 | train loss: 1.362650e-03 | valid loss: 1.414879e-03 
      	| train loss (relative): 2.664401e-02 | valid loss (relative): 2.774527e-02 
Epoch 1488 use: 419.61 second.

epoch 1489 starting......
Epoch:  1489 | train loss: 1.361871e-03 | valid loss: 1.422164e-03 
      	| train loss (relative): 2.662897e-02 | valid loss (relative): 2.776089e-02 
Epoch 1489 use: 428.10 second.

epoch 1490 starting......
Epoch:  1490 | train loss: 1.366518e-03 | valid loss: 1.420227e-03 
      	| train loss (relative): 2.671881e-02 | valid loss (relative): 2.775057e-02 
Epoch 1490 use: 420.09 second.

epoch 1491 starting......
Epoch:  1491 | train loss: 1.363870e-03 | valid loss: 1.416295e-03 
      	| train loss (relative): 2.666548e-02 | valid loss (relative): 2.776550e-02 
Epoch 1491 use: 439.73 second.

epoch 1492 starting......
Epoch:  1492 | train loss: 1.363183e-03 | valid loss: 1.419535e-03 
      	| train loss (relative): 2.665388e-02 | valid loss (relative): 2.778269e-02 
Epoch 1492 use: 418.01 second.

epoch 1493 starting......
Epoch:  1493 | train loss: 1.365559e-03 | valid loss: 1.422759e-03 
      	| train loss (relative): 2.669936e-02 | valid loss (relative): 2.794915e-02 
Epoch 1493 use: 436.56 second.

epoch 1494 starting......
Epoch:  1494 | train loss: 1.367623e-03 | valid loss: 1.419331e-03 
      	| train loss (relative): 2.674234e-02 | valid loss (relative): 2.777388e-02 
Epoch 1494 use: 414.80 second.

epoch 1495 starting......
Epoch:  1495 | train loss: 1.361566e-03 | valid loss: 1.415311e-03 
      	| train loss (relative): 2.662374e-02 | valid loss (relative): 2.770094e-02 
Epoch 1495 use: 429.78 second.

epoch 1496 starting......
Epoch:  1496 | train loss: 1.363669e-03 | valid loss: 1.422381e-03 
      	| train loss (relative): 2.666199e-02 | valid loss (relative): 2.785176e-02 
Epoch 1496 use: 421.41 second.

epoch 1497 starting......
Epoch:  1497 | train loss: 1.365367e-03 | valid loss: 1.423600e-03 
      	| train loss (relative): 2.669289e-02 | valid loss (relative): 2.798124e-02 
Epoch 1497 use: 428.93 second.

epoch 1498 starting......
Epoch:  1498 | train loss: 1.367953e-03 | valid loss: 1.422725e-03 
      	| train loss (relative): 2.675115e-02 | valid loss (relative): 2.791037e-02 
Epoch 1498 use: 420.21 second.

epoch 1499 starting......
Epoch:  1499 | train loss: 1.366085e-03 | valid loss: 1.420310e-03 
      	| train loss (relative): 2.671278e-02 | valid loss (relative): 2.774467e-02 
Epoch 1499 use: 436.83 second.

test MSE Error: 1.346231e-03 | relative MSE Error: 2.626053e-02 
 Total time used for training: 17.59 hour.
MESLoss saved to  /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_MSELoss_Latent_128_nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_1500.txt
relative MSELoss saved to  /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_reMSELoss_Latent_128_nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_1500.txt
model saved to /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_128_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_1500.pth
model_dict saved to /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_128_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_1500_dict.pth
... Training slugflow data completed, Run finished Mon 23 Aug 10:02:54 BST 2021 ...
{'vtu_dir': '/rds/general/user/jy220/home/slugflow/', 'data_dir': '/rds/general/user/jy220/home/tensors/', 'data_type': 'tensors', 'vtu_fields': 'Component1::ComponentMassFractionPhase1, phase1::Velocity', 'variational': 'False', 'optimizer': 'Adamax', 'mode': 'train', 'structured': 'False', 'changing_lr': 'False', 'self_concat': '2', 'dimension': '3', 'sfc_nums': '3', 'components': '4', 'nearest_neighbouring': 'True', 'dims_latent': '128', 'activation': 'Tanh', 'tk_file': '/rds/general/user/jy220/home/slugflow_tk.pt', 'tb_file': '/rds/general/user/jy220/home/slugflow_tb.pt', 'sfc_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/sfcs.pt', 'inv_sfc_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/inv_sfcs.pt', 'coords_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/coords.pt', 'cells_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/cells.npy', 'state_load': '/rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_128_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_1500_dict.pth', 'batch_size': '16', 'lr': '0.001', 'n_epoches': '150', 'seed': '15', 'visualize': 'False', 'reconstructed_path': 'None', 'reconstruct_start_index': '0', 'reconstruct_end_index': '100', 'save_path': '/rds/general/user/jy220/home/new_results/'}
[[ 2.50825269e-01 -1.67256990e-02 -1.22268730e-03]
 [ 2.38611959e-01 -5.50627777e-03 -5.06451913e-03]
 [ 2.39454874e-01 -1.59563257e-02  3.96778323e-03]
 ...
 [ 9.11361096e+00 -2.21908381e-02  2.22384902e-03]
 [ 9.11847310e+00 -2.84346931e-02 -2.34839590e-04]
 [ 9.11039661e+00 -2.51498573e-02 -8.70661044e-03]]
{'tetra': array([[      0,       1,       2,       3],
       [      4,       5,       6,       7],
       [      8,       9,      10,      11],
       ...,
       [1342744, 1342745, 1342746, 1342747],
       [1342748, 1342749, 1342750, 1342751],
       [1342752, 1342753, 1342754, 1342755]])}
vtu fields:  ['Component1::ComponentMassFractionPhase1', 'phase1::Velocity'] 
 structured  False 
 activation  Tanh() 
 self concat  2 
 sfc_nums  3 

dims_latent  128 
 components  4 
 nearest_neighbouring  True 

visualize  False 
 sample number  1706 

reading sfc nums......
[array([  41684,   41685,   41674, ..., 1235560, 1235564, 1235558]), array([954289, 954290, 954291, ..., 420685, 420687, 420688]), array([  40156,   40155,   40158, ..., 1123478, 1123475, 1123467])]
reading inverse sfc nums......
[array([   8146,    8145,    8144, ..., 1274444, 1274445, 1273166]), array([547280, 547283, 593037, ..., 933933, 933932, 864488]), array([ 66437,  66439, 122512, ..., 459416, 415130, 459418])]
tk:  tensor([0.7581, 0.0418, 0.1840, 0.1464], dtype=torch.float64) 

tb:  tensor([-0.3816, -0.3618, -0.0308,  0.1446], dtype=torch.float64) 

length of train set: 1505 

length of valid set: 100 

length of test set: 101 

torch device num: 4 

Let's use 4 GPUs!
epoch 1500 starting......
Epoch:  1500 | train loss: 1.557827e-03 | valid loss: 1.359264e-03 
      	| train loss (relative): 3.054405e-02 | valid loss (relative): 2.653506e-02 
Epoch 1500 use: 482.97 second.

epoch 1501 starting......
Epoch:  1501 | train loss: 1.358867e-03 | valid loss: 1.348378e-03 
      	| train loss (relative): 2.657003e-02 | valid loss (relative): 2.633857e-02 
Epoch 1501 use: 467.26 second.

epoch 1502 starting......
Epoch:  1502 | train loss: 1.351516e-03 | valid loss: 1.346211e-03 
      	| train loss (relative): 2.643051e-02 | valid loss (relative): 2.629217e-02 
Epoch 1502 use: 459.10 second.

epoch 1503 starting......
Epoch:  1503 | train loss: 1.349020e-03 | valid loss: 1.345989e-03 
      	| train loss (relative): 2.637760e-02 | valid loss (relative): 2.627995e-02 
Epoch 1503 use: 491.77 second.

epoch 1504 starting......
Epoch:  1504 | train loss: 1.347887e-03 | valid loss: 1.345930e-03 
      	| train loss (relative): 2.635457e-02 | valid loss (relative): 2.630180e-02 
Epoch 1504 use: 461.86 second.

epoch 1505 starting......
Epoch:  1505 | train loss: 1.347495e-03 | valid loss: 1.346972e-03 
      	| train loss (relative): 2.634303e-02 | valid loss (relative): 2.629771e-02 
Epoch 1505 use: 470.95 second.

epoch 1506 starting......
Epoch:  1506 | train loss: 1.347838e-03 | valid loss: 1.347442e-03 
      	| train loss (relative): 2.635335e-02 | valid loss (relative): 2.627539e-02 
Epoch 1506 use: 494.33 second.

epoch 1507 starting......
Epoch:  1507 | train loss: 1.347458e-03 | valid loss: 1.349150e-03 
      	| train loss (relative): 2.634337e-02 | valid loss (relative): 2.635002e-02 
Epoch 1507 use: 470.32 second.

epoch 1508 starting......
Epoch:  1508 | train loss: 1.348594e-03 | valid loss: 1.350685e-03 
      	| train loss (relative): 2.637164e-02 | valid loss (relative): 2.635044e-02 
Epoch 1508 use: 480.20 second.

epoch 1509 starting......
Epoch:  1509 | train loss: 1.351609e-03 | valid loss: 1.354019e-03 
      	| train loss (relative): 2.642559e-02 | valid loss (relative): 2.645681e-02 
Epoch 1509 use: 485.26 second.

epoch 1510 starting......
Epoch:  1510 | train loss: 1.352400e-03 | valid loss: 1.355926e-03 
      	| train loss (relative): 2.644182e-02 | valid loss (relative): 2.647295e-02 
Epoch 1510 use: 459.42 second.

epoch 1511 starting......
Epoch:  1511 | train loss: 1.353205e-03 | valid loss: 1.359470e-03 
      	| train loss (relative): 2.645876e-02 | valid loss (relative): 2.652888e-02 
Epoch 1511 use: 504.38 second.

epoch 1512 starting......
Epoch:  1512 | train loss: 1.354614e-03 | valid loss: 1.361410e-03 
      	| train loss (relative): 2.648191e-02 | valid loss (relative): 2.658162e-02 
Epoch 1512 use: 473.16 second.

epoch 1513 starting......
Epoch:  1513 | train loss: 1.354626e-03 | valid loss: 1.361131e-03 
      	| train loss (relative): 2.648737e-02 | valid loss (relative): 2.658652e-02 
Epoch 1513 use: 487.60 second.

epoch 1514 starting......
Epoch:  1514 | train loss: 1.356867e-03 | valid loss: 1.363122e-03 
      	| train loss (relative): 2.652723e-02 | valid loss (relative): 2.662445e-02 
Epoch 1514 use: 469.85 second.

epoch 1515 starting......
Epoch:  1515 | train loss: 1.357197e-03 | valid loss: 1.365539e-03 
      	| train loss (relative): 2.654049e-02 | valid loss (relative): 2.661694e-02 
Epoch 1515 use: 485.88 second.

epoch 1516 starting......
Epoch:  1516 | train loss: 1.360501e-03 | valid loss: 1.372519e-03 
      	| train loss (relative): 2.660047e-02 | valid loss (relative): 2.683711e-02 
Epoch 1516 use: 475.90 second.

epoch 1517 starting......
Epoch:  1517 | train loss: 1.359118e-03 | valid loss: 1.361513e-03 
      	| train loss (relative): 2.657564e-02 | valid loss (relative): 2.656731e-02 
Epoch 1517 use: 485.48 second.

epoch 1518 starting......
Epoch:  1518 | train loss: 1.355517e-03 | valid loss: 1.362523e-03 
      	| train loss (relative): 2.650313e-02 | valid loss (relative): 2.665691e-02 
Epoch 1518 use: 493.66 second.

epoch 1519 starting......
Epoch:  1519 | train loss: 1.357254e-03 | valid loss: 1.366440e-03 
      	| train loss (relative): 2.653848e-02 | valid loss (relative): 2.669580e-02 
Epoch 1519 use: 467.50 second.

epoch 1520 starting......
Epoch:  1520 | train loss: 1.361398e-03 | valid loss: 1.371632e-03 
      	| train loss (relative): 2.661810e-02 | valid loss (relative): 2.686588e-02 
Epoch 1520 use: 478.02 second.

epoch 1521 starting......
Epoch:  1521 | train loss: 1.359322e-03 | valid loss: 1.370065e-03 
      	| train loss (relative): 2.657624e-02 | valid loss (relative): 2.678030e-02 
Epoch 1521 use: 475.59 second.

epoch 1522 starting......
Epoch:  1522 | train loss: 1.361050e-03 | valid loss: 1.373452e-03 
      	| train loss (relative): 2.661222e-02 | valid loss (relative): 2.688257e-02 
Epoch 1522 use: 479.94 second.

epoch 1523 starting......
Epoch:  1523 | train loss: 1.357671e-03 | valid loss: 1.366712e-03 
      	| train loss (relative): 2.654300e-02 | valid loss (relative): 2.667769e-02 
Epoch 1523 use: 483.25 second.

epoch 1524 starting......
Epoch:  1524 | train loss: 1.358918e-03 | valid loss: 1.370203e-03 
      	| train loss (relative): 2.656499e-02 | valid loss (relative): 2.671151e-02 
Epoch 1524 use: 484.01 second.

epoch 1525 starting......
Epoch:  1525 | train loss: 1.363576e-03 | valid loss: 1.376816e-03 
      	| train loss (relative): 2.665948e-02 | valid loss (relative): 2.693169e-02 
Epoch 1525 use: 493.64 second.

epoch 1526 starting......
Epoch:  1526 | train loss: 1.367431e-03 | valid loss: 1.373176e-03 
      	| train loss (relative): 2.673580e-02 | valid loss (relative): 2.678553e-02 
Epoch 1526 use: 475.75 second.

epoch 1527 starting......
Epoch:  1527 | train loss: 1.360810e-03 | valid loss: 1.364411e-03 
      	| train loss (relative): 2.660665e-02 | valid loss (relative): 2.658749e-02 
Epoch 1527 use: 490.82 second.

epoch 1528 starting......
Epoch:  1528 | train loss: 1.355052e-03 | valid loss: 1.369094e-03 
      	| train loss (relative): 2.649228e-02 | valid loss (relative): 2.677615e-02 
Epoch 1528 use: 466.62 second.

epoch 1529 starting......
Epoch:  1529 | train loss: 1.356614e-03 | valid loss: 1.377599e-03 
      	| train loss (relative): 2.652572e-02 | valid loss (relative): 2.677024e-02 
Epoch 1529 use: 473.51 second.

epoch 1530 starting......
Epoch:  1530 | train loss: 1.357859e-03 | valid loss: 1.371210e-03 
      	| train loss (relative): 2.654378e-02 | valid loss (relative): 2.669717e-02 
Epoch 1530 use: 463.85 second.

epoch 1531 starting......
Epoch:  1531 | train loss: 1.357613e-03 | valid loss: 1.376639e-03 
      	| train loss (relative): 2.654311e-02 | valid loss (relative): 2.686765e-02 
Epoch 1531 use: 501.70 second.

epoch 1532 starting......
Epoch:  1532 | train loss: 1.359794e-03 | valid loss: 1.363945e-03 
      	| train loss (relative): 2.658495e-02 | valid loss (relative): 2.665994e-02 
Epoch 1532 use: 487.40 second.

epoch 1533 starting......
Epoch:  1533 | train loss: 1.353320e-03 | valid loss: 1.365578e-03 
      	| train loss (relative): 2.645944e-02 | valid loss (relative): 2.664295e-02 
Epoch 1533 use: 481.70 second.

epoch 1534 starting......
Epoch:  1534 | train loss: 1.356463e-03 | valid loss: 1.370860e-03 
      	| train loss (relative): 2.652177e-02 | valid loss (relative): 2.675485e-02 
Epoch 1534 use: 475.23 second.

epoch 1535 starting......
Epoch:  1535 | train loss: 1.355896e-03 | valid loss: 1.369247e-03 
      	| train loss (relative): 2.651254e-02 | valid loss (relative): 2.675429e-02 
Epoch 1535 use: 488.83 second.

epoch 1536 starting......
Epoch:  1536 | train loss: 1.358661e-03 | valid loss: 1.369183e-03 
      	| train loss (relative): 2.656078e-02 | valid loss (relative): 2.678333e-02 
Epoch 1536 use: 467.29 second.

epoch 1537 starting......
Epoch:  1537 | train loss: 1.355099e-03 | valid loss: 1.364634e-03 
      	| train loss (relative): 2.649812e-02 | valid loss (relative): 2.672685e-02 
Epoch 1537 use: 490.42 second.

epoch 1538 starting......
Epoch:  1538 | train loss: 1.353958e-03 | valid loss: 1.366671e-03 
      	| train loss (relative): 2.647121e-02 | valid loss (relative): 2.669120e-02 
Epoch 1538 use: 466.40 second.

epoch 1539 starting......
Epoch:  1539 | train loss: 1.357055e-03 | valid loss: 1.370228e-03 
      	| train loss (relative): 2.653330e-02 | valid loss (relative): 2.658896e-02 
Epoch 1539 use: 482.90 second.

epoch 1540 starting......
Epoch:  1540 | train loss: 1.355311e-03 | valid loss: 1.367745e-03 
      	| train loss (relative): 2.649184e-02 | valid loss (relative): 2.659683e-02 
Epoch 1540 use: 482.49 second.

epoch 1541 starting......
Epoch:  1541 | train loss: 1.356263e-03 | valid loss: 1.369655e-03 
      	| train loss (relative): 2.651795e-02 | valid loss (relative): 2.663717e-02 
Epoch 1541 use: 468.18 second.

epoch 1542 starting......
Epoch:  1542 | train loss: 1.356543e-03 | valid loss: 1.372977e-03 
      	| train loss (relative): 2.652044e-02 | valid loss (relative): 2.675889e-02 
Epoch 1542 use: 486.24 second.

epoch 1543 starting......
Epoch:  1543 | train loss: 1.358366e-03 | valid loss: 1.374564e-03 
      	| train loss (relative): 2.656025e-02 | valid loss (relative): 2.681660e-02 
Epoch 1543 use: 469.90 second.

epoch 1544 starting......
Epoch:  1544 | train loss: 1.357727e-03 | valid loss: 1.363267e-03 
      	| train loss (relative): 2.654614e-02 | valid loss (relative): 2.666369e-02 
Epoch 1544 use: 489.44 second.

epoch 1545 starting......
Epoch:  1545 | train loss: 1.351520e-03 | valid loss: 1.378071e-03 
      	| train loss (relative): 2.642327e-02 | valid loss (relative): 2.700176e-02 
Epoch 1545 use: 483.30 second.

epoch 1546 starting......
Epoch:  1546 | train loss: 1.356098e-03 | valid loss: 1.365744e-03 
      	| train loss (relative): 2.651035e-02 | valid loss (relative): 2.670163e-02 
Epoch 1546 use: 487.15 second.

epoch 1547 starting......
Epoch:  1547 | train loss: 1.354207e-03 | valid loss: 1.363128e-03 
      	| train loss (relative): 2.647646e-02 | valid loss (relative): 2.658760e-02 
Epoch 1547 use: 482.16 second.

epoch 1548 starting......
Epoch:  1548 | train loss: 1.353251e-03 | valid loss: 1.367935e-03 
      	| train loss (relative): 2.645616e-02 | valid loss (relative): 2.683986e-02 
Epoch 1548 use: 493.07 second.

epoch 1549 starting......
Epoch:  1549 | train loss: 1.354449e-03 | valid loss: 1.369861e-03 
      	| train loss (relative): 2.648222e-02 | valid loss (relative): 2.681682e-02 
Epoch 1549 use: 489.90 second.

epoch 1550 starting......
Epoch:  1550 | train loss: 1.356503e-03 | valid loss: 1.369998e-03 
      	| train loss (relative): 2.652241e-02 | valid loss (relative): 2.676518e-02 
Epoch 1550 use: 499.32 second.

epoch 1551 starting......
Epoch:  1551 | train loss: 1.353532e-03 | valid loss: 1.370892e-03 
      	| train loss (relative): 2.646294e-02 | valid loss (relative): 2.688450e-02 
Epoch 1551 use: 461.41 second.

epoch 1552 starting......
Epoch:  1552 | train loss: 1.356177e-03 | valid loss: 1.370036e-03 
      	| train loss (relative): 2.651190e-02 | valid loss (relative): 2.685948e-02 
Epoch 1552 use: 497.99 second.

epoch 1553 starting......
Epoch:  1553 | train loss: 1.355705e-03 | valid loss: 1.371099e-03 
      	| train loss (relative): 2.650524e-02 | valid loss (relative): 2.667227e-02 
Epoch 1553 use: 467.04 second.

epoch 1554 starting......
Epoch:  1554 | train loss: 1.356124e-03 | valid loss: 1.374063e-03 
      	| train loss (relative): 2.651301e-02 | valid loss (relative): 2.688045e-02 
Epoch 1554 use: 476.07 second.

epoch 1555 starting......
Epoch:  1555 | train loss: 1.358027e-03 | valid loss: 1.370914e-03 
      	| train loss (relative): 2.655206e-02 | valid loss (relative): 2.680268e-02 
Epoch 1555 use: 484.01 second.

epoch 1556 starting......
Epoch:  1556 | train loss: 1.351894e-03 | valid loss: 1.376588e-03 
      	| train loss (relative): 2.642572e-02 | valid loss (relative): 2.694829e-02 
Epoch 1556 use: 460.98 second.

epoch 1557 starting......
Epoch:  1557 | train loss: 1.354435e-03 | valid loss: 1.369446e-03 
      	| train loss (relative): 2.647940e-02 | valid loss (relative): 2.681471e-02 
Epoch 1557 use: 482.06 second.

epoch 1558 starting......
Epoch:  1558 | train loss: 1.354043e-03 | valid loss: 1.366184e-03 
      	| train loss (relative): 2.647574e-02 | valid loss (relative): 2.662218e-02 
Epoch 1558 use: 480.13 second.

epoch 1559 starting......
Epoch:  1559 | train loss: 1.353562e-03 | valid loss: 1.368676e-03 
      	| train loss (relative): 2.646240e-02 | valid loss (relative): 2.665328e-02 
Epoch 1559 use: 487.38 second.

epoch 1560 starting......
Epoch:  1560 | train loss: 1.354482e-03 | valid loss: 1.374433e-03 
      	| train loss (relative): 2.647863e-02 | valid loss (relative): 2.679960e-02 
Epoch 1560 use: 472.14 second.

epoch 1561 starting......
Epoch:  1561 | train loss: 1.354301e-03 | valid loss: 1.367150e-03 
      	| train loss (relative): 2.647453e-02 | valid loss (relative): 2.677453e-02 
Epoch 1561 use: 461.86 second.

epoch 1562 starting......
Epoch:  1562 | train loss: 1.350549e-03 | valid loss: 1.364724e-03 
      	| train loss (relative): 2.640170e-02 | valid loss (relative): 2.658111e-02 
Epoch 1562 use: 482.74 second.

epoch 1563 starting......
Epoch:  1563 | train loss: 1.348220e-03 | valid loss: 1.362741e-03 
      	| train loss (relative): 2.635451e-02 | valid loss (relative): 2.660136e-02 
Epoch 1563 use: 485.36 second.

epoch 1564 starting......
Epoch:  1564 | train loss: 1.352954e-03 | valid loss: 1.367690e-03 
      	| train loss (relative): 2.644876e-02 | valid loss (relative): 2.659754e-02 
Epoch 1564 use: 468.38 second.

epoch 1565 starting......
Epoch:  1565 | train loss: 1.351271e-03 | valid loss: 1.372749e-03 
      	| train loss (relative): 2.641212e-02 | valid loss (relative): 2.703706e-02 
Epoch 1565 use: 473.43 second.

epoch 1566 starting......
Epoch:  1566 | train loss: 1.352267e-03 | valid loss: 1.364369e-03 
      	| train loss (relative): 2.643137e-02 | valid loss (relative): 2.665480e-02 
Epoch 1566 use: 463.92 second.

epoch 1567 starting......
Epoch:  1567 | train loss: 1.350483e-03 | valid loss: 1.370143e-03 
      	| train loss (relative): 2.640246e-02 | valid loss (relative): 2.673799e-02 
Epoch 1567 use: 482.41 second.

epoch 1568 starting......
Epoch:  1568 | train loss: 1.352068e-03 | valid loss: 1.370007e-03 
      	| train loss (relative): 2.643411e-02 | valid loss (relative): 2.674584e-02 
Epoch 1568 use: 462.71 second.

epoch 1569 starting......
Epoch:  1569 | train loss: 1.350895e-03 | valid loss: 1.368370e-03 
      	| train loss (relative): 2.641252e-02 | valid loss (relative): 2.671199e-02 
Epoch 1569 use: 469.24 second.

epoch 1570 starting......
Epoch:  1570 | train loss: 1.352679e-03 | valid loss: 1.362403e-03 
      	| train loss (relative): 2.644581e-02 | valid loss (relative): 2.654245e-02 
Epoch 1570 use: 480.32 second.

epoch 1571 starting......
Epoch:  1571 | train loss: 1.347589e-03 | valid loss: 1.364898e-03 
      	| train loss (relative): 2.634318e-02 | valid loss (relative): 2.661738e-02 
Epoch 1571 use: 468.11 second.

epoch 1572 starting......
Epoch:  1572 | train loss: 1.348579e-03 | valid loss: 1.369483e-03 
      	| train loss (relative): 2.636425e-02 | valid loss (relative): 2.673891e-02 
Epoch 1572 use: 469.23 second.

epoch 1573 starting......
Epoch:  1573 | train loss: 1.353322e-03 | valid loss: 1.367976e-03 
      	| train loss (relative): 2.645918e-02 | valid loss (relative): 2.668615e-02 
Epoch 1573 use: 473.41 second.

epoch 1574 starting......
Epoch:  1574 | train loss: 1.349601e-03 | valid loss: 1.363428e-03 
      	| train loss (relative): 2.638277e-02 | valid loss (relative): 2.674881e-02 
Epoch 1574 use: 452.61 second.

epoch 1575 starting......
Epoch:  1575 | train loss: 1.346074e-03 | valid loss: 1.362979e-03 
      	| train loss (relative): 2.631339e-02 | valid loss (relative): 2.664308e-02 
Epoch 1575 use: 463.06 second.

epoch 1576 starting......
Epoch:  1576 | train loss: 1.348850e-03 | valid loss: 1.367171e-03 
      	| train loss (relative): 2.636820e-02 | valid loss (relative): 2.669469e-02 
Epoch 1576 use: 465.02 second.

epoch 1577 starting......
Epoch:  1577 | train loss: 1.349851e-03 | valid loss: 1.368574e-03 
      	| train loss (relative): 2.638466e-02 | valid loss (relative): 2.659864e-02 
Epoch 1577 use: 481.74 second.

epoch 1578 starting......
Epoch:  1578 | train loss: 1.352690e-03 | valid loss: 1.370544e-03 
      	| train loss (relative): 2.644169e-02 | valid loss (relative): 2.682454e-02 
Epoch 1578 use: 477.73 second.

epoch 1579 starting......
Epoch:  1579 | train loss: 1.350343e-03 | valid loss: 1.369281e-03 
      	| train loss (relative): 2.639759e-02 | valid loss (relative): 2.668152e-02 
Epoch 1579 use: 473.64 second.

epoch 1580 starting......
Epoch:  1580 | train loss: 1.352357e-03 | valid loss: 1.361100e-03 
      	| train loss (relative): 2.643511e-02 | valid loss (relative): 2.654878e-02 
Epoch 1580 use: 479.75 second.

epoch 1581 starting......
Epoch:  1581 | train loss: 1.346851e-03 | valid loss: 1.360798e-03 
      	| train loss (relative): 2.632836e-02 | valid loss (relative): 2.653925e-02 
Epoch 1581 use: 482.88 second.

epoch 1582 starting......
Epoch:  1582 | train loss: 1.345317e-03 | valid loss: 1.367864e-03 
      	| train loss (relative): 2.629808e-02 | valid loss (relative): 2.673176e-02 
Epoch 1582 use: 475.79 second.

epoch 1583 starting......
Epoch:  1583 | train loss: 1.352758e-03 | valid loss: 1.369401e-03 
      	| train loss (relative): 2.644251e-02 | valid loss (relative): 2.680139e-02 
Epoch 1583 use: 474.73 second.

epoch 1584 starting......
Epoch:  1584 | train loss: 1.348291e-03 | valid loss: 1.366151e-03 
      	| train loss (relative): 2.635642e-02 | valid loss (relative): 2.663663e-02 
Epoch 1584 use: 474.73 second.

epoch 1585 starting......
Epoch:  1585 | train loss: 1.349738e-03 | valid loss: 1.366887e-03 
      	| train loss (relative): 2.638485e-02 | valid loss (relative): 2.670266e-02 
Epoch 1585 use: 462.57 second.

epoch 1586 starting......
Epoch:  1586 | train loss: 1.350766e-03 | valid loss: 1.370153e-03 
      	| train loss (relative): 2.640614e-02 | valid loss (relative): 2.666117e-02 
Epoch 1586 use: 460.27 second.

epoch 1587 starting......
Epoch:  1587 | train loss: 1.348481e-03 | valid loss: 1.361023e-03 
      	| train loss (relative): 2.635596e-02 | valid loss (relative): 2.660102e-02 
Epoch 1587 use: 458.79 second.

epoch 1588 starting......
Epoch:  1588 | train loss: 1.343907e-03 | valid loss: 1.363642e-03 
      	| train loss (relative): 2.627028e-02 | valid loss (relative): 2.667527e-02 
Epoch 1588 use: 477.35 second.

epoch 1589 starting......
Epoch:  1589 | train loss: 1.345609e-03 | valid loss: 1.362170e-03 
      	| train loss (relative): 2.630029e-02 | valid loss (relative): 2.667226e-02 
Epoch 1589 use: 448.35 second.

epoch 1590 starting......
Epoch:  1590 | train loss: 1.346602e-03 | valid loss: 1.367267e-03 
      	| train loss (relative): 2.632415e-02 | valid loss (relative): 2.677092e-02 
Epoch 1590 use: 472.79 second.

epoch 1591 starting......
Epoch:  1591 | train loss: 1.348230e-03 | valid loss: 1.369304e-03 
      	| train loss (relative): 2.635399e-02 | valid loss (relative): 2.660807e-02 
Epoch 1591 use: 469.69 second.

epoch 1592 starting......
Epoch:  1592 | train loss: 1.349261e-03 | valid loss: 1.367320e-03 
      	| train loss (relative): 2.637399e-02 | valid loss (relative): 2.677911e-02 
Epoch 1592 use: 450.53 second.

epoch 1593 starting......
Epoch:  1593 | train loss: 1.351155e-03 | valid loss: 1.370825e-03 
      	| train loss (relative): 2.641289e-02 | valid loss (relative): 2.684431e-02 
Epoch 1593 use: 465.99 second.

epoch 1594 starting......
Epoch:  1594 | train loss: 1.349613e-03 | valid loss: 1.366356e-03 
      	| train loss (relative): 2.637818e-02 | valid loss (relative): 2.668368e-02 
Epoch 1594 use: 463.64 second.

epoch 1595 starting......
Epoch:  1595 | train loss: 1.349400e-03 | valid loss: 1.368795e-03 
      	| train loss (relative): 2.637851e-02 | valid loss (relative): 2.680685e-02 
Epoch 1595 use: 442.36 second.

epoch 1596 starting......
Epoch:  1596 | train loss: 1.347692e-03 | valid loss: 1.363770e-03 
      	| train loss (relative): 2.634623e-02 | valid loss (relative): 2.655493e-02 
Epoch 1596 use: 444.69 second.

epoch 1597 starting......
Epoch:  1597 | train loss: 1.346903e-03 | valid loss: 1.377704e-03 
      	| train loss (relative): 2.632783e-02 | valid loss (relative): 2.695599e-02 
Epoch 1597 use: 471.74 second.

epoch 1598 starting......
Epoch:  1598 | train loss: 1.350835e-03 | valid loss: 1.371057e-03 
      	| train loss (relative): 2.640627e-02 | valid loss (relative): 2.676272e-02 
Epoch 1598 use: 453.35 second.

epoch 1599 starting......
Epoch:  1599 | train loss: 1.350540e-03 | valid loss: 1.365674e-03 
      	| train loss (relative): 2.640090e-02 | valid loss (relative): 2.673995e-02 
Epoch 1599 use: 451.34 second.

epoch 1600 starting......
Epoch:  1600 | train loss: 1.347062e-03 | valid loss: 1.367330e-03 
      	| train loss (relative): 2.633203e-02 | valid loss (relative): 2.670284e-02 
Epoch 1600 use: 450.69 second.

epoch 1601 starting......
Epoch:  1601 | train loss: 1.344333e-03 | valid loss: 1.367586e-03 
      	| train loss (relative): 2.627801e-02 | valid loss (relative): 2.674727e-02 
Epoch 1601 use: 457.32 second.

epoch 1602 starting......
Epoch:  1602 | train loss: 1.347143e-03 | valid loss: 1.364086e-03 
      	| train loss (relative): 2.633477e-02 | valid loss (relative): 2.665275e-02 
Epoch 1602 use: 474.77 second.

epoch 1603 starting......
Epoch:  1603 | train loss: 1.348431e-03 | valid loss: 1.375100e-03 
      	| train loss (relative): 2.635782e-02 | valid loss (relative): 2.682430e-02 
Epoch 1603 use: 445.78 second.

epoch 1604 starting......
Epoch:  1604 | train loss: 1.349659e-03 | valid loss: 1.367243e-03 
      	| train loss (relative): 2.638394e-02 | valid loss (relative): 2.668997e-02 
Epoch 1604 use: 445.54 second.

epoch 1605 starting......
Epoch:  1605 | train loss: 1.342113e-03 | valid loss: 1.363909e-03 
      	| train loss (relative): 2.623421e-02 | valid loss (relative): 2.663085e-02 
Epoch 1605 use: 450.34 second.

epoch 1606 starting......
Epoch:  1606 | train loss: 1.340658e-03 | valid loss: 1.359713e-03 
      	| train loss (relative): 2.620388e-02 | valid loss (relative): 2.664159e-02 
Epoch 1606 use: 441.72 second.

epoch 1607 starting......
Epoch:  1607 | train loss: 1.341146e-03 | valid loss: 1.364481e-03 
      	| train loss (relative): 2.621432e-02 | valid loss (relative): 2.655416e-02 
Epoch 1607 use: 449.29 second.

epoch 1608 starting......
Epoch:  1608 | train loss: 1.346305e-03 | valid loss: 1.363787e-03 
      	| train loss (relative): 2.631368e-02 | valid loss (relative): 2.667727e-02 
Epoch 1608 use: 455.39 second.

epoch 1609 starting......
Epoch:  1609 | train loss: 1.345183e-03 | valid loss: 1.368513e-03 
      	| train loss (relative): 2.629612e-02 | valid loss (relative): 2.658864e-02 
Epoch 1609 use: 452.96 second.

epoch 1610 starting......
Epoch:  1610 | train loss: 1.347659e-03 | valid loss: 1.367925e-03 
      	| train loss (relative): 2.634143e-02 | valid loss (relative): 2.672327e-02 
Epoch 1610 use: 461.14 second.

epoch 1611 starting......
Epoch:  1611 | train loss: 1.342734e-03 | valid loss: 1.360060e-03 
      	| train loss (relative): 2.624547e-02 | valid loss (relative): 2.656020e-02 
Epoch 1611 use: 473.19 second.

epoch 1612 starting......
Epoch:  1612 | train loss: 1.343066e-03 | valid loss: 1.361374e-03 
      	| train loss (relative): 2.624865e-02 | valid loss (relative): 2.662310e-02 
Epoch 1612 use: 453.02 second.

epoch 1613 starting......
Epoch:  1613 | train loss: 1.347677e-03 | valid loss: 1.367571e-03 
      	| train loss (relative): 2.634503e-02 | valid loss (relative): 2.660033e-02 
Epoch 1613 use: 471.36 second.

epoch 1614 starting......
Epoch:  1614 | train loss: 1.346422e-03 | valid loss: 1.366390e-03 
      	| train loss (relative): 2.632242e-02 | valid loss (relative): 2.659224e-02 
Epoch 1614 use: 454.68 second.

epoch 1615 starting......
Epoch:  1615 | train loss: 1.345156e-03 | valid loss: 1.369871e-03 
      	| train loss (relative): 2.629188e-02 | valid loss (relative): 2.679353e-02 
Epoch 1615 use: 460.22 second.

epoch 1616 starting......
Epoch:  1616 | train loss: 1.348336e-03 | valid loss: 1.369952e-03 
      	| train loss (relative): 2.635310e-02 | valid loss (relative): 2.677675e-02 
Epoch 1616 use: 472.68 second.

epoch 1617 starting......
Epoch:  1617 | train loss: 1.346715e-03 | valid loss: 1.363075e-03 
      	| train loss (relative): 2.632488e-02 | valid loss (relative): 2.660490e-02 
Epoch 1617 use: 466.05 second.

epoch 1618 starting......
Epoch:  1618 | train loss: 1.343435e-03 | valid loss: 1.365180e-03 
      	| train loss (relative): 2.625962e-02 | valid loss (relative): 2.673474e-02 
Epoch 1618 use: 459.45 second.

epoch 1619 starting......
Epoch:  1619 | train loss: 1.344972e-03 | valid loss: 1.363704e-03 
      	| train loss (relative): 2.629098e-02 | valid loss (relative): 2.660636e-02 
Epoch 1619 use: 464.94 second.

epoch 1620 starting......
Epoch:  1620 | train loss: 1.343021e-03 | valid loss: 1.366119e-03 
      	| train loss (relative): 2.625541e-02 | valid loss (relative): 2.665518e-02 
Epoch 1620 use: 456.37 second.

epoch 1621 starting......
Epoch:  1621 | train loss: 1.344472e-03 | valid loss: 1.365340e-03 
      	| train loss (relative): 2.627897e-02 | valid loss (relative): 2.663258e-02 
Epoch 1621 use: 477.20 second.

epoch 1622 starting......
Epoch:  1622 | train loss: 1.344659e-03 | valid loss: 1.361897e-03 
      	| train loss (relative): 2.628268e-02 | valid loss (relative): 2.661128e-02 
Epoch 1622 use: 453.88 second.

epoch 1623 starting......
Epoch:  1623 | train loss: 1.342522e-03 | valid loss: 1.364000e-03 
      	| train loss (relative): 2.623923e-02 | valid loss (relative): 2.672224e-02 
Epoch 1623 use: 469.71 second.

epoch 1624 starting......
Epoch:  1624 | train loss: 1.345423e-03 | valid loss: 1.364494e-03 
      	| train loss (relative): 2.629932e-02 | valid loss (relative): 2.657728e-02 
Epoch 1624 use: 454.51 second.

epoch 1625 starting......
Epoch:  1625 | train loss: 1.345378e-03 | valid loss: 1.376814e-03 
      	| train loss (relative): 2.629615e-02 | valid loss (relative): 2.685867e-02 
Epoch 1625 use: 458.65 second.

epoch 1626 starting......
Epoch:  1626 | train loss: 1.346247e-03 | valid loss: 1.368846e-03 
      	| train loss (relative): 2.631265e-02 | valid loss (relative): 2.666618e-02 
Epoch 1626 use: 462.84 second.

epoch 1627 starting......
Epoch:  1627 | train loss: 1.345786e-03 | valid loss: 1.366387e-03 
      	| train loss (relative): 2.630260e-02 | valid loss (relative): 2.667740e-02 
Epoch 1627 use: 470.82 second.

epoch 1628 starting......
Epoch:  1628 | train loss: 1.347359e-03 | valid loss: 1.365114e-03 
      	| train loss (relative): 2.633824e-02 | valid loss (relative): 2.666618e-02 
Epoch 1628 use: 472.09 second.

epoch 1629 starting......
Epoch:  1629 | train loss: 1.344017e-03 | valid loss: 1.366418e-03 
      	| train loss (relative): 2.626551e-02 | valid loss (relative): 2.668154e-02 
Epoch 1629 use: 475.99 second.

epoch 1630 starting......
Epoch:  1630 | train loss: 1.343346e-03 | valid loss: 1.366908e-03 
      	| train loss (relative): 2.625853e-02 | valid loss (relative): 2.661256e-02 
Epoch 1630 use: 467.29 second.

epoch 1631 starting......
Epoch:  1631 | train loss: 1.347479e-03 | valid loss: 1.361164e-03 
      	| train loss (relative): 2.633931e-02 | valid loss (relative): 2.659438e-02 
Epoch 1631 use: 478.30 second.

epoch 1632 starting......
Epoch:  1632 | train loss: 1.340079e-03 | valid loss: 1.360833e-03 
      	| train loss (relative): 2.619740e-02 | valid loss (relative): 2.644732e-02 
Epoch 1632 use: 463.14 second.

epoch 1633 starting......
Epoch:  1633 | train loss: 1.339346e-03 | valid loss: 1.360926e-03 
      	| train loss (relative): 2.617870e-02 | valid loss (relative): 2.654712e-02 
Epoch 1633 use: 469.38 second.

epoch 1634 starting......
Epoch:  1634 | train loss: 1.342048e-03 | valid loss: 1.366033e-03 
      	| train loss (relative): 2.623344e-02 | valid loss (relative): 2.668463e-02 
Epoch 1634 use: 467.55 second.

epoch 1635 starting......
Epoch:  1635 | train loss: 1.341732e-03 | valid loss: 1.364413e-03 
      	| train loss (relative): 2.621954e-02 | valid loss (relative): 2.658107e-02 
Epoch 1635 use: 465.00 second.

epoch 1636 starting......
Epoch:  1636 | train loss: 1.340541e-03 | valid loss: 1.363576e-03 
      	| train loss (relative): 2.620442e-02 | valid loss (relative): 2.655908e-02 
Epoch 1636 use: 472.76 second.

epoch 1637 starting......
Epoch:  1637 | train loss: 1.339801e-03 | valid loss: 1.367753e-03 
      	| train loss (relative): 2.618541e-02 | valid loss (relative): 2.662364e-02 
Epoch 1637 use: 467.49 second.

epoch 1638 starting......
Epoch:  1638 | train loss: 1.345138e-03 | valid loss: 1.363061e-03 
      	| train loss (relative): 2.629285e-02 | valid loss (relative): 2.663746e-02 
Epoch 1638 use: 455.53 second.

epoch 1639 starting......
Epoch:  1639 | train loss: 1.339796e-03 | valid loss: 1.364292e-03 
      	| train loss (relative): 2.618768e-02 | valid loss (relative): 2.656616e-02 
Epoch 1639 use: 477.38 second.

epoch 1640 starting......
Epoch:  1640 | train loss: 1.342511e-03 | valid loss: 1.361639e-03 
      	| train loss (relative): 2.623641e-02 | valid loss (relative): 2.659397e-02 
Epoch 1640 use: 456.98 second.

epoch 1641 starting......
Epoch:  1641 | train loss: 1.336857e-03 | valid loss: 1.355921e-03 
      	| train loss (relative): 2.612390e-02 | valid loss (relative): 2.643142e-02 
Epoch 1641 use: 483.00 second.

epoch 1642 starting......
Epoch:  1642 | train loss: 1.334923e-03 | valid loss: 1.357027e-03 
      	| train loss (relative): 2.609018e-02 | valid loss (relative): 2.651058e-02 
Epoch 1642 use: 453.22 second.

epoch 1643 starting......
Epoch:  1643 | train loss: 1.335719e-03 | valid loss: 1.359083e-03 
      	| train loss (relative): 2.610504e-02 | valid loss (relative): 2.653066e-02 
Epoch 1643 use: 464.44 second.

epoch 1644 starting......
Epoch:  1644 | train loss: 1.342243e-03 | valid loss: 1.365950e-03 
      	| train loss (relative): 2.623441e-02 | valid loss (relative): 2.664574e-02 
Epoch 1644 use: 425.73 second.

epoch 1645 starting......
Epoch:  1645 | train loss: 1.341058e-03 | valid loss: 1.365226e-03 
      	| train loss (relative): 2.621092e-02 | valid loss (relative): 2.683313e-02 
Epoch 1645 use: 426.35 second.

epoch 1646 starting......
Epoch:  1646 | train loss: 1.337648e-03 | valid loss: 1.362679e-03 
      	| train loss (relative): 2.614344e-02 | valid loss (relative): 2.673712e-02 
Epoch 1646 use: 421.98 second.

epoch 1647 starting......
Epoch:  1647 | train loss: 1.341498e-03 | valid loss: 1.362399e-03 
      	| train loss (relative): 2.622197e-02 | valid loss (relative): 2.669424e-02 
Epoch 1647 use: 403.36 second.

epoch 1648 starting......
Epoch:  1648 | train loss: 1.338699e-03 | valid loss: 1.367192e-03 
      	| train loss (relative): 2.616198e-02 | valid loss (relative): 2.672702e-02 
Epoch 1648 use: 400.35 second.

epoch 1649 starting......
Epoch:  1649 | train loss: 1.341999e-03 | valid loss: 1.360874e-03 
      	| train loss (relative): 2.622870e-02 | valid loss (relative): 2.659779e-02 
Epoch 1649 use: 388.26 second.

test MSE Error: 1.397053e-03 | relative MSE Error: 2.740246e-02 
 Total time used for training: 19.54 hour.
MESLoss saved to  /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_MSELoss_Latent_128_nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_1650.txt
relative MSELoss saved to  /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_reMSELoss_Latent_128_nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_1650.txt
model saved to /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_128_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_1650.pth
model_dict saved to /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_128_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_1650_dict.pth
... Training slugflow data completed, Run finished Tue 24 Aug 19:10:31 BST 2021 ...
{'vtu_dir': '/rds/general/user/jy220/home/slugflow/', 'data_dir': '/rds/general/user/jy220/home/tensors/', 'data_type': 'tensors', 'vtu_fields': 'Component1::ComponentMassFractionPhase1, phase1::Velocity', 'variational': 'False', 'optimizer': 'Adamax', 'mode': 'train', 'structured': 'False', 'changing_lr': 'False', 'self_concat': '2', 'dimension': '3', 'sfc_nums': '3', 'components': '4', 'nearest_neighbouring': 'True', 'dims_latent': '128', 'activation': 'Tanh', 'tk_file': '/rds/general/user/jy220/home/slugflow_tk.pt', 'tb_file': '/rds/general/user/jy220/home/slugflow_tb.pt', 'sfc_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/sfcs.pt', 'inv_sfc_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/inv_sfcs.pt', 'coords_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/coords.pt', 'cells_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/cells.npy', 'state_load': '/rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_128_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_1500_dict.pth', 'batch_size': '16', 'lr': '0.001', 'n_epoches': '1', 'seed': '15', 'visualize': 'False', 'reconstructed_path': 'None', 'reconstruct_start_index': '0', 'reconstruct_end_index': '100', 'save_path': '/rds/general/user/jy220/home/new_results/'}
[[ 2.50825269e-01 -1.67256990e-02 -1.22268730e-03]
 [ 2.38611959e-01 -5.50627777e-03 -5.06451913e-03]
 [ 2.39454874e-01 -1.59563257e-02  3.96778323e-03]
 ...
 [ 9.11361096e+00 -2.21908381e-02  2.22384902e-03]
 [ 9.11847310e+00 -2.84346931e-02 -2.34839590e-04]
 [ 9.11039661e+00 -2.51498573e-02 -8.70661044e-03]]
{'tetra': array([[      0,       1,       2,       3],
       [      4,       5,       6,       7],
       [      8,       9,      10,      11],
       ...,
       [1342744, 1342745, 1342746, 1342747],
       [1342748, 1342749, 1342750, 1342751],
       [1342752, 1342753, 1342754, 1342755]])}
vtu fields:  ['Component1::ComponentMassFractionPhase1', 'phase1::Velocity'] 
 structured  False 
 activation  Tanh() 
 self concat  2 
 sfc_nums  3 

dims_latent  128 
 components  4 
 nearest_neighbouring  True 

visualize  False 
 sample number  1706 

reading sfc nums......
[array([  41684,   41685,   41674, ..., 1235560, 1235564, 1235558]), array([954289, 954290, 954291, ..., 420685, 420687, 420688]), array([  40156,   40155,   40158, ..., 1123478, 1123475, 1123467])]
reading inverse sfc nums......
[array([   8146,    8145,    8144, ..., 1274444, 1274445, 1273166]), array([547280, 547283, 593037, ..., 933933, 933932, 864488]), array([ 66437,  66439, 122512, ..., 459416, 415130, 459418])]
tk:  tensor([0.7581, 0.0418, 0.1840, 0.1464], dtype=torch.float64) 

tb:  tensor([-0.3816, -0.3618, -0.0308,  0.1446], dtype=torch.float64) 

length of train set: 1505 

length of valid set: 100 

length of test set: 101 

torch device num: 4 

Let's use 4 GPUs!
... Training slugflow data completed, Run finished Tue 24 Aug 19:12:26 BST 2021 ...
{'vtu_dir': '/rds/general/user/jy220/home/slugflow/', 'data_dir': '/rds/general/user/jy220/home/tensors/', 'data_type': 'tensors', 'vtu_fields': 'Component1::ComponentMassFractionPhase1, phase1::Velocity', 'variational': 'False', 'optimizer': 'Adamax', 'mode': 'train', 'structured': 'False', 'changing_lr': 'False', 'self_concat': '2', 'dimension': '3', 'sfc_nums': '3', 'components': '4', 'nearest_neighbouring': 'True', 'dims_latent': '128', 'activation': 'Tanh', 'tk_file': '/rds/general/user/jy220/home/slugflow_tk.pt', 'tb_file': '/rds/general/user/jy220/home/slugflow_tb.pt', 'sfc_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/sfcs.pt', 'inv_sfc_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/inv_sfcs.pt', 'coords_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/coords.pt', 'cells_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/cells.npy', 'state_load': '/rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_128_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_1500_dict.pth', 'batch_size': '16', 'lr': '0.001', 'n_epoches': '1', 'seed': '15', 'visualize': 'False', 'reconstructed_path': 'None', 'reconstruct_start_index': '0', 'reconstruct_end_index': '100', 'save_path': '/rds/general/user/jy220/home/new_results/'}
[[ 2.50825269e-01 -1.67256990e-02 -1.22268730e-03]
 [ 2.38611959e-01 -5.50627777e-03 -5.06451913e-03]
 [ 2.39454874e-01 -1.59563257e-02  3.96778323e-03]
 ...
 [ 9.11361096e+00 -2.21908381e-02  2.22384902e-03]
 [ 9.11847310e+00 -2.84346931e-02 -2.34839590e-04]
 [ 9.11039661e+00 -2.51498573e-02 -8.70661044e-03]]
{'tetra': array([[      0,       1,       2,       3],
       [      4,       5,       6,       7],
       [      8,       9,      10,      11],
       ...,
       [1342744, 1342745, 1342746, 1342747],
       [1342748, 1342749, 1342750, 1342751],
       [1342752, 1342753, 1342754, 1342755]])}
vtu fields:  ['Component1::ComponentMassFractionPhase1', 'phase1::Velocity'] 
 structured  False 
 activation  Tanh() 
 self concat  2 
 sfc_nums  3 

dims_latent  128 
 components  4 
 nearest_neighbouring  True 

visualize  False 
 sample number  1706 

reading sfc nums......
[array([  41684,   41685,   41674, ..., 1235560, 1235564, 1235558]), array([954289, 954290, 954291, ..., 420685, 420687, 420688]), array([  40156,   40155,   40158, ..., 1123478, 1123475, 1123467])]
reading inverse sfc nums......
[array([   8146,    8145,    8144, ..., 1274444, 1274445, 1273166]), array([547280, 547283, 593037, ..., 933933, 933932, 864488]), array([ 66437,  66439, 122512, ..., 459416, 415130, 459418])]
tk:  tensor([0.7581, 0.0418, 0.1840, 0.1464], dtype=torch.float64) 

tb:  tensor([-0.3816, -0.3618, -0.0308,  0.1446], dtype=torch.float64) 

length of train set: 1505 

length of valid set: 100 

length of test set: 101 

torch device num: 4 

Let's use 4 GPUs!
epoch 1500 starting......
Epoch:  1500 | train loss: 1.547093e-03 | valid loss: 1.317284e-03 
      	| train loss (relative): 3.023053e-02 | valid loss (relative): 2.564736e-02 
Epoch 1500 use: 465.62 second.

test MSE Error: 1.403451e-03 | relative MSE Error: 2.755564e-02 
 Total time used for training: 0.14 hour.
MESLoss saved to  /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_MSELoss_Latent_128_nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_1501.txt
relative MSELoss saved to  /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_reMSELoss_Latent_128_nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_1501.txt
model saved to /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_128_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_1501.pth
model_dict saved to /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_128_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_1501_dict.pth
... Training slugflow data completed, Run finished Tue 24 Aug 23:28:15 BST 2021 ...
{'vtu_dir': '/rds/general/user/jy220/home/slugflow/', 'data_dir': '/rds/general/user/jy220/home/tensors/', 'data_type': 'tensors', 'vtu_fields': 'Component1::ComponentMassFractionPhase1, phase1::Velocity', 'variational': 'False', 'optimizer': 'Adamax', 'mode': 'train', 'structured': 'False', 'changing_lr': 'False', 'self_concat': '2', 'dimension': '3', 'sfc_nums': '3', 'components': '4', 'nearest_neighbouring': 'True', 'dims_latent': '128', 'activation': 'Tanh', 'tk_file': '/rds/general/user/jy220/home/slugflow_tk.pt', 'tb_file': '/rds/general/user/jy220/home/slugflow_tb.pt', 'sfc_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/sfcs.pt', 'inv_sfc_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/inv_sfcs.pt', 'coords_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/coords.pt', 'cells_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/cells.npy', 'state_load': '/rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_128_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_1501_dict.pth', 'batch_size': '16', 'lr': '0.001', 'n_epoches': '150', 'seed': '15', 'visualize': 'False', 'reconstructed_path': 'None', 'reconstruct_start_index': '0', 'reconstruct_end_index': '100', 'save_path': '/rds/general/user/jy220/home/new_results/'}
[[ 2.50825269e-01 -1.67256990e-02 -1.22268730e-03]
 [ 2.38611959e-01 -5.50627777e-03 -5.06451913e-03]
 [ 2.39454874e-01 -1.59563257e-02  3.96778323e-03]
 ...
 [ 9.11361096e+00 -2.21908381e-02  2.22384902e-03]
 [ 9.11847310e+00 -2.84346931e-02 -2.34839590e-04]
 [ 9.11039661e+00 -2.51498573e-02 -8.70661044e-03]]
{'tetra': array([[      0,       1,       2,       3],
       [      4,       5,       6,       7],
       [      8,       9,      10,      11],
       ...,
       [1342744, 1342745, 1342746, 1342747],
       [1342748, 1342749, 1342750, 1342751],
       [1342752, 1342753, 1342754, 1342755]])}
vtu fields:  ['Component1::ComponentMassFractionPhase1', 'phase1::Velocity'] 
 structured  False 
 activation  Tanh() 
 self concat  2 
 sfc_nums  3 

dims_latent  128 
 components  4 
 nearest_neighbouring  True 

visualize  False 
 sample number  1706 

reading sfc nums......
[array([  41684,   41685,   41674, ..., 1235560, 1235564, 1235558]), array([954289, 954290, 954291, ..., 420685, 420687, 420688]), array([  40156,   40155,   40158, ..., 1123478, 1123475, 1123467])]
reading inverse sfc nums......
[array([   8146,    8145,    8144, ..., 1274444, 1274445, 1273166]), array([547280, 547283, 593037, ..., 933933, 933932, 864488]), array([ 66437,  66439, 122512, ..., 459416, 415130, 459418])]
tk:  tensor([0.7581, 0.0418, 0.1840, 0.1464], dtype=torch.float64) 

tb:  tensor([-0.3816, -0.3618, -0.0308,  0.1446], dtype=torch.float64) 

length of train set: 1505 

length of valid set: 100 

length of test set: 101 

torch device num: 4 

Let's use 4 GPUs!
epoch 1501 starting......
Epoch:  1501 | train loss: 1.362869e-03 | valid loss: 1.303360e-03 
      	| train loss (relative): 2.666588e-02 | valid loss (relative): 2.535942e-02 
Epoch 1501 use: 519.04 second.

epoch 1502 starting......
Epoch:  1502 | train loss: 1.355619e-03 | valid loss: 1.302024e-03 
      	| train loss (relative): 2.651921e-02 | valid loss (relative): 2.533424e-02 
Epoch 1502 use: 424.08 second.

epoch 1503 starting......
Epoch:  1503 | train loss: 1.353943e-03 | valid loss: 1.303888e-03 
      	| train loss (relative): 2.648167e-02 | valid loss (relative): 2.540332e-02 
Epoch 1503 use: 408.01 second.

epoch 1504 starting......
Epoch:  1504 | train loss: 1.352775e-03 | valid loss: 1.304015e-03 
      	| train loss (relative): 2.646017e-02 | valid loss (relative): 2.533942e-02 
Epoch 1504 use: 421.23 second.

epoch 1505 starting......
Epoch:  1505 | train loss: 1.353737e-03 | valid loss: 1.306822e-03 
      	| train loss (relative): 2.647905e-02 | valid loss (relative): 2.539618e-02 
Epoch 1505 use: 412.48 second.

epoch 1506 starting......
Epoch:  1506 | train loss: 1.355055e-03 | valid loss: 1.308705e-03 
      	| train loss (relative): 2.649940e-02 | valid loss (relative): 2.547238e-02 
Epoch 1506 use: 406.78 second.

epoch 1507 starting......
Epoch:  1507 | train loss: 1.354179e-03 | valid loss: 1.309333e-03 
      	| train loss (relative): 2.648937e-02 | valid loss (relative): 2.547913e-02 
Epoch 1507 use: 404.45 second.

epoch 1508 starting......
Epoch:  1508 | train loss: 1.352921e-03 | valid loss: 1.310176e-03 
      	| train loss (relative): 2.646021e-02 | valid loss (relative): 2.548722e-02 
Epoch 1508 use: 395.40 second.

epoch 1509 starting......
Epoch:  1509 | train loss: 1.357156e-03 | valid loss: 1.312924e-03 
      	| train loss (relative): 2.654195e-02 | valid loss (relative): 2.552745e-02 
Epoch 1509 use: 420.59 second.

epoch 1510 starting......
Epoch:  1510 | train loss: 1.358258e-03 | valid loss: 1.318509e-03 
      	| train loss (relative): 2.656481e-02 | valid loss (relative): 2.567814e-02 
Epoch 1510 use: 399.43 second.

epoch 1511 starting......
Epoch:  1511 | train loss: 1.360802e-03 | valid loss: 1.320162e-03 
      	| train loss (relative): 2.661511e-02 | valid loss (relative): 2.566886e-02 
Epoch 1511 use: 413.38 second.

epoch 1512 starting......
Epoch:  1512 | train loss: 1.360832e-03 | valid loss: 1.321385e-03 
      	| train loss (relative): 2.661207e-02 | valid loss (relative): 2.570468e-02 
Epoch 1512 use: 420.33 second.

epoch 1513 starting......
Epoch:  1513 | train loss: 1.359858e-03 | valid loss: 1.324112e-03 
      	| train loss (relative): 2.659600e-02 | valid loss (relative): 2.573219e-02 
Epoch 1513 use: 390.82 second.

epoch 1514 starting......
Epoch:  1514 | train loss: 1.363437e-03 | valid loss: 1.325428e-03 
      	| train loss (relative): 2.666491e-02 | valid loss (relative): 2.581852e-02 
Epoch 1514 use: 400.50 second.

epoch 1515 starting......
Epoch:  1515 | train loss: 1.365716e-03 | valid loss: 1.324648e-03 
      	| train loss (relative): 2.671205e-02 | valid loss (relative): 2.575616e-02 
Epoch 1515 use: 390.10 second.

epoch 1516 starting......
Epoch:  1516 | train loss: 1.367558e-03 | valid loss: 1.326485e-03 
      	| train loss (relative): 2.674875e-02 | valid loss (relative): 2.582743e-02 
Epoch 1516 use: 411.07 second.

epoch 1517 starting......
Epoch:  1517 | train loss: 1.367228e-03 | valid loss: 1.328403e-03 
      	| train loss (relative): 2.674310e-02 | valid loss (relative): 2.586612e-02 
Epoch 1517 use: 385.22 second.

epoch 1518 starting......
Epoch:  1518 | train loss: 1.367052e-03 | valid loss: 1.325752e-03 
      	| train loss (relative): 2.673894e-02 | valid loss (relative): 2.580754e-02 
Epoch 1518 use: 390.09 second.

epoch 1519 starting......
Epoch:  1519 | train loss: 1.363266e-03 | valid loss: 1.324857e-03 
      	| train loss (relative): 2.666138e-02 | valid loss (relative): 2.577795e-02 
Epoch 1519 use: 424.34 second.

epoch 1520 starting......
Epoch:  1520 | train loss: 1.365497e-03 | valid loss: 1.330929e-03 
      	| train loss (relative): 2.670698e-02 | valid loss (relative): 2.588479e-02 
Epoch 1520 use: 407.25 second.

epoch 1521 starting......
Epoch:  1521 | train loss: 1.366243e-03 | valid loss: 1.331763e-03 
      	| train loss (relative): 2.671976e-02 | valid loss (relative): 2.600085e-02 
Epoch 1521 use: 398.28 second.

epoch 1522 starting......
Epoch:  1522 | train loss: 1.365013e-03 | valid loss: 1.327700e-03 
      	| train loss (relative): 2.670095e-02 | valid loss (relative): 2.586180e-02 
Epoch 1522 use: 421.38 second.

epoch 1523 starting......
Epoch:  1523 | train loss: 1.365708e-03 | valid loss: 1.331311e-03 
      	| train loss (relative): 2.671163e-02 | valid loss (relative): 2.596011e-02 
Epoch 1523 use: 399.13 second.

epoch 1524 starting......
Epoch:  1524 | train loss: 1.366018e-03 | valid loss: 1.334372e-03 
      	| train loss (relative): 2.672179e-02 | valid loss (relative): 2.599093e-02 
Epoch 1524 use: 405.40 second.

epoch 1525 starting......
Epoch:  1525 | train loss: 1.368195e-03 | valid loss: 1.330047e-03 
      	| train loss (relative): 2.676135e-02 | valid loss (relative): 2.585489e-02 
Epoch 1525 use: 421.72 second.

epoch 1526 starting......
Epoch:  1526 | train loss: 1.363433e-03 | valid loss: 1.333427e-03 
      	| train loss (relative): 2.666622e-02 | valid loss (relative): 2.605670e-02 
Epoch 1526 use: 389.54 second.

epoch 1527 starting......
Epoch:  1527 | train loss: 1.365387e-03 | valid loss: 1.328312e-03 
      	| train loss (relative): 2.670762e-02 | valid loss (relative): 2.595154e-02 
Epoch 1527 use: 409.35 second.

epoch 1528 starting......
Epoch:  1528 | train loss: 1.364441e-03 | valid loss: 1.334236e-03 
      	| train loss (relative): 2.668455e-02 | valid loss (relative): 2.621204e-02 
Epoch 1528 use: 409.10 second.

epoch 1529 starting......
Epoch:  1529 | train loss: 1.363750e-03 | valid loss: 1.330876e-03 
      	| train loss (relative): 2.667370e-02 | valid loss (relative): 2.597115e-02 
Epoch 1529 use: 406.08 second.

epoch 1530 starting......
Epoch:  1530 | train loss: 1.362229e-03 | valid loss: 1.326194e-03 
      	| train loss (relative): 2.663986e-02 | valid loss (relative): 2.589635e-02 
Epoch 1530 use: 409.52 second.

epoch 1531 starting......
Epoch:  1531 | train loss: 1.362508e-03 | valid loss: 1.332014e-03 
      	| train loss (relative): 2.664982e-02 | valid loss (relative): 2.594211e-02 
Epoch 1531 use: 407.98 second.

epoch 1532 starting......
Epoch:  1532 | train loss: 1.365668e-03 | valid loss: 1.337557e-03 
      	| train loss (relative): 2.670956e-02 | valid loss (relative): 2.601300e-02 
Epoch 1532 use: 397.96 second.

epoch 1533 starting......
Epoch:  1533 | train loss: 1.362721e-03 | valid loss: 1.325023e-03 
      	| train loss (relative): 2.664928e-02 | valid loss (relative): 2.576511e-02 
Epoch 1533 use: 423.38 second.

epoch 1534 starting......
Epoch:  1534 | train loss: 1.358701e-03 | valid loss: 1.328328e-03 
      	| train loss (relative): 2.657134e-02 | valid loss (relative): 2.593500e-02 
Epoch 1534 use: 422.04 second.

epoch 1535 starting......
Epoch:  1535 | train loss: 1.357919e-03 | valid loss: 1.328033e-03 
      	| train loss (relative): 2.655739e-02 | valid loss (relative): 2.577949e-02 
Epoch 1535 use: 399.89 second.

epoch 1536 starting......
Epoch:  1536 | train loss: 1.357706e-03 | valid loss: 1.338323e-03 
      	| train loss (relative): 2.654997e-02 | valid loss (relative): 2.605163e-02 
Epoch 1536 use: 417.91 second.

epoch 1537 starting......
Epoch:  1537 | train loss: 1.363426e-03 | valid loss: 1.330619e-03 
      	| train loss (relative): 2.666315e-02 | valid loss (relative): 2.586083e-02 
Epoch 1537 use: 391.40 second.

epoch 1538 starting......
Epoch:  1538 | train loss: 1.360617e-03 | valid loss: 1.331227e-03 
      	| train loss (relative): 2.660960e-02 | valid loss (relative): 2.591745e-02 
Epoch 1538 use: 410.91 second.

epoch 1539 starting......
Epoch:  1539 | train loss: 1.360476e-03 | valid loss: 1.333393e-03 
      	| train loss (relative): 2.660513e-02 | valid loss (relative): 2.586363e-02 
Epoch 1539 use: 411.95 second.

epoch 1540 starting......
Epoch:  1540 | train loss: 1.363059e-03 | valid loss: 1.328874e-03 
      	| train loss (relative): 2.665845e-02 | valid loss (relative): 2.580112e-02 
Epoch 1540 use: 393.47 second.

epoch 1541 starting......
Epoch:  1541 | train loss: 1.359005e-03 | valid loss: 1.331235e-03 
      	| train loss (relative): 2.657639e-02 | valid loss (relative): 2.587831e-02 
Epoch 1541 use: 405.70 second.

epoch 1542 starting......
Epoch:  1542 | train loss: 1.361112e-03 | valid loss: 1.331446e-03 
      	| train loss (relative): 2.661573e-02 | valid loss (relative): 2.606697e-02 
Epoch 1542 use: 406.16 second.

epoch 1543 starting......
Epoch:  1543 | train loss: 1.360186e-03 | valid loss: 1.333826e-03 
      	| train loss (relative): 2.659966e-02 | valid loss (relative): 2.599357e-02 
Epoch 1543 use: 397.15 second.

epoch 1544 starting......
Epoch:  1544 | train loss: 1.362285e-03 | valid loss: 1.329901e-03 
      	| train loss (relative): 2.664436e-02 | valid loss (relative): 2.582444e-02 
Epoch 1544 use: 406.61 second.

epoch 1545 starting......
Epoch:  1545 | train loss: 1.358731e-03 | valid loss: 1.337126e-03 
      	| train loss (relative): 2.657099e-02 | valid loss (relative): 2.605730e-02 
Epoch 1545 use: 394.08 second.

epoch 1546 starting......
Epoch:  1546 | train loss: 1.357541e-03 | valid loss: 1.336410e-03 
      	| train loss (relative): 2.654624e-02 | valid loss (relative): 2.614602e-02 
Epoch 1546 use: 414.23 second.

epoch 1547 starting......
Epoch:  1547 | train loss: 1.358300e-03 | valid loss: 1.328755e-03 
      	| train loss (relative): 2.656278e-02 | valid loss (relative): 2.579190e-02 
Epoch 1547 use: 386.70 second.

epoch 1548 starting......
Epoch:  1548 | train loss: 1.358092e-03 | valid loss: 1.328704e-03 
      	| train loss (relative): 2.656114e-02 | valid loss (relative): 2.581211e-02 
Epoch 1548 use: 405.80 second.

epoch 1549 starting......
Epoch:  1549 | train loss: 1.358100e-03 | valid loss: 1.335115e-03 
      	| train loss (relative): 2.655907e-02 | valid loss (relative): 2.591671e-02 
Epoch 1549 use: 403.13 second.

epoch 1550 starting......
Epoch:  1550 | train loss: 1.361550e-03 | valid loss: 1.332430e-03 
      	| train loss (relative): 2.662660e-02 | valid loss (relative): 2.588912e-02 
Epoch 1550 use: 397.54 second.

epoch 1551 starting......
Epoch:  1551 | train loss: 1.358603e-03 | valid loss: 1.333429e-03 
      	| train loss (relative): 2.656761e-02 | valid loss (relative): 2.596758e-02 
Epoch 1551 use: 392.44 second.

epoch 1552 starting......
Epoch:  1552 | train loss: 1.358972e-03 | valid loss: 1.334880e-03 
      	| train loss (relative): 2.657484e-02 | valid loss (relative): 2.591119e-02 
Epoch 1552 use: 402.83 second.

epoch 1553 starting......
Epoch:  1553 | train loss: 1.359148e-03 | valid loss: 1.328299e-03 
      	| train loss (relative): 2.657981e-02 | valid loss (relative): 2.584114e-02 
Epoch 1553 use: 397.66 second.

epoch 1554 starting......
Epoch:  1554 | train loss: 1.357751e-03 | valid loss: 1.328813e-03 
      	| train loss (relative): 2.655298e-02 | valid loss (relative): 2.584998e-02 
Epoch 1554 use: 389.96 second.

epoch 1555 starting......
Epoch:  1555 | train loss: 1.357863e-03 | valid loss: 1.327768e-03 
      	| train loss (relative): 2.655340e-02 | valid loss (relative): 2.578486e-02 
Epoch 1555 use: 392.84 second.

epoch 1556 starting......
Epoch:  1556 | train loss: 1.357257e-03 | valid loss: 1.330228e-03 
      	| train loss (relative): 2.654421e-02 | valid loss (relative): 2.591225e-02 
Epoch 1556 use: 395.18 second.

epoch 1557 starting......
Epoch:  1557 | train loss: 1.357337e-03 | valid loss: 1.329987e-03 
      	| train loss (relative): 2.654460e-02 | valid loss (relative): 2.589495e-02 
Epoch 1557 use: 395.73 second.

epoch 1558 starting......
Epoch:  1558 | train loss: 1.355399e-03 | valid loss: 1.326247e-03 
      	| train loss (relative): 2.650691e-02 | valid loss (relative): 2.581804e-02 
Epoch 1558 use: 396.68 second.

epoch 1559 starting......
Epoch:  1559 | train loss: 1.356854e-03 | valid loss: 1.334130e-03 
      	| train loss (relative): 2.653416e-02 | valid loss (relative): 2.593020e-02 
Epoch 1559 use: 402.67 second.

epoch 1560 starting......
Epoch:  1560 | train loss: 1.359340e-03 | valid loss: 1.328803e-03 
      	| train loss (relative): 2.658101e-02 | valid loss (relative): 2.589952e-02 
Epoch 1560 use: 392.64 second.

epoch 1561 starting......
Epoch:  1561 | train loss: 1.357585e-03 | valid loss: 1.331284e-03 
      	| train loss (relative): 2.655042e-02 | valid loss (relative): 2.592579e-02 
Epoch 1561 use: 393.73 second.

epoch 1562 starting......
Epoch:  1562 | train loss: 1.359118e-03 | valid loss: 1.328968e-03 
      	| train loss (relative): 2.658235e-02 | valid loss (relative): 2.588968e-02 
Epoch 1562 use: 398.94 second.

epoch 1563 starting......
Epoch:  1563 | train loss: 1.357965e-03 | valid loss: 1.340781e-03 
      	| train loss (relative): 2.655674e-02 | valid loss (relative): 2.614856e-02 
Epoch 1563 use: 395.74 second.

epoch 1564 starting......
Epoch:  1564 | train loss: 1.359671e-03 | valid loss: 1.331066e-03 
      	| train loss (relative): 2.659029e-02 | valid loss (relative): 2.580163e-02 
Epoch 1564 use: 401.94 second.

epoch 1565 starting......
Epoch:  1565 | train loss: 1.360392e-03 | valid loss: 1.343171e-03 
      	| train loss (relative): 2.660666e-02 | valid loss (relative): 2.606428e-02 
Epoch 1565 use: 391.78 second.

epoch 1566 starting......
Epoch:  1566 | train loss: 1.364711e-03 | valid loss: 1.331384e-03 
      	| train loss (relative): 2.668624e-02 | valid loss (relative): 2.597533e-02 
Epoch 1566 use: 401.55 second.

epoch 1567 starting......
Epoch:  1567 | train loss: 1.356265e-03 | valid loss: 1.329215e-03 
      	| train loss (relative): 2.652438e-02 | valid loss (relative): 2.592240e-02 
Epoch 1567 use: 405.49 second.

epoch 1568 starting......
Epoch:  1568 | train loss: 1.354491e-03 | valid loss: 1.326034e-03 
      	| train loss (relative): 2.648938e-02 | valid loss (relative): 2.584193e-02 
Epoch 1568 use: 396.24 second.

epoch 1569 starting......
Epoch:  1569 | train loss: 1.351126e-03 | valid loss: 1.329969e-03 
      	| train loss (relative): 2.642078e-02 | valid loss (relative): 2.595814e-02 
Epoch 1569 use: 410.91 second.

epoch 1570 starting......
Epoch:  1570 | train loss: 1.354087e-03 | valid loss: 1.329808e-03 
      	| train loss (relative): 2.648232e-02 | valid loss (relative): 2.591835e-02 
Epoch 1570 use: 403.54 second.

epoch 1571 starting......
Epoch:  1571 | train loss: 1.354083e-03 | valid loss: 1.336541e-03 
      	| train loss (relative): 2.647741e-02 | valid loss (relative): 2.597469e-02 
Epoch 1571 use: 404.74 second.

epoch 1572 starting......
Epoch:  1572 | train loss: 1.356006e-03 | valid loss: 1.330032e-03 
      	| train loss (relative): 2.651651e-02 | valid loss (relative): 2.590324e-02 
Epoch 1572 use: 403.16 second.

epoch 1573 starting......
Epoch:  1573 | train loss: 1.354132e-03 | valid loss: 1.328348e-03 
      	| train loss (relative): 2.647982e-02 | valid loss (relative): 2.585364e-02 
Epoch 1573 use: 392.26 second.

epoch 1574 starting......
Epoch:  1574 | train loss: 1.354633e-03 | valid loss: 1.331805e-03 
      	| train loss (relative): 2.649270e-02 | valid loss (relative): 2.582307e-02 
Epoch 1574 use: 404.93 second.

epoch 1575 starting......
Epoch:  1575 | train loss: 1.354960e-03 | valid loss: 1.331022e-03 
      	| train loss (relative): 2.649220e-02 | valid loss (relative): 2.606170e-02 
Epoch 1575 use: 397.61 second.

epoch 1576 starting......
Epoch:  1576 | train loss: 1.354744e-03 | valid loss: 1.325706e-03 
      	| train loss (relative): 2.649372e-02 | valid loss (relative): 2.578901e-02 
Epoch 1576 use: 410.52 second.

epoch 1577 starting......
Epoch:  1577 | train loss: 1.353506e-03 | valid loss: 1.327488e-03 
      	| train loss (relative): 2.646638e-02 | valid loss (relative): 2.586874e-02 
Epoch 1577 use: 396.64 second.

epoch 1578 starting......
Epoch:  1578 | train loss: 1.353042e-03 | valid loss: 1.333685e-03 
      	| train loss (relative): 2.645734e-02 | valid loss (relative): 2.606125e-02 
Epoch 1578 use: 396.34 second.

epoch 1579 starting......
Epoch:  1579 | train loss: 1.354935e-03 | valid loss: 1.333075e-03 
      	| train loss (relative): 2.649619e-02 | valid loss (relative): 2.598276e-02 
Epoch 1579 use: 416.59 second.

epoch 1580 starting......
Epoch:  1580 | train loss: 1.355761e-03 | valid loss: 1.328096e-03 
      	| train loss (relative): 2.651407e-02 | valid loss (relative): 2.584621e-02 
Epoch 1580 use: 405.39 second.

epoch 1581 starting......
Epoch:  1581 | train loss: 1.352831e-03 | valid loss: 1.327327e-03 
      	| train loss (relative): 2.645101e-02 | valid loss (relative): 2.583827e-02 
Epoch 1581 use: 395.14 second.

epoch 1582 starting......
Epoch:  1582 | train loss: 1.351540e-03 | valid loss: 1.329199e-03 
      	| train loss (relative): 2.642651e-02 | valid loss (relative): 2.597843e-02 
Epoch 1582 use: 408.08 second.

epoch 1583 starting......
Epoch:  1583 | train loss: 1.356435e-03 | valid loss: 1.331561e-03 
      	| train loss (relative): 2.652968e-02 | valid loss (relative): 2.590628e-02 
Epoch 1583 use: 394.72 second.

epoch 1584 starting......
Epoch:  1584 | train loss: 1.354606e-03 | valid loss: 1.353434e-03 
      	| train loss (relative): 2.648805e-02 | valid loss (relative): 2.623426e-02 
Epoch 1584 use: 414.52 second.

epoch 1585 starting......
Epoch:  1585 | train loss: 1.356827e-03 | valid loss: 1.328170e-03 
      	| train loss (relative): 2.653695e-02 | valid loss (relative): 2.587142e-02 
Epoch 1585 use: 415.84 second.

epoch 1586 starting......
Epoch:  1586 | train loss: 1.353649e-03 | valid loss: 1.327893e-03 
      	| train loss (relative): 2.646925e-02 | valid loss (relative): 2.585954e-02 
Epoch 1586 use: 408.34 second.

epoch 1587 starting......
Epoch:  1587 | train loss: 1.351449e-03 | valid loss: 1.329010e-03 
      	| train loss (relative): 2.642801e-02 | valid loss (relative): 2.596555e-02 
Epoch 1587 use: 403.22 second.

epoch 1588 starting......
Epoch:  1588 | train loss: 1.353732e-03 | valid loss: 1.341912e-03 
      	| train loss (relative): 2.647460e-02 | valid loss (relative): 2.618456e-02 
Epoch 1588 use: 395.52 second.

epoch 1589 starting......
Epoch:  1589 | train loss: 1.356619e-03 | valid loss: 1.330646e-03 
      	| train loss (relative): 2.652973e-02 | valid loss (relative): 2.584276e-02 
Epoch 1589 use: 405.65 second.

epoch 1590 starting......
Epoch:  1590 | train loss: 1.356943e-03 | valid loss: 1.327006e-03 
      	| train loss (relative): 2.653512e-02 | valid loss (relative): 2.578511e-02 
Epoch 1590 use: 393.38 second.

epoch 1591 starting......
Epoch:  1591 | train loss: 1.350287e-03 | valid loss: 1.326527e-03 
      	| train loss (relative): 2.640496e-02 | valid loss (relative): 2.587556e-02 
Epoch 1591 use: 404.76 second.

epoch 1592 starting......
Epoch:  1592 | train loss: 1.350421e-03 | valid loss: 1.324450e-03 
      	| train loss (relative): 2.640766e-02 | valid loss (relative): 2.572305e-02 
Epoch 1592 use: 390.44 second.

epoch 1593 starting......
Epoch:  1593 | train loss: 1.347820e-03 | valid loss: 1.329131e-03 
      	| train loss (relative): 2.635436e-02 | valid loss (relative): 2.588375e-02 
Epoch 1593 use: 405.33 second.

epoch 1594 starting......
Epoch:  1594 | train loss: 1.346882e-03 | valid loss: 1.323410e-03 
      	| train loss (relative): 2.633498e-02 | valid loss (relative): 2.577426e-02 
Epoch 1594 use: 384.37 second.

epoch 1595 starting......
Epoch:  1595 | train loss: 1.347536e-03 | valid loss: 1.324801e-03 
      	| train loss (relative): 2.634709e-02 | valid loss (relative): 2.583441e-02 
Epoch 1595 use: 393.56 second.

epoch 1596 starting......
Epoch:  1596 | train loss: 1.347640e-03 | valid loss: 1.324888e-03 
      	| train loss (relative): 2.635107e-02 | valid loss (relative): 2.573397e-02 
Epoch 1596 use: 387.08 second.

epoch 1597 starting......
Epoch:  1597 | train loss: 1.348533e-03 | valid loss: 1.326414e-03 
      	| train loss (relative): 2.636778e-02 | valid loss (relative): 2.580081e-02 
Epoch 1597 use: 398.53 second.

epoch 1598 starting......
Epoch:  1598 | train loss: 1.347708e-03 | valid loss: 1.333476e-03 
      	| train loss (relative): 2.635053e-02 | valid loss (relative): 2.588818e-02 
Epoch 1598 use: 398.97 second.

epoch 1599 starting......
Epoch:  1599 | train loss: 1.354816e-03 | valid loss: 1.334890e-03 
      	| train loss (relative): 2.649160e-02 | valid loss (relative): 2.596215e-02 
Epoch 1599 use: 399.22 second.

epoch 1600 starting......
Epoch:  1600 | train loss: 1.349693e-03 | valid loss: 1.322474e-03 
      	| train loss (relative): 2.638975e-02 | valid loss (relative): 2.585427e-02 
Epoch 1600 use: 399.63 second.

epoch 1601 starting......
Epoch:  1601 | train loss: 1.347318e-03 | valid loss: 1.325957e-03 
      	| train loss (relative): 2.634512e-02 | valid loss (relative): 2.586194e-02 
Epoch 1601 use: 408.28 second.

epoch 1602 starting......
Epoch:  1602 | train loss: 1.349791e-03 | valid loss: 1.324852e-03 
      	| train loss (relative): 2.639184e-02 | valid loss (relative): 2.582123e-02 
Epoch 1602 use: 388.97 second.

epoch 1603 starting......
Epoch:  1603 | train loss: 1.352378e-03 | valid loss: 1.329455e-03 
      	| train loss (relative): 2.644499e-02 | valid loss (relative): 2.592935e-02 
Epoch 1603 use: 396.53 second.

epoch 1604 starting......
Epoch:  1604 | train loss: 1.349171e-03 | valid loss: 1.326274e-03 
      	| train loss (relative): 2.638074e-02 | valid loss (relative): 2.582560e-02 
Epoch 1604 use: 393.24 second.

epoch 1605 starting......
Epoch:  1605 | train loss: 1.351396e-03 | valid loss: 1.332620e-03 
      	| train loss (relative): 2.642277e-02 | valid loss (relative): 2.581249e-02 
Epoch 1605 use: 404.36 second.

epoch 1606 starting......
Epoch:  1606 | train loss: 1.353867e-03 | valid loss: 1.330065e-03 
      	| train loss (relative): 2.647226e-02 | valid loss (relative): 2.587387e-02 
Epoch 1606 use: 403.48 second.

epoch 1607 starting......
Epoch:  1607 | train loss: 1.353949e-03 | valid loss: 1.328531e-03 
      	| train loss (relative): 2.647549e-02 | valid loss (relative): 2.585208e-02 
Epoch 1607 use: 396.79 second.

epoch 1608 starting......
Epoch:  1608 | train loss: 1.350945e-03 | valid loss: 1.329944e-03 
      	| train loss (relative): 2.641743e-02 | valid loss (relative): 2.577207e-02 
Epoch 1608 use: 396.83 second.

epoch 1609 starting......
Epoch:  1609 | train loss: 1.351627e-03 | valid loss: 1.330441e-03 
      	| train loss (relative): 2.642656e-02 | valid loss (relative): 2.590844e-02 
Epoch 1609 use: 400.44 second.

epoch 1610 starting......
Epoch:  1610 | train loss: 1.348687e-03 | valid loss: 1.327132e-03 
      	| train loss (relative): 2.637045e-02 | valid loss (relative): 2.589493e-02 
Epoch 1610 use: 409.42 second.

epoch 1611 starting......
Epoch:  1611 | train loss: 1.351097e-03 | valid loss: 1.329067e-03 
      	| train loss (relative): 2.642001e-02 | valid loss (relative): 2.583119e-02 
Epoch 1611 use: 414.05 second.

epoch 1612 starting......
Epoch:  1612 | train loss: 1.347319e-03 | valid loss: 1.327226e-03 
      	| train loss (relative): 2.634097e-02 | valid loss (relative): 2.586205e-02 
Epoch 1612 use: 404.51 second.

epoch 1613 starting......
Epoch:  1613 | train loss: 1.348540e-03 | valid loss: 1.333058e-03 
      	| train loss (relative): 2.636496e-02 | valid loss (relative): 2.588799e-02 
Epoch 1613 use: 409.43 second.

epoch 1614 starting......
Epoch:  1614 | train loss: 1.354020e-03 | valid loss: 1.328527e-03 
      	| train loss (relative): 2.647631e-02 | valid loss (relative): 2.582402e-02 
Epoch 1614 use: 399.77 second.

epoch 1615 starting......
Epoch:  1615 | train loss: 1.349935e-03 | valid loss: 1.323946e-03 
      	| train loss (relative): 2.639495e-02 | valid loss (relative): 2.575132e-02 
Epoch 1615 use: 412.09 second.

epoch 1616 starting......
Epoch:  1616 | train loss: 1.347304e-03 | valid loss: 1.327179e-03 
      	| train loss (relative): 2.634500e-02 | valid loss (relative): 2.584635e-02 
Epoch 1616 use: 385.59 second.

epoch 1617 starting......
Epoch:  1617 | train loss: 1.347074e-03 | valid loss: 1.325639e-03 
      	| train loss (relative): 2.633670e-02 | valid loss (relative): 2.582523e-02 
Epoch 1617 use: 417.95 second.

epoch 1618 starting......
Epoch:  1618 | train loss: 1.346048e-03 | valid loss: 1.333914e-03 
      	| train loss (relative): 2.631991e-02 | valid loss (relative): 2.595837e-02 
Epoch 1618 use: 395.43 second.

epoch 1619 starting......
Epoch:  1619 | train loss: 1.350139e-03 | valid loss: 1.331479e-03 
      	| train loss (relative): 2.639554e-02 | valid loss (relative): 2.584705e-02 
Epoch 1619 use: 398.86 second.

epoch 1620 starting......
Epoch:  1620 | train loss: 1.354624e-03 | valid loss: 1.331107e-03 
      	| train loss (relative): 2.648908e-02 | valid loss (relative): 2.591353e-02 
Epoch 1620 use: 392.63 second.

epoch 1621 starting......
Epoch:  1621 | train loss: 1.349468e-03 | valid loss: 1.329592e-03 
      	| train loss (relative): 2.638968e-02 | valid loss (relative): 2.582424e-02 
Epoch 1621 use: 394.35 second.

epoch 1622 starting......
Epoch:  1622 | train loss: 1.348652e-03 | valid loss: 1.328935e-03 
      	| train loss (relative): 2.636858e-02 | valid loss (relative): 2.585823e-02 
Epoch 1622 use: 404.47 second.

epoch 1623 starting......
Epoch:  1623 | train loss: 1.349308e-03 | valid loss: 1.329361e-03 
      	| train loss (relative): 2.638030e-02 | valid loss (relative): 2.586262e-02 
Epoch 1623 use: 395.55 second.

epoch 1624 starting......
Epoch:  1624 | train loss: 1.349743e-03 | valid loss: 1.331872e-03 
      	| train loss (relative): 2.639148e-02 | valid loss (relative): 2.597805e-02 
Epoch 1624 use: 391.43 second.

epoch 1625 starting......
Epoch:  1625 | train loss: 1.348678e-03 | valid loss: 1.327242e-03 
      	| train loss (relative): 2.637137e-02 | valid loss (relative): 2.577396e-02 
Epoch 1625 use: 404.97 second.

epoch 1626 starting......
Epoch:  1626 | train loss: 1.347234e-03 | valid loss: 1.323094e-03 
      	| train loss (relative): 2.634173e-02 | valid loss (relative): 2.582866e-02 
Epoch 1626 use: 422.27 second.

epoch 1627 starting......
Epoch:  1627 | train loss: 1.344609e-03 | valid loss: 1.326459e-03 
      	| train loss (relative): 2.628889e-02 | valid loss (relative): 2.585403e-02 
Epoch 1627 use: 406.98 second.

epoch 1628 starting......
Epoch:  1628 | train loss: 1.347546e-03 | valid loss: 1.330229e-03 
      	| train loss (relative): 2.634990e-02 | valid loss (relative): 2.580101e-02 
Epoch 1628 use: 417.17 second.

epoch 1629 starting......
Epoch:  1629 | train loss: 1.349569e-03 | valid loss: 1.325901e-03 
      	| train loss (relative): 2.638694e-02 | valid loss (relative): 2.581468e-02 
Epoch 1629 use: 413.78 second.

epoch 1630 starting......
Epoch:  1630 | train loss: 1.347571e-03 | valid loss: 1.330663e-03 
      	| train loss (relative): 2.634873e-02 | valid loss (relative): 2.596934e-02 
Epoch 1630 use: 399.07 second.

epoch 1631 starting......
Epoch:  1631 | train loss: 1.350407e-03 | valid loss: 1.337409e-03 
      	| train loss (relative): 2.640239e-02 | valid loss (relative): 2.597505e-02 
Epoch 1631 use: 417.49 second.

epoch 1632 starting......
Epoch:  1632 | train loss: 1.350652e-03 | valid loss: 1.330015e-03 
      	| train loss (relative): 2.640773e-02 | valid loss (relative): 2.582291e-02 
Epoch 1632 use: 401.38 second.

epoch 1633 starting......
Epoch:  1633 | train loss: 1.349011e-03 | valid loss: 1.327136e-03 
      	| train loss (relative): 2.637227e-02 | valid loss (relative): 2.582462e-02 
Epoch 1633 use: 408.65 second.

epoch 1634 starting......
Epoch:  1634 | train loss: 1.347268e-03 | valid loss: 1.334653e-03 
      	| train loss (relative): 2.634094e-02 | valid loss (relative): 2.597313e-02 
Epoch 1634 use: 407.13 second.

epoch 1635 starting......
Epoch:  1635 | train loss: 1.345829e-03 | valid loss: 1.325915e-03 
      	| train loss (relative): 2.631957e-02 | valid loss (relative): 2.583649e-02 
Epoch 1635 use: 397.08 second.

epoch 1636 starting......
Epoch:  1636 | train loss: 1.347723e-03 | valid loss: 1.327284e-03 
      	| train loss (relative): 2.635341e-02 | valid loss (relative): 2.591881e-02 
Epoch 1636 use: 401.74 second.

epoch 1637 starting......
Epoch:  1637 | train loss: 1.344959e-03 | valid loss: 1.325959e-03 
      	| train loss (relative): 2.629535e-02 | valid loss (relative): 2.575237e-02 
Epoch 1637 use: 413.30 second.

epoch 1638 starting......
Epoch:  1638 | train loss: 1.345874e-03 | valid loss: 1.327309e-03 
      	| train loss (relative): 2.631040e-02 | valid loss (relative): 2.584773e-02 
Epoch 1638 use: 402.65 second.

epoch 1639 starting......
Epoch:  1639 | train loss: 1.347873e-03 | valid loss: 1.326971e-03 
      	| train loss (relative): 2.635302e-02 | valid loss (relative): 2.578341e-02 
Epoch 1639 use: 401.52 second.

epoch 1640 starting......
Epoch:  1640 | train loss: 1.346389e-03 | valid loss: 1.326813e-03 
      	| train loss (relative): 2.632607e-02 | valid loss (relative): 2.581185e-02 
Epoch 1640 use: 398.29 second.

epoch 1641 starting......
Epoch:  1641 | train loss: 1.345013e-03 | valid loss: 1.325502e-03 
      	| train loss (relative): 2.629287e-02 | valid loss (relative): 2.595676e-02 
Epoch 1641 use: 398.60 second.

epoch 1642 starting......
Epoch:  1642 | train loss: 1.345786e-03 | valid loss: 1.329327e-03 
      	| train loss (relative): 2.631195e-02 | valid loss (relative): 2.588062e-02 
Epoch 1642 use: 406.64 second.

epoch 1643 starting......
Epoch:  1643 | train loss: 1.344027e-03 | valid loss: 1.321195e-03 
      	| train loss (relative): 2.628166e-02 | valid loss (relative): 2.563600e-02 
Epoch 1643 use: 407.02 second.

epoch 1644 starting......
Epoch:  1644 | train loss: 1.341096e-03 | valid loss: 1.321358e-03 
      	| train loss (relative): 2.622097e-02 | valid loss (relative): 2.577767e-02 
Epoch 1644 use: 407.90 second.

epoch 1645 starting......
Epoch:  1645 | train loss: 1.342013e-03 | valid loss: 1.320641e-03 
      	| train loss (relative): 2.623934e-02 | valid loss (relative): 2.565118e-02 
Epoch 1645 use: 410.29 second.

epoch 1646 starting......
Epoch:  1646 | train loss: 1.342791e-03 | valid loss: 1.331602e-03 
      	| train loss (relative): 2.624981e-02 | valid loss (relative): 2.587978e-02 
Epoch 1646 use: 410.81 second.

epoch 1647 starting......
Epoch:  1647 | train loss: 1.345999e-03 | valid loss: 1.327253e-03 
      	| train loss (relative): 2.631811e-02 | valid loss (relative): 2.567018e-02 
Epoch 1647 use: 414.65 second.

epoch 1648 starting......
Epoch:  1648 | train loss: 1.345027e-03 | valid loss: 1.324476e-03 
      	| train loss (relative): 2.629527e-02 | valid loss (relative): 2.573930e-02 
Epoch 1648 use: 403.11 second.

epoch 1649 starting......
Epoch:  1649 | train loss: 1.344467e-03 | valid loss: 1.326245e-03 
      	| train loss (relative): 2.628618e-02 | valid loss (relative): 2.566934e-02 
Epoch 1649 use: 398.05 second.

epoch 1650 starting......
Epoch:  1650 | train loss: 1.344978e-03 | valid loss: 1.325788e-03 
      	| train loss (relative): 2.629329e-02 | valid loss (relative): 2.587194e-02 
Epoch 1650 use: 399.25 second.

test MSE Error: 1.367322e-03 | relative MSE Error: 2.680889e-02 
 Total time used for training: 16.83 hour.
MESLoss saved to  /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_MSELoss_Latent_128_nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_1651.txt
relative MSELoss saved to  /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_reMSELoss_Latent_128_nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_1651.txt
model saved to /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_128_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_1651.pth
model_dict saved to /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_128_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_1651_dict.pth
... Training slugflow data completed, Run finished Wed 25 Aug 23:58:49 BST 2021 ...
