{'data_dir': '/rds/general/user/jy220/home/tensors/', 'data_type': 'tensors', 'vtu_fields': 'Component1::ComponentMassFractionPhase1, phase1::Velocity', 'variational': 'False', 'structured': 'False', 'changing_lr': 'False', 'self_concat': '2', 'dimension': '3', 'sfc_nums': '3', 'components': '4', 'nearest_neighbouring': 'True', 'dims_latent': '64', 'activation': 'Tanh', 'sfc_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/sfcs.pt', 'inv_sfc_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/inv_sfcs.pt', 'coords_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/coords.pt', 'cells_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/cells.npy', 'state_load': '/rds/general/user/jy220/home/results/Variational_False_Changelr_False_Latent_64_Nearest_neighbouring_True_SFC_nums_3_startlr_0.0005_n_epoches_200_dict.pth', 'batch_size': '16', 'lr': '0.0005', 'n_epoches': '200', 'seed': '15', 'visualize': 'False', 'reconstructed_path': 'None', 'reconstruct_start_index': '0', 'reconstruct_end_index': '50', 'save_path': '/rds/general/user/jy220/home/new_results/'}
[[ 2.50825269e-01 -1.67256990e-02 -1.22268730e-03]
 [ 2.38611959e-01 -5.50627777e-03 -5.06451913e-03]
 [ 2.39454874e-01 -1.59563257e-02  3.96778323e-03]
 ...
 [ 9.11361096e+00 -2.21908381e-02  2.22384902e-03]
 [ 9.11847310e+00 -2.84346931e-02 -2.34839590e-04]
 [ 9.11039661e+00 -2.51498573e-02 -8.70661044e-03]]
{'tetra': array([[      0,       1,       2,       3],
       [      4,       5,       6,       7],
       [      8,       9,      10,      11],
       ...,
       [1342744, 1342745, 1342746, 1342747],
       [1342748, 1342749, 1342750, 1342751],
       [1342752, 1342753, 1342754, 1342755]])}
structured  False 
 activation  Tanh() 
 self concat  2 
 sfc_nums  3 

dims_latent  64 
 components  4 
 nearest_neighbouring  True 

visualize  False 
 sample number  1706 

reading sfc nums......
[array([  41684,   41685,   41674, ..., 1235560, 1235564, 1235558]), array([954289, 954290, 954291, ..., 420685, 420687, 420688]), array([  40156,   40155,   40158, ..., 1123478, 1123475, 1123467])]
reading inverse sfc nums......
[array([   8146,    8145,    8144, ..., 1274444, 1274445, 1273166]), array([547280, 547283, 593037, ..., 933933, 933932, 864488]), array([ 66437,  66439, 122512, ..., 459416, 415130, 459418])]
Computing min and max......

tk:  tensor([0.7581, 0.0418, 0.1840, 0.1464], dtype=torch.float64) 

tb:  tensor([-0.3816, -0.3618, -0.0308,  0.1446], dtype=torch.float64) 

length of train set: 1505 

length of valid set: 100 

length of test set: 101 

torch device num: 4 

Let's use 4 GPUs!
epoch 200 starting......
Epoch:  200 | train loss: 1.153881e-02 | valid loss: 1.075020e-02 
      	| train loss (relative): 2.810929e-01 | valid loss (relative): 2.628657e-01 
Epoch 200 use: 417.10 second.

epoch 201 starting......
Epoch:  201 | train loss: 1.153481e-02 | valid loss: 1.074817e-02 
      	| train loss (relative): 2.829790e-01 | valid loss (relative): 2.627204e-01 
Epoch 201 use: 401.74 second.

epoch 202 starting......
Epoch:  202 | train loss: 1.153694e-02 | valid loss: 1.075081e-02 
      	| train loss (relative): 2.808629e-01 | valid loss (relative): 2.625288e-01 
Epoch 202 use: 406.45 second.

epoch 203 starting......
Epoch:  203 | train loss: 1.153818e-02 | valid loss: 1.074305e-02 
      	| train loss (relative): 2.838261e-01 | valid loss (relative): 2.632031e-01 
Epoch 203 use: 390.43 second.

epoch 204 starting......
Epoch:  204 | train loss: 1.153814e-02 | valid loss: 1.075125e-02 
      	| train loss (relative): 2.820140e-01 | valid loss (relative): 2.651531e-01 
Epoch 204 use: 432.38 second.

epoch 205 starting......
Epoch:  205 | train loss: 1.153792e-02 | valid loss: 1.074723e-02 
      	| train loss (relative): 2.818970e-01 | valid loss (relative): 2.618501e-01 
Epoch 205 use: 383.33 second.

epoch 206 starting......
Epoch:  206 | train loss: 1.153340e-02 | valid loss: 1.075192e-02 
      	| train loss (relative): 2.823398e-01 | valid loss (relative): 2.642156e-01 
Epoch 206 use: 381.46 second.

epoch 207 starting......
Epoch:  207 | train loss: 1.153693e-02 | valid loss: 1.075742e-02 
      	| train loss (relative): 2.817515e-01 | valid loss (relative): 2.636883e-01 
Epoch 207 use: 410.96 second.

epoch 208 starting......
Epoch:  208 | train loss: 1.153369e-02 | valid loss: 1.075263e-02 
      	| train loss (relative): 2.821007e-01 | valid loss (relative): 2.625859e-01 
Epoch 208 use: 384.85 second.

epoch 209 starting......
Epoch:  209 | train loss: 1.153793e-02 | valid loss: 1.075539e-02 
      	| train loss (relative): 2.815655e-01 | valid loss (relative): 2.640506e-01 
Epoch 209 use: 388.54 second.

epoch 210 starting......
Epoch:  210 | train loss: 1.153351e-02 | valid loss: 1.075694e-02 
      	| train loss (relative): 2.812696e-01 | valid loss (relative): 2.656257e-01 
Epoch 210 use: 388.42 second.

epoch 211 starting......
Epoch:  211 | train loss: 1.154345e-02 | valid loss: 1.075011e-02 
      	| train loss (relative): 2.856989e-01 | valid loss (relative): 2.620376e-01 
Epoch 211 use: 377.72 second.

epoch 212 starting......
Epoch:  212 | train loss: 1.153835e-02 | valid loss: 1.075059e-02 
      	| train loss (relative): 2.831214e-01 | valid loss (relative): 2.621923e-01 
Epoch 212 use: 389.66 second.

epoch 213 starting......
Epoch:  213 | train loss: 1.153398e-02 | valid loss: 1.074935e-02 
      	| train loss (relative): 2.816668e-01 | valid loss (relative): 2.614036e-01 
Epoch 213 use: 411.21 second.

epoch 214 starting......
Epoch:  214 | train loss: 1.153366e-02 | valid loss: 1.075233e-02 
      	| train loss (relative): 2.817791e-01 | valid loss (relative): 2.619518e-01 
Epoch 214 use: 387.55 second.

epoch 215 starting......
Epoch:  215 | train loss: 1.153594e-02 | valid loss: 1.074568e-02 
      	| train loss (relative): 2.829379e-01 | valid loss (relative): 2.618656e-01 
Epoch 215 use: 363.28 second.

epoch 216 starting......
Epoch:  216 | train loss: 1.153662e-02 | valid loss: 1.075056e-02 
      	| train loss (relative): 2.803725e-01 | valid loss (relative): 2.625756e-01 
Epoch 216 use: 386.45 second.

epoch 217 starting......
Epoch:  217 | train loss: 1.153388e-02 | valid loss: 1.075924e-02 
      	| train loss (relative): 2.812050e-01 | valid loss (relative): 2.641144e-01 
Epoch 217 use: 395.31 second.

epoch 218 starting......
Epoch:  218 | train loss: 1.153556e-02 | valid loss: 1.075234e-02 
      	| train loss (relative): 2.838953e-01 | valid loss (relative): 2.623856e-01 
Epoch 218 use: 384.63 second.

epoch 219 starting......
Epoch:  219 | train loss: 1.153866e-02 | valid loss: 1.075614e-02 
      	| train loss (relative): 2.812544e-01 | valid loss (relative): 2.629723e-01 
Epoch 219 use: 378.91 second.

epoch 220 starting......
Epoch:  220 | train loss: 1.153516e-02 | valid loss: 1.075217e-02 
      	| train loss (relative): 2.822542e-01 | valid loss (relative): 2.638221e-01 
Epoch 220 use: 375.93 second.

epoch 221 starting......
Epoch:  221 | train loss: 1.153500e-02 | valid loss: 1.074729e-02 
      	| train loss (relative): 2.830202e-01 | valid loss (relative): 2.637461e-01 
Epoch 221 use: 382.45 second.

epoch 222 starting......
Epoch:  222 | train loss: 1.153254e-02 | valid loss: 1.074785e-02 
      	| train loss (relative): 2.823199e-01 | valid loss (relative): 2.620859e-01 
Epoch 222 use: 389.75 second.

epoch 223 starting......
Epoch:  223 | train loss: 1.153532e-02 | valid loss: 1.074584e-02 
      	| train loss (relative): 2.840280e-01 | valid loss (relative): 2.636682e-01 
Epoch 223 use: 368.71 second.

epoch 224 starting......
Epoch:  224 | train loss: 1.153680e-02 | valid loss: 1.074318e-02 
      	| train loss (relative): 2.839793e-01 | valid loss (relative): 2.617254e-01 
Epoch 224 use: 384.38 second.

epoch 225 starting......
Epoch:  225 | train loss: 1.153542e-02 | valid loss: 1.075236e-02 
      	| train loss (relative): 2.821746e-01 | valid loss (relative): 2.632096e-01 
Epoch 225 use: 380.65 second.

epoch 226 starting......
Epoch:  226 | train loss: 1.153305e-02 | valid loss: 1.074865e-02 
      	| train loss (relative): 2.811588e-01 | valid loss (relative): 2.633643e-01 
Epoch 226 use: 385.50 second.

epoch 227 starting......
Epoch:  227 | train loss: 1.153384e-02 | valid loss: 1.074995e-02 
      	| train loss (relative): 2.825457e-01 | valid loss (relative): 2.620711e-01 
Epoch 227 use: 380.05 second.

epoch 228 starting......
Epoch:  228 | train loss: 1.153920e-02 | valid loss: 1.074212e-02 
      	| train loss (relative): 2.828105e-01 | valid loss (relative): 2.616694e-01 
Epoch 228 use: 395.24 second.

epoch 229 starting......
Epoch:  229 | train loss: 1.153253e-02 | valid loss: 1.075590e-02 
      	| train loss (relative): 2.813208e-01 | valid loss (relative): 2.631553e-01 
Epoch 229 use: 389.48 second.

epoch 230 starting......
Epoch:  230 | train loss: 1.153767e-02 | valid loss: 1.075018e-02 
      	| train loss (relative): 2.836155e-01 | valid loss (relative): 2.622833e-01 
Epoch 230 use: 393.67 second.

epoch 231 starting......
Epoch:  231 | train loss: 1.153533e-02 | valid loss: 1.074433e-02 
      	| train loss (relative): 2.806572e-01 | valid loss (relative): 2.627715e-01 
Epoch 231 use: 362.65 second.

epoch 232 starting......
Epoch:  232 | train loss: 1.153519e-02 | valid loss: 1.074626e-02 
      	| train loss (relative): 2.824683e-01 | valid loss (relative): 2.625268e-01 
Epoch 232 use: 386.71 second.

epoch 233 starting......
Epoch:  233 | train loss: 1.153425e-02 | valid loss: 1.074731e-02 
      	| train loss (relative): 2.816826e-01 | valid loss (relative): 2.621989e-01 
Epoch 233 use: 384.07 second.

epoch 234 starting......
Epoch:  234 | train loss: 1.153777e-02 | valid loss: 1.075505e-02 
      	| train loss (relative): 2.818847e-01 | valid loss (relative): 2.622493e-01 
Epoch 234 use: 371.11 second.

epoch 235 starting......
Epoch:  235 | train loss: 1.153672e-02 | valid loss: 1.074838e-02 
      	| train loss (relative): 2.823508e-01 | valid loss (relative): 2.610777e-01 
Epoch 235 use: 374.17 second.

epoch 236 starting......
Epoch:  236 | train loss: 1.153881e-02 | valid loss: 1.074628e-02 
      	| train loss (relative): 2.817127e-01 | valid loss (relative): 2.638570e-01 
Epoch 236 use: 360.37 second.

epoch 237 starting......
Epoch:  237 | train loss: 1.153786e-02 | valid loss: 1.076167e-02 
      	| train loss (relative): 2.828171e-01 | valid loss (relative): 2.662988e-01 
Epoch 237 use: 372.47 second.

epoch 238 starting......
Epoch:  238 | train loss: 1.154062e-02 | valid loss: 1.076383e-02 
      	| train loss (relative): 2.847996e-01 | valid loss (relative): 2.648967e-01 
Epoch 238 use: 384.30 second.

epoch 239 starting......
Epoch:  239 | train loss: 1.153592e-02 | valid loss: 1.075709e-02 
      	| train loss (relative): 2.815792e-01 | valid loss (relative): 2.651538e-01 
Epoch 239 use: 388.56 second.

epoch 240 starting......
Epoch:  240 | train loss: 1.153770e-02 | valid loss: 1.075583e-02 
      	| train loss (relative): 2.832005e-01 | valid loss (relative): 2.634407e-01 
Epoch 240 use: 391.14 second.

epoch 241 starting......
Epoch:  241 | train loss: 1.153610e-02 | valid loss: 1.075649e-02 
      	| train loss (relative): 2.814513e-01 | valid loss (relative): 2.647591e-01 
Epoch 241 use: 375.50 second.

epoch 242 starting......
Epoch:  242 | train loss: 1.153630e-02 | valid loss: 1.075615e-02 
      	| train loss (relative): 2.820949e-01 | valid loss (relative): 2.638399e-01 
Epoch 242 use: 379.99 second.

epoch 243 starting......
Epoch:  243 | train loss: 1.153509e-02 | valid loss: 1.074534e-02 
      	| train loss (relative): 2.827136e-01 | valid loss (relative): 2.649921e-01 
Epoch 243 use: 405.52 second.

epoch 244 starting......
Epoch:  244 | train loss: 1.153792e-02 | valid loss: 1.074558e-02 
      	| train loss (relative): 2.837782e-01 | valid loss (relative): 2.625320e-01 
Epoch 244 use: 364.55 second.

epoch 245 starting......
Epoch:  245 | train loss: 1.153751e-02 | valid loss: 1.074788e-02 
      	| train loss (relative): 2.831750e-01 | valid loss (relative): 2.622501e-01 
Epoch 245 use: 374.14 second.

epoch 246 starting......
Epoch:  246 | train loss: 1.153365e-02 | valid loss: 1.074581e-02 
      	| train loss (relative): 2.820962e-01 | valid loss (relative): 2.622297e-01 
Epoch 246 use: 379.82 second.

epoch 247 starting......
Epoch:  247 | train loss: 1.153624e-02 | valid loss: 1.074982e-02 
      	| train loss (relative): 2.810140e-01 | valid loss (relative): 2.637770e-01 
Epoch 247 use: 386.05 second.

epoch 248 starting......
Epoch:  248 | train loss: 1.153464e-02 | valid loss: 1.076306e-02 
      	| train loss (relative): 2.815904e-01 | valid loss (relative): 2.658236e-01 
Epoch 248 use: 414.98 second.

epoch 249 starting......
Epoch:  249 | train loss: 1.153687e-02 | valid loss: 1.074621e-02 
      	| train loss (relative): 2.832793e-01 | valid loss (relative): 2.623085e-01 
Epoch 249 use: 383.45 second.

epoch 250 starting......
Epoch:  250 | train loss: 1.153415e-02 | valid loss: 1.075139e-02 
      	| train loss (relative): 2.814507e-01 | valid loss (relative): 2.627636e-01 
Epoch 250 use: 391.80 second.

epoch 251 starting......
Epoch:  251 | train loss: 1.153702e-02 | valid loss: 1.074105e-02 
      	| train loss (relative): 2.823552e-01 | valid loss (relative): 2.622195e-01 
Epoch 251 use: 409.84 second.

epoch 252 starting......
Epoch:  252 | train loss: 1.153581e-02 | valid loss: 1.075548e-02 
      	| train loss (relative): 2.803188e-01 | valid loss (relative): 2.638701e-01 
Epoch 252 use: 383.30 second.

epoch 253 starting......
Epoch:  253 | train loss: 1.153462e-02 | valid loss: 1.075794e-02 
      	| train loss (relative): 2.827955e-01 | valid loss (relative): 2.648113e-01 
Epoch 253 use: 375.35 second.

epoch 254 starting......
Epoch:  254 | train loss: 1.153504e-02 | valid loss: 1.075241e-02 
      	| train loss (relative): 2.819831e-01 | valid loss (relative): 2.626176e-01 
Epoch 254 use: 382.89 second.

epoch 255 starting......
Epoch:  255 | train loss: 1.153499e-02 | valid loss: 1.075139e-02 
      	| train loss (relative): 2.828259e-01 | valid loss (relative): 2.633213e-01 
Epoch 255 use: 376.31 second.

epoch 256 starting......
Epoch:  256 | train loss: 1.153259e-02 | valid loss: 1.074899e-02 
      	| train loss (relative): 2.813254e-01 | valid loss (relative): 2.637270e-01 
Epoch 256 use: 373.21 second.

epoch 257 starting......
Epoch:  257 | train loss: 1.153659e-02 | valid loss: 1.074997e-02 
      	| train loss (relative): 2.824496e-01 | valid loss (relative): 2.633547e-01 
Epoch 257 use: 384.49 second.

epoch 258 starting......
Epoch:  258 | train loss: 1.153782e-02 | valid loss: 1.074701e-02 
      	| train loss (relative): 2.828996e-01 | valid loss (relative): 2.621113e-01 
Epoch 258 use: 394.00 second.

epoch 259 starting......
Epoch:  259 | train loss: 1.153746e-02 | valid loss: 1.075474e-02 
      	| train loss (relative): 2.817941e-01 | valid loss (relative): 2.624506e-01 
Epoch 259 use: 389.54 second.

epoch 260 starting......
Epoch:  260 | train loss: 1.153716e-02 | valid loss: 1.075766e-02 
      	| train loss (relative): 2.813232e-01 | valid loss (relative): 2.633185e-01 
Epoch 260 use: 385.58 second.

epoch 261 starting......
Epoch:  261 | train loss: 1.153494e-02 | valid loss: 1.075182e-02 
      	| train loss (relative): 2.836597e-01 | valid loss (relative): 2.630010e-01 
Epoch 261 use: 381.78 second.

epoch 262 starting......
Epoch:  262 | train loss: 1.153856e-02 | valid loss: 1.076041e-02 
      	| train loss (relative): 2.811897e-01 | valid loss (relative): 2.634594e-01 
Epoch 262 use: 390.19 second.

epoch 263 starting......
Epoch:  263 | train loss: 1.153762e-02 | valid loss: 1.074633e-02 
      	| train loss (relative): 2.814065e-01 | valid loss (relative): 2.627572e-01 
Epoch 263 use: 366.37 second.

epoch 264 starting......
Epoch:  264 | train loss: 1.153939e-02 | valid loss: 1.075182e-02 
      	| train loss (relative): 2.823994e-01 | valid loss (relative): 2.619722e-01 
Epoch 264 use: 378.05 second.

epoch 265 starting......
Epoch:  265 | train loss: 1.153706e-02 | valid loss: 1.075640e-02 
      	| train loss (relative): 2.814651e-01 | valid loss (relative): 2.637704e-01 
Epoch 265 use: 387.37 second.

epoch 266 starting......
Epoch:  266 | train loss: 1.153436e-02 | valid loss: 1.074205e-02 
      	| train loss (relative): 2.832900e-01 | valid loss (relative): 2.609702e-01 
Epoch 266 use: 386.84 second.

epoch 267 starting......
Epoch:  267 | train loss: 1.153578e-02 | valid loss: 1.074873e-02 
      	| train loss (relative): 2.815586e-01 | valid loss (relative): 2.623760e-01 
Epoch 267 use: 408.99 second.

epoch 268 starting......
Epoch:  268 | train loss: 1.153414e-02 | valid loss: 1.074816e-02 
      	| train loss (relative): 2.807855e-01 | valid loss (relative): 2.634961e-01 
Epoch 268 use: 395.78 second.

epoch 269 starting......
Epoch:  269 | train loss: 1.153490e-02 | valid loss: 1.074620e-02 
      	| train loss (relative): 2.828276e-01 | valid loss (relative): 2.624952e-01 
Epoch 269 use: 386.89 second.

epoch 270 starting......
Epoch:  270 | train loss: 1.153383e-02 | valid loss: 1.075206e-02 
      	| train loss (relative): 2.817315e-01 | valid loss (relative): 2.625592e-01 
Epoch 270 use: 369.15 second.

epoch 271 starting......
Epoch:  271 | train loss: 1.153701e-02 | valid loss: 1.075174e-02 
      	| train loss (relative): 2.824890e-01 | valid loss (relative): 2.638639e-01 
Epoch 271 use: 379.94 second.

epoch 272 starting......
Epoch:  272 | train loss: 1.153492e-02 | valid loss: 1.075682e-02 
      	| train loss (relative): 2.831768e-01 | valid loss (relative): 2.634408e-01 
Epoch 272 use: 370.74 second.

epoch 273 starting......
Epoch:  273 | train loss: 1.153406e-02 | valid loss: 1.074652e-02 
      	| train loss (relative): 2.818389e-01 | valid loss (relative): 2.631201e-01 
Epoch 273 use: 373.97 second.

epoch 274 starting......
Epoch:  274 | train loss: 1.153355e-02 | valid loss: 1.074413e-02 
      	| train loss (relative): 2.824533e-01 | valid loss (relative): 2.631754e-01 
Epoch 274 use: 377.81 second.

epoch 275 starting......
Epoch:  275 | train loss: 1.153367e-02 | valid loss: 1.075188e-02 
      	| train loss (relative): 2.832575e-01 | valid loss (relative): 2.639034e-01 
Epoch 275 use: 377.67 second.

epoch 276 starting......
Epoch:  276 | train loss: 1.153456e-02 | valid loss: 1.075045e-02 
      	| train loss (relative): 2.820621e-01 | valid loss (relative): 2.609280e-01 
Epoch 276 use: 377.77 second.

epoch 277 starting......
Epoch:  277 | train loss: 1.153473e-02 | valid loss: 1.075666e-02 
      	| train loss (relative): 2.814064e-01 | valid loss (relative): 2.649658e-01 
Epoch 277 use: 381.44 second.

epoch 278 starting......
Epoch:  278 | train loss: 1.154190e-02 | valid loss: 1.075285e-02 
      	| train loss (relative): 2.837828e-01 | valid loss (relative): 2.646607e-01 
Epoch 278 use: 397.82 second.

epoch 279 starting......
Epoch:  279 | train loss: 1.153570e-02 | valid loss: 1.075293e-02 
      	| train loss (relative): 2.829795e-01 | valid loss (relative): 2.638224e-01 
Epoch 279 use: 409.20 second.

epoch 280 starting......
Epoch:  280 | train loss: 1.153735e-02 | valid loss: 1.075197e-02 
      	| train loss (relative): 2.842596e-01 | valid loss (relative): 2.642165e-01 
Epoch 280 use: 464.48 second.

epoch 281 starting......
Epoch:  281 | train loss: 1.153377e-02 | valid loss: 1.076108e-02 
      	| train loss (relative): 2.829707e-01 | valid loss (relative): 2.682029e-01 
Epoch 281 use: 440.44 second.

epoch 282 starting......
Epoch:  282 | train loss: 1.154424e-02 | valid loss: 1.075193e-02 
      	| train loss (relative): 2.845030e-01 | valid loss (relative): 2.613279e-01 
Epoch 282 use: 396.44 second.

epoch 283 starting......
Epoch:  283 | train loss: 1.153660e-02 | valid loss: 1.074457e-02 
      	| train loss (relative): 2.818781e-01 | valid loss (relative): 2.633080e-01 
Epoch 283 use: 425.02 second.

epoch 284 starting......
Epoch:  284 | train loss: 1.153620e-02 | valid loss: 1.075401e-02 
      	| train loss (relative): 2.810595e-01 | valid loss (relative): 2.635501e-01 
Epoch 284 use: 378.77 second.

epoch 285 starting......
Epoch:  285 | train loss: 1.153563e-02 | valid loss: 1.074815e-02 
      	| train loss (relative): 2.829535e-01 | valid loss (relative): 2.626009e-01 
Epoch 285 use: 527.81 second.

epoch 286 starting......
Epoch:  286 | train loss: 1.153653e-02 | valid loss: 1.075040e-02 
      	| train loss (relative): 2.820307e-01 | valid loss (relative): 2.616106e-01 
Epoch 286 use: 382.72 second.

epoch 287 starting......
Epoch:  287 | train loss: 1.153733e-02 | valid loss: 1.075304e-02 
      	| train loss (relative): 2.818351e-01 | valid loss (relative): 2.629860e-01 
Epoch 287 use: 380.14 second.

epoch 288 starting......
Epoch:  288 | train loss: 1.154107e-02 | valid loss: 1.075186e-02 
      	| train loss (relative): 2.842910e-01 | valid loss (relative): 2.628486e-01 
Epoch 288 use: 401.62 second.

epoch 289 starting......
Epoch:  289 | train loss: 1.153802e-02 | valid loss: 1.075790e-02 
      	| train loss (relative): 2.814757e-01 | valid loss (relative): 2.648331e-01 
Epoch 289 use: 383.22 second.

epoch 290 starting......
Epoch:  290 | train loss: 1.153515e-02 | valid loss: 1.076330e-02 
      	| train loss (relative): 2.818364e-01 | valid loss (relative): 2.643407e-01 
Epoch 290 use: 392.82 second.

epoch 291 starting......
Epoch:  291 | train loss: 1.153242e-02 | valid loss: 1.075253e-02 
      	| train loss (relative): 2.824866e-01 | valid loss (relative): 2.626401e-01 
Epoch 291 use: 486.20 second.

epoch 292 starting......
Epoch:  292 | train loss: 1.153573e-02 | valid loss: 1.075273e-02 
      	| train loss (relative): 2.816652e-01 | valid loss (relative): 2.640991e-01 
Epoch 292 use: 390.25 second.

epoch 293 starting......
Epoch:  293 | train loss: 1.153360e-02 | valid loss: 1.075395e-02 
      	| train loss (relative): 2.823414e-01 | valid loss (relative): 2.625734e-01 
Epoch 293 use: 384.16 second.

epoch 294 starting......
Epoch:  294 | train loss: 1.153720e-02 | valid loss: 1.075421e-02 
      	| train loss (relative): 2.817145e-01 | valid loss (relative): 2.636773e-01 
Epoch 294 use: 394.39 second.

epoch 295 starting......
Epoch:  295 | train loss: 1.153485e-02 | valid loss: 1.074643e-02 
      	| train loss (relative): 2.818814e-01 | valid loss (relative): 2.637303e-01 
Epoch 295 use: 398.09 second.

epoch 296 starting......
Epoch:  296 | train loss: 1.154284e-02 | valid loss: 1.074541e-02 
      	| train loss (relative): 2.854706e-01 | valid loss (relative): 2.623319e-01 
Epoch 296 use: 489.50 second.

epoch 297 starting......
Epoch:  297 | train loss: 1.154121e-02 | valid loss: 1.075704e-02 
      	| train loss (relative): 2.807478e-01 | valid loss (relative): 2.645480e-01 
Epoch 297 use: 419.77 second.

epoch 298 starting......
Epoch:  298 | train loss: 1.153684e-02 | valid loss: 1.074785e-02 
      	| train loss (relative): 2.814428e-01 | valid loss (relative): 2.644334e-01 
Epoch 298 use: 398.12 second.

epoch 299 starting......
Epoch:  299 | train loss: 1.153895e-02 | valid loss: 1.075021e-02 
      	| train loss (relative): 2.839262e-01 | valid loss (relative): 2.647696e-01 
Epoch 299 use: 409.91 second.

epoch 300 starting......
Epoch:  300 | train loss: 1.153873e-02 | valid loss: 1.075464e-02 
      	| train loss (relative): 2.844093e-01 | valid loss (relative): 2.644327e-01 
Epoch 300 use: 394.75 second.

epoch 301 starting......
Epoch:  301 | train loss: 1.153660e-02 | valid loss: 1.075654e-02 
      	| train loss (relative): 2.820532e-01 | valid loss (relative): 2.633535e-01 
Epoch 301 use: 459.85 second.

epoch 302 starting......
Epoch:  302 | train loss: 1.153482e-02 | valid loss: 1.075564e-02 
      	| train loss (relative): 2.812282e-01 | valid loss (relative): 2.636371e-01 
Epoch 302 use: 569.53 second.

epoch 303 starting......
Epoch:  303 | train loss: 1.153336e-02 | valid loss: 1.074343e-02 
      	| train loss (relative): 2.826954e-01 | valid loss (relative): 2.620347e-01 
Epoch 303 use: 391.91 second.

epoch 304 starting......
Epoch:  304 | train loss: 1.153947e-02 | valid loss: 1.074435e-02 
      	| train loss (relative): 2.824036e-01 | valid loss (relative): 2.618665e-01 
Epoch 304 use: 403.44 second.

epoch 305 starting......
Epoch:  305 | train loss: 1.153456e-02 | valid loss: 1.074543e-02 
      	| train loss (relative): 2.821566e-01 | valid loss (relative): 2.622102e-01 
Epoch 305 use: 402.20 second.

epoch 306 starting......
Epoch:  306 | train loss: 1.153481e-02 | valid loss: 1.074931e-02 
      	| train loss (relative): 2.812244e-01 | valid loss (relative): 2.633879e-01 
Epoch 306 use: 401.30 second.

epoch 307 starting......
Epoch:  307 | train loss: 1.153318e-02 | valid loss: 1.074924e-02 
      	| train loss (relative): 2.828740e-01 | valid loss (relative): 2.623968e-01 
Epoch 307 use: 515.46 second.

epoch 308 starting......
Epoch:  308 | train loss: 1.153568e-02 | valid loss: 1.075822e-02 
      	| train loss (relative): 2.803653e-01 | valid loss (relative): 2.646315e-01 
Epoch 308 use: 455.07 second.

epoch 309 starting......
Epoch:  309 | train loss: 1.153381e-02 | valid loss: 1.074160e-02 
      	| train loss (relative): 2.828913e-01 | valid loss (relative): 2.622877e-01 
Epoch 309 use: 437.94 second.

epoch 310 starting......
Epoch:  310 | train loss: 1.153376e-02 | valid loss: 1.074626e-02 
      	| train loss (relative): 2.818341e-01 | valid loss (relative): 2.623031e-01 
Epoch 310 use: 385.92 second.

epoch 311 starting......
Epoch:  311 | train loss: 1.153390e-02 | valid loss: 1.074232e-02 
      	| train loss (relative): 2.823491e-01 | valid loss (relative): 2.624945e-01 
Epoch 311 use: 440.48 second.

epoch 312 starting......
Epoch:  312 | train loss: 1.153447e-02 | valid loss: 1.075013e-02 
      	| train loss (relative): 2.822962e-01 | valid loss (relative): 2.638337e-01 
Epoch 312 use: 471.41 second.

epoch 313 starting......
Epoch:  313 | train loss: 1.153438e-02 | valid loss: 1.074648e-02 
      	| train loss (relative): 2.824265e-01 | valid loss (relative): 2.624109e-01 
Epoch 313 use: 410.61 second.

epoch 314 starting......
Epoch:  314 | train loss: 1.153781e-02 | valid loss: 1.075246e-02 
      	| train loss (relative): 2.819429e-01 | valid loss (relative): 2.630623e-01 
Epoch 314 use: 412.37 second.

epoch 315 starting......
Epoch:  315 | train loss: 1.153462e-02 | valid loss: 1.075018e-02 
      	| train loss (relative): 2.820747e-01 | valid loss (relative): 2.635574e-01 
Epoch 315 use: 440.49 second.

epoch 316 starting......
Epoch:  316 | train loss: 1.153613e-02 | valid loss: 1.075028e-02 
      	| train loss (relative): 2.819047e-01 | valid loss (relative): 2.619136e-01 
Epoch 316 use: 394.04 second.

epoch 317 starting......
Epoch:  317 | train loss: 1.153410e-02 | valid loss: 1.074837e-02 
      	| train loss (relative): 2.823267e-01 | valid loss (relative): 2.635539e-01 
Epoch 317 use: 408.94 second.

epoch 318 starting......
Epoch:  318 | train loss: 1.153727e-02 | valid loss: 1.074232e-02 
      	| train loss (relative): 2.836948e-01 | valid loss (relative): 2.618329e-01 
Epoch 318 use: 447.68 second.

epoch 319 starting......
Epoch:  319 | train loss: 1.153642e-02 | valid loss: 1.074876e-02 
      	| train loss (relative): 2.811436e-01 | valid loss (relative): 2.645235e-01 
Epoch 319 use: 379.28 second.

epoch 320 starting......
Epoch:  320 | train loss: 1.153922e-02 | valid loss: 1.074836e-02 
      	| train loss (relative): 2.837195e-01 | valid loss (relative): 2.633372e-01 
Epoch 320 use: 385.70 second.

epoch 321 starting......
Epoch:  321 | train loss: 1.153585e-02 | valid loss: 1.075407e-02 
      	| train loss (relative): 2.812429e-01 | valid loss (relative): 2.641709e-01 
Epoch 321 use: 406.55 second.

epoch 322 starting......
Epoch:  322 | train loss: 1.153531e-02 | valid loss: 1.074644e-02 
      	| train loss (relative): 2.831253e-01 | valid loss (relative): 2.620276e-01 
Epoch 322 use: 382.23 second.

epoch 323 starting......
Epoch:  323 | train loss: 1.153431e-02 | valid loss: 1.076693e-02 
      	| train loss (relative): 2.809738e-01 | valid loss (relative): 2.658601e-01 
Epoch 323 use: 440.85 second.

epoch 324 starting......
Epoch:  324 | train loss: 1.153544e-02 | valid loss: 1.075814e-02 
      	| train loss (relative): 2.823035e-01 | valid loss (relative): 2.637827e-01 
Epoch 324 use: 420.54 second.

epoch 325 starting......
Epoch:  325 | train loss: 1.153538e-02 | valid loss: 1.075477e-02 
      	| train loss (relative): 2.825114e-01 | valid loss (relative): 2.648917e-01 
Epoch 325 use: 390.83 second.

epoch 326 starting......
Epoch:  326 | train loss: 1.153517e-02 | valid loss: 1.074601e-02 
      	| train loss (relative): 2.836684e-01 | valid loss (relative): 2.638301e-01 
Epoch 326 use: 390.63 second.

epoch 327 starting......
Epoch:  327 | train loss: 1.153708e-02 | valid loss: 1.075788e-02 
      	| train loss (relative): 2.826135e-01 | valid loss (relative): 2.641182e-01 
Epoch 327 use: 390.62 second.

epoch 328 starting......
Epoch:  328 | train loss: 1.153371e-02 | valid loss: 1.074490e-02 
      	| train loss (relative): 2.820941e-01 | valid loss (relative): 2.622560e-01 
Epoch 328 use: 398.86 second.

epoch 329 starting......
Epoch:  329 | train loss: 1.153390e-02 | valid loss: 1.074715e-02 
      	| train loss (relative): 2.820283e-01 | valid loss (relative): 2.618523e-01 
Epoch 329 use: 461.34 second.

epoch 330 starting......
Epoch:  330 | train loss: 1.153541e-02 | valid loss: 1.075029e-02 
      	| train loss (relative): 2.819588e-01 | valid loss (relative): 2.624577e-01 
Epoch 330 use: 405.70 second.

epoch 331 starting......
Epoch:  331 | train loss: 1.153559e-02 | valid loss: 1.074668e-02 
      	| train loss (relative): 2.828119e-01 | valid loss (relative): 2.605729e-01 
Epoch 331 use: 408.20 second.

epoch 332 starting......
Epoch:  332 | train loss: 1.154380e-02 | valid loss: 1.075184e-02 
      	| train loss (relative): 2.811879e-01 | valid loss (relative): 2.631453e-01 
Epoch 332 use: 383.78 second.

epoch 333 starting......
Epoch:  333 | train loss: 1.153543e-02 | valid loss: 1.074520e-02 
      	| train loss (relative): 2.845333e-01 | valid loss (relative): 2.626441e-01 
Epoch 333 use: 410.05 second.

epoch 334 starting......
Epoch:  334 | train loss: 1.153577e-02 | valid loss: 1.074666e-02 
      	| train loss (relative): 2.830060e-01 | valid loss (relative): 2.618640e-01 
Epoch 334 use: 402.73 second.

epoch 335 starting......
Epoch:  335 | train loss: 1.153740e-02 | valid loss: 1.075526e-02 
      	| train loss (relative): 2.810599e-01 | valid loss (relative): 2.625037e-01 
Epoch 335 use: 468.61 second.

epoch 336 starting......
Epoch:  336 | train loss: 1.153619e-02 | valid loss: 1.075156e-02 
      	| train loss (relative): 2.810277e-01 | valid loss (relative): 2.646530e-01 
Epoch 336 use: 403.78 second.

epoch 337 starting......
Epoch:  337 | train loss: 1.153794e-02 | valid loss: 1.074726e-02 
      	| train loss (relative): 2.840604e-01 | valid loss (relative): 2.635385e-01 
Epoch 337 use: 418.22 second.

epoch 338 starting......
Epoch:  338 | train loss: 1.153450e-02 | valid loss: 1.075718e-02 
      	| train loss (relative): 2.820088e-01 | valid loss (relative): 2.633983e-01 
Epoch 338 use: 389.33 second.

epoch 339 starting......
Epoch:  339 | train loss: 1.153453e-02 | valid loss: 1.075035e-02 
      	| train loss (relative): 2.835460e-01 | valid loss (relative): 2.628983e-01 
Epoch 339 use: 375.98 second.

epoch 340 starting......
Epoch:  340 | train loss: 1.153288e-02 | valid loss: 1.075082e-02 
      	| train loss (relative): 2.807554e-01 | valid loss (relative): 2.630580e-01 
Epoch 340 use: 411.40 second.

epoch 341 starting......
Epoch:  341 | train loss: 1.153458e-02 | valid loss: 1.075016e-02 
      	| train loss (relative): 2.827566e-01 | valid loss (relative): 2.632373e-01 
Epoch 341 use: 409.26 second.

epoch 342 starting......
Epoch:  342 | train loss: 1.153578e-02 | valid loss: 1.075570e-02 
      	| train loss (relative): 2.813835e-01 | valid loss (relative): 2.638633e-01 
Epoch 342 use: 375.57 second.

epoch 343 starting......
Epoch:  343 | train loss: 1.153285e-02 | valid loss: 1.075393e-02 
      	| train loss (relative): 2.827448e-01 | valid loss (relative): 2.618611e-01 
Epoch 343 use: 420.30 second.

epoch 344 starting......
Epoch:  344 | train loss: 1.153401e-02 | valid loss: 1.074846e-02 
      	| train loss (relative): 2.825313e-01 | valid loss (relative): 2.636614e-01 
Epoch 344 use: 413.56 second.

epoch 345 starting......
Epoch:  345 | train loss: 1.153753e-02 | valid loss: 1.074623e-02 
      	| train loss (relative): 2.818655e-01 | valid loss (relative): 2.634264e-01 
Epoch 345 use: 389.12 second.

epoch 346 starting......
Epoch:  346 | train loss: 1.153760e-02 | valid loss: 1.074306e-02 
      	| train loss (relative): 2.820626e-01 | valid loss (relative): 2.629170e-01 
Epoch 346 use: 461.11 second.

epoch 347 starting......
Epoch:  347 | train loss: 1.153415e-02 | valid loss: 1.074381e-02 
      	| train loss (relative): 2.815614e-01 | valid loss (relative): 2.617051e-01 
Epoch 347 use: 413.90 second.

epoch 348 starting......
Epoch:  348 | train loss: 1.153571e-02 | valid loss: 1.075485e-02 
      	| train loss (relative): 2.817942e-01 | valid loss (relative): 2.642226e-01 
Epoch 348 use: 401.31 second.

epoch 349 starting......
Epoch:  349 | train loss: 1.153777e-02 | valid loss: 1.074680e-02 
      	| train loss (relative): 2.856805e-01 | valid loss (relative): 2.617631e-01 
Epoch 349 use: 370.01 second.

epoch 350 starting......
Epoch:  350 | train loss: 1.153461e-02 | valid loss: 1.074450e-02 
      	| train loss (relative): 2.814380e-01 | valid loss (relative): 2.623412e-01 
Epoch 350 use: 397.34 second.

epoch 351 starting......
Epoch:  351 | train loss: 1.153348e-02 | valid loss: 1.074714e-02 
      	| train loss (relative): 2.826136e-01 | valid loss (relative): 2.626014e-01 
Epoch 351 use: 389.78 second.

epoch 352 starting......
Epoch:  352 | train loss: 1.153712e-02 | valid loss: 1.075160e-02 
      	| train loss (relative): 2.816683e-01 | valid loss (relative): 2.630866e-01 
Epoch 352 use: 466.45 second.

epoch 353 starting......
Epoch:  353 | train loss: 1.153489e-02 | valid loss: 1.075886e-02 
      	| train loss (relative): 2.808936e-01 | valid loss (relative): 2.642781e-01 
Epoch 353 use: 402.60 second.

epoch 354 starting......
Epoch:  354 | train loss: 1.153175e-02 | valid loss: 1.074475e-02 
      	| train loss (relative): 2.833320e-01 | valid loss (relative): 2.613631e-01 
Epoch 354 use: 473.15 second.

epoch 355 starting......
Epoch:  355 | train loss: 1.153452e-02 | valid loss: 1.075100e-02 
      	| train loss (relative): 2.810526e-01 | valid loss (relative): 2.621744e-01 
Epoch 355 use: 416.44 second.

epoch 356 starting......
Epoch:  356 | train loss: 1.153884e-02 | valid loss: 1.074974e-02 
      	| train loss (relative): 2.823115e-01 | valid loss (relative): 2.626487e-01 
Epoch 356 use: 426.33 second.

epoch 357 starting......
Epoch:  357 | train loss: 1.153618e-02 | valid loss: 1.075324e-02 
      	| train loss (relative): 2.826582e-01 | valid loss (relative): 2.627660e-01 
Epoch 357 use: 483.61 second.

epoch 358 starting......
Epoch:  358 | train loss: 1.153560e-02 | valid loss: 1.074619e-02 
      	| train loss (relative): 2.824441e-01 | valid loss (relative): 2.618896e-01 
Epoch 358 use: 421.13 second.

epoch 359 starting......
Epoch:  359 | train loss: 1.153490e-02 | valid loss: 1.075960e-02 
      	| train loss (relative): 2.810663e-01 | valid loss (relative): 2.635997e-01 
Epoch 359 use: 421.46 second.

epoch 360 starting......
Epoch:  360 | train loss: 1.153627e-02 | valid loss: 1.074680e-02 
      	| train loss (relative): 2.821481e-01 | valid loss (relative): 2.619680e-01 
Epoch 360 use: 384.12 second.

epoch 361 starting......
Epoch:  361 | train loss: 1.153399e-02 | valid loss: 1.075158e-02 
      	| train loss (relative): 2.820447e-01 | valid loss (relative): 2.627073e-01 
Epoch 361 use: 405.65 second.

epoch 362 starting......
Epoch:  362 | train loss: 1.153365e-02 | valid loss: 1.076035e-02 
      	| train loss (relative): 2.814191e-01 | valid loss (relative): 2.637365e-01 
Epoch 362 use: 407.84 second.

epoch 363 starting......
Epoch:  363 | train loss: 1.154275e-02 | valid loss: 1.075303e-02 
      	| train loss (relative): 2.822959e-01 | valid loss (relative): 2.640317e-01 
Epoch 363 use: 459.70 second.

epoch 364 starting......
Epoch:  364 | train loss: 1.153619e-02 | valid loss: 1.075079e-02 
      	| train loss (relative): 2.823597e-01 | valid loss (relative): 2.617854e-01 
Epoch 364 use: 443.46 second.

epoch 365 starting......
Epoch:  365 | train loss: 1.153253e-02 | valid loss: 1.075012e-02 
      	| train loss (relative): 2.824191e-01 | valid loss (relative): 2.622487e-01 
Epoch 365 use: 472.32 second.

epoch 366 starting......
Epoch:  366 | train loss: 1.153860e-02 | valid loss: 1.075574e-02 
      	| train loss (relative): 2.811551e-01 | valid loss (relative): 2.646692e-01 
Epoch 366 use: 501.54 second.

epoch 367 starting......
Epoch:  367 | train loss: 1.153588e-02 | valid loss: 1.075408e-02 
      	| train loss (relative): 2.819901e-01 | valid loss (relative): 2.634541e-01 
Epoch 367 use: 500.52 second.

epoch 368 starting......
Epoch:  368 | train loss: 1.153408e-02 | valid loss: 1.074742e-02 
      	| train loss (relative): 2.817007e-01 | valid loss (relative): 2.632800e-01 
Epoch 368 use: 514.32 second.

epoch 369 starting......
Epoch:  369 | train loss: 1.153320e-02 | valid loss: 1.075012e-02 
      	| train loss (relative): 2.818021e-01 | valid loss (relative): 2.631335e-01 
Epoch 369 use: 446.04 second.

epoch 370 starting......
Epoch:  370 | train loss: 1.153450e-02 | valid loss: 1.074561e-02 
      	| train loss (relative): 2.828618e-01 | valid loss (relative): 2.624999e-01 
Epoch 370 use: 428.03 second.

epoch 371 starting......
Epoch:  371 | train loss: 1.153560e-02 | valid loss: 1.074698e-02 
      	| train loss (relative): 2.816431e-01 | valid loss (relative): 2.626957e-01 
Epoch 371 use: 379.46 second.

epoch 372 starting......
Epoch:  372 | train loss: 1.153806e-02 | valid loss: 1.075176e-02 
      	| train loss (relative): 2.808795e-01 | valid loss (relative): 2.642589e-01 
Epoch 372 use: 452.82 second.

epoch 373 starting......
Epoch:  373 | train loss: 1.153546e-02 | valid loss: 1.075446e-02 
      	| train loss (relative): 2.825541e-01 | valid loss (relative): 2.653914e-01 
Epoch 373 use: 433.55 second.

epoch 374 starting......
Epoch:  374 | train loss: 1.153756e-02 | valid loss: 1.074593e-02 
      	| train loss (relative): 2.833075e-01 | valid loss (relative): 2.630306e-01 
Epoch 374 use: 406.90 second.

epoch 375 starting......
Epoch:  375 | train loss: 1.153379e-02 | valid loss: 1.074567e-02 
      	| train loss (relative): 2.823950e-01 | valid loss (relative): 2.635245e-01 
Epoch 375 use: 406.94 second.

epoch 376 starting......
Epoch:  376 | train loss: 1.153923e-02 | valid loss: 1.074353e-02 
      	| train loss (relative): 2.821714e-01 | valid loss (relative): 2.636380e-01 
Epoch 376 use: 418.65 second.

epoch 377 starting......
Epoch:  377 | train loss: 1.153306e-02 | valid loss: 1.074443e-02 
      	| train loss (relative): 2.814242e-01 | valid loss (relative): 2.617845e-01 
Epoch 377 use: 414.10 second.

epoch 378 starting......
Epoch:  378 | train loss: 1.153736e-02 | valid loss: 1.075613e-02 
      	| train loss (relative): 2.827353e-01 | valid loss (relative): 2.632504e-01 
Epoch 378 use: 411.87 second.

epoch 379 starting......
Epoch:  379 | train loss: 1.153785e-02 | valid loss: 1.074954e-02 
      	| train loss (relative): 2.818559e-01 | valid loss (relative): 2.623943e-01 
Epoch 379 use: 405.77 second.

epoch 380 starting......
Epoch:  380 | train loss: 1.153270e-02 | valid loss: 1.074785e-02 
      	| train loss (relative): 2.823859e-01 | valid loss (relative): 2.617368e-01 
Epoch 380 use: 399.65 second.

epoch 381 starting......
Epoch:  381 | train loss: 1.153721e-02 | valid loss: 1.075055e-02 
      	| train loss (relative): 2.818910e-01 | valid loss (relative): 2.617418e-01 
Epoch 381 use: 414.64 second.

epoch 382 starting......
Epoch:  382 | train loss: 1.153764e-02 | valid loss: 1.075639e-02 
      	| train loss (relative): 2.816736e-01 | valid loss (relative): 2.631781e-01 
Epoch 382 use: 414.77 second.

epoch 383 starting......
Epoch:  383 | train loss: 1.153526e-02 | valid loss: 1.074961e-02 
      	| train loss (relative): 2.832090e-01 | valid loss (relative): 2.627186e-01 
Epoch 383 use: 403.22 second.

epoch 384 starting......
Epoch:  384 | train loss: 1.153601e-02 | valid loss: 1.074824e-02 
      	| train loss (relative): 2.818759e-01 | valid loss (relative): 2.618048e-01 
Epoch 384 use: 433.36 second.

epoch 385 starting......
Epoch:  385 | train loss: 1.153613e-02 | valid loss: 1.075539e-02 
      	| train loss (relative): 2.809336e-01 | valid loss (relative): 2.632966e-01 
Epoch 385 use: 395.20 second.

epoch 386 starting......
Epoch:  386 | train loss: 1.153385e-02 | valid loss: 1.075155e-02 
      	| train loss (relative): 2.821470e-01 | valid loss (relative): 2.625341e-01 
Epoch 386 use: 409.09 second.

epoch 387 starting......
Epoch:  387 | train loss: 1.153563e-02 | valid loss: 1.074754e-02 
      	| train loss (relative): 2.822043e-01 | valid loss (relative): 2.629125e-01 
Epoch 387 use: 379.38 second.

epoch 388 starting......
Epoch:  388 | train loss: 1.153326e-02 | valid loss: 1.075294e-02 
      	| train loss (relative): 2.834124e-01 | valid loss (relative): 2.613131e-01 
Epoch 388 use: 432.64 second.

epoch 389 starting......
Epoch:  389 | train loss: 1.153759e-02 | valid loss: 1.075271e-02 
      	| train loss (relative): 2.811046e-01 | valid loss (relative): 2.630716e-01 
Epoch 389 use: 386.27 second.

epoch 390 starting......
Epoch:  390 | train loss: 1.153729e-02 | valid loss: 1.075133e-02 
      	| train loss (relative): 2.824113e-01 | valid loss (relative): 2.639386e-01 
Epoch 390 use: 380.10 second.

epoch 391 starting......
Epoch:  391 | train loss: 1.153848e-02 | valid loss: 1.075818e-02 
      	| train loss (relative): 2.828470e-01 | valid loss (relative): 2.606400e-01 
Epoch 391 use: 396.44 second.

epoch 392 starting......
Epoch:  392 | train loss: 1.153588e-02 | valid loss: 1.074550e-02 
      	| train loss (relative): 2.818889e-01 | valid loss (relative): 2.623131e-01 
Epoch 392 use: 408.74 second.

epoch 393 starting......
Epoch:  393 | train loss: 1.153498e-02 | valid loss: 1.074785e-02 
      	| train loss (relative): 2.804151e-01 | valid loss (relative): 2.643343e-01 
Epoch 393 use: 432.91 second.

epoch 394 starting......
Epoch:  394 | train loss: 1.153792e-02 | valid loss: 1.075597e-02 
      	| train loss (relative): 2.830935e-01 | valid loss (relative): 2.641987e-01 
Epoch 394 use: 383.28 second.

epoch 395 starting......
Epoch:  395 | train loss: 1.153687e-02 | valid loss: 1.074915e-02 
      	| train loss (relative): 2.812910e-01 | valid loss (relative): 2.636755e-01 
Epoch 395 use: 392.10 second.

epoch 396 starting......
Epoch:  396 | train loss: 1.153548e-02 | valid loss: 1.074714e-02 
      	| train loss (relative): 2.819597e-01 | valid loss (relative): 2.638104e-01 
Epoch 396 use: 392.80 second.

epoch 397 starting......
Epoch:  397 | train loss: 1.153658e-02 | valid loss: 1.075287e-02 
      	| train loss (relative): 2.829974e-01 | valid loss (relative): 2.634067e-01 
Epoch 397 use: 413.94 second.

epoch 398 starting......
Epoch:  398 | train loss: 1.153809e-02 | valid loss: 1.074893e-02 
      	| train loss (relative): 2.821106e-01 | valid loss (relative): 2.634956e-01 
Epoch 398 use: 392.80 second.

epoch 399 starting......
Epoch:  399 | train loss: 1.153530e-02 | valid loss: 1.074957e-02 
      	| train loss (relative): 2.815103e-01 | valid loss (relative): 2.637497e-01 
Epoch 399 use: 365.43 second.

test MSE Error: 1.131092e-02 | relative MSE Error: 2.775229e-01 
 Total time used for training: 22.52 hour.
MESLoss saved to  /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_MSELoss_Latent_64_nearest_neighbouring_True_SFC_nums_3_startlr_0.0005_n_epoches_400.txt
relative MSELoss saved to  /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_reMSELoss_Latent_64_nearest_neighbouring_True_SFC_nums_3_startlr_0.0005_n_epoches_400.txt
model saved to /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_64_Nearest_neighbouring_True_SFC_nums_3_startlr_0.0005_n_epoches_400.pth
model_dict saved to /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_64_Nearest_neighbouring_True_SFC_nums_3_startlr_0.0005_n_epoches_400_dict.pth
... Training slugflow data group 3 completed, Run finished Wed  4 Aug 06:47:13 BST 2021 ...
{'data_dir': '/rds/general/user/jy220/home/tensors/', 'data_type': 'tensors', 'vtu_fields': 'Component1::ComponentMassFractionPhase1, phase1::Velocity', 'variational': 'False', 'structured': 'False', 'changing_lr': 'False', 'self_concat': '2', 'dimension': '3', 'sfc_nums': '3', 'components': '4', 'nearest_neighbouring': 'True', 'dims_latent': '64', 'activation': 'Tanh', 'sfc_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/sfcs.pt', 'inv_sfc_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/inv_sfcs.pt', 'coords_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/coords.pt', 'cells_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/cells.npy', 'state_load': '/rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_64_Nearest_neighbouring_True_SFC_nums_3_startlr_0.0005_n_epoches_400_dict.pth', 'batch_size': '16', 'lr': '0.01', 'n_epoches': '200', 'seed': '15', 'visualize': 'False', 'reconstructed_path': 'None', 'reconstruct_start_index': '0', 'reconstruct_end_index': '50', 'save_path': '/rds/general/user/jy220/home/new_results/'}
[[ 2.50825269e-01 -1.67256990e-02 -1.22268730e-03]
 [ 2.38611959e-01 -5.50627777e-03 -5.06451913e-03]
 [ 2.39454874e-01 -1.59563257e-02  3.96778323e-03]
 ...
 [ 9.11361096e+00 -2.21908381e-02  2.22384902e-03]
 [ 9.11847310e+00 -2.84346931e-02 -2.34839590e-04]
 [ 9.11039661e+00 -2.51498573e-02 -8.70661044e-03]]
{'tetra': array([[      0,       1,       2,       3],
       [      4,       5,       6,       7],
       [      8,       9,      10,      11],
       ...,
       [1342744, 1342745, 1342746, 1342747],
       [1342748, 1342749, 1342750, 1342751],
       [1342752, 1342753, 1342754, 1342755]])}
structured  False 
 activation  Tanh() 
 self concat  2 
 sfc_nums  3 

dims_latent  64 
 components  4 
 nearest_neighbouring  True 

visualize  False 
 sample number  1706 

reading sfc nums......
[array([  41684,   41685,   41674, ..., 1235560, 1235564, 1235558]), array([954289, 954290, 954291, ..., 420685, 420687, 420688]), array([  40156,   40155,   40158, ..., 1123478, 1123475, 1123467])]
reading inverse sfc nums......
[array([   8146,    8145,    8144, ..., 1274444, 1274445, 1273166]), array([547280, 547283, 593037, ..., 933933, 933932, 864488]), array([ 66437,  66439, 122512, ..., 459416, 415130, 459418])]
Computing min and max......

tk:  tensor([0.7581, 0.0418, 0.1840, 0.1464], dtype=torch.float64) 

tb:  tensor([-0.3816, -0.3618, -0.0308,  0.1446], dtype=torch.float64) 

length of train set: 1505 

length of valid set: 100 

length of test set: 101 

torch device num: 4 

Let's use 4 GPUs!
epoch 400 starting......
Epoch:  400 | train loss: 1.156564e-02 | valid loss: 1.011904e-02 
      	| train loss (relative): 2.836805e-01 | valid loss (relative): 2.464658e-01 
Epoch 400 use: 423.71 second.

epoch 401 starting......
Epoch:  401 | train loss: 1.156319e-02 | valid loss: 1.015145e-02 
      	| train loss (relative): 2.826393e-01 | valid loss (relative): 2.493852e-01 
Epoch 401 use: 400.52 second.

epoch 402 starting......
Epoch:  402 | train loss: 1.156967e-02 | valid loss: 1.016358e-02 
      	| train loss (relative): 2.848625e-01 | valid loss (relative): 2.491986e-01 
Epoch 402 use: 396.62 second.

epoch 403 starting......
Epoch:  403 | train loss: 1.156483e-02 | valid loss: 1.014235e-02 
      	| train loss (relative): 2.834816e-01 | valid loss (relative): 2.490483e-01 
Epoch 403 use: 423.14 second.

epoch 404 starting......
Epoch:  404 | train loss: 1.156407e-02 | valid loss: 1.013635e-02 
      	| train loss (relative): 2.825553e-01 | valid loss (relative): 2.491128e-01 
Epoch 404 use: 405.89 second.

epoch 405 starting......
Epoch:  405 | train loss: 1.156557e-02 | valid loss: 1.015319e-02 
      	| train loss (relative): 2.822600e-01 | valid loss (relative): 2.487676e-01 
Epoch 405 use: 438.79 second.

epoch 406 starting......
Epoch:  406 | train loss: 1.156327e-02 | valid loss: 1.013510e-02 
      	| train loss (relative): 2.833894e-01 | valid loss (relative): 2.482946e-01 
Epoch 406 use: 445.51 second.

epoch 407 starting......
Epoch:  407 | train loss: 1.156360e-02 | valid loss: 1.015715e-02 
      	| train loss (relative): 2.821233e-01 | valid loss (relative): 2.496710e-01 
Epoch 407 use: 382.41 second.

epoch 408 starting......
Epoch:  408 | train loss: 1.156342e-02 | valid loss: 1.014802e-02 
      	| train loss (relative): 2.834501e-01 | valid loss (relative): 2.496016e-01 
Epoch 408 use: 386.52 second.

epoch 409 starting......
Epoch:  409 | train loss: 1.156790e-02 | valid loss: 1.013723e-02 
      	| train loss (relative): 2.854745e-01 | valid loss (relative): 2.484612e-01 
Epoch 409 use: 393.12 second.

epoch 410 starting......
Epoch:  410 | train loss: 1.156180e-02 | valid loss: 1.011009e-02 
      	| train loss (relative): 2.831047e-01 | valid loss (relative): 2.455811e-01 
Epoch 410 use: 369.82 second.

epoch 411 starting......
Epoch:  411 | train loss: 1.156538e-02 | valid loss: 1.013209e-02 
      	| train loss (relative): 2.824033e-01 | valid loss (relative): 2.472175e-01 
Epoch 411 use: 417.71 second.

epoch 412 starting......
Epoch:  412 | train loss: 1.156587e-02 | valid loss: 1.015388e-02 
      	| train loss (relative): 2.822692e-01 | valid loss (relative): 2.490616e-01 
Epoch 412 use: 387.53 second.

epoch 413 starting......
Epoch:  413 | train loss: 1.156349e-02 | valid loss: 1.013813e-02 
      	| train loss (relative): 2.838673e-01 | valid loss (relative): 2.482681e-01 
Epoch 413 use: 375.40 second.

epoch 414 starting......
Epoch:  414 | train loss: 1.156198e-02 | valid loss: 1.015454e-02 
      	| train loss (relative): 2.822011e-01 | valid loss (relative): 2.490176e-01 
Epoch 414 use: 373.03 second.

epoch 415 starting......
Epoch:  415 | train loss: 1.156756e-02 | valid loss: 1.014800e-02 
      	| train loss (relative): 2.845883e-01 | valid loss (relative): 2.487634e-01 
Epoch 415 use: 404.09 second.

epoch 416 starting......
Epoch:  416 | train loss: 1.156418e-02 | valid loss: 1.014675e-02 
      	| train loss (relative): 2.831058e-01 | valid loss (relative): 2.505855e-01 
Epoch 416 use: 367.09 second.

epoch 417 starting......
Epoch:  417 | train loss: 1.156568e-02 | valid loss: 1.013086e-02 
      	| train loss (relative): 2.841440e-01 | valid loss (relative): 2.472508e-01 
Epoch 417 use: 481.47 second.

epoch 418 starting......
Epoch:  418 | train loss: 1.156236e-02 | valid loss: 1.013092e-02 
      	| train loss (relative): 2.831682e-01 | valid loss (relative): 2.471639e-01 
Epoch 418 use: 405.79 second.

epoch 419 starting......
Epoch:  419 | train loss: 1.156718e-02 | valid loss: 1.015531e-02 
      	| train loss (relative): 2.822087e-01 | valid loss (relative): 2.483762e-01 
Epoch 419 use: 382.74 second.

epoch 420 starting......
Epoch:  420 | train loss: 1.156633e-02 | valid loss: 1.014133e-02 
      	| train loss (relative): 2.841344e-01 | valid loss (relative): 2.484449e-01 
Epoch 420 use: 385.90 second.

epoch 421 starting......
Epoch:  421 | train loss: 1.156806e-02 | valid loss: 1.015108e-02 
      	| train loss (relative): 2.818431e-01 | valid loss (relative): 2.492884e-01 
Epoch 421 use: 386.46 second.

epoch 422 starting......
Epoch:  422 | train loss: 1.156359e-02 | valid loss: 1.012664e-02 
      	| train loss (relative): 2.831340e-01 | valid loss (relative): 2.471383e-01 
Epoch 422 use: 421.19 second.

epoch 423 starting......
Epoch:  423 | train loss: 1.156457e-02 | valid loss: 1.014834e-02 
      	| train loss (relative): 2.826805e-01 | valid loss (relative): 2.485661e-01 
Epoch 423 use: 440.64 second.

epoch 424 starting......
Epoch:  424 | train loss: 1.156177e-02 | valid loss: 1.013154e-02 
      	| train loss (relative): 2.837872e-01 | valid loss (relative): 2.489003e-01 
Epoch 424 use: 407.37 second.

epoch 425 starting......
Epoch:  425 | train loss: 1.156493e-02 | valid loss: 1.014623e-02 
      	| train loss (relative): 2.838421e-01 | valid loss (relative): 2.488893e-01 
Epoch 425 use: 408.07 second.

epoch 426 starting......
Epoch:  426 | train loss: 1.156551e-02 | valid loss: 1.016952e-02 
      	| train loss (relative): 2.828964e-01 | valid loss (relative): 2.520167e-01 
Epoch 426 use: 380.47 second.

epoch 427 starting......
Epoch:  427 | train loss: 1.156770e-02 | valid loss: 1.014874e-02 
      	| train loss (relative): 2.852772e-01 | valid loss (relative): 2.494895e-01 
Epoch 427 use: 434.47 second.

epoch 428 starting......
Epoch:  428 | train loss: 1.156857e-02 | valid loss: 1.011886e-02 
      	| train loss (relative): 2.853941e-01 | valid loss (relative): 2.468926e-01 
Epoch 428 use: 369.28 second.

epoch 429 starting......
Epoch:  429 | train loss: 1.156309e-02 | valid loss: 1.014375e-02 
      	| train loss (relative): 2.824450e-01 | valid loss (relative): 2.486485e-01 
Epoch 429 use: 370.18 second.

epoch 430 starting......
Epoch:  430 | train loss: 1.156361e-02 | valid loss: 1.014653e-02 
      	| train loss (relative): 2.830180e-01 | valid loss (relative): 2.484615e-01 
Epoch 430 use: 374.99 second.

epoch 431 starting......
Epoch:  431 | train loss: 1.156387e-02 | valid loss: 1.014978e-02 
      	| train loss (relative): 2.823113e-01 | valid loss (relative): 2.498279e-01 
Epoch 431 use: 389.90 second.

epoch 432 starting......
Epoch:  432 | train loss: 1.156497e-02 | valid loss: 1.013151e-02 
      	| train loss (relative): 2.852180e-01 | valid loss (relative): 2.475763e-01 
Epoch 432 use: 388.39 second.

epoch 433 starting......
Epoch:  433 | train loss: 1.156596e-02 | valid loss: 1.010540e-02 
      	| train loss (relative): 2.829297e-01 | valid loss (relative): 2.458456e-01 
Epoch 433 use: 378.07 second.

epoch 434 starting......
Epoch:  434 | train loss: 1.156498e-02 | valid loss: 1.012121e-02 
      	| train loss (relative): 2.826898e-01 | valid loss (relative): 2.464086e-01 
Epoch 434 use: 394.73 second.

epoch 435 starting......
Epoch:  435 | train loss: 1.156731e-02 | valid loss: 1.011165e-02 
      	| train loss (relative): 2.825603e-01 | valid loss (relative): 2.463401e-01 
Epoch 435 use: 482.97 second.

epoch 436 starting......
Epoch:  436 | train loss: 1.156450e-02 | valid loss: 1.013406e-02 
      	| train loss (relative): 2.830857e-01 | valid loss (relative): 2.476568e-01 
Epoch 436 use: 444.76 second.

epoch 437 starting......
Epoch:  437 | train loss: 1.156383e-02 | valid loss: 1.014477e-02 
      	| train loss (relative): 2.829683e-01 | valid loss (relative): 2.484375e-01 
Epoch 437 use: 435.85 second.

epoch 438 starting......
Epoch:  438 | train loss: 1.156559e-02 | valid loss: 1.015852e-02 
      	| train loss (relative): 2.824936e-01 | valid loss (relative): 2.497084e-01 
Epoch 438 use: 438.28 second.

epoch 439 starting......
Epoch:  439 | train loss: 1.156332e-02 | valid loss: 1.012463e-02 
      	| train loss (relative): 2.831894e-01 | valid loss (relative): 2.471478e-01 
Epoch 439 use: 445.13 second.

epoch 440 starting......
Epoch:  440 | train loss: 1.156426e-02 | valid loss: 1.012849e-02 
      	| train loss (relative): 2.833640e-01 | valid loss (relative): 2.483129e-01 
Epoch 440 use: 437.63 second.

epoch 441 starting......
Epoch:  441 | train loss: 1.156327e-02 | valid loss: 1.012711e-02 
      	| train loss (relative): 2.830154e-01 | valid loss (relative): 2.475036e-01 
Epoch 441 use: 434.32 second.

epoch 442 starting......
Epoch:  442 | train loss: 1.156425e-02 | valid loss: 1.014562e-02 
      	| train loss (relative): 2.825056e-01 | valid loss (relative): 2.481939e-01 
Epoch 442 use: 483.77 second.

epoch 443 starting......
Epoch:  443 | train loss: 1.156448e-02 | valid loss: 1.015192e-02 
      	| train loss (relative): 2.831940e-01 | valid loss (relative): 2.488972e-01 
Epoch 443 use: 451.81 second.

epoch 444 starting......
Epoch:  444 | train loss: 1.156215e-02 | valid loss: 1.013748e-02 
      	| train loss (relative): 2.827615e-01 | valid loss (relative): 2.480264e-01 
Epoch 444 use: 436.24 second.

epoch 445 starting......
Epoch:  445 | train loss: 1.156292e-02 | valid loss: 1.013156e-02 
      	| train loss (relative): 2.824710e-01 | valid loss (relative): 2.487947e-01 
Epoch 445 use: 403.51 second.

epoch 446 starting......
Epoch:  446 | train loss: 1.156351e-02 | valid loss: 1.015405e-02 
      	| train loss (relative): 2.829260e-01 | valid loss (relative): 2.490447e-01 
Epoch 446 use: 402.84 second.

epoch 447 starting......
Epoch:  447 | train loss: 1.156344e-02 | valid loss: 1.013042e-02 
      	| train loss (relative): 2.833846e-01 | valid loss (relative): 2.478754e-01 
Epoch 447 use: 445.25 second.

epoch 448 starting......
Epoch:  448 | train loss: 1.156501e-02 | valid loss: 1.013454e-02 
      	| train loss (relative): 2.828557e-01 | valid loss (relative): 2.484852e-01 
Epoch 448 use: 473.31 second.

epoch 449 starting......
Epoch:  449 | train loss: 1.156437e-02 | valid loss: 1.014158e-02 
      	| train loss (relative): 2.829504e-01 | valid loss (relative): 2.487416e-01 
Epoch 449 use: 515.55 second.

epoch 450 starting......
Epoch:  450 | train loss: 1.156200e-02 | valid loss: 1.013100e-02 
      	| train loss (relative): 2.829002e-01 | valid loss (relative): 2.481346e-01 
Epoch 450 use: 439.96 second.

epoch 451 starting......
Epoch:  451 | train loss: 1.156461e-02 | valid loss: 1.011450e-02 
      	| train loss (relative): 2.826933e-01 | valid loss (relative): 2.471992e-01 
Epoch 451 use: 489.85 second.

epoch 452 starting......
Epoch:  452 | train loss: 1.156537e-02 | valid loss: 1.011627e-02 
      	| train loss (relative): 2.826417e-01 | valid loss (relative): 2.467986e-01 
Epoch 452 use: 435.57 second.

epoch 453 starting......
Epoch:  453 | train loss: 1.156191e-02 | valid loss: 1.012621e-02 
      	| train loss (relative): 2.828818e-01 | valid loss (relative): 2.468794e-01 
Epoch 453 use: 484.42 second.

epoch 454 starting......
Epoch:  454 | train loss: 1.156479e-02 | valid loss: 1.012272e-02 
      	| train loss (relative): 2.820623e-01 | valid loss (relative): 2.479431e-01 
Epoch 454 use: 399.32 second.

epoch 455 starting......
Epoch:  455 | train loss: 1.156317e-02 | valid loss: 1.014437e-02 
      	| train loss (relative): 2.830092e-01 | valid loss (relative): 2.493922e-01 
Epoch 455 use: 423.04 second.

epoch 456 starting......
Epoch:  456 | train loss: 1.156122e-02 | valid loss: 1.011605e-02 
      	| train loss (relative): 2.833796e-01 | valid loss (relative): 2.465351e-01 
Epoch 456 use: 457.28 second.

epoch 457 starting......
Epoch:  457 | train loss: 1.156133e-02 | valid loss: 1.016413e-02 
      	| train loss (relative): 2.817415e-01 | valid loss (relative): 2.503932e-01 
Epoch 457 use: 500.29 second.

epoch 458 starting......
Epoch:  458 | train loss: 1.156816e-02 | valid loss: 1.013033e-02 
      	| train loss (relative): 2.828275e-01 | valid loss (relative): 2.483111e-01 
Epoch 458 use: 416.68 second.

epoch 459 starting......
Epoch:  459 | train loss: 1.156923e-02 | valid loss: 1.014180e-02 
      	| train loss (relative): 2.833336e-01 | valid loss (relative): 2.493754e-01 
Epoch 459 use: 436.77 second.

epoch 460 starting......
Epoch:  460 | train loss: 1.156554e-02 | valid loss: 1.014642e-02 
      	| train loss (relative): 2.825212e-01 | valid loss (relative): 2.491019e-01 
Epoch 460 use: 418.48 second.

epoch 461 starting......
Epoch:  461 | train loss: 1.156482e-02 | valid loss: 1.012053e-02 
      	| train loss (relative): 2.832040e-01 | valid loss (relative): 2.478993e-01 
Epoch 461 use: 413.63 second.

epoch 462 starting......
Epoch:  462 | train loss: 1.156692e-02 | valid loss: 1.015355e-02 
      	| train loss (relative): 2.840089e-01 | valid loss (relative): 2.496020e-01 
Epoch 462 use: 451.55 second.

epoch 463 starting......
Epoch:  463 | train loss: 1.156551e-02 | valid loss: 1.012395e-02 
      	| train loss (relative): 2.838464e-01 | valid loss (relative): 2.482143e-01 
Epoch 463 use: 407.85 second.

epoch 464 starting......
Epoch:  464 | train loss: 1.156472e-02 | valid loss: 1.012760e-02 
      	| train loss (relative): 2.831545e-01 | valid loss (relative): 2.478362e-01 
Epoch 464 use: 387.38 second.

epoch 465 starting......
Epoch:  465 | train loss: 1.156472e-02 | valid loss: 1.013055e-02 
      	| train loss (relative): 2.837050e-01 | valid loss (relative): 2.480685e-01 
Epoch 465 use: 404.84 second.

epoch 466 starting......
Epoch:  466 | train loss: 1.156530e-02 | valid loss: 1.011589e-02 
      	| train loss (relative): 2.829905e-01 | valid loss (relative): 2.463010e-01 
Epoch 466 use: 415.35 second.

epoch 467 starting......
Epoch:  467 | train loss: 1.156501e-02 | valid loss: 1.013786e-02 
      	| train loss (relative): 2.823582e-01 | valid loss (relative): 2.484917e-01 
Epoch 467 use: 561.46 second.

epoch 468 starting......
Epoch:  468 | train loss: 1.156507e-02 | valid loss: 1.011981e-02 
      	| train loss (relative): 2.842495e-01 | valid loss (relative): 2.467987e-01 
Epoch 468 use: 396.88 second.

epoch 469 starting......
Epoch:  469 | train loss: 1.156426e-02 | valid loss: 1.014814e-02 
      	| train loss (relative): 2.822307e-01 | valid loss (relative): 2.485322e-01 
Epoch 469 use: 412.99 second.

epoch 470 starting......
Epoch:  470 | train loss: 1.156235e-02 | valid loss: 1.012661e-02 
      	| train loss (relative): 2.826623e-01 | valid loss (relative): 2.478444e-01 
Epoch 470 use: 412.61 second.

epoch 471 starting......
Epoch:  471 | train loss: 1.156493e-02 | valid loss: 1.016327e-02 
      	| train loss (relative): 2.820571e-01 | valid loss (relative): 2.496025e-01 
Epoch 471 use: 384.92 second.

epoch 472 starting......
Epoch:  472 | train loss: 1.156597e-02 | valid loss: 1.010881e-02 
      	| train loss (relative): 2.846829e-01 | valid loss (relative): 2.460215e-01 
Epoch 472 use: 379.05 second.

epoch 473 starting......
Epoch:  473 | train loss: 1.156496e-02 | valid loss: 1.010537e-02 
      	| train loss (relative): 2.821329e-01 | valid loss (relative): 2.464309e-01 
Epoch 473 use: 430.05 second.

epoch 474 starting......
Epoch:  474 | train loss: 1.156339e-02 | valid loss: 1.013260e-02 
      	| train loss (relative): 2.825021e-01 | valid loss (relative): 2.473115e-01 
Epoch 474 use: 388.94 second.

epoch 475 starting......
Epoch:  475 | train loss: 1.156543e-02 | valid loss: 1.013085e-02 
      	| train loss (relative): 2.831826e-01 | valid loss (relative): 2.483478e-01 
Epoch 475 use: 409.94 second.

epoch 476 starting......
Epoch:  476 | train loss: 1.156661e-02 | valid loss: 1.014296e-02 
      	| train loss (relative): 2.841131e-01 | valid loss (relative): 2.491485e-01 
Epoch 476 use: 383.49 second.

epoch 477 starting......
Epoch:  477 | train loss: 1.156988e-02 | valid loss: 1.015597e-02 
      	| train loss (relative): 2.855108e-01 | valid loss (relative): 2.489373e-01 
Epoch 477 use: 403.66 second.

epoch 478 starting......
Epoch:  478 | train loss: 1.156386e-02 | valid loss: 1.012152e-02 
      	| train loss (relative): 2.835210e-01 | valid loss (relative): 2.467105e-01 
Epoch 478 use: 408.28 second.

epoch 479 starting......
Epoch:  479 | train loss: 1.156352e-02 | valid loss: 1.013695e-02 
      	| train loss (relative): 2.821023e-01 | valid loss (relative): 2.486497e-01 
Epoch 479 use: 374.00 second.

epoch 480 starting......
Epoch:  480 | train loss: 1.156473e-02 | valid loss: 1.012852e-02 
      	| train loss (relative): 2.843890e-01 | valid loss (relative): 2.482211e-01 
Epoch 480 use: 400.77 second.

epoch 481 starting......
Epoch:  481 | train loss: 1.156197e-02 | valid loss: 1.014381e-02 
      	| train loss (relative): 2.826008e-01 | valid loss (relative): 2.489031e-01 
Epoch 481 use: 399.32 second.

epoch 482 starting......
Epoch:  482 | train loss: 1.156383e-02 | valid loss: 1.014142e-02 
      	| train loss (relative): 2.829809e-01 | valid loss (relative): 2.490845e-01 
Epoch 482 use: 467.29 second.

epoch 483 starting......
Epoch:  483 | train loss: 1.156200e-02 | valid loss: 1.012245e-02 
      	| train loss (relative): 2.832422e-01 | valid loss (relative): 2.475855e-01 
Epoch 483 use: 379.12 second.

epoch 484 starting......
Epoch:  484 | train loss: 1.156469e-02 | valid loss: 1.012241e-02 
      	| train loss (relative): 2.825150e-01 | valid loss (relative): 2.475466e-01 
Epoch 484 use: 377.57 second.

epoch 485 starting......
Epoch:  485 | train loss: 1.156353e-02 | valid loss: 1.013225e-02 
      	| train loss (relative): 2.833051e-01 | valid loss (relative): 2.479432e-01 
Epoch 485 use: 412.99 second.

epoch 486 starting......
Epoch:  486 | train loss: 1.156536e-02 | valid loss: 1.016356e-02 
      	| train loss (relative): 2.835605e-01 | valid loss (relative): 2.494181e-01 
Epoch 486 use: 440.16 second.

epoch 487 starting......
Epoch:  487 | train loss: 1.156671e-02 | valid loss: 1.016268e-02 
      	| train loss (relative): 2.832056e-01 | valid loss (relative): 2.497761e-01 
Epoch 487 use: 470.68 second.

epoch 488 starting......
Epoch:  488 | train loss: 1.156628e-02 | valid loss: 1.016990e-02 
      	| train loss (relative): 2.826011e-01 | valid loss (relative): 2.506030e-01 
Epoch 488 use: 434.10 second.

epoch 489 starting......
Epoch:  489 | train loss: 1.156152e-02 | valid loss: 1.011961e-02 
      	| train loss (relative): 2.838234e-01 | valid loss (relative): 2.474283e-01 
Epoch 489 use: 395.99 second.

epoch 490 starting......
Epoch:  490 | train loss: 1.156628e-02 | valid loss: 1.013973e-02 
      	| train loss (relative): 2.827750e-01 | valid loss (relative): 2.485564e-01 
Epoch 490 use: 379.91 second.

epoch 491 starting......
Epoch:  491 | train loss: 1.156374e-02 | valid loss: 1.010788e-02 
      	| train loss (relative): 2.840545e-01 | valid loss (relative): 2.461297e-01 
Epoch 491 use: 393.21 second.

epoch 492 starting......
Epoch:  492 | train loss: 1.156424e-02 | valid loss: 1.014241e-02 
      	| train loss (relative): 2.812874e-01 | valid loss (relative): 2.488722e-01 
Epoch 492 use: 376.51 second.

epoch 493 starting......
Epoch:  493 | train loss: 1.156566e-02 | valid loss: 1.011812e-02 
      	| train loss (relative): 2.826960e-01 | valid loss (relative): 2.466037e-01 
Epoch 493 use: 391.01 second.

epoch 494 starting......
Epoch:  494 | train loss: 1.156301e-02 | valid loss: 1.013637e-02 
      	| train loss (relative): 2.835523e-01 | valid loss (relative): 2.483968e-01 
Epoch 494 use: 389.83 second.

epoch 495 starting......
Epoch:  495 | train loss: 1.156372e-02 | valid loss: 1.014823e-02 
      	| train loss (relative): 2.820839e-01 | valid loss (relative): 2.493736e-01 
Epoch 495 use: 375.50 second.

epoch 496 starting......
Epoch:  496 | train loss: 1.156403e-02 | valid loss: 1.016249e-02 
      	| train loss (relative): 2.827672e-01 | valid loss (relative): 2.491156e-01 
Epoch 496 use: 421.60 second.

epoch 497 starting......
Epoch:  497 | train loss: 1.156355e-02 | valid loss: 1.014409e-02 
      	| train loss (relative): 2.835142e-01 | valid loss (relative): 2.492549e-01 
Epoch 497 use: 397.56 second.

epoch 498 starting......
Epoch:  498 | train loss: 1.156253e-02 | valid loss: 1.011950e-02 
      	| train loss (relative): 2.828970e-01 | valid loss (relative): 2.466898e-01 
Epoch 498 use: 364.06 second.

epoch 499 starting......
Epoch:  499 | train loss: 1.156564e-02 | valid loss: 1.010772e-02 
      	| train loss (relative): 2.828628e-01 | valid loss (relative): 2.465496e-01 
Epoch 499 use: 369.01 second.

epoch 500 starting......
Epoch:  500 | train loss: 1.156390e-02 | valid loss: 1.012031e-02 
      	| train loss (relative): 2.836490e-01 | valid loss (relative): 2.478800e-01 
Epoch 500 use: 379.85 second.

epoch 501 starting......
Epoch:  501 | train loss: 1.156869e-02 | valid loss: 1.012218e-02 
      	| train loss (relative): 2.836764e-01 | valid loss (relative): 2.477730e-01 
Epoch 501 use: 360.80 second.

epoch 502 starting......
Epoch:  502 | train loss: 1.156445e-02 | valid loss: 1.014165e-02 
      	| train loss (relative): 2.820853e-01 | valid loss (relative): 2.487324e-01 
Epoch 502 use: 372.50 second.

epoch 503 starting......
Epoch:  503 | train loss: 1.156372e-02 | valid loss: 1.016291e-02 
      	| train loss (relative): 2.826994e-01 | valid loss (relative): 2.498324e-01 
Epoch 503 use: 415.80 second.

epoch 504 starting......
Epoch:  504 | train loss: 1.156450e-02 | valid loss: 1.016197e-02 
      	| train loss (relative): 2.832386e-01 | valid loss (relative): 2.492977e-01 
Epoch 504 use: 398.63 second.

epoch 505 starting......
Epoch:  505 | train loss: 1.156400e-02 | valid loss: 1.014209e-02 
      	| train loss (relative): 2.831166e-01 | valid loss (relative): 2.482925e-01 
Epoch 505 use: 368.82 second.

epoch 506 starting......
Epoch:  506 | train loss: 1.157044e-02 | valid loss: 1.014770e-02 
      	| train loss (relative): 2.848631e-01 | valid loss (relative): 2.488571e-01 
Epoch 506 use: 404.81 second.

epoch 507 starting......
Epoch:  507 | train loss: 1.156543e-02 | valid loss: 1.014341e-02 
      	| train loss (relative): 2.837096e-01 | valid loss (relative): 2.494690e-01 
Epoch 507 use: 366.86 second.

epoch 508 starting......
Epoch:  508 | train loss: 1.156668e-02 | valid loss: 1.013026e-02 
      	| train loss (relative): 2.832090e-01 | valid loss (relative): 2.482861e-01 
Epoch 508 use: 374.74 second.

epoch 509 starting......
Epoch:  509 | train loss: 1.156601e-02 | valid loss: 1.012361e-02 
      	| train loss (relative): 2.856078e-01 | valid loss (relative): 2.476500e-01 
Epoch 509 use: 370.44 second.

epoch 510 starting......
Epoch:  510 | train loss: 1.156817e-02 | valid loss: 1.011393e-02 
      	| train loss (relative): 2.835910e-01 | valid loss (relative): 2.465690e-01 
Epoch 510 use: 368.85 second.

epoch 511 starting......
Epoch:  511 | train loss: 1.156768e-02 | valid loss: 1.012610e-02 
      	| train loss (relative): 2.818116e-01 | valid loss (relative): 2.478297e-01 
Epoch 511 use: 362.54 second.

epoch 512 starting......
Epoch:  512 | train loss: 1.156358e-02 | valid loss: 1.012523e-02 
      	| train loss (relative): 2.832647e-01 | valid loss (relative): 2.477125e-01 
Epoch 512 use: 389.76 second.

epoch 513 starting......
Epoch:  513 | train loss: 1.156615e-02 | valid loss: 1.018054e-02 
      	| train loss (relative): 2.815791e-01 | valid loss (relative): 2.501671e-01 
Epoch 513 use: 401.71 second.

epoch 514 starting......
Epoch:  514 | train loss: 1.156481e-02 | valid loss: 1.013600e-02 
      	| train loss (relative): 2.839921e-01 | valid loss (relative): 2.483393e-01 
Epoch 514 use: 415.00 second.

epoch 515 starting......
Epoch:  515 | train loss: 1.156730e-02 | valid loss: 1.015476e-02 
      	| train loss (relative): 2.831892e-01 | valid loss (relative): 2.495525e-01 
Epoch 515 use: 411.19 second.

epoch 516 starting......
Epoch:  516 | train loss: 1.156477e-02 | valid loss: 1.013658e-02 
      	| train loss (relative): 2.845544e-01 | valid loss (relative): 2.479578e-01 
Epoch 516 use: 366.18 second.

epoch 517 starting......
Epoch:  517 | train loss: 1.156341e-02 | valid loss: 1.011405e-02 
      	| train loss (relative): 2.821127e-01 | valid loss (relative): 2.470053e-01 
Epoch 517 use: 391.02 second.

epoch 518 starting......
Epoch:  518 | train loss: 1.156775e-02 | valid loss: 1.011857e-02 
      	| train loss (relative): 2.827821e-01 | valid loss (relative): 2.472788e-01 
Epoch 518 use: 364.94 second.

epoch 519 starting......
Epoch:  519 | train loss: 1.156152e-02 | valid loss: 1.014376e-02 
      	| train loss (relative): 2.818230e-01 | valid loss (relative): 2.497329e-01 
Epoch 519 use: 382.59 second.

epoch 520 starting......
Epoch:  520 | train loss: 1.156585e-02 | valid loss: 1.015232e-02 
      	| train loss (relative): 2.836693e-01 | valid loss (relative): 2.497908e-01 
Epoch 520 use: 355.41 second.

epoch 521 starting......
Epoch:  521 | train loss: 1.156368e-02 | valid loss: 1.012690e-02 
      	| train loss (relative): 2.831184e-01 | valid loss (relative): 2.472430e-01 
Epoch 521 use: 363.50 second.

epoch 522 starting......
Epoch:  522 | train loss: 1.156394e-02 | valid loss: 1.012297e-02 
      	| train loss (relative): 2.831323e-01 | valid loss (relative): 2.479804e-01 
Epoch 522 use: 371.47 second.

epoch 523 starting......
Epoch:  523 | train loss: 1.156345e-02 | valid loss: 1.011284e-02 
      	| train loss (relative): 2.831592e-01 | valid loss (relative): 2.464214e-01 
Epoch 523 use: 361.13 second.

epoch 524 starting......
Epoch:  524 | train loss: 1.156514e-02 | valid loss: 1.012330e-02 
      	| train loss (relative): 2.817320e-01 | valid loss (relative): 2.477699e-01 
Epoch 524 use: 362.30 second.

epoch 525 starting......
Epoch:  525 | train loss: 1.156670e-02 | valid loss: 1.015115e-02 
      	| train loss (relative): 2.862450e-01 | valid loss (relative): 2.486158e-01 
Epoch 525 use: 370.68 second.

epoch 526 starting......
Epoch:  526 | train loss: 1.156287e-02 | valid loss: 1.015140e-02 
      	| train loss (relative): 2.818761e-01 | valid loss (relative): 2.489722e-01 
Epoch 526 use: 463.19 second.

epoch 527 starting......
Epoch:  527 | train loss: 1.156262e-02 | valid loss: 1.012176e-02 
      	| train loss (relative): 2.841192e-01 | valid loss (relative): 2.473285e-01 
Epoch 527 use: 407.72 second.

epoch 528 starting......
Epoch:  528 | train loss: 1.156400e-02 | valid loss: 1.011400e-02 
      	| train loss (relative): 2.824804e-01 | valid loss (relative): 2.465705e-01 
Epoch 528 use: 377.55 second.

epoch 529 starting......
Epoch:  529 | train loss: 1.156280e-02 | valid loss: 1.013979e-02 
      	| train loss (relative): 2.817594e-01 | valid loss (relative): 2.485286e-01 
Epoch 529 use: 377.80 second.

epoch 530 starting......
Epoch:  530 | train loss: 1.156283e-02 | valid loss: 1.014492e-02 
      	| train loss (relative): 2.829143e-01 | valid loss (relative): 2.491046e-01 
Epoch 530 use: 371.23 second.

epoch 531 starting......
Epoch:  531 | train loss: 1.156412e-02 | valid loss: 1.013403e-02 
      	| train loss (relative): 2.838855e-01 | valid loss (relative): 2.482151e-01 
Epoch 531 use: 421.80 second.

epoch 532 starting......
Epoch:  532 | train loss: 1.156236e-02 | valid loss: 1.013682e-02 
      	| train loss (relative): 2.823347e-01 | valid loss (relative): 2.474308e-01 
Epoch 532 use: 393.51 second.

epoch 533 starting......
Epoch:  533 | train loss: 1.156441e-02 | valid loss: 1.014422e-02 
      	| train loss (relative): 2.830209e-01 | valid loss (relative): 2.487291e-01 
Epoch 533 use: 372.58 second.

epoch 534 starting......
Epoch:  534 | train loss: 1.156327e-02 | valid loss: 1.011626e-02 
      	| train loss (relative): 2.845367e-01 | valid loss (relative): 2.469459e-01 
Epoch 534 use: 367.77 second.

epoch 535 starting......
Epoch:  535 | train loss: 1.156622e-02 | valid loss: 1.010438e-02 
      	| train loss (relative): 2.816864e-01 | valid loss (relative): 2.461980e-01 
Epoch 535 use: 369.75 second.

epoch 536 starting......
Epoch:  536 | train loss: 1.156380e-02 | valid loss: 1.011841e-02 
      	| train loss (relative): 2.832829e-01 | valid loss (relative): 2.471319e-01 
Epoch 536 use: 372.24 second.

epoch 537 starting......
Epoch:  537 | train loss: 1.157150e-02 | valid loss: 1.010007e-02 
      	| train loss (relative): 2.838146e-01 | valid loss (relative): 2.455249e-01 
Epoch 537 use: 396.25 second.

epoch 538 starting......
Epoch:  538 | train loss: 1.156772e-02 | valid loss: 1.011719e-02 
      	| train loss (relative): 2.809420e-01 | valid loss (relative): 2.468686e-01 
Epoch 538 use: 366.63 second.

epoch 539 starting......
Epoch:  539 | train loss: 1.156563e-02 | valid loss: 1.013771e-02 
      	| train loss (relative): 2.825326e-01 | valid loss (relative): 2.486341e-01 
Epoch 539 use: 403.97 second.

epoch 540 starting......
Epoch:  540 | train loss: 1.156450e-02 | valid loss: 1.012679e-02 
      	| train loss (relative): 2.830132e-01 | valid loss (relative): 2.473138e-01 
Epoch 540 use: 372.90 second.

epoch 541 starting......
Epoch:  541 | train loss: 1.156262e-02 | valid loss: 1.013192e-02 
      	| train loss (relative): 2.830112e-01 | valid loss (relative): 2.472404e-01 
Epoch 541 use: 365.89 second.

epoch 542 starting......
Epoch:  542 | train loss: 1.156581e-02 | valid loss: 1.013321e-02 
      	| train loss (relative): 2.825471e-01 | valid loss (relative): 2.474842e-01 
Epoch 542 use: 352.08 second.

epoch 543 starting......
Epoch:  543 | train loss: 1.156307e-02 | valid loss: 1.014126e-02 
      	| train loss (relative): 2.825623e-01 | valid loss (relative): 2.482781e-01 
Epoch 543 use: 368.77 second.

epoch 544 starting......
Epoch:  544 | train loss: 1.156462e-02 | valid loss: 1.013100e-02 
      	| train loss (relative): 2.834946e-01 | valid loss (relative): 2.483203e-01 
Epoch 544 use: 357.89 second.

epoch 545 starting......
Epoch:  545 | train loss: 1.156623e-02 | valid loss: 1.019576e-02 
      	| train loss (relative): 2.829241e-01 | valid loss (relative): 2.517794e-01 
Epoch 545 use: 400.79 second.

epoch 546 starting......
Epoch:  546 | train loss: 1.156333e-02 | valid loss: 1.013647e-02 
      	| train loss (relative): 2.840137e-01 | valid loss (relative): 2.481395e-01 
Epoch 546 use: 371.77 second.

epoch 547 starting......
Epoch:  547 | train loss: 1.156101e-02 | valid loss: 1.011168e-02 
      	| train loss (relative): 2.834936e-01 | valid loss (relative): 2.459877e-01 
Epoch 547 use: 375.47 second.

epoch 548 starting......
Epoch:  548 | train loss: 1.156560e-02 | valid loss: 1.012419e-02 
      	| train loss (relative): 2.822720e-01 | valid loss (relative): 2.473060e-01 
Epoch 548 use: 350.09 second.

epoch 549 starting......
Epoch:  549 | train loss: 1.157133e-02 | valid loss: 1.010330e-02 
      	| train loss (relative): 2.830923e-01 | valid loss (relative): 2.463115e-01 
Epoch 549 use: 362.82 second.

epoch 550 starting......
Epoch:  550 | train loss: 1.156436e-02 | valid loss: 1.012406e-02 
      	| train loss (relative): 2.828437e-01 | valid loss (relative): 2.471803e-01 
Epoch 550 use: 424.41 second.

epoch 551 starting......
Epoch:  551 | train loss: 1.156746e-02 | valid loss: 1.012231e-02 
      	| train loss (relative): 2.835820e-01 | valid loss (relative): 2.470778e-01 
Epoch 551 use: 420.35 second.

epoch 552 starting......
Epoch:  552 | train loss: 1.156367e-02 | valid loss: 1.013335e-02 
      	| train loss (relative): 2.821820e-01 | valid loss (relative): 2.477563e-01 
Epoch 552 use: 356.44 second.

epoch 553 starting......
Epoch:  553 | train loss: 1.156283e-02 | valid loss: 1.014106e-02 
      	| train loss (relative): 2.826962e-01 | valid loss (relative): 2.488497e-01 
Epoch 553 use: 443.61 second.

epoch 554 starting......
Epoch:  554 | train loss: 1.156330e-02 | valid loss: 1.014411e-02 
      	| train loss (relative): 2.828535e-01 | valid loss (relative): 2.480936e-01 
Epoch 554 use: 367.22 second.

epoch 555 starting......
Epoch:  555 | train loss: 1.156289e-02 | valid loss: 1.013392e-02 
      	| train loss (relative): 2.830296e-01 | valid loss (relative): 2.479783e-01 
Epoch 555 use: 386.37 second.

epoch 556 starting......
Epoch:  556 | train loss: 1.156415e-02 | valid loss: 1.011396e-02 
      	| train loss (relative): 2.837754e-01 | valid loss (relative): 2.460052e-01 
Epoch 556 use: 367.41 second.

epoch 557 starting......
Epoch:  557 | train loss: 1.156334e-02 | valid loss: 1.013863e-02 
      	| train loss (relative): 2.823216e-01 | valid loss (relative): 2.484468e-01 
Epoch 557 use: 407.61 second.

epoch 558 starting......
Epoch:  558 | train loss: 1.156435e-02 | valid loss: 1.011428e-02 
      	| train loss (relative): 2.829355e-01 | valid loss (relative): 2.465261e-01 
Epoch 558 use: 357.65 second.

epoch 559 starting......
Epoch:  559 | train loss: 1.156237e-02 | valid loss: 1.017443e-02 
      	| train loss (relative): 2.818187e-01 | valid loss (relative): 2.501742e-01 
Epoch 559 use: 405.06 second.

epoch 560 starting......
Epoch:  560 | train loss: 1.156161e-02 | valid loss: 1.013414e-02 
      	| train loss (relative): 2.837585e-01 | valid loss (relative): 2.479733e-01 
Epoch 560 use: 444.23 second.

epoch 561 starting......
Epoch:  561 | train loss: 1.156742e-02 | valid loss: 1.011967e-02 
      	| train loss (relative): 2.828732e-01 | valid loss (relative): 2.472755e-01 
Epoch 561 use: 389.54 second.

epoch 562 starting......
Epoch:  562 | train loss: 1.156673e-02 | valid loss: 1.013183e-02 
      	| train loss (relative): 2.815889e-01 | valid loss (relative): 2.484605e-01 
Epoch 562 use: 367.00 second.

epoch 563 starting......
Epoch:  563 | train loss: 1.156563e-02 | valid loss: 1.013688e-02 
      	| train loss (relative): 2.839096e-01 | valid loss (relative): 2.493474e-01 
Epoch 563 use: 375.87 second.

epoch 564 starting......
Epoch:  564 | train loss: 1.156603e-02 | valid loss: 1.014268e-02 
      	| train loss (relative): 2.837602e-01 | valid loss (relative): 2.488554e-01 
Epoch 564 use: 389.17 second.

epoch 565 starting......
Epoch:  565 | train loss: 1.156555e-02 | valid loss: 1.014911e-02 
      	| train loss (relative): 2.827332e-01 | valid loss (relative): 2.495047e-01 
Epoch 565 use: 374.87 second.

epoch 566 starting......
Epoch:  566 | train loss: 1.156254e-02 | valid loss: 1.014482e-02 
      	| train loss (relative): 2.825025e-01 | valid loss (relative): 2.483494e-01 
Epoch 566 use: 369.77 second.

epoch 567 starting......
Epoch:  567 | train loss: 1.156281e-02 | valid loss: 1.015564e-02 
      	| train loss (relative): 2.818797e-01 | valid loss (relative): 2.502960e-01 
Epoch 567 use: 406.47 second.

epoch 568 starting......
Epoch:  568 | train loss: 1.156616e-02 | valid loss: 1.015503e-02 
      	| train loss (relative): 2.840773e-01 | valid loss (relative): 2.490517e-01 
Epoch 568 use: 380.02 second.

epoch 569 starting......
Epoch:  569 | train loss: 1.156409e-02 | valid loss: 1.012626e-02 
      	| train loss (relative): 2.830504e-01 | valid loss (relative): 2.478416e-01 
Epoch 569 use: 365.41 second.

epoch 570 starting......
Epoch:  570 | train loss: 1.156498e-02 | valid loss: 1.012435e-02 
      	| train loss (relative): 2.841357e-01 | valid loss (relative): 2.477449e-01 
Epoch 570 use: 373.43 second.

epoch 571 starting......
Epoch:  571 | train loss: 1.156368e-02 | valid loss: 1.014347e-02 
      	| train loss (relative): 2.826961e-01 | valid loss (relative): 2.474632e-01 
Epoch 571 use: 375.79 second.

epoch 572 starting......
Epoch:  572 | train loss: 1.156651e-02 | valid loss: 1.012479e-02 
      	| train loss (relative): 2.820074e-01 | valid loss (relative): 2.483002e-01 
Epoch 572 use: 444.22 second.

epoch 573 starting......
Epoch:  573 | train loss: 1.156588e-02 | valid loss: 1.014651e-02 
      	| train loss (relative): 2.833509e-01 | valid loss (relative): 2.494761e-01 
Epoch 573 use: 369.97 second.

epoch 574 starting......
Epoch:  574 | train loss: 1.156318e-02 | valid loss: 1.010577e-02 
      	| train loss (relative): 2.838209e-01 | valid loss (relative): 2.464515e-01 
Epoch 574 use: 463.78 second.

epoch 575 starting......
Epoch:  575 | train loss: 1.157078e-02 | valid loss: 1.014898e-02 
      	| train loss (relative): 2.843086e-01 | valid loss (relative): 2.483474e-01 
Epoch 575 use: 403.72 second.

epoch 576 starting......
Epoch:  576 | train loss: 1.156419e-02 | valid loss: 1.014692e-02 
      	| train loss (relative): 2.830218e-01 | valid loss (relative): 2.483325e-01 
Epoch 576 use: 370.73 second.

epoch 577 starting......
Epoch:  577 | train loss: 1.156352e-02 | valid loss: 1.012718e-02 
      	| train loss (relative): 2.836494e-01 | valid loss (relative): 2.480566e-01 
Epoch 577 use: 379.24 second.

epoch 578 starting......
Epoch:  578 | train loss: 1.156248e-02 | valid loss: 1.013070e-02 
      	| train loss (relative): 2.814651e-01 | valid loss (relative): 2.483984e-01 
Epoch 578 use: 375.71 second.

epoch 579 starting......
Epoch:  579 | train loss: 1.156439e-02 | valid loss: 1.013297e-02 
      	| train loss (relative): 2.826737e-01 | valid loss (relative): 2.479181e-01 
Epoch 579 use: 408.34 second.

epoch 580 starting......
Epoch:  580 | train loss: 1.156493e-02 | valid loss: 1.012105e-02 
      	| train loss (relative): 2.841054e-01 | valid loss (relative): 2.473676e-01 
Epoch 580 use: 372.25 second.

epoch 581 starting......
Epoch:  581 | train loss: 1.156839e-02 | valid loss: 1.013150e-02 
      	| train loss (relative): 2.819813e-01 | valid loss (relative): 2.484177e-01 
Epoch 581 use: 407.60 second.

epoch 582 starting......
Epoch:  582 | train loss: 1.156248e-02 | valid loss: 1.012502e-02 
      	| train loss (relative): 2.838253e-01 | valid loss (relative): 2.471239e-01 
Epoch 582 use: 380.04 second.

epoch 583 starting......
Epoch:  583 | train loss: 1.156340e-02 | valid loss: 1.012530e-02 
      	| train loss (relative): 2.829833e-01 | valid loss (relative): 2.479635e-01 
Epoch 583 use: 369.79 second.

epoch 584 starting......
Epoch:  584 | train loss: 1.156618e-02 | valid loss: 1.013114e-02 
      	| train loss (relative): 2.825215e-01 | valid loss (relative): 2.474803e-01 
Epoch 584 use: 386.87 second.

epoch 585 starting......
Epoch:  585 | train loss: 1.156616e-02 | valid loss: 1.010253e-02 
      	| train loss (relative): 2.830458e-01 | valid loss (relative): 2.461455e-01 
Epoch 585 use: 381.34 second.

epoch 586 starting......
Epoch:  586 | train loss: 1.156375e-02 | valid loss: 1.013626e-02 
      	| train loss (relative): 2.810778e-01 | valid loss (relative): 2.484965e-01 
Epoch 586 use: 422.00 second.

epoch 587 starting......
Epoch:  587 | train loss: 1.156214e-02 | valid loss: 1.014139e-02 
      	| train loss (relative): 2.830344e-01 | valid loss (relative): 2.488992e-01 
Epoch 587 use: 396.77 second.

epoch 588 starting......
Epoch:  588 | train loss: 1.156355e-02 | valid loss: 1.012206e-02 
      	| train loss (relative): 2.834544e-01 | valid loss (relative): 2.469080e-01 
Epoch 588 use: 378.80 second.

epoch 589 starting......
Epoch:  589 | train loss: 1.156279e-02 | valid loss: 1.014157e-02 
      	| train loss (relative): 2.828708e-01 | valid loss (relative): 2.488612e-01 
Epoch 589 use: 376.70 second.

epoch 590 starting......
Epoch:  590 | train loss: 1.156439e-02 | valid loss: 1.014157e-02 
      	| train loss (relative): 2.833800e-01 | valid loss (relative): 2.483382e-01 
Epoch 590 use: 409.41 second.

epoch 591 starting......
Epoch:  591 | train loss: 1.156404e-02 | valid loss: 1.013315e-02 
      	| train loss (relative): 2.826453e-01 | valid loss (relative): 2.479156e-01 
Epoch 591 use: 380.36 second.

epoch 592 starting......
Epoch:  592 | train loss: 1.156342e-02 | valid loss: 1.011219e-02 
      	| train loss (relative): 2.831929e-01 | valid loss (relative): 2.465592e-01 
Epoch 592 use: 457.03 second.

epoch 593 starting......
Epoch:  593 | train loss: 1.156401e-02 | valid loss: 1.014320e-02 
      	| train loss (relative): 2.827017e-01 | valid loss (relative): 2.488750e-01 
Epoch 593 use: 379.86 second.

epoch 594 starting......
Epoch:  594 | train loss: 1.156294e-02 | valid loss: 1.013168e-02 
      	| train loss (relative): 2.832910e-01 | valid loss (relative): 2.480460e-01 
Epoch 594 use: 437.11 second.

epoch 595 starting......
Epoch:  595 | train loss: 1.157225e-02 | valid loss: 1.011712e-02 
      	| train loss (relative): 2.840120e-01 | valid loss (relative): 2.475516e-01 
Epoch 595 use: 378.60 second.

epoch 596 starting......
Epoch:  596 | train loss: 1.157119e-02 | valid loss: 1.015890e-02 
      	| train loss (relative): 2.850796e-01 | valid loss (relative): 2.503313e-01 
Epoch 596 use: 410.34 second.

epoch 597 starting......
Epoch:  597 | train loss: 1.156461e-02 | valid loss: 1.013402e-02 
      	| train loss (relative): 2.833703e-01 | valid loss (relative): 2.480622e-01 
Epoch 597 use: 420.22 second.

epoch 598 starting......
Epoch:  598 | train loss: 1.156359e-02 | valid loss: 1.012791e-02 
      	| train loss (relative): 2.827654e-01 | valid loss (relative): 2.479913e-01 
Epoch 598 use: 423.65 second.

epoch 599 starting......
Epoch:  599 | train loss: 1.156344e-02 | valid loss: 1.013368e-02 
      	| train loss (relative): 2.833599e-01 | valid loss (relative): 2.477503e-01 
Epoch 599 use: 442.95 second.

test MSE Error: 1.148577e-02 | relative MSE Error: 2.808065e-01 
 Total time used for training: 22.31 hour.
MESLoss saved to  /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_MSELoss_Latent_64_nearest_neighbouring_True_SFC_nums_3_startlr_0.0005_n_epoches_600.txt
relative MSELoss saved to  /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_reMSELoss_Latent_64_nearest_neighbouring_True_SFC_nums_3_startlr_0.0005_n_epoches_600.txt
model saved to /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_64_Nearest_neighbouring_True_SFC_nums_3_startlr_0.0005_n_epoches_600.pth
model_dict saved to /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_64_Nearest_neighbouring_True_SFC_nums_3_startlr_0.0005_n_epoches_600_dict.pth
... Training slugflow data group 3 completed, Run finished Thu  5 Aug 22:02:14 BST 2021 ...
{'data_dir': '/rds/general/user/jy220/home/tensors/', 'data_type': 'tensors', 'vtu_fields': 'Component1::ComponentMassFractionPhase1, phase1::Velocity', 'variational': 'False', 'structured': 'False', 'changing_lr': 'False', 'self_concat': '2', 'dimension': '3', 'sfc_nums': '3', 'components': '4', 'nearest_neighbouring': 'True', 'dims_latent': '64', 'activation': 'Tanh', 'sfc_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/sfcs.pt', 'inv_sfc_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/inv_sfcs.pt', 'coords_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/coords.pt', 'cells_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/cells.npy', 'state_load': 'None', 'batch_size': '16', 'lr': '0.05', 'n_epoches': '100', 'seed': '32', 'visualize': 'False', 'reconstructed_path': 'None', 'reconstruct_start_index': '0', 'reconstruct_end_index': '50', 'save_path': '/rds/general/user/jy220/home/new_results/'}
[[ 2.50825269e-01 -1.67256990e-02 -1.22268730e-03]
 [ 2.38611959e-01 -5.50627777e-03 -5.06451913e-03]
 [ 2.39454874e-01 -1.59563257e-02  3.96778323e-03]
 ...
 [ 9.11361096e+00 -2.21908381e-02  2.22384902e-03]
 [ 9.11847310e+00 -2.84346931e-02 -2.34839590e-04]
 [ 9.11039661e+00 -2.51498573e-02 -8.70661044e-03]]
{'tetra': array([[      0,       1,       2,       3],
       [      4,       5,       6,       7],
       [      8,       9,      10,      11],
       ...,
       [1342744, 1342745, 1342746, 1342747],
       [1342748, 1342749, 1342750, 1342751],
       [1342752, 1342753, 1342754, 1342755]])}
structured  False 
 activation  Tanh() 
 self concat  2 
 sfc_nums  3 

dims_latent  64 
 components  4 
 nearest_neighbouring  True 

visualize  False 
 sample number  1706 

reading sfc nums......
[array([  41684,   41685,   41674, ..., 1235560, 1235564, 1235558]), array([954289, 954290, 954291, ..., 420685, 420687, 420688]), array([  40156,   40155,   40158, ..., 1123478, 1123475, 1123467])]
reading inverse sfc nums......
[array([   8146,    8145,    8144, ..., 1274444, 1274445, 1273166]), array([547280, 547283, 593037, ..., 933933, 933932, 864488]), array([ 66437,  66439, 122512, ..., 459416, 415130, 459418])]
Computing min and max......

tk:  tensor([0.7581, 0.0418, 0.1840, 0.1464], dtype=torch.float64) 

tb:  tensor([-0.3816, -0.3618, -0.0308,  0.1446], dtype=torch.float64) 

length of train set: 1505 

length of valid set: 100 

length of test set: 101 

torch device num: 4 

Let's use 4 GPUs!
epoch 0 starting......
... Training slugflow data group 3 completed, Run finished Fri  6 Aug 02:07:56 BST 2021 ...
{'data_dir': '/rds/general/user/jy220/home/tensors/', 'data_type': 'tensors', 'vtu_fields': 'Component1::ComponentMassFractionPhase1, phase1::Velocity', 'variational': 'False', 'structured': 'False', 'changing_lr': 'False', 'self_concat': '2', 'dimension': '3', 'sfc_nums': '3', 'components': '4', 'nearest_neighbouring': 'True', 'dims_latent': '64', 'activation': 'Tanh', 'sfc_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/sfcs.pt', 'inv_sfc_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/inv_sfcs.pt', 'coords_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/coords.pt', 'cells_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/cells.npy', 'state_load': 'None', 'batch_size': '16', 'lr': '0.05', 'n_epoches': '100', 'seed': '32', 'visualize': 'False', 'reconstructed_path': 'None', 'reconstruct_start_index': '0', 'reconstruct_end_index': '50', 'save_path': '/rds/general/user/jy220/home/new_results/'}
[[ 2.50825269e-01 -1.67256990e-02 -1.22268730e-03]
 [ 2.38611959e-01 -5.50627777e-03 -5.06451913e-03]
 [ 2.39454874e-01 -1.59563257e-02  3.96778323e-03]
 ...
 [ 9.11361096e+00 -2.21908381e-02  2.22384902e-03]
 [ 9.11847310e+00 -2.84346931e-02 -2.34839590e-04]
 [ 9.11039661e+00 -2.51498573e-02 -8.70661044e-03]]
{'tetra': array([[      0,       1,       2,       3],
       [      4,       5,       6,       7],
       [      8,       9,      10,      11],
       ...,
       [1342744, 1342745, 1342746, 1342747],
       [1342748, 1342749, 1342750, 1342751],
       [1342752, 1342753, 1342754, 1342755]])}
structured  False 
 activation  Tanh() 
 self concat  2 
 sfc_nums  3 

dims_latent  64 
 components  4 
 nearest_neighbouring  True 

visualize  False 
 sample number  1706 

reading sfc nums......
[array([  41684,   41685,   41674, ..., 1235560, 1235564, 1235558]), array([954289, 954290, 954291, ..., 420685, 420687, 420688]), array([  40156,   40155,   40158, ..., 1123478, 1123475, 1123467])]
reading inverse sfc nums......
[array([   8146,    8145,    8144, ..., 1274444, 1274445, 1273166]), array([547280, 547283, 593037, ..., 933933, 933932, 864488]), array([ 66437,  66439, 122512, ..., 459416, 415130, 459418])]
Computing min and max......

tk:  tensor([0.7581, 0.0418, 0.1840, 0.1464], dtype=torch.float64) 

tb:  tensor([-0.3816, -0.3618, -0.0308,  0.1446], dtype=torch.float64) 

length of train set: 1505 

length of valid set: 100 

length of test set: 101 

torch device num: 4 

Let's use 4 GPUs!
epoch 0 starting......
Epoch:  0 | train loss: 2.154709e-02 | valid loss: 1.129893e-02 
      	| train loss (relative): 7.293718e+00 | valid loss (relative): 2.783321e-01 
Epoch 0 use: 540.52 second.

epoch 1 starting......
Epoch:  1 | train loss: 1.179458e-02 | valid loss: 1.131897e-02 
      	| train loss (relative): 2.871651e-01 | valid loss (relative): 2.777713e-01 
Epoch 1 use: 450.35 second.

epoch 2 starting......
Epoch:  2 | train loss: 1.169066e-02 | valid loss: 1.132599e-02 
      	| train loss (relative): 2.833663e-01 | valid loss (relative): 2.857604e-01 
Epoch 2 use: 448.42 second.

epoch 3 starting......
Epoch:  3 | train loss: 1.171947e-02 | valid loss: 1.121819e-02 
      	| train loss (relative): 2.850313e-01 | valid loss (relative): 2.717052e-01 
Epoch 3 use: 448.60 second.

epoch 4 starting......
Epoch:  4 | train loss: 1.170489e-02 | valid loss: 1.144527e-02 
      	| train loss (relative): 2.844076e-01 | valid loss (relative): 2.799474e-01 
Epoch 4 use: 474.05 second.

epoch 5 starting......
Epoch:  5 | train loss: 1.172484e-02 | valid loss: 1.129854e-02 
      	| train loss (relative): 2.865229e-01 | valid loss (relative): 2.625617e-01 
Epoch 5 use: 492.30 second.

epoch 6 starting......
Epoch:  6 | train loss: 1.167741e-02 | valid loss: 1.126712e-02 
      	| train loss (relative): 2.836142e-01 | valid loss (relative): 2.763257e-01 
Epoch 6 use: 478.20 second.

epoch 7 starting......
Epoch:  7 | train loss: 1.169782e-02 | valid loss: 1.127944e-02 
      	| train loss (relative): 2.846760e-01 | valid loss (relative): 2.744616e-01 
Epoch 7 use: 462.52 second.

epoch 8 starting......
Epoch:  8 | train loss: 1.168730e-02 | valid loss: 1.129065e-02 
      	| train loss (relative): 2.842901e-01 | valid loss (relative): 2.685503e-01 
Epoch 8 use: 465.09 second.

epoch 9 starting......
Epoch:  9 | train loss: 1.169303e-02 | valid loss: 1.154924e-02 
      	| train loss (relative): 2.835799e-01 | valid loss (relative): 2.985277e-01 
Epoch 9 use: 505.86 second.

epoch 10 starting......
Epoch:  10 | train loss: 1.167481e-02 | valid loss: 1.128417e-02 
      	| train loss (relative): 2.849921e-01 | valid loss (relative): 2.720276e-01 
Epoch 10 use: 537.21 second.

epoch 11 starting......
Epoch:  11 | train loss: 1.174541e-02 | valid loss: 1.126252e-02 
      	| train loss (relative): 2.849493e-01 | valid loss (relative): 2.700249e-01 
Epoch 11 use: 482.58 second.

epoch 12 starting......
Epoch:  12 | train loss: 1.167785e-02 | valid loss: 1.135965e-02 
      	| train loss (relative): 2.858005e-01 | valid loss (relative): 2.639961e-01 
Epoch 12 use: 483.41 second.

epoch 13 starting......
Epoch:  13 | train loss: 1.167091e-02 | valid loss: 1.150676e-02 
      	| train loss (relative): 2.829122e-01 | valid loss (relative): 2.826052e-01 
Epoch 13 use: 474.61 second.

epoch 14 starting......
Epoch:  14 | train loss: 1.176061e-02 | valid loss: 1.130527e-02 
      	| train loss (relative): 2.858228e-01 | valid loss (relative): 2.791640e-01 
Epoch 14 use: 486.07 second.

epoch 15 starting......
Epoch:  15 | train loss: 1.172824e-02 | valid loss: 1.128377e-02 
      	| train loss (relative): 2.850903e-01 | valid loss (relative): 2.751129e-01 
Epoch 15 use: 467.48 second.

epoch 16 starting......
Epoch:  16 | train loss: 1.175173e-02 | valid loss: 1.123614e-02 
      	| train loss (relative): 2.853709e-01 | valid loss (relative): 2.762790e-01 
Epoch 16 use: 502.59 second.

epoch 17 starting......
Epoch:  17 | train loss: 1.171179e-02 | valid loss: 1.133548e-02 
      	| train loss (relative): 2.844677e-01 | valid loss (relative): 2.773287e-01 
Epoch 17 use: 492.66 second.

epoch 18 starting......
Epoch:  18 | train loss: 1.167179e-02 | valid loss: 1.152791e-02 
      	| train loss (relative): 2.832983e-01 | valid loss (relative): 2.975788e-01 
Epoch 18 use: 496.84 second.

epoch 19 starting......
Epoch:  19 | train loss: 1.174919e-02 | valid loss: 1.135982e-02 
      	| train loss (relative): 2.880074e-01 | valid loss (relative): 2.864711e-01 
Epoch 19 use: 471.67 second.

epoch 20 starting......
Epoch:  20 | train loss: 1.164752e-02 | valid loss: 1.118163e-02 
      	| train loss (relative): 2.833097e-01 | valid loss (relative): 2.700488e-01 
Epoch 20 use: 514.74 second.

epoch 21 starting......
Epoch:  21 | train loss: 1.167259e-02 | valid loss: 1.129851e-02 
      	| train loss (relative): 2.840098e-01 | valid loss (relative): 2.636801e-01 
Epoch 21 use: 489.36 second.

epoch 22 starting......
Epoch:  22 | train loss: 1.171308e-02 | valid loss: 1.124971e-02 
      	| train loss (relative): 2.839787e-01 | valid loss (relative): 2.756158e-01 
Epoch 22 use: 454.69 second.

epoch 23 starting......
Epoch:  23 | train loss: 1.168937e-02 | valid loss: 1.127388e-02 
      	| train loss (relative): 2.851436e-01 | valid loss (relative): 2.701836e-01 
Epoch 23 use: 523.58 second.

epoch 24 starting......
Epoch:  24 | train loss: 1.170099e-02 | valid loss: 1.125382e-02 
      	| train loss (relative): 2.841142e-01 | valid loss (relative): 2.653854e-01 
Epoch 24 use: 487.60 second.

epoch 25 starting......
Epoch:  25 | train loss: 1.171907e-02 | valid loss: 1.136740e-02 
      	| train loss (relative): 2.838596e-01 | valid loss (relative): 2.793620e-01 
Epoch 25 use: 512.25 second.

epoch 26 starting......
Epoch:  26 | train loss: 1.168215e-02 | valid loss: 1.140319e-02 
      	| train loss (relative): 2.847306e-01 | valid loss (relative): 2.815113e-01 
Epoch 26 use: 462.31 second.

epoch 27 starting......
Epoch:  27 | train loss: 1.171005e-02 | valid loss: 1.124717e-02 
      	| train loss (relative): 2.852512e-01 | valid loss (relative): 2.754245e-01 
Epoch 27 use: 461.26 second.

epoch 28 starting......
Epoch:  28 | train loss: 1.164242e-02 | valid loss: 1.136986e-02 
      	| train loss (relative): 2.826707e-01 | valid loss (relative): 2.879614e-01 
Epoch 28 use: 454.54 second.

epoch 29 starting......
Epoch:  29 | train loss: 1.169577e-02 | valid loss: 1.133791e-02 
      	| train loss (relative): 2.860717e-01 | valid loss (relative): 2.666505e-01 
Epoch 29 use: 492.30 second.

epoch 30 starting......
Epoch:  30 | train loss: 1.166648e-02 | valid loss: 1.129620e-02 
      	| train loss (relative): 2.828106e-01 | valid loss (relative): 2.706066e-01 
Epoch 30 use: 430.08 second.

epoch 31 starting......
Epoch:  31 | train loss: 1.167027e-02 | valid loss: 1.127635e-02 
      	| train loss (relative): 2.838748e-01 | valid loss (relative): 2.800278e-01 
Epoch 31 use: 467.56 second.

epoch 32 starting......
Epoch:  32 | train loss: 1.173665e-02 | valid loss: 1.126382e-02 
      	| train loss (relative): 2.869862e-01 | valid loss (relative): 2.787202e-01 
Epoch 32 use: 463.56 second.

epoch 33 starting......
Epoch:  33 | train loss: 1.172881e-02 | valid loss: 1.125841e-02 
      	| train loss (relative): 2.869835e-01 | valid loss (relative): 2.807496e-01 
Epoch 33 use: 466.51 second.

epoch 34 starting......
Epoch:  34 | train loss: 1.170282e-02 | valid loss: 1.123338e-02 
      	| train loss (relative): 2.847449e-01 | valid loss (relative): 2.742949e-01 
Epoch 34 use: 488.71 second.

epoch 35 starting......
Epoch:  35 | train loss: 1.166438e-02 | valid loss: 1.130626e-02 
      	| train loss (relative): 2.838368e-01 | valid loss (relative): 2.852949e-01 
Epoch 35 use: 563.83 second.

epoch 36 starting......
Epoch:  36 | train loss: 1.172232e-02 | valid loss: 1.124502e-02 
      	| train loss (relative): 2.849920e-01 | valid loss (relative): 2.771060e-01 
Epoch 36 use: 453.41 second.

epoch 37 starting......
Epoch:  37 | train loss: 1.164635e-02 | valid loss: 1.126928e-02 
      	| train loss (relative): 2.832224e-01 | valid loss (relative): 2.710919e-01 
Epoch 37 use: 494.18 second.

epoch 38 starting......
Epoch:  38 | train loss: 1.171752e-02 | valid loss: 1.126593e-02 
      	| train loss (relative): 2.845212e-01 | valid loss (relative): 2.665963e-01 
Epoch 38 use: 487.99 second.

epoch 39 starting......
Epoch:  39 | train loss: 1.171809e-02 | valid loss: 1.124097e-02 
      	| train loss (relative): 2.846873e-01 | valid loss (relative): 2.693909e-01 
Epoch 39 use: 468.65 second.

epoch 40 starting......
Epoch:  40 | train loss: 1.166513e-02 | valid loss: 1.125232e-02 
      	| train loss (relative): 2.835379e-01 | valid loss (relative): 2.642514e-01 
Epoch 40 use: 600.86 second.

epoch 41 starting......
Epoch:  41 | train loss: 1.166248e-02 | valid loss: 1.134085e-02 
      	| train loss (relative): 2.840383e-01 | valid loss (relative): 2.620755e-01 
Epoch 41 use: 519.09 second.

epoch 42 starting......
Epoch:  42 | train loss: 1.177199e-02 | valid loss: 1.137410e-02 
      	| train loss (relative): 2.856155e-01 | valid loss (relative): 2.756687e-01 
Epoch 42 use: 459.81 second.

epoch 43 starting......
Epoch:  43 | train loss: 1.170096e-02 | valid loss: 1.120613e-02 
      	| train loss (relative): 2.846385e-01 | valid loss (relative): 2.725880e-01 
Epoch 43 use: 459.15 second.

epoch 44 starting......
Epoch:  44 | train loss: 1.169001e-02 | valid loss: 1.136684e-02 
      	| train loss (relative): 2.855682e-01 | valid loss (relative): 2.602680e-01 
Epoch 44 use: 420.44 second.

epoch 45 starting......
Epoch:  45 | train loss: 1.172027e-02 | valid loss: 1.131391e-02 
      	| train loss (relative): 2.835778e-01 | valid loss (relative): 2.784863e-01 
Epoch 45 use: 420.81 second.

epoch 46 starting......
Epoch:  46 | train loss: 1.167763e-02 | valid loss: 1.152097e-02 
      	| train loss (relative): 2.844582e-01 | valid loss (relative): 2.939042e-01 
Epoch 46 use: 447.37 second.

epoch 47 starting......
Epoch:  47 | train loss: 1.173338e-02 | valid loss: 1.125861e-02 
      	| train loss (relative): 2.866590e-01 | valid loss (relative): 2.759584e-01 
Epoch 47 use: 432.69 second.

epoch 48 starting......
Epoch:  48 | train loss: 1.169444e-02 | valid loss: 1.121753e-02 
      	| train loss (relative): 2.844771e-01 | valid loss (relative): 2.702633e-01 
Epoch 48 use: 419.50 second.

epoch 49 starting......
Epoch:  49 | train loss: 1.170754e-02 | valid loss: 1.125914e-02 
      	| train loss (relative): 2.843576e-01 | valid loss (relative): 2.815054e-01 
Epoch 49 use: 446.33 second.

epoch 50 starting......
Epoch:  50 | train loss: 1.165400e-02 | valid loss: 1.131923e-02 
      	| train loss (relative): 2.836322e-01 | valid loss (relative): 2.696395e-01 
Epoch 50 use: 452.67 second.

epoch 51 starting......
Epoch:  51 | train loss: 1.165134e-02 | valid loss: 1.137902e-02 
      	| train loss (relative): 2.837578e-01 | valid loss (relative): 2.593799e-01 
Epoch 51 use: 451.06 second.

epoch 52 starting......
Epoch:  52 | train loss: 1.169350e-02 | valid loss: 1.137687e-02 
      	| train loss (relative): 2.835379e-01 | valid loss (relative): 2.798824e-01 
Epoch 52 use: 409.66 second.

epoch 53 starting......
Epoch:  53 | train loss: 1.173125e-02 | valid loss: 1.124970e-02 
      	| train loss (relative): 2.853794e-01 | valid loss (relative): 2.835627e-01 
Epoch 53 use: 431.15 second.

epoch 54 starting......
Epoch:  54 | train loss: 1.183925e-02 | valid loss: 1.133262e-02 
      	| train loss (relative): 2.897846e-01 | valid loss (relative): 2.650001e-01 
Epoch 54 use: 635.10 second.

epoch 55 starting......
Epoch:  55 | train loss: 1.167649e-02 | valid loss: 1.133267e-02 
      	| train loss (relative): 2.841191e-01 | valid loss (relative): 2.770986e-01 
Epoch 55 use: 670.87 second.

epoch 56 starting......
Epoch:  56 | train loss: 1.166010e-02 | valid loss: 1.131089e-02 
      	| train loss (relative): 2.840558e-01 | valid loss (relative): 2.804471e-01 
Epoch 56 use: 782.79 second.

epoch 57 starting......
Epoch:  57 | train loss: 1.165838e-02 | valid loss: 1.123700e-02 
      	| train loss (relative): 2.839944e-01 | valid loss (relative): 2.683741e-01 
Epoch 57 use: 740.15 second.

epoch 58 starting......
Epoch:  58 | train loss: 1.172404e-02 | valid loss: 1.135417e-02 
      	| train loss (relative): 2.847609e-01 | valid loss (relative): 2.627079e-01 
Epoch 58 use: 664.82 second.

epoch 59 starting......
Epoch:  59 | train loss: 1.171625e-02 | valid loss: 1.125813e-02 
      	| train loss (relative): 2.847642e-01 | valid loss (relative): 2.800204e-01 
Epoch 59 use: 658.48 second.

epoch 60 starting......
Epoch:  60 | train loss: 1.170863e-02 | valid loss: 1.187948e-02 
      	| train loss (relative): 2.835506e-01 | valid loss (relative): 3.113482e-01 
Epoch 60 use: 489.68 second.

epoch 61 starting......
Epoch:  61 | train loss: 1.176325e-02 | valid loss: 1.123498e-02 
      	| train loss (relative): 2.864471e-01 | valid loss (relative): 2.750775e-01 
Epoch 61 use: 423.90 second.

epoch 62 starting......
Epoch:  62 | train loss: 1.166179e-02 | valid loss: 1.140539e-02 
      	| train loss (relative): 2.833615e-01 | valid loss (relative): 2.880884e-01 
Epoch 62 use: 410.32 second.

epoch 63 starting......
Epoch:  63 | train loss: 1.169826e-02 | valid loss: 1.156601e-02 
      	| train loss (relative): 2.849090e-01 | valid loss (relative): 2.922316e-01 
Epoch 63 use: 418.93 second.

epoch 64 starting......
Epoch:  64 | train loss: 1.172374e-02 | valid loss: 1.124464e-02 
      	| train loss (relative): 2.868253e-01 | valid loss (relative): 2.688588e-01 
Epoch 64 use: 470.59 second.

epoch 65 starting......
Epoch:  65 | train loss: 1.168806e-02 | valid loss: 1.125990e-02 
      	| train loss (relative): 2.835525e-01 | valid loss (relative): 2.699184e-01 
Epoch 65 use: 440.23 second.

epoch 66 starting......
Epoch:  66 | train loss: 1.175140e-02 | valid loss: 1.129480e-02 
      	| train loss (relative): 2.881300e-01 | valid loss (relative): 2.666875e-01 
Epoch 66 use: 407.32 second.

epoch 67 starting......
Epoch:  67 | train loss: 1.171544e-02 | valid loss: 1.129261e-02 
      	| train loss (relative): 2.845796e-01 | valid loss (relative): 2.669444e-01 
Epoch 67 use: 425.06 second.

epoch 68 starting......
Epoch:  68 | train loss: 1.165215e-02 | valid loss: 1.133865e-02 
      	| train loss (relative): 2.837647e-01 | valid loss (relative): 2.729712e-01 
Epoch 68 use: 408.49 second.

epoch 69 starting......
Epoch:  69 | train loss: 1.181805e-02 | valid loss: 1.149935e-02 
      	| train loss (relative): 2.874693e-01 | valid loss (relative): 2.976304e-01 
Epoch 69 use: 460.92 second.

epoch 70 starting......
Epoch:  70 | train loss: 1.173784e-02 | valid loss: 1.137793e-02 
      	| train loss (relative): 2.862482e-01 | valid loss (relative): 2.703870e-01 
Epoch 70 use: 420.95 second.

epoch 71 starting......
Epoch:  71 | train loss: 1.171878e-02 | valid loss: 1.129963e-02 
      	| train loss (relative): 2.840179e-01 | valid loss (relative): 2.797251e-01 
Epoch 71 use: 405.86 second.

epoch 72 starting......
Epoch:  72 | train loss: 1.167032e-02 | valid loss: 1.127902e-02 
      	| train loss (relative): 2.847497e-01 | valid loss (relative): 2.739553e-01 
Epoch 72 use: 430.94 second.

epoch 73 starting......
Epoch:  73 | train loss: 1.171749e-02 | valid loss: 1.136584e-02 
      	| train loss (relative): 2.837472e-01 | valid loss (relative): 2.827148e-01 
Epoch 73 use: 431.59 second.

epoch 74 starting......
Epoch:  74 | train loss: 1.168093e-02 | valid loss: 1.123153e-02 
      	| train loss (relative): 2.837328e-01 | valid loss (relative): 2.770762e-01 
Epoch 74 use: 401.08 second.

epoch 75 starting......
Epoch:  75 | train loss: 1.165890e-02 | valid loss: 1.129290e-02 
      	| train loss (relative): 2.837996e-01 | valid loss (relative): 2.732341e-01 
Epoch 75 use: 399.32 second.

epoch 76 starting......
Epoch:  76 | train loss: 1.171449e-02 | valid loss: 1.125530e-02 
      	| train loss (relative): 2.851841e-01 | valid loss (relative): 2.710474e-01 
Epoch 76 use: 406.93 second.

epoch 77 starting......
Epoch:  77 | train loss: 1.166981e-02 | valid loss: 1.136041e-02 
      	| train loss (relative): 2.835937e-01 | valid loss (relative): 2.764608e-01 
Epoch 77 use: 510.43 second.

epoch 78 starting......
Epoch:  78 | train loss: 1.174530e-02 | valid loss: 1.122812e-02 
      	| train loss (relative): 2.855469e-01 | valid loss (relative): 2.700813e-01 
Epoch 78 use: 642.11 second.

epoch 79 starting......
Epoch:  79 | train loss: 1.173201e-02 | valid loss: 1.122764e-02 
      	| train loss (relative): 2.845531e-01 | valid loss (relative): 2.724579e-01 
Epoch 79 use: 678.36 second.

epoch 80 starting......
Epoch:  80 | train loss: 1.168221e-02 | valid loss: 1.134241e-02 
      	| train loss (relative): 2.845773e-01 | valid loss (relative): 2.682201e-01 
Epoch 80 use: 659.60 second.

epoch 81 starting......
Epoch:  81 | train loss: 1.170737e-02 | valid loss: 1.132050e-02 
      	| train loss (relative): 2.837693e-01 | valid loss (relative): 2.684277e-01 
Epoch 81 use: 788.59 second.

epoch 82 starting......
Epoch:  82 | train loss: 1.171341e-02 | valid loss: 1.133397e-02 
      	| train loss (relative): 2.845394e-01 | valid loss (relative): 2.841718e-01 
Epoch 82 use: 720.99 second.

epoch 83 starting......
Epoch:  83 | train loss: 1.172342e-02 | valid loss: 1.119665e-02 
      	| train loss (relative): 2.857023e-01 | valid loss (relative): 2.678028e-01 
Epoch 83 use: 653.24 second.

epoch 84 starting......
Epoch:  84 | train loss: 1.171121e-02 | valid loss: 1.126963e-02 
      	| train loss (relative): 2.846019e-01 | valid loss (relative): 2.719951e-01 
Epoch 84 use: 426.37 second.

epoch 85 starting......
Epoch:  85 | train loss: 1.174209e-02 | valid loss: 1.132420e-02 
      	| train loss (relative): 2.850161e-01 | valid loss (relative): 2.720233e-01 
Epoch 85 use: 497.30 second.

epoch 86 starting......
Epoch:  86 | train loss: 1.166786e-02 | valid loss: 1.126233e-02 
      	| train loss (relative): 2.838847e-01 | valid loss (relative): 2.738837e-01 
Epoch 86 use: 479.54 second.

epoch 87 starting......
Epoch:  87 | train loss: 1.167890e-02 | valid loss: 1.120227e-02 
      	| train loss (relative): 2.840920e-01 | valid loss (relative): 2.752713e-01 
Epoch 87 use: 416.12 second.

epoch 88 starting......
Epoch:  88 | train loss: 1.177526e-02 | valid loss: 1.130857e-02 
      	| train loss (relative): 2.854544e-01 | valid loss (relative): 2.712863e-01 
Epoch 88 use: 461.09 second.

epoch 89 starting......
Epoch:  89 | train loss: 1.167720e-02 | valid loss: 1.129107e-02 
      	| train loss (relative): 2.848323e-01 | valid loss (relative): 2.757698e-01 
Epoch 89 use: 488.70 second.

epoch 90 starting......
Epoch:  90 | train loss: 1.167293e-02 | valid loss: 1.130337e-02 
      	| train loss (relative): 2.829826e-01 | valid loss (relative): 2.762728e-01 
Epoch 90 use: 513.06 second.

epoch 91 starting......
Epoch:  91 | train loss: 1.164646e-02 | valid loss: 1.138083e-02 
      	| train loss (relative): 2.834308e-01 | valid loss (relative): 2.632051e-01 
Epoch 91 use: 428.77 second.

epoch 92 starting......
Epoch:  92 | train loss: 1.171254e-02 | valid loss: 1.135314e-02 
      	| train loss (relative): 2.841328e-01 | valid loss (relative): 2.862711e-01 
Epoch 92 use: 481.48 second.

epoch 93 starting......
Epoch:  93 | train loss: 1.169157e-02 | valid loss: 1.134984e-02 
      	| train loss (relative): 2.848333e-01 | valid loss (relative): 2.731161e-01 
Epoch 93 use: 429.81 second.

epoch 94 starting......
Epoch:  94 | train loss: 1.169995e-02 | valid loss: 1.128174e-02 
      	| train loss (relative): 2.834091e-01 | valid loss (relative): 2.734686e-01 
Epoch 94 use: 489.59 second.

epoch 95 starting......
Epoch:  95 | train loss: 1.167585e-02 | valid loss: 1.132389e-02 
      	| train loss (relative): 2.840521e-01 | valid loss (relative): 2.720128e-01 
Epoch 95 use: 408.97 second.

epoch 96 starting......
Epoch:  96 | train loss: 1.170728e-02 | valid loss: 1.122032e-02 
      	| train loss (relative): 2.844921e-01 | valid loss (relative): 2.732411e-01 
Epoch 96 use: 433.55 second.

epoch 97 starting......
Epoch:  97 | train loss: 1.171841e-02 | valid loss: 1.144505e-02 
      	| train loss (relative): 2.847732e-01 | valid loss (relative): 2.616712e-01 
Epoch 97 use: 445.32 second.

epoch 98 starting......
Epoch:  98 | train loss: 1.166837e-02 | valid loss: 1.131109e-02 
      	| train loss (relative): 2.838867e-01 | valid loss (relative): 2.643386e-01 
Epoch 98 use: 403.99 second.

epoch 99 starting......
Epoch:  99 | train loss: 1.167025e-02 | valid loss: 1.124595e-02 
      	| train loss (relative): 2.838877e-01 | valid loss (relative): 2.779772e-01 
Epoch 99 use: 407.05 second.

test MSE Error: 1.152613e-02 | relative MSE Error: 2.849026e-01 
 Total time used for training: 13.60 hour.
MESLoss saved to  /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_MSELoss_Latent_64_nearest_neighbouring_True_SFC_nums_3_startlr_0.05_n_epoches_100.txt
relative MSELoss saved to  /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_reMSELoss_Latent_64_nearest_neighbouring_True_SFC_nums_3_startlr_0.05_n_epoches_100.txt
model saved to /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_64_Nearest_neighbouring_True_SFC_nums_3_startlr_0.05_n_epoches_100.pth
model_dict saved to /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_64_Nearest_neighbouring_True_SFC_nums_3_startlr_0.05_n_epoches_100_dict.pth
... Training slugflow data group 3 completed, Run finished Fri  6 Aug 21:45:19 BST 2021 ...
{'data_dir': '/rds/general/user/jy220/home/tensors/', 'data_type': 'tensors', 'vtu_fields': 'Component1::ComponentMassFractionPhase1, phase1::Velocity', 'variational': 'False', 'optimizer': 'Adamax', 'structured': 'False', 'changing_lr': 'False', 'self_concat': '2', 'dimension': '3', 'sfc_nums': '3', 'components': '4', 'nearest_neighbouring': 'True', 'dims_latent': '64', 'activation': 'Tanh', 'sfc_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/sfcs.pt', 'inv_sfc_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/inv_sfcs.pt', 'coords_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/coords.pt', 'cells_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/cells.npy', 'state_load': 'None', 'batch_size': '16', 'lr': '0.001', 'n_epoches': '100', 'seed': '32', 'visualize': 'False', 'reconstructed_path': 'None', 'reconstruct_start_index': '0', 'reconstruct_end_index': '50', 'save_path': '/rds/general/user/jy220/home/new_results/'}
[[ 2.50825269e-01 -1.67256990e-02 -1.22268730e-03]
 [ 2.38611959e-01 -5.50627777e-03 -5.06451913e-03]
 [ 2.39454874e-01 -1.59563257e-02  3.96778323e-03]
 ...
 [ 9.11361096e+00 -2.21908381e-02  2.22384902e-03]
 [ 9.11847310e+00 -2.84346931e-02 -2.34839590e-04]
 [ 9.11039661e+00 -2.51498573e-02 -8.70661044e-03]]
{'tetra': array([[      0,       1,       2,       3],
       [      4,       5,       6,       7],
       [      8,       9,      10,      11],
       ...,
       [1342744, 1342745, 1342746, 1342747],
       [1342748, 1342749, 1342750, 1342751],
       [1342752, 1342753, 1342754, 1342755]])}
vtu fields:  ['Component1::ComponentMassFractionPhase1', 'phase1::Velocity'] 
 structured  False 
 activation  Tanh() 
 self concat  2 
 sfc_nums  3 

dims_latent  64 
 components  4 
 nearest_neighbouring  True 

visualize  False 
 sample number  1706 

reading sfc nums......
[array([  41684,   41685,   41674, ..., 1235560, 1235564, 1235558]), array([954289, 954290, 954291, ..., 420685, 420687, 420688]), array([  40156,   40155,   40158, ..., 1123478, 1123475, 1123467])]
reading inverse sfc nums......
[array([   8146,    8145,    8144, ..., 1274444, 1274445, 1273166]), array([547280, 547283, 593037, ..., 933933, 933932, 864488]), array([ 66437,  66439, 122512, ..., 459416, 415130, 459418])]
Computing min and max......

tk:  tensor([0.7581, 0.0418, 0.1840, 0.1464], dtype=torch.float64) 

tb:  tensor([-0.3816, -0.3618, -0.0308,  0.1446], dtype=torch.float64) 

length of train set: 1505 

length of valid set: 100 

length of test set: 101 

torch device num: 4 

Let's use 4 GPUs!
epoch 0 starting......
Epoch:  0 | train loss: 2.163623e-02 | valid loss: 8.987115e-03 
      	| train loss (relative): 2.452555e+00 | valid loss (relative): 2.503128e-01 
Epoch 0 use: 516.44 second.

epoch 1 starting......
Epoch:  1 | train loss: 7.261473e-03 | valid loss: 6.445129e-03 
      	| train loss (relative): 1.671942e-01 | valid loss (relative): 1.422123e-01 
Epoch 1 use: 462.30 second.

epoch 2 starting......
Epoch:  2 | train loss: 6.215007e-03 | valid loss: 6.015787e-03 
      	| train loss (relative): 1.348244e-01 | valid loss (relative): 1.289511e-01 
Epoch 2 use: 431.96 second.

epoch 3 starting......
Epoch:  3 | train loss: 5.877745e-03 | valid loss: 5.658918e-03 
      	| train loss (relative): 1.269822e-01 | valid loss (relative): 1.245937e-01 
Epoch 3 use: 414.34 second.

epoch 4 starting......
Epoch:  4 | train loss: 5.442416e-03 | valid loss: 5.126130e-03 
      	| train loss (relative): 1.172618e-01 | valid loss (relative): 1.138358e-01 
Epoch 4 use: 613.20 second.

epoch 5 starting......
Epoch:  5 | train loss: 4.973710e-03 | valid loss: 4.726499e-03 
      	| train loss (relative): 1.061277e-01 | valid loss (relative): 1.008487e-01 
Epoch 5 use: 996.37 second.

epoch 6 starting......
Epoch:  6 | train loss: 4.661352e-03 | valid loss: 4.503509e-03 
      	| train loss (relative): 9.829634e-02 | valid loss (relative): 9.357224e-02 
Epoch 6 use: 1076.77 second.

epoch 7 starting......
Epoch:  7 | train loss: 4.506260e-03 | valid loss: 4.407510e-03 
      	| train loss (relative): 9.438784e-02 | valid loss (relative): 9.392932e-02 
Epoch 7 use: 971.31 second.

epoch 8 starting......
Epoch:  8 | train loss: 4.435817e-03 | valid loss: 4.340563e-03 
      	| train loss (relative): 9.283994e-02 | valid loss (relative): 8.916536e-02 
Epoch 8 use: 620.25 second.

epoch 9 starting......
Epoch:  9 | train loss: 4.308939e-03 | valid loss: 4.219875e-03 
      	| train loss (relative): 8.993257e-02 | valid loss (relative): 8.646236e-02 
Epoch 9 use: 481.52 second.

epoch 10 starting......
Epoch:  10 | train loss: 4.219325e-03 | valid loss: 4.151626e-03 
      	| train loss (relative): 8.791169e-02 | valid loss (relative): 8.524089e-02 
Epoch 10 use: 456.55 second.

epoch 11 starting......
Epoch:  11 | train loss: 4.119766e-03 | valid loss: 4.066154e-03 
      	| train loss (relative): 8.570405e-02 | valid loss (relative): 8.426547e-02 
Epoch 11 use: 420.06 second.

epoch 12 starting......
Epoch:  12 | train loss: 4.036796e-03 | valid loss: 3.985376e-03 
      	| train loss (relative): 8.373111e-02 | valid loss (relative): 8.179360e-02 
Epoch 12 use: 424.36 second.

epoch 13 starting......
Epoch:  13 | train loss: 3.990142e-03 | valid loss: 3.910778e-03 
      	| train loss (relative): 8.263765e-02 | valid loss (relative): 8.012745e-02 
Epoch 13 use: 405.71 second.

epoch 14 starting......
Epoch:  14 | train loss: 3.883729e-03 | valid loss: 3.911123e-03 
      	| train loss (relative): 8.031285e-02 | valid loss (relative): 8.263841e-02 
Epoch 14 use: 421.36 second.

epoch 15 starting......
Epoch:  15 | train loss: 3.863030e-03 | valid loss: 3.843199e-03 
      	| train loss (relative): 7.978369e-02 | valid loss (relative): 7.700752e-02 
Epoch 15 use: 370.67 second.

epoch 16 starting......
Epoch:  16 | train loss: 3.790708e-03 | valid loss: 3.735381e-03 
      	| train loss (relative): 7.815734e-02 | valid loss (relative): 7.582442e-02 
Epoch 16 use: 369.01 second.

epoch 17 starting......
Epoch:  17 | train loss: 3.736690e-03 | valid loss: 3.688470e-03 
      	| train loss (relative): 7.688566e-02 | valid loss (relative): 7.651263e-02 
Epoch 17 use: 381.80 second.

epoch 18 starting......
Epoch:  18 | train loss: 3.694221e-03 | valid loss: 3.690152e-03 
      	| train loss (relative): 7.595719e-02 | valid loss (relative): 7.565487e-02 
Epoch 18 use: 406.38 second.

epoch 19 starting......
Epoch:  19 | train loss: 3.674524e-03 | valid loss: 3.568208e-03 
      	| train loss (relative): 7.546841e-02 | valid loss (relative): 7.386309e-02 
Epoch 19 use: 347.96 second.

epoch 20 starting......
Epoch:  20 | train loss: 3.579345e-03 | valid loss: 3.549230e-03 
      	| train loss (relative): 7.338035e-02 | valid loss (relative): 7.314712e-02 
Epoch 20 use: 357.52 second.

epoch 21 starting......
Epoch:  21 | train loss: 3.547231e-03 | valid loss: 3.522500e-03 
      	| train loss (relative): 7.267541e-02 | valid loss (relative): 7.216723e-02 
Epoch 21 use: 356.19 second.

epoch 22 starting......
Epoch:  22 | train loss: 3.530854e-03 | valid loss: 3.454528e-03 
      	| train loss (relative): 7.228033e-02 | valid loss (relative): 7.052317e-02 
Epoch 22 use: 347.33 second.

epoch 23 starting......
Epoch:  23 | train loss: 3.472091e-03 | valid loss: 3.459844e-03 
      	| train loss (relative): 7.100765e-02 | valid loss (relative): 7.120072e-02 
Epoch 23 use: 345.83 second.

epoch 24 starting......
Epoch:  24 | train loss: 3.444487e-03 | valid loss: 3.412407e-03 
      	| train loss (relative): 7.034868e-02 | valid loss (relative): 6.843024e-02 
Epoch 24 use: 340.99 second.

epoch 25 starting......
Epoch:  25 | train loss: 3.408493e-03 | valid loss: 3.341722e-03 
      	| train loss (relative): 6.962072e-02 | valid loss (relative): 6.821475e-02 
Epoch 25 use: 333.18 second.

epoch 26 starting......
Epoch:  26 | train loss: 3.371450e-03 | valid loss: 3.333733e-03 
      	| train loss (relative): 6.876685e-02 | valid loss (relative): 6.847563e-02 
Epoch 26 use: 362.52 second.

epoch 27 starting......
Epoch:  27 | train loss: 3.356673e-03 | valid loss: 3.304803e-03 
      	| train loss (relative): 6.846365e-02 | valid loss (relative): 6.755965e-02 
Epoch 27 use: 364.25 second.

epoch 28 starting......
Epoch:  28 | train loss: 3.339527e-03 | valid loss: 3.308232e-03 
      	| train loss (relative): 6.806356e-02 | valid loss (relative): 6.784976e-02 
Epoch 28 use: 374.53 second.

epoch 29 starting......
Epoch:  29 | train loss: 3.317595e-03 | valid loss: 3.280769e-03 
      	| train loss (relative): 6.760363e-02 | valid loss (relative): 6.756236e-02 
Epoch 29 use: 431.36 second.

epoch 30 starting......
Epoch:  30 | train loss: 3.287214e-03 | valid loss: 3.226838e-03 
      	| train loss (relative): 6.692828e-02 | valid loss (relative): 6.516369e-02 
Epoch 30 use: 437.63 second.

epoch 31 starting......
Epoch:  31 | train loss: 3.260400e-03 | valid loss: 3.228604e-03 
      	| train loss (relative): 6.637448e-02 | valid loss (relative): 6.632371e-02 
Epoch 31 use: 470.12 second.

epoch 32 starting......
Epoch:  32 | train loss: 3.258308e-03 | valid loss: 3.214558e-03 
      	| train loss (relative): 6.633671e-02 | valid loss (relative): 6.514690e-02 
Epoch 32 use: 485.75 second.

epoch 33 starting......
Epoch:  33 | train loss: 3.222967e-03 | valid loss: 3.173787e-03 
      	| train loss (relative): 6.555407e-02 | valid loss (relative): 6.479397e-02 
Epoch 33 use: 613.65 second.

epoch 34 starting......
Epoch:  34 | train loss: 3.223461e-03 | valid loss: 3.230332e-03 
      	| train loss (relative): 6.558656e-02 | valid loss (relative): 6.791591e-02 
Epoch 34 use: 400.15 second.

epoch 35 starting......
Epoch:  35 | train loss: 3.192483e-03 | valid loss: 3.130984e-03 
      	| train loss (relative): 6.488413e-02 | valid loss (relative): 6.405561e-02 
Epoch 35 use: 375.15 second.

epoch 36 starting......
Epoch:  36 | train loss: 3.165372e-03 | valid loss: 3.140584e-03 
      	| train loss (relative): 6.428501e-02 | valid loss (relative): 6.426215e-02 
Epoch 36 use: 367.49 second.

epoch 37 starting......
Epoch:  37 | train loss: 3.159501e-03 | valid loss: 3.109454e-03 
      	| train loss (relative): 6.419177e-02 | valid loss (relative): 6.284512e-02 
Epoch 37 use: 398.86 second.

epoch 38 starting......
Epoch:  38 | train loss: 3.135387e-03 | valid loss: 3.092712e-03 
      	| train loss (relative): 6.365817e-02 | valid loss (relative): 6.280867e-02 
Epoch 38 use: 367.86 second.

epoch 39 starting......
Epoch:  39 | train loss: 3.129541e-03 | valid loss: 3.085693e-03 
      	| train loss (relative): 6.353477e-02 | valid loss (relative): 6.266079e-02 
Epoch 39 use: 332.86 second.

epoch 40 starting......
Epoch:  40 | train loss: 3.107511e-03 | valid loss: 3.082075e-03 
      	| train loss (relative): 6.304142e-02 | valid loss (relative): 6.269538e-02 
Epoch 40 use: 333.18 second.

epoch 41 starting......
Epoch:  41 | train loss: 3.097009e-03 | valid loss: 3.068275e-03 
      	| train loss (relative): 6.279955e-02 | valid loss (relative): 6.298709e-02 
Epoch 41 use: 331.42 second.

epoch 42 starting......
Epoch:  42 | train loss: 3.078991e-03 | valid loss: 3.038544e-03 
      	| train loss (relative): 6.244656e-02 | valid loss (relative): 6.137201e-02 
Epoch 42 use: 350.55 second.

epoch 43 starting......
Epoch:  43 | train loss: 3.069069e-03 | valid loss: 3.042673e-03 
      	| train loss (relative): 6.221605e-02 | valid loss (relative): 6.169578e-02 
Epoch 43 use: 409.64 second.

epoch 44 starting......
Epoch:  44 | train loss: 3.074647e-03 | valid loss: 3.013003e-03 
      	| train loss (relative): 6.232324e-02 | valid loss (relative): 6.109327e-02 
Epoch 44 use: 334.77 second.

epoch 45 starting......
Epoch:  45 | train loss: 3.042292e-03 | valid loss: 3.007015e-03 
      	| train loss (relative): 6.164240e-02 | valid loss (relative): 6.036180e-02 
Epoch 45 use: 340.51 second.

epoch 46 starting......
Epoch:  46 | train loss: 3.031926e-03 | valid loss: 2.987008e-03 
      	| train loss (relative): 6.139886e-02 | valid loss (relative): 6.065698e-02 
Epoch 46 use: 337.29 second.

epoch 47 starting......
Epoch:  47 | train loss: 3.018945e-03 | valid loss: 2.998734e-03 
      	| train loss (relative): 6.113549e-02 | valid loss (relative): 6.102600e-02 
Epoch 47 use: 341.18 second.

epoch 48 starting......
Epoch:  48 | train loss: 2.993292e-03 | valid loss: 2.961553e-03 
      	| train loss (relative): 6.056083e-02 | valid loss (relative): 6.020105e-02 
Epoch 48 use: 343.78 second.

epoch 49 starting......
Epoch:  49 | train loss: 2.991254e-03 | valid loss: 2.942205e-03 
      	| train loss (relative): 6.054851e-02 | valid loss (relative): 6.043703e-02 
Epoch 49 use: 334.93 second.

epoch 50 starting......
Epoch:  50 | train loss: 2.967620e-03 | valid loss: 2.952159e-03 
      	| train loss (relative): 6.003595e-02 | valid loss (relative): 6.005938e-02 
Epoch 50 use: 335.55 second.

epoch 51 starting......
Epoch:  51 | train loss: 2.960446e-03 | valid loss: 2.945295e-03 
      	| train loss (relative): 5.990088e-02 | valid loss (relative): 5.987864e-02 
Epoch 51 use: 350.29 second.

epoch 52 starting......
Epoch:  52 | train loss: 2.938503e-03 | valid loss: 2.908026e-03 
      	| train loss (relative): 5.942730e-02 | valid loss (relative): 5.866758e-02 
Epoch 52 use: 340.21 second.

epoch 53 starting......
Epoch:  53 | train loss: 2.944390e-03 | valid loss: 2.888933e-03 
      	| train loss (relative): 5.953440e-02 | valid loss (relative): 5.802874e-02 
Epoch 53 use: 350.77 second.

epoch 54 starting......
Epoch:  54 | train loss: 2.907135e-03 | valid loss: 2.885698e-03 
      	| train loss (relative): 5.878503e-02 | valid loss (relative): 5.753075e-02 
Epoch 54 use: 350.45 second.

epoch 55 starting......
Epoch:  55 | train loss: 2.892583e-03 | valid loss: 2.858380e-03 
      	| train loss (relative): 5.844931e-02 | valid loss (relative): 5.746473e-02 
Epoch 55 use: 407.49 second.

epoch 56 starting......
Epoch:  56 | train loss: 2.869676e-03 | valid loss: 2.844802e-03 
      	| train loss (relative): 5.793367e-02 | valid loss (relative): 5.848396e-02 
Epoch 56 use: 353.24 second.

epoch 57 starting......
Epoch:  57 | train loss: 2.851217e-03 | valid loss: 2.843588e-03 
      	| train loss (relative): 5.755430e-02 | valid loss (relative): 5.794902e-02 
Epoch 57 use: 352.53 second.

epoch 58 starting......
Epoch:  58 | train loss: 2.866046e-03 | valid loss: 2.841241e-03 
      	| train loss (relative): 5.787966e-02 | valid loss (relative): 5.634557e-02 
Epoch 58 use: 335.56 second.

epoch 59 starting......
Epoch:  59 | train loss: 2.833439e-03 | valid loss: 2.826488e-03 
      	| train loss (relative): 5.718376e-02 | valid loss (relative): 5.580341e-02 
Epoch 59 use: 344.73 second.

epoch 60 starting......
Epoch:  60 | train loss: 2.829556e-03 | valid loss: 2.806003e-03 
      	| train loss (relative): 5.710016e-02 | valid loss (relative): 5.656536e-02 
Epoch 60 use: 348.67 second.

epoch 61 starting......
Epoch:  61 | train loss: 2.811363e-03 | valid loss: 2.794934e-03 
      	| train loss (relative): 5.673494e-02 | valid loss (relative): 5.544104e-02 
Epoch 61 use: 348.88 second.

epoch 62 starting......
Epoch:  62 | train loss: 2.794153e-03 | valid loss: 2.769155e-03 
      	| train loss (relative): 5.635119e-02 | valid loss (relative): 5.615490e-02 
Epoch 62 use: 353.62 second.

epoch 63 starting......
Epoch:  63 | train loss: 2.775968e-03 | valid loss: 2.775723e-03 
      	| train loss (relative): 5.596893e-02 | valid loss (relative): 5.585257e-02 
Epoch 63 use: 351.75 second.

epoch 64 starting......
Epoch:  64 | train loss: 2.784233e-03 | valid loss: 2.739355e-03 
      	| train loss (relative): 5.615386e-02 | valid loss (relative): 5.512856e-02 
Epoch 64 use: 344.03 second.

epoch 65 starting......
Epoch:  65 | train loss: 2.745259e-03 | valid loss: 2.736626e-03 
      	| train loss (relative): 5.532920e-02 | valid loss (relative): 5.529691e-02 
Epoch 65 use: 345.24 second.

epoch 66 starting......
Epoch:  66 | train loss: 2.739846e-03 | valid loss: 2.731878e-03 
      	| train loss (relative): 5.522085e-02 | valid loss (relative): 5.493481e-02 
Epoch 66 use: 314.49 second.

epoch 67 starting......
Epoch:  67 | train loss: 2.746709e-03 | valid loss: 2.722936e-03 
      	| train loss (relative): 5.533940e-02 | valid loss (relative): 5.471551e-02 
Epoch 67 use: 319.34 second.

epoch 68 starting......
Epoch:  68 | train loss: 2.714507e-03 | valid loss: 2.694136e-03 
      	| train loss (relative): 5.467771e-02 | valid loss (relative): 5.454178e-02 
Epoch 68 use: 326.43 second.

epoch 69 starting......
Epoch:  69 | train loss: 2.699057e-03 | valid loss: 2.694199e-03 
      	| train loss (relative): 5.437281e-02 | valid loss (relative): 5.367330e-02 
Epoch 69 use: 318.62 second.

epoch 70 starting......
Epoch:  70 | train loss: 2.704661e-03 | valid loss: 2.712946e-03 
      	| train loss (relative): 5.445426e-02 | valid loss (relative): 5.456655e-02 
Epoch 70 use: 310.15 second.

epoch 71 starting......
Epoch:  71 | train loss: 2.702107e-03 | valid loss: 2.694157e-03 
      	| train loss (relative): 5.441377e-02 | valid loss (relative): 5.462105e-02 
Epoch 71 use: 327.80 second.

epoch 72 starting......
Epoch:  72 | train loss: 2.687471e-03 | valid loss: 2.695960e-03 
      	| train loss (relative): 5.410299e-02 | valid loss (relative): 5.465623e-02 
Epoch 72 use: 328.83 second.

epoch 73 starting......
Epoch:  73 | train loss: 2.663748e-03 | valid loss: 2.657508e-03 
      	| train loss (relative): 5.359710e-02 | valid loss (relative): 5.395691e-02 
Epoch 73 use: 324.02 second.

epoch 74 starting......
Epoch:  74 | train loss: 2.639822e-03 | valid loss: 2.635117e-03 
      	| train loss (relative): 5.309788e-02 | valid loss (relative): 5.292977e-02 
Epoch 74 use: 335.96 second.

epoch 75 starting......
Epoch:  75 | train loss: 2.632869e-03 | valid loss: 2.649821e-03 
      	| train loss (relative): 5.295769e-02 | valid loss (relative): 5.330330e-02 
Epoch 75 use: 352.53 second.

epoch 76 starting......
Epoch:  76 | train loss: 2.630120e-03 | valid loss: 2.621270e-03 
      	| train loss (relative): 5.289749e-02 | valid loss (relative): 5.264203e-02 
Epoch 76 use: 340.86 second.

epoch 77 starting......
Epoch:  77 | train loss: 2.629004e-03 | valid loss: 2.616744e-03 
      	| train loss (relative): 5.286792e-02 | valid loss (relative): 5.233582e-02 
Epoch 77 use: 323.16 second.

epoch 78 starting......
Epoch:  78 | train loss: 2.605165e-03 | valid loss: 2.598491e-03 
      	| train loss (relative): 5.236128e-02 | valid loss (relative): 5.232278e-02 
Epoch 78 use: 332.67 second.

epoch 79 starting......
Epoch:  79 | train loss: 2.601169e-03 | valid loss: 2.599252e-03 
      	| train loss (relative): 5.229121e-02 | valid loss (relative): 5.219603e-02 
Epoch 79 use: 346.38 second.

epoch 80 starting......
Epoch:  80 | train loss: 2.589526e-03 | valid loss: 2.592443e-03 
      	| train loss (relative): 5.202864e-02 | valid loss (relative): 5.240858e-02 
Epoch 80 use: 347.25 second.

epoch 81 starting......
Epoch:  81 | train loss: 2.573361e-03 | valid loss: 2.558430e-03 
      	| train loss (relative): 5.168839e-02 | valid loss (relative): 5.150232e-02 
Epoch 81 use: 338.97 second.

epoch 82 starting......
Epoch:  82 | train loss: 2.570040e-03 | valid loss: 2.572567e-03 
      	| train loss (relative): 5.162809e-02 | valid loss (relative): 5.177451e-02 
Epoch 82 use: 329.42 second.

epoch 83 starting......
Epoch:  83 | train loss: 2.575143e-03 | valid loss: 2.556162e-03 
      	| train loss (relative): 5.173175e-02 | valid loss (relative): 5.117108e-02 
Epoch 83 use: 334.25 second.

epoch 84 starting......
Epoch:  84 | train loss: 2.552250e-03 | valid loss: 2.550740e-03 
      	| train loss (relative): 5.124772e-02 | valid loss (relative): 5.152456e-02 
Epoch 84 use: 325.55 second.

epoch 85 starting......
Epoch:  85 | train loss: 2.541777e-03 | valid loss: 2.549533e-03 
      	| train loss (relative): 5.101749e-02 | valid loss (relative): 5.111872e-02 
Epoch 85 use: 347.80 second.

epoch 86 starting......
Epoch:  86 | train loss: 2.529020e-03 | valid loss: 2.514082e-03 
      	| train loss (relative): 5.076000e-02 | valid loss (relative): 5.038538e-02 
Epoch 86 use: 336.29 second.

epoch 87 starting......
Epoch:  87 | train loss: 2.519086e-03 | valid loss: 2.530246e-03 
      	| train loss (relative): 5.054518e-02 | valid loss (relative): 5.121381e-02 
Epoch 87 use: 340.04 second.

epoch 88 starting......
Epoch:  88 | train loss: 2.512109e-03 | valid loss: 2.504725e-03 
      	| train loss (relative): 5.041192e-02 | valid loss (relative): 5.031554e-02 
Epoch 88 use: 335.20 second.

epoch 89 starting......
Epoch:  89 | train loss: 2.502921e-03 | valid loss: 2.531312e-03 
      	| train loss (relative): 5.021861e-02 | valid loss (relative): 5.013423e-02 
Epoch 89 use: 334.65 second.

epoch 90 starting......
Epoch:  90 | train loss: 2.496847e-03 | valid loss: 2.488135e-03 
      	| train loss (relative): 5.007304e-02 | valid loss (relative): 5.013911e-02 
Epoch 90 use: 339.98 second.

epoch 91 starting......
Epoch:  91 | train loss: 2.482144e-03 | valid loss: 2.479353e-03 
      	| train loss (relative): 4.977413e-02 | valid loss (relative): 4.980285e-02 
Epoch 91 use: 331.45 second.

epoch 92 starting......
Epoch:  92 | train loss: 2.472298e-03 | valid loss: 2.463153e-03 
      	| train loss (relative): 4.955281e-02 | valid loss (relative): 4.924091e-02 
Epoch 92 use: 345.13 second.

epoch 93 starting......
Epoch:  93 | train loss: 2.464461e-03 | valid loss: 2.467273e-03 
      	| train loss (relative): 4.939205e-02 | valid loss (relative): 4.941830e-02 
Epoch 93 use: 346.68 second.

epoch 94 starting......
Epoch:  94 | train loss: 2.465164e-03 | valid loss: 2.474295e-03 
      	| train loss (relative): 4.940916e-02 | valid loss (relative): 4.958499e-02 
Epoch 94 use: 357.43 second.

epoch 95 starting......
Epoch:  95 | train loss: 2.470208e-03 | valid loss: 2.469702e-03 
      	| train loss (relative): 4.950571e-02 | valid loss (relative): 4.916912e-02 
Epoch 95 use: 370.06 second.

epoch 96 starting......
Epoch:  96 | train loss: 2.440888e-03 | valid loss: 2.435152e-03 
      	| train loss (relative): 4.890105e-02 | valid loss (relative): 4.870916e-02 
Epoch 96 use: 348.05 second.

epoch 97 starting......
Epoch:  97 | train loss: 2.433532e-03 | valid loss: 2.460894e-03 
      	| train loss (relative): 4.873524e-02 | valid loss (relative): 4.965540e-02 
Epoch 97 use: 347.99 second.

epoch 98 starting......
Epoch:  98 | train loss: 2.439030e-03 | valid loss: 2.442336e-03 
      	| train loss (relative): 4.886310e-02 | valid loss (relative): 4.885325e-02 
Epoch 98 use: 360.80 second.

epoch 99 starting......
Epoch:  99 | train loss: 2.424616e-03 | valid loss: 2.421225e-03 
      	| train loss (relative): 4.855043e-02 | valid loss (relative): 4.883723e-02 
Epoch 99 use: 361.19 second.

test MSE Error: 2.428984e-03 | relative MSE Error: 4.872412e-02 
 Total time used for training: 10.84 hour.
MESLoss saved to  /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_MSELoss_Latent_64_nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_100.txt
relative MSELoss saved to  /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_reMSELoss_Latent_64_nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_100.txt
model saved to /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_64_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_100.pth
model_dict saved to /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_64_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_100_dict.pth
... Training slugflow data group 3 completed, Run finished Thu 12 Aug 03:17:35 BST 2021 ...
{'data_dir': '/rds/general/user/jy220/home/tensors/', 'data_type': 'tensors', 'vtu_fields': 'Component1::ComponentMassFractionPhase1, phase1::Velocity', 'variational': 'False', 'optimizer': 'Adamax', 'structured': 'False', 'changing_lr': 'False', 'self_concat': '2', 'dimension': '3', 'sfc_nums': '3', 'components': '4', 'nearest_neighbouring': 'True', 'dims_latent': '64', 'activation': 'Tanh', 'sfc_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/sfcs.pt', 'inv_sfc_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/inv_sfcs.pt', 'coords_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/coords.pt', 'cells_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/cells.npy', 'state_load': '/rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_64_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_100_dict.pth', 'batch_size': '16', 'lr': '0.001', 'n_epoches': '100', 'seed': '32', 'visualize': 'False', 'reconstructed_path': 'None', 'reconstruct_start_index': '0', 'reconstruct_end_index': '50', 'save_path': '/rds/general/user/jy220/home/new_results/'}
[[ 2.50825269e-01 -1.67256990e-02 -1.22268730e-03]
 [ 2.38611959e-01 -5.50627777e-03 -5.06451913e-03]
 [ 2.39454874e-01 -1.59563257e-02  3.96778323e-03]
 ...
 [ 9.11361096e+00 -2.21908381e-02  2.22384902e-03]
 [ 9.11847310e+00 -2.84346931e-02 -2.34839590e-04]
 [ 9.11039661e+00 -2.51498573e-02 -8.70661044e-03]]
{'tetra': array([[      0,       1,       2,       3],
       [      4,       5,       6,       7],
       [      8,       9,      10,      11],
       ...,
       [1342744, 1342745, 1342746, 1342747],
       [1342748, 1342749, 1342750, 1342751],
       [1342752, 1342753, 1342754, 1342755]])}
vtu fields:  ['Component1::ComponentMassFractionPhase1', 'phase1::Velocity'] 
 structured  False 
 activation  Tanh() 
 self concat  2 
 sfc_nums  3 

dims_latent  64 
 components  4 
 nearest_neighbouring  True 

visualize  False 
 sample number  1706 

reading sfc nums......
[array([  41684,   41685,   41674, ..., 1235560, 1235564, 1235558]), array([954289, 954290, 954291, ..., 420685, 420687, 420688]), array([  40156,   40155,   40158, ..., 1123478, 1123475, 1123467])]
reading inverse sfc nums......
[array([   8146,    8145,    8144, ..., 1274444, 1274445, 1273166]), array([547280, 547283, 593037, ..., 933933, 933932, 864488]), array([ 66437,  66439, 122512, ..., 459416, 415130, 459418])]
Computing min and max......

tk:  tensor([0.7581, 0.0418, 0.1840, 0.1464], dtype=torch.float64) 

tb:  tensor([-0.3816, -0.3618, -0.0308,  0.1446], dtype=torch.float64) 

length of train set: 1505 

length of valid set: 100 

length of test set: 101 

torch device num: 4 

Let's use 4 GPUs!
epoch 100 starting......
Epoch:  100 | train loss: 3.823547e-03 | valid loss: 2.592397e-03 
      	| train loss (relative): 7.569181e-02 | valid loss (relative): 5.207073e-02 
Epoch 100 use: 408.13 second.

epoch 101 starting......
Epoch:  101 | train loss: 2.545922e-03 | valid loss: 2.491316e-03 
      	| train loss (relative): 5.108007e-02 | valid loss (relative): 5.007637e-02 
Epoch 101 use: 407.25 second.

epoch 102 starting......
Epoch:  102 | train loss: 2.481517e-03 | valid loss: 2.453516e-03 
      	| train loss (relative): 4.974442e-02 | valid loss (relative): 4.923451e-02 
Epoch 102 use: 385.15 second.

epoch 103 starting......
Epoch:  103 | train loss: 2.450692e-03 | valid loss: 2.432823e-03 
      	| train loss (relative): 4.909068e-02 | valid loss (relative): 4.885740e-02 
Epoch 103 use: 374.98 second.

epoch 104 starting......
Epoch:  104 | train loss: 2.432999e-03 | valid loss: 2.419469e-03 
      	| train loss (relative): 4.871240e-02 | valid loss (relative): 4.849608e-02 
Epoch 104 use: 367.01 second.

epoch 105 starting......
Epoch:  105 | train loss: 2.418554e-03 | valid loss: 2.410530e-03 
      	| train loss (relative): 4.840939e-02 | valid loss (relative): 4.845148e-02 
Epoch 105 use: 361.92 second.

epoch 106 starting......
Epoch:  106 | train loss: 2.408017e-03 | valid loss: 2.400074e-03 
      	| train loss (relative): 4.819126e-02 | valid loss (relative): 4.809906e-02 
Epoch 106 use: 391.05 second.

epoch 107 starting......
Epoch:  107 | train loss: 2.399550e-03 | valid loss: 2.394273e-03 
      	| train loss (relative): 4.801933e-02 | valid loss (relative): 4.802166e-02 
Epoch 107 use: 373.79 second.

epoch 108 starting......
Epoch:  108 | train loss: 2.393899e-03 | valid loss: 2.386691e-03 
      	| train loss (relative): 4.789132e-02 | valid loss (relative): 4.785737e-02 
Epoch 108 use: 404.81 second.

epoch 109 starting......
Epoch:  109 | train loss: 2.383906e-03 | valid loss: 2.379562e-03 
      	| train loss (relative): 4.767570e-02 | valid loss (relative): 4.780423e-02 
Epoch 109 use: 454.78 second.

epoch 110 starting......
Epoch:  110 | train loss: 2.378253e-03 | valid loss: 2.378130e-03 
      	| train loss (relative): 4.756014e-02 | valid loss (relative): 4.757840e-02 
Epoch 110 use: 381.67 second.

epoch 111 starting......
Epoch:  111 | train loss: 2.372539e-03 | valid loss: 2.373024e-03 
      	| train loss (relative): 4.743094e-02 | valid loss (relative): 4.764805e-02 
Epoch 111 use: 399.62 second.

epoch 112 starting......
Epoch:  112 | train loss: 2.367977e-03 | valid loss: 2.366725e-03 
      	| train loss (relative): 4.733827e-02 | valid loss (relative): 4.742660e-02 
Epoch 112 use: 387.18 second.

epoch 113 starting......
Epoch:  113 | train loss: 2.362229e-03 | valid loss: 2.361779e-03 
      	| train loss (relative): 4.722816e-02 | valid loss (relative): 4.732918e-02 
Epoch 113 use: 387.44 second.

epoch 114 starting......
Epoch:  114 | train loss: 2.358018e-03 | valid loss: 2.360610e-03 
      	| train loss (relative): 4.712724e-02 | valid loss (relative): 4.739530e-02 
Epoch 114 use: 396.40 second.

epoch 115 starting......
Epoch:  115 | train loss: 2.354175e-03 | valid loss: 2.356081e-03 
      	| train loss (relative): 4.704756e-02 | valid loss (relative): 4.728974e-02 
Epoch 115 use: 370.42 second.

epoch 116 starting......
Epoch:  116 | train loss: 2.351107e-03 | valid loss: 2.354630e-03 
      	| train loss (relative): 4.698980e-02 | valid loss (relative): 4.724406e-02 
Epoch 116 use: 357.83 second.

epoch 117 starting......
Epoch:  117 | train loss: 2.344792e-03 | valid loss: 2.347366e-03 
      	| train loss (relative): 4.684583e-02 | valid loss (relative): 4.704393e-02 
Epoch 117 use: 375.58 second.

epoch 118 starting......
Epoch:  118 | train loss: 2.342738e-03 | valid loss: 2.347652e-03 
      	| train loss (relative): 4.681547e-02 | valid loss (relative): 4.679848e-02 
Epoch 118 use: 352.99 second.

epoch 119 starting......
Epoch:  119 | train loss: 2.341968e-03 | valid loss: 2.347596e-03 
      	| train loss (relative): 4.677831e-02 | valid loss (relative): 4.692826e-02 
Epoch 119 use: 411.86 second.

epoch 120 starting......
Epoch:  120 | train loss: 2.335637e-03 | valid loss: 2.347576e-03 
      	| train loss (relative): 4.665364e-02 | valid loss (relative): 4.703459e-02 
Epoch 120 use: 375.84 second.

epoch 121 starting......
Epoch:  121 | train loss: 2.337146e-03 | valid loss: 2.349504e-03 
      	| train loss (relative): 4.669990e-02 | valid loss (relative): 4.701209e-02 
Epoch 121 use: 354.17 second.

epoch 122 starting......
Epoch:  122 | train loss: 2.331182e-03 | valid loss: 2.338200e-03 
      	| train loss (relative): 4.655683e-02 | valid loss (relative): 4.702802e-02 
Epoch 122 use: 366.25 second.

epoch 123 starting......
Epoch:  123 | train loss: 2.327004e-03 | valid loss: 2.327801e-03 
      	| train loss (relative): 4.648579e-02 | valid loss (relative): 4.658312e-02 
Epoch 123 use: 363.67 second.

epoch 124 starting......
Epoch:  124 | train loss: 2.315092e-03 | valid loss: 2.326664e-03 
      	| train loss (relative): 4.621704e-02 | valid loss (relative): 4.681642e-02 
Epoch 124 use: 344.24 second.

epoch 125 starting......
Epoch:  125 | train loss: 2.311017e-03 | valid loss: 2.320469e-03 
      	| train loss (relative): 4.614934e-02 | valid loss (relative): 4.628003e-02 
Epoch 125 use: 342.27 second.

epoch 126 starting......
Epoch:  126 | train loss: 2.309362e-03 | valid loss: 2.327357e-03 
      	| train loss (relative): 4.610265e-02 | valid loss (relative): 4.637857e-02 
Epoch 126 use: 356.81 second.

epoch 127 starting......
Epoch:  127 | train loss: 2.312475e-03 | valid loss: 2.313804e-03 
      	| train loss (relative): 4.615485e-02 | valid loss (relative): 4.633975e-02 
Epoch 127 use: 355.58 second.

epoch 128 starting......
Epoch:  128 | train loss: 2.305926e-03 | valid loss: 2.322249e-03 
      	| train loss (relative): 4.602332e-02 | valid loss (relative): 4.618999e-02 
Epoch 128 use: 359.72 second.

epoch 129 starting......
Epoch:  129 | train loss: 2.318115e-03 | valid loss: 2.328092e-03 
      	| train loss (relative): 4.627591e-02 | valid loss (relative): 4.655837e-02 
Epoch 129 use: 350.99 second.

epoch 130 starting......
Epoch:  130 | train loss: 2.304214e-03 | valid loss: 2.315614e-03 
      	| train loss (relative): 4.598290e-02 | valid loss (relative): 4.675398e-02 
Epoch 130 use: 360.26 second.

epoch 131 starting......
Epoch:  131 | train loss: 2.304642e-03 | valid loss: 2.315218e-03 
      	| train loss (relative): 4.599953e-02 | valid loss (relative): 4.631874e-02 
Epoch 131 use: 356.01 second.

epoch 132 starting......
Epoch:  132 | train loss: 2.297170e-03 | valid loss: 2.314206e-03 
      	| train loss (relative): 4.583928e-02 | valid loss (relative): 4.642360e-02 
Epoch 132 use: 354.41 second.

epoch 133 starting......
Epoch:  133 | train loss: 2.290148e-03 | valid loss: 2.292929e-03 
      	| train loss (relative): 4.569276e-02 | valid loss (relative): 4.573974e-02 
Epoch 133 use: 368.27 second.

epoch 134 starting......
Epoch:  134 | train loss: 2.285905e-03 | valid loss: 2.292621e-03 
      	| train loss (relative): 4.559704e-02 | valid loss (relative): 4.586986e-02 
Epoch 134 use: 339.31 second.

epoch 135 starting......
Epoch:  135 | train loss: 2.290582e-03 | valid loss: 2.313835e-03 
      	| train loss (relative): 4.568476e-02 | valid loss (relative): 4.638608e-02 
Epoch 135 use: 345.60 second.

epoch 136 starting......
Epoch:  136 | train loss: 2.286923e-03 | valid loss: 2.311361e-03 
      	| train loss (relative): 4.561021e-02 | valid loss (relative): 4.595238e-02 
Epoch 136 use: 373.31 second.

epoch 137 starting......
Epoch:  137 | train loss: 2.280979e-03 | valid loss: 2.291432e-03 
      	| train loss (relative): 4.550158e-02 | valid loss (relative): 4.594897e-02 
Epoch 137 use: 370.32 second.

epoch 138 starting......
Epoch:  138 | train loss: 2.278941e-03 | valid loss: 2.299045e-03 
      	| train loss (relative): 4.545306e-02 | valid loss (relative): 4.587259e-02 
Epoch 138 use: 422.58 second.

epoch 139 starting......
Epoch:  139 | train loss: 2.275529e-03 | valid loss: 2.290864e-03 
      	| train loss (relative): 4.537945e-02 | valid loss (relative): 4.607468e-02 
Epoch 139 use: 369.26 second.

epoch 140 starting......
Epoch:  140 | train loss: 2.271850e-03 | valid loss: 2.287327e-03 
      	| train loss (relative): 4.530283e-02 | valid loss (relative): 4.585199e-02 
Epoch 140 use: 430.09 second.

epoch 141 starting......
Epoch:  141 | train loss: 2.267320e-03 | valid loss: 2.281272e-03 
      	| train loss (relative): 4.520783e-02 | valid loss (relative): 4.546715e-02 
Epoch 141 use: 362.20 second.

epoch 142 starting......
Epoch:  142 | train loss: 2.260164e-03 | valid loss: 2.282382e-03 
      	| train loss (relative): 4.506138e-02 | valid loss (relative): 4.560322e-02 
Epoch 142 use: 396.59 second.

epoch 143 starting......
Epoch:  143 | train loss: 2.270111e-03 | valid loss: 2.297032e-03 
      	| train loss (relative): 4.525770e-02 | valid loss (relative): 4.532855e-02 
Epoch 143 use: 376.19 second.

epoch 144 starting......
Epoch:  144 | train loss: 2.266484e-03 | valid loss: 2.273548e-03 
      	| train loss (relative): 4.518056e-02 | valid loss (relative): 4.543528e-02 
Epoch 144 use: 420.09 second.

epoch 145 starting......
Epoch:  145 | train loss: 2.261321e-03 | valid loss: 2.268420e-03 
      	| train loss (relative): 4.508558e-02 | valid loss (relative): 4.536226e-02 
Epoch 145 use: 375.77 second.

epoch 146 starting......
Epoch:  146 | train loss: 2.244193e-03 | valid loss: 2.262306e-03 
      	| train loss (relative): 4.471837e-02 | valid loss (relative): 4.505604e-02 
Epoch 146 use: 406.28 second.

epoch 147 starting......
Epoch:  147 | train loss: 2.244100e-03 | valid loss: 2.261042e-03 
      	| train loss (relative): 4.472513e-02 | valid loss (relative): 4.528728e-02 
Epoch 147 use: 380.75 second.

epoch 148 starting......
Epoch:  148 | train loss: 2.245188e-03 | valid loss: 2.282294e-03 
      	| train loss (relative): 4.473711e-02 | valid loss (relative): 4.595871e-02 
Epoch 148 use: 588.07 second.

epoch 149 starting......
Epoch:  149 | train loss: 2.256859e-03 | valid loss: 2.288618e-03 
      	| train loss (relative): 4.497844e-02 | valid loss (relative): 4.593449e-02 
Epoch 149 use: 663.36 second.

epoch 150 starting......
Epoch:  150 | train loss: 2.247891e-03 | valid loss: 2.255397e-03 
      	| train loss (relative): 4.478975e-02 | valid loss (relative): 4.506085e-02 
Epoch 150 use: 398.65 second.

epoch 151 starting......
Epoch:  151 | train loss: 2.235971e-03 | valid loss: 2.300114e-03 
      	| train loss (relative): 4.455546e-02 | valid loss (relative): 4.515159e-02 
Epoch 151 use: 429.99 second.

epoch 152 starting......
Epoch:  152 | train loss: 2.243014e-03 | valid loss: 2.263336e-03 
      	| train loss (relative): 4.468558e-02 | valid loss (relative): 4.509770e-02 
Epoch 152 use: 402.29 second.

epoch 153 starting......
Epoch:  153 | train loss: 2.235477e-03 | valid loss: 2.267334e-03 
      	| train loss (relative): 4.453733e-02 | valid loss (relative): 4.474515e-02 
Epoch 153 use: 906.78 second.

epoch 154 starting......
Epoch:  154 | train loss: 2.228528e-03 | valid loss: 2.260536e-03 
      	| train loss (relative): 4.438769e-02 | valid loss (relative): 4.553343e-02 
Epoch 154 use: 895.98 second.

epoch 155 starting......
Epoch:  155 | train loss: 2.232867e-03 | valid loss: 2.242536e-03 
      	| train loss (relative): 4.448798e-02 | valid loss (relative): 4.472455e-02 
Epoch 155 use: 902.70 second.

epoch 156 starting......
Epoch:  156 | train loss: 2.226698e-03 | valid loss: 2.259305e-03 
      	| train loss (relative): 4.435219e-02 | valid loss (relative): 4.514736e-02 
Epoch 156 use: 1050.69 second.

epoch 157 starting......
Epoch:  157 | train loss: 2.225169e-03 | valid loss: 2.240644e-03 
      	| train loss (relative): 4.431587e-02 | valid loss (relative): 4.434635e-02 
Epoch 157 use: 758.96 second.

epoch 158 starting......
Epoch:  158 | train loss: 2.216565e-03 | valid loss: 2.233990e-03 
      	| train loss (relative): 4.414564e-02 | valid loss (relative): 4.455503e-02 
Epoch 158 use: 663.59 second.

epoch 159 starting......
Epoch:  159 | train loss: 2.217687e-03 | valid loss: 2.233435e-03 
      	| train loss (relative): 4.416153e-02 | valid loss (relative): 4.492046e-02 
Epoch 159 use: 719.73 second.

epoch 160 starting......
Epoch:  160 | train loss: 2.210417e-03 | valid loss: 2.237699e-03 
      	| train loss (relative): 4.401472e-02 | valid loss (relative): 4.470445e-02 
Epoch 160 use: 969.11 second.

epoch 161 starting......
Epoch:  161 | train loss: 2.215978e-03 | valid loss: 2.235909e-03 
      	| train loss (relative): 4.412787e-02 | valid loss (relative): 4.505182e-02 
Epoch 161 use: 567.46 second.

epoch 162 starting......
Epoch:  162 | train loss: 2.219732e-03 | valid loss: 2.221142e-03 
      	| train loss (relative): 4.419654e-02 | valid loss (relative): 4.429429e-02 
Epoch 162 use: 469.96 second.

epoch 163 starting......
Epoch:  163 | train loss: 2.193374e-03 | valid loss: 2.213398e-03 
      	| train loss (relative): 4.365532e-02 | valid loss (relative): 4.422069e-02 
Epoch 163 use: 341.77 second.

epoch 164 starting......
Epoch:  164 | train loss: 2.194627e-03 | valid loss: 2.215463e-03 
      	| train loss (relative): 4.368343e-02 | valid loss (relative): 4.414620e-02 
Epoch 164 use: 345.03 second.

epoch 165 starting......
Epoch:  165 | train loss: 2.196186e-03 | valid loss: 2.219961e-03 
      	| train loss (relative): 4.372075e-02 | valid loss (relative): 4.397817e-02 
Epoch 165 use: 373.15 second.

epoch 166 starting......
Epoch:  166 | train loss: 2.205139e-03 | valid loss: 2.217140e-03 
      	| train loss (relative): 4.388868e-02 | valid loss (relative): 4.429711e-02 
Epoch 166 use: 364.29 second.

epoch 167 starting......
Epoch:  167 | train loss: 2.186809e-03 | valid loss: 2.198661e-03 
      	| train loss (relative): 4.352004e-02 | valid loss (relative): 4.375486e-02 
Epoch 167 use: 366.61 second.

epoch 168 starting......
Epoch:  168 | train loss: 2.178619e-03 | valid loss: 2.203507e-03 
      	| train loss (relative): 4.335206e-02 | valid loss (relative): 4.405451e-02 
Epoch 168 use: 321.58 second.

epoch 169 starting......
Epoch:  169 | train loss: 2.181838e-03 | valid loss: 2.206076e-03 
      	| train loss (relative): 4.341561e-02 | valid loss (relative): 4.432597e-02 
Epoch 169 use: 339.39 second.

epoch 170 starting......
Epoch:  170 | train loss: 2.189815e-03 | valid loss: 2.247401e-03 
      	| train loss (relative): 4.358506e-02 | valid loss (relative): 4.428077e-02 
Epoch 170 use: 321.99 second.

epoch 171 starting......
Epoch:  171 | train loss: 2.190187e-03 | valid loss: 2.206015e-03 
      	| train loss (relative): 4.357678e-02 | valid loss (relative): 4.411770e-02 
Epoch 171 use: 348.03 second.

epoch 172 starting......
Epoch:  172 | train loss: 2.181038e-03 | valid loss: 2.213541e-03 
      	| train loss (relative): 4.339128e-02 | valid loss (relative): 4.375841e-02 
Epoch 172 use: 326.10 second.

epoch 173 starting......
Epoch:  173 | train loss: 2.177061e-03 | valid loss: 2.209075e-03 
      	| train loss (relative): 4.330357e-02 | valid loss (relative): 4.373329e-02 
Epoch 173 use: 334.26 second.

epoch 174 starting......
Epoch:  174 | train loss: 2.185193e-03 | valid loss: 2.235449e-03 
      	| train loss (relative): 4.347581e-02 | valid loss (relative): 4.416886e-02 
Epoch 174 use: 362.00 second.

epoch 175 starting......
Epoch:  175 | train loss: 2.185342e-03 | valid loss: 2.202953e-03 
      	| train loss (relative): 4.347567e-02 | valid loss (relative): 4.389071e-02 
Epoch 175 use: 364.32 second.

epoch 176 starting......
Epoch:  176 | train loss: 2.175369e-03 | valid loss: 2.197789e-03 
      	| train loss (relative): 4.327073e-02 | valid loss (relative): 4.384336e-02 
Epoch 176 use: 354.76 second.

epoch 177 starting......
Epoch:  177 | train loss: 2.169795e-03 | valid loss: 2.202534e-03 
      	| train loss (relative): 4.315427e-02 | valid loss (relative): 4.424629e-02 
Epoch 177 use: 354.61 second.

epoch 178 starting......
Epoch:  178 | train loss: 2.174407e-03 | valid loss: 2.206721e-03 
      	| train loss (relative): 4.324873e-02 | valid loss (relative): 4.381064e-02 
Epoch 178 use: 338.65 second.

epoch 179 starting......
Epoch:  179 | train loss: 2.178065e-03 | valid loss: 2.198863e-03 
      	| train loss (relative): 4.332858e-02 | valid loss (relative): 4.374670e-02 
Epoch 179 use: 343.35 second.

epoch 180 starting......
Epoch:  180 | train loss: 2.168620e-03 | valid loss: 2.185844e-03 
      	| train loss (relative): 4.313419e-02 | valid loss (relative): 4.339059e-02 
Epoch 180 use: 333.97 second.

epoch 181 starting......
Epoch:  181 | train loss: 2.160540e-03 | valid loss: 2.212671e-03 
      	| train loss (relative): 4.296960e-02 | valid loss (relative): 4.354873e-02 
Epoch 181 use: 359.70 second.

epoch 182 starting......
Epoch:  182 | train loss: 2.162149e-03 | valid loss: 2.178720e-03 
      	| train loss (relative): 4.298858e-02 | valid loss (relative): 4.310717e-02 
Epoch 182 use: 382.99 second.

epoch 183 starting......
Epoch:  183 | train loss: 2.151650e-03 | valid loss: 2.186721e-03 
      	| train loss (relative): 4.278177e-02 | valid loss (relative): 4.348224e-02 
Epoch 183 use: 373.78 second.

epoch 184 starting......
Epoch:  184 | train loss: 2.154752e-03 | valid loss: 2.233502e-03 
      	| train loss (relative): 4.284538e-02 | valid loss (relative): 4.461915e-02 
Epoch 184 use: 380.57 second.

epoch 185 starting......
Epoch:  185 | train loss: 2.157802e-03 | valid loss: 2.177918e-03 
      	| train loss (relative): 4.289499e-02 | valid loss (relative): 4.330891e-02 
Epoch 185 use: 386.02 second.

epoch 186 starting......
Epoch:  186 | train loss: 2.150987e-03 | valid loss: 2.178472e-03 
      	| train loss (relative): 4.276370e-02 | valid loss (relative): 4.355957e-02 
Epoch 186 use: 388.01 second.

epoch 187 starting......
Epoch:  187 | train loss: 2.149708e-03 | valid loss: 2.179365e-03 
      	| train loss (relative): 4.274866e-02 | valid loss (relative): 4.332547e-02 
Epoch 187 use: 359.30 second.

epoch 188 starting......
Epoch:  188 | train loss: 2.150810e-03 | valid loss: 2.179895e-03 
      	| train loss (relative): 4.275580e-02 | valid loss (relative): 4.359442e-02 
Epoch 188 use: 351.06 second.

epoch 189 starting......
Epoch:  189 | train loss: 2.147290e-03 | valid loss: 2.177316e-03 
      	| train loss (relative): 4.269288e-02 | valid loss (relative): 4.322630e-02 
Epoch 189 use: 400.72 second.

epoch 190 starting......
Epoch:  190 | train loss: 2.149330e-03 | valid loss: 2.163883e-03 
      	| train loss (relative): 4.272671e-02 | valid loss (relative): 4.311369e-02 
Epoch 190 use: 428.02 second.

epoch 191 starting......
Epoch:  191 | train loss: 2.139846e-03 | valid loss: 2.175343e-03 
      	| train loss (relative): 4.253278e-02 | valid loss (relative): 4.314870e-02 
Epoch 191 use: 401.59 second.

epoch 192 starting......
Epoch:  192 | train loss: 2.136357e-03 | valid loss: 2.160027e-03 
      	| train loss (relative): 4.246520e-02 | valid loss (relative): 4.339841e-02 
Epoch 192 use: 726.98 second.

epoch 193 starting......
Epoch:  193 | train loss: 2.138248e-03 | valid loss: 2.161603e-03 
      	| train loss (relative): 4.249046e-02 | valid loss (relative): 4.301651e-02 
Epoch 193 use: 444.58 second.

epoch 194 starting......
Epoch:  194 | train loss: 2.135272e-03 | valid loss: 2.183448e-03 
      	| train loss (relative): 4.243302e-02 | valid loss (relative): 4.373579e-02 
Epoch 194 use: 531.48 second.

epoch 195 starting......
Epoch:  195 | train loss: 2.132850e-03 | valid loss: 2.153458e-03 
      	| train loss (relative): 4.238321e-02 | valid loss (relative): 4.291235e-02 
Epoch 195 use: 410.74 second.

epoch 196 starting......
Epoch:  196 | train loss: 2.131373e-03 | valid loss: 2.151134e-03 
      	| train loss (relative): 4.235093e-02 | valid loss (relative): 4.280537e-02 
Epoch 196 use: 391.21 second.

epoch 197 starting......
Epoch:  197 | train loss: 2.124987e-03 | valid loss: 2.149343e-03 
      	| train loss (relative): 4.222269e-02 | valid loss (relative): 4.287356e-02 
Epoch 197 use: 620.76 second.

epoch 198 starting......
Epoch:  198 | train loss: 2.123333e-03 | valid loss: 2.151356e-03 
      	| train loss (relative): 4.218953e-02 | valid loss (relative): 4.291421e-02 
Epoch 198 use: 443.65 second.

epoch 199 starting......
Epoch:  199 | train loss: 2.125876e-03 | valid loss: 2.142651e-03 
      	| train loss (relative): 4.223602e-02 | valid loss (relative): 4.284290e-02 
Epoch 199 use: 360.92 second.

test MSE Error: 2.181128e-03 | relative MSE Error: 4.369530e-02 
 Total time used for training: 11.92 hour.
MESLoss saved to  /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_MSELoss_Latent_64_nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_200.txt
relative MSELoss saved to  /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_reMSELoss_Latent_64_nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_200.txt
model saved to /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_64_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_200.pth
model_dict saved to /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_64_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_200_dict.pth
... Training slugflow data group 3 completed, Run finished Thu 12 Aug 20:17:50 BST 2021 ...
{'data_dir': '/rds/general/user/jy220/home/tensors/', 'data_type': 'tensors', 'vtu_fields': 'Component1::ComponentMassFractionPhase1, phase1::Velocity', 'variational': 'False', 'optimizer': 'Adamax', 'structured': 'False', 'changing_lr': 'False', 'self_concat': '2', 'dimension': '3', 'sfc_nums': '3', 'components': '4', 'nearest_neighbouring': 'True', 'dims_latent': '64', 'activation': 'Tanh', 'sfc_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/sfcs.pt', 'inv_sfc_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/inv_sfcs.pt', 'coords_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/coords.pt', 'cells_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/cells.npy', 'state_load': '/rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_64_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_200_dict.pth', 'batch_size': '16', 'lr': '0.001', 'n_epoches': '100', 'seed': '32', 'visualize': 'False', 'reconstructed_path': 'None', 'reconstruct_start_index': '0', 'reconstruct_end_index': '50', 'save_path': '/rds/general/user/jy220/home/new_results/'}
[[ 2.50825269e-01 -1.67256990e-02 -1.22268730e-03]
 [ 2.38611959e-01 -5.50627777e-03 -5.06451913e-03]
 [ 2.39454874e-01 -1.59563257e-02  3.96778323e-03]
 ...
 [ 9.11361096e+00 -2.21908381e-02  2.22384902e-03]
 [ 9.11847310e+00 -2.84346931e-02 -2.34839590e-04]
 [ 9.11039661e+00 -2.51498573e-02 -8.70661044e-03]]
{'tetra': array([[      0,       1,       2,       3],
       [      4,       5,       6,       7],
       [      8,       9,      10,      11],
       ...,
       [1342744, 1342745, 1342746, 1342747],
       [1342748, 1342749, 1342750, 1342751],
       [1342752, 1342753, 1342754, 1342755]])}
vtu fields:  ['Component1::ComponentMassFractionPhase1', 'phase1::Velocity'] 
 structured  False 
 activation  Tanh() 
 self concat  2 
 sfc_nums  3 

dims_latent  64 
 components  4 
 nearest_neighbouring  True 

visualize  False 
 sample number  1706 

reading sfc nums......
[array([  41684,   41685,   41674, ..., 1235560, 1235564, 1235558]), array([954289, 954290, 954291, ..., 420685, 420687, 420688]), array([  40156,   40155,   40158, ..., 1123478, 1123475, 1123467])]
reading inverse sfc nums......
[array([   8146,    8145,    8144, ..., 1274444, 1274445, 1273166]), array([547280, 547283, 593037, ..., 933933, 933932, 864488]), array([ 66437,  66439, 122512, ..., 459416, 415130, 459418])]
Computing min and max......

tk:  tensor([0.7581, 0.0418, 0.1840, 0.1464], dtype=torch.float64) 

tb:  tensor([-0.3816, -0.3618, -0.0308,  0.1446], dtype=torch.float64) 

length of train set: 1505 

length of valid set: 100 

length of test set: 101 

torch device num: 4 

Let's use 4 GPUs!
epoch 200 starting......
Epoch:  200 | train loss: 2.388570e-03 | valid loss: 2.101066e-03 
      	| train loss (relative): 4.743295e-02 | valid loss (relative): 4.172627e-02 
Epoch 200 use: 488.07 second.

epoch 201 starting......
Epoch:  201 | train loss: 2.115573e-03 | valid loss: 2.082078e-03 
      	| train loss (relative): 4.203366e-02 | valid loss (relative): 4.126299e-02 
Epoch 201 use: 525.42 second.

epoch 202 starting......
Epoch:  202 | train loss: 2.102549e-03 | valid loss: 2.074949e-03 
      	| train loss (relative): 4.176039e-02 | valid loss (relative): 4.118314e-02 
Epoch 202 use: 460.75 second.

epoch 203 starting......
Epoch:  203 | train loss: 2.096638e-03 | valid loss: 2.073730e-03 
      	| train loss (relative): 4.165094e-02 | valid loss (relative): 4.127145e-02 
Epoch 203 use: 489.13 second.

epoch 204 starting......
Epoch:  204 | train loss: 2.096426e-03 | valid loss: 2.073506e-03 
      	| train loss (relative): 4.164716e-02 | valid loss (relative): 4.112041e-02 
Epoch 204 use: 449.39 second.

epoch 205 starting......
Epoch:  205 | train loss: 2.095292e-03 | valid loss: 2.072352e-03 
      	| train loss (relative): 4.161327e-02 | valid loss (relative): 4.110307e-02 
Epoch 205 use: 591.10 second.

epoch 206 starting......
Epoch:  206 | train loss: 2.090679e-03 | valid loss: 2.074081e-03 
      	| train loss (relative): 4.152304e-02 | valid loss (relative): 4.109274e-02 
Epoch 206 use: 503.71 second.

epoch 207 starting......
Epoch:  207 | train loss: 2.091342e-03 | valid loss: 2.073799e-03 
      	| train loss (relative): 4.153490e-02 | valid loss (relative): 4.103101e-02 
Epoch 207 use: 427.84 second.

epoch 208 starting......
Epoch:  208 | train loss: 2.092911e-03 | valid loss: 2.073823e-03 
      	| train loss (relative): 4.155874e-02 | valid loss (relative): 4.126868e-02 
Epoch 208 use: 474.94 second.

epoch 209 starting......
Epoch:  209 | train loss: 2.092917e-03 | valid loss: 2.078322e-03 
      	| train loss (relative): 4.156736e-02 | valid loss (relative): 4.120979e-02 
Epoch 209 use: 449.26 second.

epoch 210 starting......
Epoch:  210 | train loss: 2.093124e-03 | valid loss: 2.076960e-03 
      	| train loss (relative): 4.156338e-02 | valid loss (relative): 4.121593e-02 
Epoch 210 use: 416.64 second.

epoch 211 starting......
Epoch:  211 | train loss: 2.091663e-03 | valid loss: 2.077168e-03 
      	| train loss (relative): 4.153642e-02 | valid loss (relative): 4.122804e-02 
Epoch 211 use: 427.64 second.

epoch 212 starting......
Epoch:  212 | train loss: 2.094389e-03 | valid loss: 2.080138e-03 
      	| train loss (relative): 4.158937e-02 | valid loss (relative): 4.126092e-02 
Epoch 212 use: 432.63 second.

epoch 213 starting......
Epoch:  213 | train loss: 2.096886e-03 | valid loss: 2.081361e-03 
      	| train loss (relative): 4.163956e-02 | valid loss (relative): 4.142746e-02 
Epoch 213 use: 424.40 second.

epoch 214 starting......
Epoch:  214 | train loss: 2.093363e-03 | valid loss: 2.079302e-03 
      	| train loss (relative): 4.156677e-02 | valid loss (relative): 4.124914e-02 
Epoch 214 use: 411.54 second.

epoch 215 starting......
Epoch:  215 | train loss: 2.093675e-03 | valid loss: 2.076914e-03 
      	| train loss (relative): 4.156604e-02 | valid loss (relative): 4.133295e-02 
Epoch 215 use: 448.27 second.

epoch 216 starting......
Epoch:  216 | train loss: 2.089091e-03 | valid loss: 2.081739e-03 
      	| train loss (relative): 4.148851e-02 | valid loss (relative): 4.118053e-02 
Epoch 216 use: 424.87 second.

epoch 217 starting......
Epoch:  217 | train loss: 2.091797e-03 | valid loss: 2.079505e-03 
      	| train loss (relative): 4.153455e-02 | valid loss (relative): 4.126582e-02 
Epoch 217 use: 414.36 second.

epoch 218 starting......
Epoch:  218 | train loss: 2.088255e-03 | valid loss: 2.081661e-03 
      	| train loss (relative): 4.146000e-02 | valid loss (relative): 4.123014e-02 
Epoch 218 use: 417.82 second.

epoch 219 starting......
Epoch:  219 | train loss: 2.092238e-03 | valid loss: 2.088869e-03 
      	| train loss (relative): 4.153439e-02 | valid loss (relative): 4.133780e-02 
Epoch 219 use: 436.10 second.

epoch 220 starting......
Epoch:  220 | train loss: 2.089967e-03 | valid loss: 2.094140e-03 
      	| train loss (relative): 4.149369e-02 | valid loss (relative): 4.102518e-02 
Epoch 220 use: 419.41 second.

epoch 221 starting......
Epoch:  221 | train loss: 2.088142e-03 | valid loss: 2.073820e-03 
      	| train loss (relative): 4.145614e-02 | valid loss (relative): 4.112248e-02 
Epoch 221 use: 407.45 second.

epoch 222 starting......
Epoch:  222 | train loss: 2.086376e-03 | valid loss: 2.074309e-03 
      	| train loss (relative): 4.142160e-02 | valid loss (relative): 4.116128e-02 
Epoch 222 use: 413.82 second.

epoch 223 starting......
Epoch:  223 | train loss: 2.092140e-03 | valid loss: 2.085291e-03 
      	| train loss (relative): 4.153207e-02 | valid loss (relative): 4.150843e-02 
Epoch 223 use: 431.21 second.

epoch 224 starting......
Epoch:  224 | train loss: 2.087444e-03 | valid loss: 2.083268e-03 
      	| train loss (relative): 4.143704e-02 | valid loss (relative): 4.121004e-02 
Epoch 224 use: 412.58 second.

epoch 225 starting......
Epoch:  225 | train loss: 2.085007e-03 | valid loss: 2.080464e-03 
      	| train loss (relative): 4.138879e-02 | valid loss (relative): 4.109596e-02 
Epoch 225 use: 409.55 second.

epoch 226 starting......
Epoch:  226 | train loss: 2.077480e-03 | valid loss: 2.070327e-03 
      	| train loss (relative): 4.123117e-02 | valid loss (relative): 4.104252e-02 
Epoch 226 use: 417.33 second.

epoch 227 starting......
Epoch:  227 | train loss: 2.076571e-03 | valid loss: 2.071217e-03 
      	| train loss (relative): 4.120751e-02 | valid loss (relative): 4.106677e-02 
Epoch 227 use: 416.51 second.

epoch 228 starting......
Epoch:  228 | train loss: 2.084437e-03 | valid loss: 2.067443e-03 
      	| train loss (relative): 4.137283e-02 | valid loss (relative): 4.078632e-02 
Epoch 228 use: 422.42 second.

epoch 229 starting......
Epoch:  229 | train loss: 2.074537e-03 | valid loss: 2.085406e-03 
      	| train loss (relative): 4.117371e-02 | valid loss (relative): 4.158099e-02 
Epoch 229 use: 411.50 second.

epoch 230 starting......
Epoch:  230 | train loss: 2.074312e-03 | valid loss: 2.075824e-03 
      	| train loss (relative): 4.116376e-02 | valid loss (relative): 4.102636e-02 
Epoch 230 use: 421.17 second.

epoch 231 starting......
Epoch:  231 | train loss: 2.073219e-03 | valid loss: 2.084762e-03 
      	| train loss (relative): 4.115089e-02 | valid loss (relative): 4.146846e-02 
Epoch 231 use: 404.15 second.

epoch 232 starting......
Epoch:  232 | train loss: 2.070459e-03 | valid loss: 2.059769e-03 
      	| train loss (relative): 4.109460e-02 | valid loss (relative): 4.071527e-02 
Epoch 232 use: 431.97 second.

epoch 233 starting......
Epoch:  233 | train loss: 2.062639e-03 | valid loss: 2.060968e-03 
      	| train loss (relative): 4.092946e-02 | valid loss (relative): 4.113697e-02 
Epoch 233 use: 410.09 second.

epoch 234 starting......
Epoch:  234 | train loss: 2.069510e-03 | valid loss: 2.056000e-03 
      	| train loss (relative): 4.106620e-02 | valid loss (relative): 4.093735e-02 
Epoch 234 use: 463.62 second.

epoch 235 starting......
Epoch:  235 | train loss: 2.058519e-03 | valid loss: 2.049624e-03 
      	| train loss (relative): 4.083697e-02 | valid loss (relative): 4.080676e-02 
Epoch 235 use: 411.56 second.

epoch 236 starting......
Epoch:  236 | train loss: 2.053440e-03 | valid loss: 2.052894e-03 
      	| train loss (relative): 4.073529e-02 | valid loss (relative): 4.058988e-02 
Epoch 236 use: 409.11 second.

epoch 237 starting......
Epoch:  237 | train loss: 2.058540e-03 | valid loss: 2.054732e-03 
      	| train loss (relative): 4.083563e-02 | valid loss (relative): 4.089412e-02 
Epoch 237 use: 415.73 second.

epoch 238 starting......
Epoch:  238 | train loss: 2.057636e-03 | valid loss: 2.047268e-03 
      	| train loss (relative): 4.081912e-02 | valid loss (relative): 4.058904e-02 
Epoch 238 use: 408.95 second.

epoch 239 starting......
Epoch:  239 | train loss: 2.055742e-03 | valid loss: 2.045630e-03 
      	| train loss (relative): 4.078031e-02 | valid loss (relative): 4.042553e-02 
Epoch 239 use: 466.89 second.

epoch 240 starting......
Epoch:  240 | train loss: 2.050657e-03 | valid loss: 2.069971e-03 
      	| train loss (relative): 4.067874e-02 | valid loss (relative): 4.114017e-02 
Epoch 240 use: 413.72 second.

epoch 241 starting......
Epoch:  241 | train loss: 2.052231e-03 | valid loss: 2.044083e-03 
      	| train loss (relative): 4.070490e-02 | valid loss (relative): 4.044156e-02 
Epoch 241 use: 419.98 second.

epoch 242 starting......
Epoch:  242 | train loss: 2.049299e-03 | valid loss: 2.055263e-03 
      	| train loss (relative): 4.064346e-02 | valid loss (relative): 4.056090e-02 
Epoch 242 use: 408.47 second.

epoch 243 starting......
Epoch:  243 | train loss: 2.048393e-03 | valid loss: 2.050249e-03 
      	| train loss (relative): 4.063183e-02 | valid loss (relative): 4.055818e-02 
Epoch 243 use: 408.21 second.

epoch 244 starting......
Epoch:  244 | train loss: 2.053863e-03 | valid loss: 2.046663e-03 
      	| train loss (relative): 4.073488e-02 | valid loss (relative): 4.074206e-02 
Epoch 244 use: 430.36 second.

epoch 245 starting......
Epoch:  245 | train loss: 2.050394e-03 | valid loss: 2.051896e-03 
      	| train loss (relative): 4.066224e-02 | valid loss (relative): 4.083230e-02 
Epoch 245 use: 429.73 second.

epoch 246 starting......
Epoch:  246 | train loss: 2.047770e-03 | valid loss: 2.072989e-03 
      	| train loss (relative): 4.061951e-02 | valid loss (relative): 4.154928e-02 
Epoch 246 use: 428.07 second.

epoch 247 starting......
Epoch:  247 | train loss: 2.052099e-03 | valid loss: 2.038463e-03 
      	| train loss (relative): 4.072126e-02 | valid loss (relative): 4.030390e-02 
Epoch 247 use: 404.00 second.

epoch 248 starting......
Epoch:  248 | train loss: 2.033588e-03 | valid loss: 2.035782e-03 
      	| train loss (relative): 4.032675e-02 | valid loss (relative): 4.030509e-02 
Epoch 248 use: 405.44 second.

epoch 249 starting......
Epoch:  249 | train loss: 2.032705e-03 | valid loss: 2.032649e-03 
      	| train loss (relative): 4.030333e-02 | valid loss (relative): 4.033386e-02 
Epoch 249 use: 448.12 second.

epoch 250 starting......
Epoch:  250 | train loss: 2.035307e-03 | valid loss: 2.033760e-03 
      	| train loss (relative): 4.035705e-02 | valid loss (relative): 4.009160e-02 
Epoch 250 use: 406.68 second.

epoch 251 starting......
Epoch:  251 | train loss: 2.038039e-03 | valid loss: 2.037605e-03 
      	| train loss (relative): 4.041904e-02 | valid loss (relative): 4.020105e-02 
Epoch 251 use: 407.36 second.

epoch 252 starting......
Epoch:  252 | train loss: 2.036918e-03 | valid loss: 2.027162e-03 
      	| train loss (relative): 4.039400e-02 | valid loss (relative): 4.012815e-02 
Epoch 252 use: 406.84 second.

epoch 253 starting......
Epoch:  253 | train loss: 2.031625e-03 | valid loss: 2.029678e-03 
      	| train loss (relative): 4.028233e-02 | valid loss (relative): 4.021511e-02 
Epoch 253 use: 411.37 second.

epoch 254 starting......
Epoch:  254 | train loss: 2.032496e-03 | valid loss: 2.033690e-03 
      	| train loss (relative): 4.029387e-02 | valid loss (relative): 4.033405e-02 
Epoch 254 use: 406.71 second.

epoch 255 starting......
Epoch:  255 | train loss: 2.031767e-03 | valid loss: 2.037685e-03 
      	| train loss (relative): 4.028667e-02 | valid loss (relative): 4.001622e-02 
Epoch 255 use: 444.23 second.

epoch 256 starting......
Epoch:  256 | train loss: 2.033331e-03 | valid loss: 2.029794e-03 
      	| train loss (relative): 4.030710e-02 | valid loss (relative): 4.025258e-02 
Epoch 256 use: 446.79 second.

epoch 257 starting......
Epoch:  257 | train loss: 2.028891e-03 | valid loss: 2.026432e-03 
      	| train loss (relative): 4.021500e-02 | valid loss (relative): 4.020817e-02 
Epoch 257 use: 402.44 second.

epoch 258 starting......
Epoch:  258 | train loss: 2.026029e-03 | valid loss: 2.022761e-03 
      	| train loss (relative): 4.015636e-02 | valid loss (relative): 4.006538e-02 
Epoch 258 use: 435.42 second.

epoch 259 starting......
Epoch:  259 | train loss: 2.024926e-03 | valid loss: 2.041428e-03 
      	| train loss (relative): 4.013697e-02 | valid loss (relative): 4.028813e-02 
Epoch 259 use: 404.89 second.

epoch 260 starting......
Epoch:  260 | train loss: 2.025674e-03 | valid loss: 2.018700e-03 
      	| train loss (relative): 4.015663e-02 | valid loss (relative): 3.980627e-02 
Epoch 260 use: 417.28 second.

epoch 261 starting......
Epoch:  261 | train loss: 2.017747e-03 | valid loss: 2.021100e-03 
      	| train loss (relative): 3.999416e-02 | valid loss (relative): 4.002373e-02 
Epoch 261 use: 404.29 second.

epoch 262 starting......
Epoch:  262 | train loss: 2.018484e-03 | valid loss: 2.018275e-03 
      	| train loss (relative): 4.000861e-02 | valid loss (relative): 3.978682e-02 
Epoch 262 use: 411.78 second.

epoch 263 starting......
Epoch:  263 | train loss: 2.020834e-03 | valid loss: 2.030841e-03 
      	| train loss (relative): 4.005283e-02 | valid loss (relative): 3.982691e-02 
Epoch 263 use: 419.47 second.

epoch 264 starting......
Epoch:  264 | train loss: 2.023268e-03 | valid loss: 2.018440e-03 
      	| train loss (relative): 4.009274e-02 | valid loss (relative): 4.002374e-02 
Epoch 264 use: 455.10 second.

epoch 265 starting......
Epoch:  265 | train loss: 2.017983e-03 | valid loss: 2.014135e-03 
      	| train loss (relative): 3.999878e-02 | valid loss (relative): 4.016560e-02 
Epoch 265 use: 466.56 second.

epoch 266 starting......
Epoch:  266 | train loss: 2.015300e-03 | valid loss: 2.033434e-03 
      	| train loss (relative): 3.993610e-02 | valid loss (relative): 4.052738e-02 
Epoch 266 use: 419.49 second.

epoch 267 starting......
Epoch:  267 | train loss: 2.016512e-03 | valid loss: 2.014686e-03 
      	| train loss (relative): 3.996246e-02 | valid loss (relative): 3.957013e-02 
Epoch 267 use: 415.26 second.

epoch 268 starting......
Epoch:  268 | train loss: 2.009043e-03 | valid loss: 2.009602e-03 
      	| train loss (relative): 3.980708e-02 | valid loss (relative): 3.969411e-02 
Epoch 268 use: 434.41 second.

epoch 269 starting......
Epoch:  269 | train loss: 2.017714e-03 | valid loss: 2.023974e-03 
      	| train loss (relative): 3.998048e-02 | valid loss (relative): 4.012723e-02 
Epoch 269 use: 416.14 second.

epoch 270 starting......
Epoch:  270 | train loss: 2.010491e-03 | valid loss: 2.019141e-03 
      	| train loss (relative): 3.984696e-02 | valid loss (relative): 3.965120e-02 
Epoch 270 use: 417.67 second.

epoch 271 starting......
Epoch:  271 | train loss: 2.005550e-03 | valid loss: 2.000245e-03 
      	| train loss (relative): 3.973947e-02 | valid loss (relative): 3.942789e-02 
Epoch 271 use: 449.59 second.

epoch 272 starting......
Epoch:  272 | train loss: 2.001639e-03 | valid loss: 2.012826e-03 
      	| train loss (relative): 3.965374e-02 | valid loss (relative): 4.003518e-02 
Epoch 272 use: 424.69 second.

epoch 273 starting......
Epoch:  273 | train loss: 2.007376e-03 | valid loss: 2.008340e-03 
      	| train loss (relative): 3.976827e-02 | valid loss (relative): 3.995671e-02 
Epoch 273 use: 449.97 second.

epoch 274 starting......
Epoch:  274 | train loss: 2.006715e-03 | valid loss: 2.002569e-03 
      	| train loss (relative): 3.975528e-02 | valid loss (relative): 3.951835e-02 
Epoch 274 use: 450.91 second.

epoch 275 starting......
Epoch:  275 | train loss: 2.002750e-03 | valid loss: 1.995363e-03 
      	| train loss (relative): 3.967204e-02 | valid loss (relative): 3.978973e-02 
Epoch 275 use: 418.41 second.

epoch 276 starting......
Epoch:  276 | train loss: 1.992902e-03 | valid loss: 2.000237e-03 
      	| train loss (relative): 3.948084e-02 | valid loss (relative): 3.987883e-02 
Epoch 276 use: 414.96 second.

epoch 277 starting......
Epoch:  277 | train loss: 1.995753e-03 | valid loss: 1.997385e-03 
      	| train loss (relative): 3.953979e-02 | valid loss (relative): 3.926216e-02 
Epoch 277 use: 410.45 second.

epoch 278 starting......
Epoch:  278 | train loss: 1.993617e-03 | valid loss: 1.988522e-03 
      	| train loss (relative): 3.948097e-02 | valid loss (relative): 3.935652e-02 
Epoch 278 use: 413.77 second.

epoch 279 starting......
Epoch:  279 | train loss: 1.983020e-03 | valid loss: 1.977967e-03 
      	| train loss (relative): 3.927161e-02 | valid loss (relative): 3.897308e-02 
Epoch 279 use: 423.00 second.

epoch 280 starting......
Epoch:  280 | train loss: 1.983849e-03 | valid loss: 1.999290e-03 
      	| train loss (relative): 3.928640e-02 | valid loss (relative): 3.984198e-02 
Epoch 280 use: 414.76 second.

epoch 281 starting......
Epoch:  281 | train loss: 1.999751e-03 | valid loss: 2.003866e-03 
      	| train loss (relative): 3.961148e-02 | valid loss (relative): 3.942697e-02 
Epoch 281 use: 413.77 second.

epoch 282 starting......
Epoch:  282 | train loss: 1.984810e-03 | valid loss: 1.977540e-03 
      	| train loss (relative): 3.931271e-02 | valid loss (relative): 3.909356e-02 
Epoch 282 use: 407.00 second.

epoch 283 starting......
Epoch:  283 | train loss: 1.976163e-03 | valid loss: 1.976392e-03 
      	| train loss (relative): 3.913193e-02 | valid loss (relative): 3.908864e-02 
Epoch 283 use: 405.39 second.

epoch 284 starting......
Epoch:  284 | train loss: 1.974912e-03 | valid loss: 1.976446e-03 
      	| train loss (relative): 3.910496e-02 | valid loss (relative): 3.900286e-02 
Epoch 284 use: 408.00 second.

epoch 285 starting......
Epoch:  285 | train loss: 1.977554e-03 | valid loss: 1.981278e-03 
      	| train loss (relative): 3.915567e-02 | valid loss (relative): 3.913428e-02 
Epoch 285 use: 422.37 second.

epoch 286 starting......
Epoch:  286 | train loss: 1.980803e-03 | valid loss: 1.992950e-03 
      	| train loss (relative): 3.922328e-02 | valid loss (relative): 3.896817e-02 
Epoch 286 use: 414.66 second.

epoch 287 starting......
Epoch:  287 | train loss: 1.986836e-03 | valid loss: 1.984731e-03 
      	| train loss (relative): 3.934235e-02 | valid loss (relative): 3.938336e-02 
Epoch 287 use: 416.80 second.

epoch 288 starting......
Epoch:  288 | train loss: 1.976149e-03 | valid loss: 1.976046e-03 
      	| train loss (relative): 3.913145e-02 | valid loss (relative): 3.922308e-02 
Epoch 288 use: 410.14 second.

epoch 289 starting......
Epoch:  289 | train loss: 1.976694e-03 | valid loss: 1.988408e-03 
      	| train loss (relative): 3.914160e-02 | valid loss (relative): 3.962100e-02 
Epoch 289 use: 421.21 second.

epoch 290 starting......
Epoch:  290 | train loss: 1.975077e-03 | valid loss: 1.983465e-03 
      	| train loss (relative): 3.910649e-02 | valid loss (relative): 3.930602e-02 
Epoch 290 use: 415.31 second.

epoch 291 starting......
Epoch:  291 | train loss: 1.969310e-03 | valid loss: 1.961625e-03 
      	| train loss (relative): 3.898486e-02 | valid loss (relative): 3.868870e-02 
Epoch 291 use: 412.96 second.

epoch 292 starting......
Epoch:  292 | train loss: 1.958480e-03 | valid loss: 1.961726e-03 
      	| train loss (relative): 3.876751e-02 | valid loss (relative): 3.901133e-02 
Epoch 292 use: 426.89 second.

epoch 293 starting......
Epoch:  293 | train loss: 1.959573e-03 | valid loss: 1.967705e-03 
      	| train loss (relative): 3.879043e-02 | valid loss (relative): 3.923045e-02 
Epoch 293 use: 426.44 second.

epoch 294 starting......
Epoch:  294 | train loss: 1.967103e-03 | valid loss: 1.968555e-03 
      	| train loss (relative): 3.894684e-02 | valid loss (relative): 3.926213e-02 
Epoch 294 use: 447.84 second.

epoch 295 starting......
Epoch:  295 | train loss: 1.962702e-03 | valid loss: 1.970290e-03 
      	| train loss (relative): 3.884607e-02 | valid loss (relative): 3.906395e-02 
Epoch 295 use: 429.96 second.

epoch 296 starting......
Epoch:  296 | train loss: 1.963580e-03 | valid loss: 2.006634e-03 
      	| train loss (relative): 3.886100e-02 | valid loss (relative): 4.012776e-02 
Epoch 296 use: 423.16 second.

epoch 297 starting......
Epoch:  297 | train loss: 1.965428e-03 | valid loss: 1.960272e-03 
      	| train loss (relative): 3.889782e-02 | valid loss (relative): 3.900116e-02 
Epoch 297 use: 471.36 second.

epoch 298 starting......
Epoch:  298 | train loss: 1.956257e-03 | valid loss: 1.961474e-03 
      	| train loss (relative): 3.872036e-02 | valid loss (relative): 3.873311e-02 
Epoch 298 use: 422.62 second.

epoch 299 starting......
Epoch:  299 | train loss: 1.960194e-03 | valid loss: 1.976135e-03 
      	| train loss (relative): 3.879997e-02 | valid loss (relative): 3.893016e-02 
Epoch 299 use: 419.77 second.

test MSE Error: 2.070535e-03 | relative MSE Error: 4.100632e-02 
 Total time used for training: 11.91 hour.
MESLoss saved to  /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_MSELoss_Latent_64_nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_300.txt
relative MSELoss saved to  /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_reMSELoss_Latent_64_nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_300.txt
model saved to /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_64_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_300.pth
model_dict saved to /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_64_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_300_dict.pth
... Training slugflow data group 3 completed, Run finished Fri 13 Aug 10:51:38 BST 2021 ...
{'data_dir': '/rds/general/user/jy220/home/tensors/', 'data_type': 'tensors', 'vtu_fields': 'Component1::ComponentMassFractionPhase1, phase1::Velocity', 'variational': 'False', 'optimizer': 'Adamax', 'structured': 'False', 'changing_lr': 'False', 'self_concat': '2', 'dimension': '3', 'sfc_nums': '3', 'components': '4', 'nearest_neighbouring': 'True', 'dims_latent': '64', 'activation': 'Tanh', 'sfc_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/sfcs.pt', 'inv_sfc_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/inv_sfcs.pt', 'coords_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/coords.pt', 'cells_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/cells.npy', 'state_load': '/rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_64_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_300_dict.pth', 'batch_size': '16', 'lr': '0.001', 'n_epoches': '150', 'seed': '17', 'visualize': 'False', 'reconstructed_path': 'None', 'reconstruct_start_index': '0', 'reconstruct_end_index': '50', 'save_path': '/rds/general/user/jy220/home/new_results/'}
[[ 2.50825269e-01 -1.67256990e-02 -1.22268730e-03]
 [ 2.38611959e-01 -5.50627777e-03 -5.06451913e-03]
 [ 2.39454874e-01 -1.59563257e-02  3.96778323e-03]
 ...
 [ 9.11361096e+00 -2.21908381e-02  2.22384902e-03]
 [ 9.11847310e+00 -2.84346931e-02 -2.34839590e-04]
 [ 9.11039661e+00 -2.51498573e-02 -8.70661044e-03]]
{'tetra': array([[      0,       1,       2,       3],
       [      4,       5,       6,       7],
       [      8,       9,      10,      11],
       ...,
       [1342744, 1342745, 1342746, 1342747],
       [1342748, 1342749, 1342750, 1342751],
       [1342752, 1342753, 1342754, 1342755]])}
vtu fields:  ['Component1::ComponentMassFractionPhase1', 'phase1::Velocity'] 
 structured  False 
 activation  Tanh() 
 self concat  2 
 sfc_nums  3 

dims_latent  64 
 components  4 
 nearest_neighbouring  True 

visualize  False 
 sample number  1706 

reading sfc nums......
[array([  41684,   41685,   41674, ..., 1235560, 1235564, 1235558]), array([954289, 954290, 954291, ..., 420685, 420687, 420688]), array([  40156,   40155,   40158, ..., 1123478, 1123475, 1123467])]
reading inverse sfc nums......
[array([   8146,    8145,    8144, ..., 1274444, 1274445, 1273166]), array([547280, 547283, 593037, ..., 933933, 933932, 864488]), array([ 66437,  66439, 122512, ..., 459416, 415130, 459418])]
Computing min and max......

tk:  tensor([0.7581, 0.0418, 0.1840, 0.1464], dtype=torch.float64) 

tb:  tensor([-0.3816, -0.3618, -0.0308,  0.1446], dtype=torch.float64) 

length of train set: 1505 

length of valid set: 100 

length of test set: 101 

torch device num: 4 

Let's use 4 GPUs!
epoch 300 starting......
Epoch:  300 | train loss: 2.220483e-03 | valid loss: 1.898671e-03 
      	| train loss (relative): 4.399357e-02 | valid loss (relative): 3.742744e-02 
Epoch 300 use: 470.78 second.

epoch 301 starting......
Epoch:  301 | train loss: 1.964240e-03 | valid loss: 1.879069e-03 
      	| train loss (relative): 3.887070e-02 | valid loss (relative): 3.710521e-02 
Epoch 301 use: 410.17 second.

epoch 302 starting......
Epoch:  302 | train loss: 1.951548e-03 | valid loss: 1.873497e-03 
      	| train loss (relative): 3.862111e-02 | valid loss (relative): 3.699448e-02 
Epoch 302 use: 351.74 second.

epoch 303 starting......
Epoch:  303 | train loss: 1.945766e-03 | valid loss: 1.872226e-03 
      	| train loss (relative): 3.850570e-02 | valid loss (relative): 3.697152e-02 
Epoch 303 use: 355.30 second.

epoch 304 starting......
Epoch:  304 | train loss: 1.943145e-03 | valid loss: 1.874759e-03 
      	| train loss (relative): 3.845753e-02 | valid loss (relative): 3.702231e-02 
Epoch 304 use: 344.03 second.

epoch 305 starting......
Epoch:  305 | train loss: 1.941813e-03 | valid loss: 1.875498e-03 
      	| train loss (relative): 3.842314e-02 | valid loss (relative): 3.698622e-02 
Epoch 305 use: 401.49 second.

epoch 306 starting......
Epoch:  306 | train loss: 1.940381e-03 | valid loss: 1.874099e-03 
      	| train loss (relative): 3.839342e-02 | valid loss (relative): 3.695749e-02 
Epoch 306 use: 343.40 second.

epoch 307 starting......
Epoch:  307 | train loss: 1.939930e-03 | valid loss: 1.875032e-03 
      	| train loss (relative): 3.838563e-02 | valid loss (relative): 3.705626e-02 
Epoch 307 use: 389.63 second.

epoch 308 starting......
Epoch:  308 | train loss: 1.940152e-03 | valid loss: 1.874670e-03 
      	| train loss (relative): 3.838960e-02 | valid loss (relative): 3.689532e-02 
Epoch 308 use: 362.13 second.

epoch 309 starting......
Epoch:  309 | train loss: 1.940592e-03 | valid loss: 1.879753e-03 
      	| train loss (relative): 3.839900e-02 | valid loss (relative): 3.706938e-02 
Epoch 309 use: 322.03 second.

epoch 310 starting......
Epoch:  310 | train loss: 1.947379e-03 | valid loss: 1.880519e-03 
      	| train loss (relative): 3.852699e-02 | valid loss (relative): 3.721051e-02 
Epoch 310 use: 352.47 second.

epoch 311 starting......
Epoch:  311 | train loss: 1.946253e-03 | valid loss: 1.883313e-03 
      	| train loss (relative): 3.851312e-02 | valid loss (relative): 3.715901e-02 
Epoch 311 use: 356.81 second.

epoch 312 starting......
Epoch:  312 | train loss: 1.944227e-03 | valid loss: 1.884417e-03 
      	| train loss (relative): 3.847435e-02 | valid loss (relative): 3.728022e-02 
Epoch 312 use: 344.67 second.

epoch 313 starting......
Epoch:  313 | train loss: 1.947197e-03 | valid loss: 1.888432e-03 
      	| train loss (relative): 3.853512e-02 | valid loss (relative): 3.733028e-02 
Epoch 313 use: 356.58 second.

epoch 314 starting......
Epoch:  314 | train loss: 1.948378e-03 | valid loss: 1.891853e-03 
      	| train loss (relative): 3.855486e-02 | valid loss (relative): 3.703017e-02 
Epoch 314 use: 344.62 second.

epoch 315 starting......
Epoch:  315 | train loss: 1.950115e-03 | valid loss: 1.879405e-03 
      	| train loss (relative): 3.858507e-02 | valid loss (relative): 3.720129e-02 
Epoch 315 use: 344.44 second.

epoch 316 starting......
Epoch:  316 | train loss: 1.937624e-03 | valid loss: 1.871752e-03 
      	| train loss (relative): 3.833479e-02 | valid loss (relative): 3.693354e-02 
Epoch 316 use: 318.78 second.

epoch 317 starting......
Epoch:  317 | train loss: 1.932623e-03 | valid loss: 1.882697e-03 
      	| train loss (relative): 3.823611e-02 | valid loss (relative): 3.697599e-02 
Epoch 317 use: 340.70 second.

epoch 318 starting......
Epoch:  318 | train loss: 1.936598e-03 | valid loss: 1.877860e-03 
      	| train loss (relative): 3.831623e-02 | valid loss (relative): 3.689290e-02 
Epoch 318 use: 350.59 second.

epoch 319 starting......
Epoch:  319 | train loss: 1.940195e-03 | valid loss: 1.883607e-03 
      	| train loss (relative): 3.838266e-02 | valid loss (relative): 3.734551e-02 
Epoch 319 use: 357.58 second.

epoch 320 starting......
Epoch:  320 | train loss: 1.939814e-03 | valid loss: 1.886800e-03 
      	| train loss (relative): 3.837628e-02 | valid loss (relative): 3.724621e-02 
Epoch 320 use: 353.38 second.

epoch 321 starting......
Epoch:  321 | train loss: 1.940496e-03 | valid loss: 1.886975e-03 
      	| train loss (relative): 3.838689e-02 | valid loss (relative): 3.726177e-02 
Epoch 321 use: 355.08 second.

epoch 322 starting......
Epoch:  322 | train loss: 1.939085e-03 | valid loss: 1.879347e-03 
      	| train loss (relative): 3.836051e-02 | valid loss (relative): 3.684912e-02 
Epoch 322 use: 352.07 second.

epoch 323 starting......
Epoch:  323 | train loss: 1.939466e-03 | valid loss: 1.885393e-03 
      	| train loss (relative): 3.837179e-02 | valid loss (relative): 3.735589e-02 
Epoch 323 use: 492.99 second.

epoch 324 starting......
Epoch:  324 | train loss: 1.938794e-03 | valid loss: 1.888547e-03 
      	| train loss (relative): 3.834735e-02 | valid loss (relative): 3.703110e-02 
Epoch 324 use: 401.26 second.

epoch 325 starting......
Epoch:  325 | train loss: 1.934680e-03 | valid loss: 1.868594e-03 
      	| train loss (relative): 3.826257e-02 | valid loss (relative): 3.677720e-02 
Epoch 325 use: 358.19 second.

epoch 326 starting......
Epoch:  326 | train loss: 1.930936e-03 | valid loss: 1.872950e-03 
      	| train loss (relative): 3.819024e-02 | valid loss (relative): 3.693628e-02 
Epoch 326 use: 366.77 second.

epoch 327 starting......
Epoch:  327 | train loss: 1.935491e-03 | valid loss: 1.890547e-03 
      	| train loss (relative): 3.828590e-02 | valid loss (relative): 3.712179e-02 
Epoch 327 use: 348.08 second.

epoch 328 starting......
Epoch:  328 | train loss: 1.941054e-03 | valid loss: 1.868886e-03 
      	| train loss (relative): 3.839536e-02 | valid loss (relative): 3.702286e-02 
Epoch 328 use: 337.28 second.

epoch 329 starting......
Epoch:  329 | train loss: 1.933283e-03 | valid loss: 1.875831e-03 
      	| train loss (relative): 3.823812e-02 | valid loss (relative): 3.720558e-02 
Epoch 329 use: 351.11 second.

epoch 330 starting......
Epoch:  330 | train loss: 1.926134e-03 | valid loss: 1.893035e-03 
      	| train loss (relative): 3.809490e-02 | valid loss (relative): 3.699337e-02 
Epoch 330 use: 356.24 second.

epoch 331 starting......
Epoch:  331 | train loss: 1.933695e-03 | valid loss: 1.876436e-03 
      	| train loss (relative): 3.823844e-02 | valid loss (relative): 3.693309e-02 
Epoch 331 use: 339.19 second.

epoch 332 starting......
Epoch:  332 | train loss: 1.928347e-03 | valid loss: 1.874095e-03 
      	| train loss (relative): 3.813701e-02 | valid loss (relative): 3.716315e-02 
Epoch 332 use: 347.54 second.

epoch 333 starting......
Epoch:  333 | train loss: 1.922818e-03 | valid loss: 1.891157e-03 
      	| train loss (relative): 3.802915e-02 | valid loss (relative): 3.694475e-02 
Epoch 333 use: 351.44 second.

epoch 334 starting......
Epoch:  334 | train loss: 1.933013e-03 | valid loss: 1.874276e-03 
      	| train loss (relative): 3.823011e-02 | valid loss (relative): 3.681286e-02 
Epoch 334 use: 342.57 second.

epoch 335 starting......
Epoch:  335 | train loss: 1.918854e-03 | valid loss: 1.861289e-03 
      	| train loss (relative): 3.794970e-02 | valid loss (relative): 3.664708e-02 
Epoch 335 use: 358.34 second.

epoch 336 starting......
Epoch:  336 | train loss: 1.915852e-03 | valid loss: 1.860860e-03 
      	| train loss (relative): 3.788206e-02 | valid loss (relative): 3.659616e-02 
Epoch 336 use: 388.12 second.

epoch 337 starting......
Epoch:  337 | train loss: 1.914958e-03 | valid loss: 1.867401e-03 
      	| train loss (relative): 3.785674e-02 | valid loss (relative): 3.693621e-02 
Epoch 337 use: 348.04 second.

epoch 338 starting......
Epoch:  338 | train loss: 1.916283e-03 | valid loss: 1.858447e-03 
      	| train loss (relative): 3.788393e-02 | valid loss (relative): 3.652973e-02 
Epoch 338 use: 357.41 second.

epoch 339 starting......
Epoch:  339 | train loss: 1.915773e-03 | valid loss: 1.868347e-03 
      	| train loss (relative): 3.788374e-02 | valid loss (relative): 3.684681e-02 
Epoch 339 use: 347.69 second.

epoch 340 starting......
Epoch:  340 | train loss: 1.919963e-03 | valid loss: 1.856834e-03 
      	| train loss (relative): 3.796172e-02 | valid loss (relative): 3.643893e-02 
Epoch 340 use: 391.40 second.

epoch 341 starting......
Epoch:  341 | train loss: 1.907442e-03 | valid loss: 1.869522e-03 
      	| train loss (relative): 3.770823e-02 | valid loss (relative): 3.690943e-02 
Epoch 341 use: 357.29 second.

epoch 342 starting......
Epoch:  342 | train loss: 1.914545e-03 | valid loss: 1.858700e-03 
      	| train loss (relative): 3.785379e-02 | valid loss (relative): 3.651419e-02 
Epoch 342 use: 355.12 second.

epoch 343 starting......
Epoch:  343 | train loss: 1.903551e-03 | valid loss: 1.845724e-03 
      	| train loss (relative): 3.762675e-02 | valid loss (relative): 3.636781e-02 
Epoch 343 use: 394.11 second.

epoch 344 starting......
Epoch:  344 | train loss: 1.900262e-03 | valid loss: 1.855519e-03 
      	| train loss (relative): 3.756949e-02 | valid loss (relative): 3.616216e-02 
Epoch 344 use: 379.47 second.

epoch 345 starting......
Epoch:  345 | train loss: 1.900803e-03 | valid loss: 1.839159e-03 
      	| train loss (relative): 3.756449e-02 | valid loss (relative): 3.626648e-02 
Epoch 345 use: 360.89 second.

epoch 346 starting......
Epoch:  346 | train loss: 1.897882e-03 | valid loss: 1.856688e-03 
      	| train loss (relative): 3.752478e-02 | valid loss (relative): 3.651808e-02 
Epoch 346 use: 354.55 second.

epoch 347 starting......
Epoch:  347 | train loss: 1.894639e-03 | valid loss: 1.841617e-03 
      	| train loss (relative): 3.744391e-02 | valid loss (relative): 3.630224e-02 
Epoch 347 use: 350.94 second.

epoch 348 starting......
Epoch:  348 | train loss: 1.894237e-03 | valid loss: 1.840952e-03 
      	| train loss (relative): 3.743755e-02 | valid loss (relative): 3.626977e-02 
Epoch 348 use: 362.75 second.

epoch 349 starting......
Epoch:  349 | train loss: 1.895598e-03 | valid loss: 1.849062e-03 
      	| train loss (relative): 3.746837e-02 | valid loss (relative): 3.666931e-02 
Epoch 349 use: 350.67 second.

epoch 350 starting......
Epoch:  350 | train loss: 1.900803e-03 | valid loss: 1.840159e-03 
      	| train loss (relative): 3.756500e-02 | valid loss (relative): 3.643099e-02 
Epoch 350 use: 355.96 second.

epoch 351 starting......
Epoch:  351 | train loss: 1.892995e-03 | valid loss: 1.860602e-03 
      	| train loss (relative): 3.741082e-02 | valid loss (relative): 3.642382e-02 
Epoch 351 use: 343.17 second.

epoch 352 starting......
Epoch:  352 | train loss: 1.899384e-03 | valid loss: 1.848167e-03 
      	| train loss (relative): 3.753807e-02 | valid loss (relative): 3.626974e-02 
Epoch 352 use: 353.96 second.

epoch 353 starting......
Epoch:  353 | train loss: 1.893447e-03 | valid loss: 1.839366e-03 
      	| train loss (relative): 3.741671e-02 | valid loss (relative): 3.648021e-02 
Epoch 353 use: 340.57 second.

epoch 354 starting......
Epoch:  354 | train loss: 1.891991e-03 | valid loss: 1.846292e-03 
      	| train loss (relative): 3.738872e-02 | valid loss (relative): 3.657842e-02 
Epoch 354 use: 350.24 second.

epoch 355 starting......
Epoch:  355 | train loss: 1.897002e-03 | valid loss: 1.870390e-03 
      	| train loss (relative): 3.749272e-02 | valid loss (relative): 3.648385e-02 
Epoch 355 use: 334.57 second.

epoch 356 starting......
Epoch:  356 | train loss: 1.895562e-03 | valid loss: 1.838113e-03 
      	| train loss (relative): 3.746039e-02 | valid loss (relative): 3.609293e-02 
Epoch 356 use: 348.56 second.

epoch 357 starting......
Epoch:  357 | train loss: 1.885117e-03 | valid loss: 1.829802e-03 
      	| train loss (relative): 3.724170e-02 | valid loss (relative): 3.607613e-02 
Epoch 357 use: 355.09 second.

epoch 358 starting......
Epoch:  358 | train loss: 1.883061e-03 | valid loss: 1.829060e-03 
      	| train loss (relative): 3.720719e-02 | valid loss (relative): 3.609956e-02 
Epoch 358 use: 362.91 second.

epoch 359 starting......
Epoch:  359 | train loss: 1.885907e-03 | valid loss: 1.835090e-03 
      	| train loss (relative): 3.726511e-02 | valid loss (relative): 3.633460e-02 
Epoch 359 use: 343.97 second.

epoch 360 starting......
Epoch:  360 | train loss: 1.885762e-03 | valid loss: 1.837911e-03 
      	| train loss (relative): 3.727053e-02 | valid loss (relative): 3.616658e-02 
Epoch 360 use: 353.52 second.

epoch 361 starting......
Epoch:  361 | train loss: 1.887349e-03 | valid loss: 1.833873e-03 
      	| train loss (relative): 3.729696e-02 | valid loss (relative): 3.607541e-02 
Epoch 361 use: 350.57 second.

epoch 362 starting......
Epoch:  362 | train loss: 1.884691e-03 | valid loss: 1.840706e-03 
      	| train loss (relative): 3.723924e-02 | valid loss (relative): 3.596268e-02 
Epoch 362 use: 351.75 second.

epoch 363 starting......
Epoch:  363 | train loss: 1.882063e-03 | valid loss: 1.834096e-03 
      	| train loss (relative): 3.718425e-02 | valid loss (relative): 3.633375e-02 
Epoch 363 use: 336.11 second.

epoch 364 starting......
Epoch:  364 | train loss: 1.879687e-03 | valid loss: 1.831720e-03 
      	| train loss (relative): 3.713489e-02 | valid loss (relative): 3.631767e-02 
Epoch 364 use: 352.62 second.

epoch 365 starting......
Epoch:  365 | train loss: 1.881418e-03 | valid loss: 1.832605e-03 
      	| train loss (relative): 3.717216e-02 | valid loss (relative): 3.612120e-02 
Epoch 365 use: 343.54 second.

epoch 366 starting......
Epoch:  366 | train loss: 1.882197e-03 | valid loss: 1.829854e-03 
      	| train loss (relative): 3.718805e-02 | valid loss (relative): 3.575661e-02 
Epoch 366 use: 364.24 second.

epoch 367 starting......
Epoch:  367 | train loss: 1.874141e-03 | valid loss: 1.820242e-03 
      	| train loss (relative): 3.702005e-02 | valid loss (relative): 3.585659e-02 
Epoch 367 use: 352.43 second.

epoch 368 starting......
Epoch:  368 | train loss: 1.875386e-03 | valid loss: 1.829718e-03 
      	| train loss (relative): 3.705447e-02 | valid loss (relative): 3.618087e-02 
Epoch 368 use: 351.61 second.

epoch 369 starting......
Epoch:  369 | train loss: 1.874548e-03 | valid loss: 1.824098e-03 
      	| train loss (relative): 3.702899e-02 | valid loss (relative): 3.612200e-02 
Epoch 369 use: 353.75 second.

epoch 370 starting......
Epoch:  370 | train loss: 1.868929e-03 | valid loss: 1.824659e-03 
      	| train loss (relative): 3.691083e-02 | valid loss (relative): 3.589643e-02 
Epoch 370 use: 360.08 second.

epoch 371 starting......
Epoch:  371 | train loss: 1.871643e-03 | valid loss: 1.824289e-03 
      	| train loss (relative): 3.697255e-02 | valid loss (relative): 3.581601e-02 
Epoch 371 use: 343.06 second.

epoch 372 starting......
Epoch:  372 | train loss: 1.873827e-03 | valid loss: 1.828323e-03 
      	| train loss (relative): 3.700782e-02 | valid loss (relative): 3.598068e-02 
Epoch 372 use: 357.06 second.

epoch 373 starting......
Epoch:  373 | train loss: 1.870739e-03 | valid loss: 1.822020e-03 
      	| train loss (relative): 3.695104e-02 | valid loss (relative): 3.570210e-02 
Epoch 373 use: 350.54 second.

epoch 374 starting......
Epoch:  374 | train loss: 1.862294e-03 | valid loss: 1.809655e-03 
      	| train loss (relative): 3.678207e-02 | valid loss (relative): 3.568881e-02 
Epoch 374 use: 360.94 second.

epoch 375 starting......
Epoch:  375 | train loss: 1.861155e-03 | valid loss: 1.813446e-03 
      	| train loss (relative): 3.675966e-02 | valid loss (relative): 3.579916e-02 
Epoch 375 use: 351.47 second.

epoch 376 starting......
Epoch:  376 | train loss: 1.867607e-03 | valid loss: 1.828766e-03 
      	| train loss (relative): 3.688955e-02 | valid loss (relative): 3.626350e-02 
Epoch 376 use: 358.43 second.

epoch 377 starting......
Epoch:  377 | train loss: 1.865099e-03 | valid loss: 1.822519e-03 
      	| train loss (relative): 3.683298e-02 | valid loss (relative): 3.585050e-02 
Epoch 377 use: 345.00 second.

epoch 378 starting......
Epoch:  378 | train loss: 1.864562e-03 | valid loss: 1.811897e-03 
      	| train loss (relative): 3.681917e-02 | valid loss (relative): 3.580237e-02 
Epoch 378 use: 356.53 second.

epoch 379 starting......
Epoch:  379 | train loss: 1.862924e-03 | valid loss: 1.817851e-03 
      	| train loss (relative): 3.679169e-02 | valid loss (relative): 3.572183e-02 
Epoch 379 use: 350.66 second.

epoch 380 starting......
Epoch:  380 | train loss: 1.870388e-03 | valid loss: 1.861410e-03 
      	| train loss (relative): 3.694469e-02 | valid loss (relative): 3.629741e-02 
Epoch 380 use: 348.60 second.

epoch 381 starting......
Epoch:  381 | train loss: 1.866019e-03 | valid loss: 1.804207e-03 
      	| train loss (relative): 3.685511e-02 | valid loss (relative): 3.555351e-02 
Epoch 381 use: 356.50 second.

epoch 382 starting......
Epoch:  382 | train loss: 1.850289e-03 | valid loss: 1.799563e-03 
      	| train loss (relative): 3.654157e-02 | valid loss (relative): 3.529266e-02 
Epoch 382 use: 362.23 second.

epoch 383 starting......
Epoch:  383 | train loss: 1.849970e-03 | valid loss: 1.843942e-03 
      	| train loss (relative): 3.652854e-02 | valid loss (relative): 3.562510e-02 
Epoch 383 use: 344.14 second.

epoch 384 starting......
Epoch:  384 | train loss: 1.860498e-03 | valid loss: 1.799330e-03 
      	| train loss (relative): 3.672111e-02 | valid loss (relative): 3.524176e-02 
Epoch 384 use: 356.67 second.

epoch 385 starting......
Epoch:  385 | train loss: 1.845129e-03 | valid loss: 1.789379e-03 
      	| train loss (relative): 3.642304e-02 | valid loss (relative): 3.522719e-02 
Epoch 385 use: 361.11 second.

epoch 386 starting......
Epoch:  386 | train loss: 1.840634e-03 | valid loss: 1.795732e-03 
      	| train loss (relative): 3.634163e-02 | valid loss (relative): 3.518209e-02 
Epoch 386 use: 374.78 second.

epoch 387 starting......
Epoch:  387 | train loss: 1.847372e-03 | valid loss: 1.798527e-03 
      	| train loss (relative): 3.647166e-02 | valid loss (relative): 3.539143e-02 
Epoch 387 use: 360.74 second.

epoch 388 starting......
Epoch:  388 | train loss: 1.846485e-03 | valid loss: 1.809451e-03 
      	| train loss (relative): 3.645654e-02 | valid loss (relative): 3.543571e-02 
Epoch 388 use: 374.40 second.

epoch 389 starting......
Epoch:  389 | train loss: 1.848084e-03 | valid loss: 1.793980e-03 
      	| train loss (relative): 3.649472e-02 | valid loss (relative): 3.530836e-02 
Epoch 389 use: 363.83 second.

epoch 390 starting......
Epoch:  390 | train loss: 1.841280e-03 | valid loss: 1.793087e-03 
      	| train loss (relative): 3.634874e-02 | valid loss (relative): 3.534942e-02 
Epoch 390 use: 392.95 second.

epoch 391 starting......
Epoch:  391 | train loss: 1.843064e-03 | valid loss: 1.800015e-03 
      	| train loss (relative): 3.637873e-02 | valid loss (relative): 3.552980e-02 
Epoch 391 use: 387.46 second.

epoch 392 starting......
Epoch:  392 | train loss: 1.849203e-03 | valid loss: 1.799764e-03 
      	| train loss (relative): 3.650668e-02 | valid loss (relative): 3.545646e-02 
Epoch 392 use: 397.36 second.

epoch 393 starting......
Epoch:  393 | train loss: 1.844985e-03 | valid loss: 1.792104e-03 
      	| train loss (relative): 3.641903e-02 | valid loss (relative): 3.541205e-02 
Epoch 393 use: 394.56 second.

epoch 394 starting......
Epoch:  394 | train loss: 1.841219e-03 | valid loss: 1.794808e-03 
      	| train loss (relative): 3.634807e-02 | valid loss (relative): 3.534866e-02 
Epoch 394 use: 382.83 second.

epoch 395 starting......
Epoch:  395 | train loss: 1.844921e-03 | valid loss: 1.795033e-03 
      	| train loss (relative): 3.641922e-02 | valid loss (relative): 3.542442e-02 
Epoch 395 use: 387.78 second.

epoch 396 starting......
Epoch:  396 | train loss: 1.846353e-03 | valid loss: 1.806630e-03 
      	| train loss (relative): 3.644593e-02 | valid loss (relative): 3.568563e-02 
Epoch 396 use: 375.94 second.

epoch 397 starting......
Epoch:  397 | train loss: 1.849566e-03 | valid loss: 1.819924e-03 
      	| train loss (relative): 3.651178e-02 | valid loss (relative): 3.547715e-02 
Epoch 397 use: 376.37 second.

epoch 398 starting......
Epoch:  398 | train loss: 1.841765e-03 | valid loss: 1.782469e-03 
      	| train loss (relative): 3.635286e-02 | valid loss (relative): 3.505336e-02 
Epoch 398 use: 368.72 second.

epoch 399 starting......
Epoch:  399 | train loss: 1.833953e-03 | valid loss: 1.783003e-03 
      	| train loss (relative): 3.619592e-02 | valid loss (relative): 3.486168e-02 
Epoch 399 use: 366.20 second.

epoch 400 starting......
Epoch:  400 | train loss: 1.832813e-03 | valid loss: 1.781902e-03 
      	| train loss (relative): 3.616915e-02 | valid loss (relative): 3.486951e-02 
Epoch 400 use: 358.73 second.

epoch 401 starting......
Epoch:  401 | train loss: 1.828121e-03 | valid loss: 1.776345e-03 
      	| train loss (relative): 3.608206e-02 | valid loss (relative): 3.494741e-02 
Epoch 401 use: 356.48 second.

epoch 402 starting......
Epoch:  402 | train loss: 1.827265e-03 | valid loss: 1.781592e-03 
      	| train loss (relative): 3.605925e-02 | valid loss (relative): 3.508643e-02 
Epoch 402 use: 366.40 second.

epoch 403 starting......
Epoch:  403 | train loss: 1.831140e-03 | valid loss: 1.792307e-03 
      	| train loss (relative): 3.613291e-02 | valid loss (relative): 3.513528e-02 
Epoch 403 use: 361.43 second.

epoch 404 starting......
Epoch:  404 | train loss: 1.829118e-03 | valid loss: 1.781219e-03 
      	| train loss (relative): 3.609332e-02 | valid loss (relative): 3.498406e-02 
Epoch 404 use: 366.06 second.

epoch 405 starting......
Epoch:  405 | train loss: 1.823438e-03 | valid loss: 1.778432e-03 
      	| train loss (relative): 3.598320e-02 | valid loss (relative): 3.472763e-02 
Epoch 405 use: 355.40 second.

epoch 406 starting......
Epoch:  406 | train loss: 1.826383e-03 | valid loss: 1.790834e-03 
      	| train loss (relative): 3.603178e-02 | valid loss (relative): 3.516448e-02 
Epoch 406 use: 353.00 second.

epoch 407 starting......
Epoch:  407 | train loss: 1.833831e-03 | valid loss: 1.787766e-03 
      	| train loss (relative): 3.619052e-02 | valid loss (relative): 3.520816e-02 
Epoch 407 use: 364.32 second.

epoch 408 starting......
Epoch:  408 | train loss: 1.828223e-03 | valid loss: 1.778834e-03 
      	| train loss (relative): 3.608017e-02 | valid loss (relative): 3.487040e-02 
Epoch 408 use: 356.16 second.

epoch 409 starting......
Epoch:  409 | train loss: 1.821829e-03 | valid loss: 1.778036e-03 
      	| train loss (relative): 3.593826e-02 | valid loss (relative): 3.499151e-02 
Epoch 409 use: 369.02 second.

epoch 410 starting......
Epoch:  410 | train loss: 1.821476e-03 | valid loss: 1.774683e-03 
      	| train loss (relative): 3.594167e-02 | valid loss (relative): 3.495985e-02 
Epoch 410 use: 373.34 second.

epoch 411 starting......
Epoch:  411 | train loss: 1.825467e-03 | valid loss: 1.778833e-03 
      	| train loss (relative): 3.602312e-02 | valid loss (relative): 3.488219e-02 
Epoch 411 use: 380.79 second.

epoch 412 starting......
Epoch:  412 | train loss: 1.824370e-03 | valid loss: 1.777714e-03 
      	| train loss (relative): 3.598998e-02 | valid loss (relative): 3.505610e-02 
Epoch 412 use: 386.58 second.

epoch 413 starting......
Epoch:  413 | train loss: 1.825623e-03 | valid loss: 1.822149e-03 
      	| train loss (relative): 3.602638e-02 | valid loss (relative): 3.537509e-02 
Epoch 413 use: 372.27 second.

epoch 414 starting......
Epoch:  414 | train loss: 1.834630e-03 | valid loss: 1.768458e-03 
      	| train loss (relative): 3.620013e-02 | valid loss (relative): 3.471500e-02 
Epoch 414 use: 375.72 second.

epoch 415 starting......
Epoch:  415 | train loss: 1.814534e-03 | valid loss: 1.780065e-03 
      	| train loss (relative): 3.579827e-02 | valid loss (relative): 3.506052e-02 
Epoch 415 use: 409.05 second.

epoch 416 starting......
Epoch:  416 | train loss: 1.816650e-03 | valid loss: 1.764756e-03 
      	| train loss (relative): 3.583940e-02 | valid loss (relative): 3.470352e-02 
Epoch 416 use: 383.26 second.

epoch 417 starting......
Epoch:  417 | train loss: 1.810367e-03 | valid loss: 1.763068e-03 
      	| train loss (relative): 3.570979e-02 | valid loss (relative): 3.454658e-02 
Epoch 417 use: 377.85 second.

epoch 418 starting......
Epoch:  418 | train loss: 1.813516e-03 | valid loss: 1.765968e-03 
      	| train loss (relative): 3.577675e-02 | valid loss (relative): 3.466303e-02 
Epoch 418 use: 379.36 second.

epoch 419 starting......
Epoch:  419 | train loss: 1.810263e-03 | valid loss: 1.777290e-03 
      	| train loss (relative): 3.570952e-02 | valid loss (relative): 3.480255e-02 
Epoch 419 use: 371.52 second.

epoch 420 starting......
Epoch:  420 | train loss: 1.816618e-03 | valid loss: 1.765215e-03 
      	| train loss (relative): 3.583561e-02 | valid loss (relative): 3.479037e-02 
Epoch 420 use: 387.41 second.

epoch 421 starting......
Epoch:  421 | train loss: 1.813169e-03 | valid loss: 1.766708e-03 
      	| train loss (relative): 3.577075e-02 | valid loss (relative): 3.478511e-02 
Epoch 421 use: 386.98 second.

epoch 422 starting......
Epoch:  422 | train loss: 1.813111e-03 | valid loss: 1.781795e-03 
      	| train loss (relative): 3.576284e-02 | valid loss (relative): 3.549100e-02 
Epoch 422 use: 397.77 second.

epoch 423 starting......
Epoch:  423 | train loss: 1.815945e-03 | valid loss: 1.765823e-03 
      	| train loss (relative): 3.583312e-02 | valid loss (relative): 3.471574e-02 
Epoch 423 use: 390.41 second.

epoch 424 starting......
Epoch:  424 | train loss: 1.807272e-03 | valid loss: 1.758987e-03 
      	| train loss (relative): 3.564916e-02 | valid loss (relative): 3.449173e-02 
Epoch 424 use: 369.72 second.

epoch 425 starting......
Epoch:  425 | train loss: 1.801440e-03 | valid loss: 1.760483e-03 
      	| train loss (relative): 3.552588e-02 | valid loss (relative): 3.475479e-02 
Epoch 425 use: 377.46 second.

epoch 426 starting......
Epoch:  426 | train loss: 1.805122e-03 | valid loss: 1.767332e-03 
      	| train loss (relative): 3.560422e-02 | valid loss (relative): 3.502525e-02 
Epoch 426 use: 362.43 second.

epoch 427 starting......
Epoch:  427 | train loss: 1.804325e-03 | valid loss: 1.767548e-03 
      	| train loss (relative): 3.558706e-02 | valid loss (relative): 3.503649e-02 
Epoch 427 use: 359.42 second.

epoch 428 starting......
Epoch:  428 | train loss: 1.807803e-03 | valid loss: 1.763479e-03 
      	| train loss (relative): 3.565338e-02 | valid loss (relative): 3.470212e-02 
Epoch 428 use: 367.60 second.

epoch 429 starting......
Epoch:  429 | train loss: 1.808756e-03 | valid loss: 1.765928e-03 
      	| train loss (relative): 3.567288e-02 | valid loss (relative): 3.483462e-02 
Epoch 429 use: 367.44 second.

epoch 430 starting......
Epoch:  430 | train loss: 1.807529e-03 | valid loss: 1.756604e-03 
      	| train loss (relative): 3.565367e-02 | valid loss (relative): 3.450254e-02 
Epoch 430 use: 365.84 second.

epoch 431 starting......
Epoch:  431 | train loss: 1.800208e-03 | valid loss: 1.767761e-03 
      	| train loss (relative): 3.550284e-02 | valid loss (relative): 3.462750e-02 
Epoch 431 use: 363.99 second.

epoch 432 starting......
Epoch:  432 | train loss: 1.804857e-03 | valid loss: 1.757862e-03 
      	| train loss (relative): 3.559754e-02 | valid loss (relative): 3.457199e-02 
Epoch 432 use: 370.95 second.

epoch 433 starting......
Epoch:  433 | train loss: 1.801529e-03 | valid loss: 1.750288e-03 
      	| train loss (relative): 3.552514e-02 | valid loss (relative): 3.430806e-02 
Epoch 433 use: 375.95 second.

epoch 434 starting......
Epoch:  434 | train loss: 1.796448e-03 | valid loss: 1.788317e-03 
      	| train loss (relative): 3.542094e-02 | valid loss (relative): 3.552243e-02 
Epoch 434 use: 363.06 second.

epoch 435 starting......
Epoch:  435 | train loss: 1.801813e-03 | valid loss: 1.753929e-03 
      	| train loss (relative): 3.553314e-02 | valid loss (relative): 3.450656e-02 
Epoch 435 use: 355.63 second.

epoch 436 starting......
Epoch:  436 | train loss: 1.792808e-03 | valid loss: 1.752807e-03 
      	| train loss (relative): 3.534893e-02 | valid loss (relative): 3.435125e-02 
Epoch 436 use: 362.16 second.

epoch 437 starting......
Epoch:  437 | train loss: 1.794898e-03 | valid loss: 1.749216e-03 
      	| train loss (relative): 3.539314e-02 | valid loss (relative): 3.443557e-02 
Epoch 437 use: 352.10 second.

epoch 438 starting......
Epoch:  438 | train loss: 1.795320e-03 | valid loss: 1.751074e-03 
      	| train loss (relative): 3.540093e-02 | valid loss (relative): 3.452341e-02 
Epoch 438 use: 364.39 second.

epoch 439 starting......
Epoch:  439 | train loss: 1.793149e-03 | valid loss: 1.739823e-03 
      	| train loss (relative): 3.535685e-02 | valid loss (relative): 3.434547e-02 
Epoch 439 use: 360.22 second.

epoch 440 starting......
Epoch:  440 | train loss: 1.788309e-03 | valid loss: 1.743520e-03 
      	| train loss (relative): 3.526360e-02 | valid loss (relative): 3.412311e-02 
Epoch 440 use: 368.99 second.

epoch 441 starting......
Epoch:  441 | train loss: 1.789951e-03 | valid loss: 1.744598e-03 
      	| train loss (relative): 3.529045e-02 | valid loss (relative): 3.431496e-02 
Epoch 441 use: 353.91 second.

epoch 442 starting......
Epoch:  442 | train loss: 1.787130e-03 | valid loss: 1.747666e-03 
      	| train loss (relative): 3.524184e-02 | valid loss (relative): 3.437142e-02 
Epoch 442 use: 367.03 second.

epoch 443 starting......
Epoch:  443 | train loss: 1.787185e-03 | valid loss: 1.745107e-03 
      	| train loss (relative): 3.523927e-02 | valid loss (relative): 3.417772e-02 
Epoch 443 use: 348.82 second.

epoch 444 starting......
Epoch:  444 | train loss: 1.792064e-03 | valid loss: 1.757331e-03 
      	| train loss (relative): 3.533062e-02 | valid loss (relative): 3.468633e-02 
Epoch 444 use: 363.33 second.

epoch 445 starting......
Epoch:  445 | train loss: 1.791981e-03 | valid loss: 1.749132e-03 
      	| train loss (relative): 3.532967e-02 | valid loss (relative): 3.466669e-02 
Epoch 445 use: 370.42 second.

epoch 446 starting......
Epoch:  446 | train loss: 1.789669e-03 | valid loss: 1.741114e-03 
      	| train loss (relative): 3.528795e-02 | valid loss (relative): 3.412773e-02 
Epoch 446 use: 347.64 second.

epoch 447 starting......
Epoch:  447 | train loss: 1.781444e-03 | valid loss: 1.739233e-03 
      	| train loss (relative): 3.511669e-02 | valid loss (relative): 3.428535e-02 
Epoch 447 use: 370.23 second.

epoch 448 starting......
Epoch:  448 | train loss: 1.783487e-03 | valid loss: 1.745480e-03 
      	| train loss (relative): 3.515799e-02 | valid loss (relative): 3.448628e-02 
Epoch 448 use: 361.04 second.

epoch 449 starting......
Epoch:  449 | train loss: 1.784234e-03 | valid loss: 1.740630e-03 
      	| train loss (relative): 3.517251e-02 | valid loss (relative): 3.446836e-02 
Epoch 449 use: 348.10 second.

test MSE Error: 1.825871e-03 | relative MSE Error: 3.635195e-02 
 Total time used for training: 15.15 hour.
MESLoss saved to  /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_MSELoss_Latent_64_nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_450.txt
relative MSELoss saved to  /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_reMSELoss_Latent_64_nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_450.txt
model saved to /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_64_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_450.pth
model_dict saved to /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_64_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_450_dict.pth
... Training slugflow data group 3 completed, Run finished Sat 14 Aug 04:30:45 BST 2021 ...
{'data_dir': '/rds/general/user/jy220/home/tensors/', 'mode': 'train', 'data_type': 'tensors', 'vtu_fields': 'Component1::ComponentMassFractionPhase1, phase1::Velocity', 'variational': 'False', 'optimizer': 'Adamax', 'structured': 'False', 'changing_lr': 'False', 'self_concat': '2', 'dimension': '3', 'sfc_nums': '3', 'components': '4', 'nearest_neighbouring': 'True', 'dims_latent': '64', 'activation': 'Tanh', 'tk_file': '/rds/general/user/jy220/home/slugflow_tk.pt', 'tb_file': '/rds/general/user/jy220/home/slugflow_tb.pt', 'sfc_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/sfcs.pt', 'inv_sfc_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/inv_sfcs.pt', 'coords_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/coords.pt', 'cells_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/cells.npy', 'state_load': '/rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_64_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_450_dict.pth', 'batch_size': '16', 'lr': '0.001', 'n_epoches': '150', 'seed': '17', 'visualize': 'False', 'reconstructed_path': 'None', 'reconstruct_start_index': '0', 'reconstruct_end_index': '50', 'save_path': '/rds/general/user/jy220/home/new_results/'}
[[ 2.50825269e-01 -1.67256990e-02 -1.22268730e-03]
 [ 2.38611959e-01 -5.50627777e-03 -5.06451913e-03]
 [ 2.39454874e-01 -1.59563257e-02  3.96778323e-03]
 ...
 [ 9.11361096e+00 -2.21908381e-02  2.22384902e-03]
 [ 9.11847310e+00 -2.84346931e-02 -2.34839590e-04]
 [ 9.11039661e+00 -2.51498573e-02 -8.70661044e-03]]
{'tetra': array([[      0,       1,       2,       3],
       [      4,       5,       6,       7],
       [      8,       9,      10,      11],
       ...,
       [1342744, 1342745, 1342746, 1342747],
       [1342748, 1342749, 1342750, 1342751],
       [1342752, 1342753, 1342754, 1342755]])}
vtu fields:  ['Component1::ComponentMassFractionPhase1', 'phase1::Velocity'] 
 structured  False 
 activation  Tanh() 
 self concat  2 
 sfc_nums  3 

dims_latent  64 
 components  4 
 nearest_neighbouring  True 

visualize  False 
 sample number  1706 

reading sfc nums......
[array([  41684,   41685,   41674, ..., 1235560, 1235564, 1235558]), array([954289, 954290, 954291, ..., 420685, 420687, 420688]), array([  40156,   40155,   40158, ..., 1123478, 1123475, 1123467])]
reading inverse sfc nums......
[array([   8146,    8145,    8144, ..., 1274444, 1274445, 1273166]), array([547280, 547283, 593037, ..., 933933, 933932, 864488]), array([ 66437,  66439, 122512, ..., 459416, 415130, 459418])]
tk:  tensor([0.7581, 0.0418, 0.1840, 0.1464], dtype=torch.float64) 

tb:  tensor([-0.3816, -0.3618, -0.0308,  0.1446], dtype=torch.float64) 

length of train set: 1505 

length of valid set: 100 

length of test set: 101 

torch device num: 4 

Let's use 4 GPUs!
epoch 450 starting......
Epoch:  450 | train loss: 2.117339e-03 | valid loss: 1.792414e-03 
      	| train loss (relative): 4.117611e-02 | valid loss (relative): 3.529140e-02 
Epoch 450 use: 477.66 second.

epoch 451 starting......
Epoch:  451 | train loss: 1.791088e-03 | valid loss: 1.763528e-03 
      	| train loss (relative): 3.534601e-02 | valid loss (relative): 3.469243e-02 
Epoch 451 use: 448.37 second.

epoch 452 starting......
Epoch:  452 | train loss: 1.774513e-03 | valid loss: 1.757684e-03 
      	| train loss (relative): 3.500251e-02 | valid loss (relative): 3.458306e-02 
Epoch 452 use: 433.74 second.

epoch 453 starting......
Epoch:  453 | train loss: 1.767652e-03 | valid loss: 1.754320e-03 
      	| train loss (relative): 3.486038e-02 | valid loss (relative): 3.453995e-02 
Epoch 453 use: 430.53 second.

epoch 454 starting......
Epoch:  454 | train loss: 1.763653e-03 | valid loss: 1.752451e-03 
      	| train loss (relative): 3.478892e-02 | valid loss (relative): 3.449138e-02 
Epoch 454 use: 425.15 second.

epoch 455 starting......
Epoch:  455 | train loss: 1.760289e-03 | valid loss: 1.750936e-03 
      	| train loss (relative): 3.470666e-02 | valid loss (relative): 3.445683e-02 
Epoch 455 use: 434.75 second.

epoch 456 starting......
Epoch:  456 | train loss: 1.759294e-03 | valid loss: 1.749159e-03 
      	| train loss (relative): 3.469127e-02 | valid loss (relative): 3.440992e-02 
Epoch 456 use: 436.08 second.

epoch 457 starting......
Epoch:  457 | train loss: 1.757031e-03 | valid loss: 1.749566e-03 
      	| train loss (relative): 3.464850e-02 | valid loss (relative): 3.444500e-02 
Epoch 457 use: 430.30 second.

epoch 458 starting......
Epoch:  458 | train loss: 1.756814e-03 | valid loss: 1.748349e-03 
      	| train loss (relative): 3.463652e-02 | valid loss (relative): 3.436758e-02 
Epoch 458 use: 427.94 second.

epoch 459 starting......
Epoch:  459 | train loss: 1.756038e-03 | valid loss: 1.750485e-03 
      	| train loss (relative): 3.462211e-02 | valid loss (relative): 3.442171e-02 
Epoch 459 use: 428.54 second.

epoch 460 starting......
Epoch:  460 | train loss: 1.758390e-03 | valid loss: 1.754058e-03 
      	| train loss (relative): 3.466726e-02 | valid loss (relative): 3.457557e-02 
Epoch 460 use: 515.62 second.

epoch 461 starting......
Epoch:  461 | train loss: 1.759374e-03 | valid loss: 1.755550e-03 
      	| train loss (relative): 3.469073e-02 | valid loss (relative): 3.443338e-02 
Epoch 461 use: 558.09 second.

epoch 462 starting......
Epoch:  462 | train loss: 1.758951e-03 | valid loss: 1.754411e-03 
      	| train loss (relative): 3.468566e-02 | valid loss (relative): 3.443402e-02 
Epoch 462 use: 452.42 second.

epoch 463 starting......
Epoch:  463 | train loss: 1.762433e-03 | valid loss: 1.757215e-03 
      	| train loss (relative): 3.474564e-02 | valid loss (relative): 3.455388e-02 
Epoch 463 use: 440.15 second.

epoch 464 starting......
Epoch:  464 | train loss: 1.759830e-03 | valid loss: 1.758083e-03 
      	| train loss (relative): 3.469271e-02 | valid loss (relative): 3.448385e-02 
Epoch 464 use: 443.26 second.

epoch 465 starting......
Epoch:  465 | train loss: 1.763894e-03 | valid loss: 1.763124e-03 
      	| train loss (relative): 3.477626e-02 | valid loss (relative): 3.477764e-02 
Epoch 465 use: 447.18 second.

epoch 466 starting......
Epoch:  466 | train loss: 1.761539e-03 | valid loss: 1.763722e-03 
      	| train loss (relative): 3.473079e-02 | valid loss (relative): 3.483896e-02 
Epoch 466 use: 443.83 second.

epoch 467 starting......
Epoch:  467 | train loss: 1.765585e-03 | valid loss: 1.764630e-03 
      	| train loss (relative): 3.481586e-02 | valid loss (relative): 3.480691e-02 
Epoch 467 use: 457.16 second.

epoch 468 starting......
Epoch:  468 | train loss: 1.761553e-03 | valid loss: 1.761324e-03 
      	| train loss (relative): 3.473030e-02 | valid loss (relative): 3.468363e-02 
Epoch 468 use: 479.64 second.

epoch 469 starting......
Epoch:  469 | train loss: 1.760372e-03 | valid loss: 1.758810e-03 
      	| train loss (relative): 3.470083e-02 | valid loss (relative): 3.455418e-02 
Epoch 469 use: 492.04 second.

epoch 470 starting......
Epoch:  470 | train loss: 1.756925e-03 | valid loss: 1.760618e-03 
      	| train loss (relative): 3.463403e-02 | valid loss (relative): 3.459309e-02 
Epoch 470 use: 469.26 second.

epoch 471 starting......
Epoch:  471 | train loss: 1.757792e-03 | valid loss: 1.753387e-03 
      	| train loss (relative): 3.465124e-02 | valid loss (relative): 3.459064e-02 
Epoch 471 use: 472.72 second.

epoch 472 starting......
Epoch:  472 | train loss: 1.757476e-03 | valid loss: 1.761231e-03 
      	| train loss (relative): 3.464839e-02 | valid loss (relative): 3.465437e-02 
Epoch 472 use: 474.77 second.

epoch 473 starting......
Epoch:  473 | train loss: 1.757708e-03 | valid loss: 1.759386e-03 
      	| train loss (relative): 3.465099e-02 | valid loss (relative): 3.474202e-02 
Epoch 473 use: 453.91 second.

epoch 474 starting......
Epoch:  474 | train loss: 1.759018e-03 | valid loss: 1.763841e-03 
      	| train loss (relative): 3.467680e-02 | valid loss (relative): 3.479495e-02 
Epoch 474 use: 452.40 second.

epoch 475 starting......
Epoch:  475 | train loss: 1.759775e-03 | valid loss: 1.762524e-03 
      	| train loss (relative): 3.468732e-02 | valid loss (relative): 3.483976e-02 
Epoch 475 use: 458.58 second.

epoch 476 starting......
Epoch:  476 | train loss: 1.758634e-03 | valid loss: 1.759700e-03 
      	| train loss (relative): 3.466512e-02 | valid loss (relative): 3.454670e-02 
Epoch 476 use: 430.87 second.

epoch 477 starting......
Epoch:  477 | train loss: 1.753145e-03 | valid loss: 1.761642e-03 
      	| train loss (relative): 3.455891e-02 | valid loss (relative): 3.467961e-02 
Epoch 477 use: 443.43 second.

epoch 478 starting......
Epoch:  478 | train loss: 1.755767e-03 | valid loss: 1.762551e-03 
      	| train loss (relative): 3.460959e-02 | valid loss (relative): 3.463878e-02 
Epoch 478 use: 440.84 second.

epoch 479 starting......
Epoch:  479 | train loss: 1.755785e-03 | valid loss: 1.754726e-03 
      	| train loss (relative): 3.460323e-02 | valid loss (relative): 3.461827e-02 
Epoch 479 use: 433.40 second.

epoch 480 starting......
Epoch:  480 | train loss: 1.753133e-03 | valid loss: 1.751652e-03 
      	| train loss (relative): 3.455547e-02 | valid loss (relative): 3.460877e-02 
Epoch 480 use: 438.19 second.

epoch 481 starting......
Epoch:  481 | train loss: 1.751393e-03 | valid loss: 1.756874e-03 
      	| train loss (relative): 3.452297e-02 | valid loss (relative): 3.455332e-02 
Epoch 481 use: 429.84 second.

epoch 482 starting......
Epoch:  482 | train loss: 1.747165e-03 | valid loss: 1.749796e-03 
      	| train loss (relative): 3.443200e-02 | valid loss (relative): 3.455797e-02 
Epoch 482 use: 570.74 second.

epoch 483 starting......
Epoch:  483 | train loss: 1.746967e-03 | valid loss: 1.749087e-03 
      	| train loss (relative): 3.443116e-02 | valid loss (relative): 3.429831e-02 
Epoch 483 use: 465.82 second.

epoch 484 starting......
Epoch:  484 | train loss: 1.751245e-03 | valid loss: 1.753179e-03 
      	| train loss (relative): 3.451553e-02 | valid loss (relative): 3.437842e-02 
Epoch 484 use: 441.15 second.

epoch 485 starting......
Epoch:  485 | train loss: 1.748046e-03 | valid loss: 1.757732e-03 
      	| train loss (relative): 3.445560e-02 | valid loss (relative): 3.445734e-02 
Epoch 485 use: 429.24 second.

epoch 486 starting......
Epoch:  486 | train loss: 1.751842e-03 | valid loss: 1.757682e-03 
      	| train loss (relative): 3.452332e-02 | valid loss (relative): 3.477053e-02 
Epoch 486 use: 433.64 second.

epoch 487 starting......
Epoch:  487 | train loss: 1.749492e-03 | valid loss: 1.771605e-03 
      	| train loss (relative): 3.447743e-02 | valid loss (relative): 3.492866e-02 
Epoch 487 use: 424.76 second.

epoch 488 starting......
Epoch:  488 | train loss: 1.751184e-03 | valid loss: 1.756617e-03 
      	| train loss (relative): 3.451311e-02 | valid loss (relative): 3.451217e-02 
Epoch 488 use: 421.68 second.

epoch 489 starting......
Epoch:  489 | train loss: 1.743686e-03 | valid loss: 1.743622e-03 
      	| train loss (relative): 3.435379e-02 | valid loss (relative): 3.425150e-02 
Epoch 489 use: 419.60 second.

epoch 490 starting......
Epoch:  490 | train loss: 1.734073e-03 | valid loss: 1.743970e-03 
      	| train loss (relative): 3.416696e-02 | valid loss (relative): 3.406595e-02 
Epoch 490 use: 423.28 second.

epoch 491 starting......
Epoch:  491 | train loss: 1.738083e-03 | valid loss: 1.741439e-03 
      	| train loss (relative): 3.424385e-02 | valid loss (relative): 3.400819e-02 
Epoch 491 use: 427.85 second.

epoch 492 starting......
Epoch:  492 | train loss: 1.735922e-03 | valid loss: 1.752504e-03 
      	| train loss (relative): 3.419713e-02 | valid loss (relative): 3.484618e-02 
Epoch 492 use: 436.14 second.

epoch 493 starting......
Epoch:  493 | train loss: 1.738201e-03 | valid loss: 1.752805e-03 
      	| train loss (relative): 3.424712e-02 | valid loss (relative): 3.431542e-02 
Epoch 493 use: 423.53 second.

epoch 494 starting......
Epoch:  494 | train loss: 1.736855e-03 | valid loss: 1.747607e-03 
      	| train loss (relative): 3.421668e-02 | valid loss (relative): 3.438873e-02 
Epoch 494 use: 426.84 second.

epoch 495 starting......
Epoch:  495 | train loss: 1.737562e-03 | valid loss: 1.752321e-03 
      	| train loss (relative): 3.423238e-02 | valid loss (relative): 3.460867e-02 
Epoch 495 use: 423.26 second.

epoch 496 starting......
Epoch:  496 | train loss: 1.740204e-03 | valid loss: 1.746006e-03 
      	| train loss (relative): 3.429230e-02 | valid loss (relative): 3.427750e-02 
Epoch 496 use: 427.79 second.

epoch 497 starting......
Epoch:  497 | train loss: 1.734809e-03 | valid loss: 1.746404e-03 
      	| train loss (relative): 3.417907e-02 | valid loss (relative): 3.454499e-02 
Epoch 497 use: 429.29 second.

epoch 498 starting......
Epoch:  498 | train loss: 1.737430e-03 | valid loss: 1.739339e-03 
      	| train loss (relative): 3.422317e-02 | valid loss (relative): 3.419271e-02 
Epoch 498 use: 422.81 second.

epoch 499 starting......
Epoch:  499 | train loss: 1.733952e-03 | valid loss: 1.747810e-03 
      	| train loss (relative): 3.416270e-02 | valid loss (relative): 3.444909e-02 
Epoch 499 use: 408.70 second.

epoch 500 starting......
Epoch:  500 | train loss: 1.733731e-03 | valid loss: 1.738234e-03 
      	| train loss (relative): 3.416160e-02 | valid loss (relative): 3.406303e-02 
Epoch 500 use: 425.36 second.

epoch 501 starting......
Epoch:  501 | train loss: 1.730769e-03 | valid loss: 1.740456e-03 
      	| train loss (relative): 3.409503e-02 | valid loss (relative): 3.418051e-02 
Epoch 501 use: 409.95 second.

epoch 502 starting......
Epoch:  502 | train loss: 1.731571e-03 | valid loss: 1.742534e-03 
      	| train loss (relative): 3.411262e-02 | valid loss (relative): 3.403724e-02 
Epoch 502 use: 430.78 second.

epoch 503 starting......
Epoch:  503 | train loss: 1.734802e-03 | valid loss: 1.753857e-03 
      	| train loss (relative): 3.418147e-02 | valid loss (relative): 3.453029e-02 
Epoch 503 use: 419.44 second.

epoch 504 starting......
Epoch:  504 | train loss: 1.733758e-03 | valid loss: 1.736381e-03 
      	| train loss (relative): 3.415717e-02 | valid loss (relative): 3.416376e-02 
Epoch 504 use: 419.12 second.

epoch 505 starting......
Epoch:  505 | train loss: 1.730278e-03 | valid loss: 1.736927e-03 
      	| train loss (relative): 3.408364e-02 | valid loss (relative): 3.429741e-02 
Epoch 505 use: 420.48 second.

epoch 506 starting......
Epoch:  506 | train loss: 1.724846e-03 | valid loss: 1.737700e-03 
      	| train loss (relative): 3.397685e-02 | valid loss (relative): 3.429462e-02 
Epoch 506 use: 426.94 second.

epoch 507 starting......
Epoch:  507 | train loss: 1.726439e-03 | valid loss: 1.740327e-03 
      	| train loss (relative): 3.401361e-02 | valid loss (relative): 3.420895e-02 
Epoch 507 use: 425.27 second.

epoch 508 starting......
Epoch:  508 | train loss: 1.729834e-03 | valid loss: 1.731356e-03 
      	| train loss (relative): 3.408066e-02 | valid loss (relative): 3.396044e-02 
Epoch 508 use: 427.84 second.

epoch 509 starting......
Epoch:  509 | train loss: 1.719338e-03 | valid loss: 1.744288e-03 
      	| train loss (relative): 3.386219e-02 | valid loss (relative): 3.413108e-02 
Epoch 509 use: 418.65 second.

epoch 510 starting......
Epoch:  510 | train loss: 1.722730e-03 | valid loss: 1.742193e-03 
      	| train loss (relative): 3.393887e-02 | valid loss (relative): 3.411329e-02 
Epoch 510 use: 423.82 second.

epoch 511 starting......
Epoch:  511 | train loss: 1.729414e-03 | valid loss: 1.736016e-03 
      	| train loss (relative): 3.407059e-02 | valid loss (relative): 3.408102e-02 
Epoch 511 use: 415.56 second.

epoch 512 starting......
Epoch:  512 | train loss: 1.722222e-03 | valid loss: 1.749797e-03 
      	| train loss (relative): 3.392551e-02 | valid loss (relative): 3.400606e-02 
Epoch 512 use: 426.17 second.

epoch 513 starting......
Epoch:  513 | train loss: 1.724216e-03 | valid loss: 1.726418e-03 
      	| train loss (relative): 3.395761e-02 | valid loss (relative): 3.385581e-02 
Epoch 513 use: 426.59 second.

epoch 514 starting......
Epoch:  514 | train loss: 1.715428e-03 | valid loss: 1.734751e-03 
      	| train loss (relative): 3.378410e-02 | valid loss (relative): 3.405243e-02 
Epoch 514 use: 420.76 second.

epoch 515 starting......
Epoch:  515 | train loss: 1.720912e-03 | valid loss: 1.728745e-03 
      	| train loss (relative): 3.389471e-02 | valid loss (relative): 3.397661e-02 
Epoch 515 use: 405.68 second.

epoch 516 starting......
Epoch:  516 | train loss: 1.720475e-03 | valid loss: 1.738906e-03 
      	| train loss (relative): 3.388676e-02 | valid loss (relative): 3.418360e-02 
Epoch 516 use: 411.52 second.

epoch 517 starting......
Epoch:  517 | train loss: 1.723900e-03 | valid loss: 1.732635e-03 
      	| train loss (relative): 3.395682e-02 | valid loss (relative): 3.378854e-02 
Epoch 517 use: 415.07 second.

epoch 518 starting......
Epoch:  518 | train loss: 1.720221e-03 | valid loss: 1.737545e-03 
      	| train loss (relative): 3.387907e-02 | valid loss (relative): 3.399507e-02 
Epoch 518 use: 433.85 second.

epoch 519 starting......
Epoch:  519 | train loss: 1.719480e-03 | valid loss: 1.730562e-03 
      	| train loss (relative): 3.387079e-02 | valid loss (relative): 3.372205e-02 
Epoch 519 use: 421.11 second.

epoch 520 starting......
Epoch:  520 | train loss: 1.719265e-03 | valid loss: 1.751949e-03 
      	| train loss (relative): 3.386450e-02 | valid loss (relative): 3.420325e-02 
Epoch 520 use: 423.22 second.

epoch 521 starting......
Epoch:  521 | train loss: 1.719097e-03 | valid loss: 1.723763e-03 
      	| train loss (relative): 3.385773e-02 | valid loss (relative): 3.403668e-02 
Epoch 521 use: 415.67 second.

epoch 522 starting......
Epoch:  522 | train loss: 1.709452e-03 | valid loss: 1.723943e-03 
      	| train loss (relative): 3.366828e-02 | valid loss (relative): 3.403495e-02 
Epoch 522 use: 429.07 second.

epoch 523 starting......
Epoch:  523 | train loss: 1.710723e-03 | valid loss: 1.731207e-03 
      	| train loss (relative): 3.368826e-02 | valid loss (relative): 3.409088e-02 
Epoch 523 use: 433.20 second.

epoch 524 starting......
Epoch:  524 | train loss: 1.715537e-03 | valid loss: 1.717910e-03 
      	| train loss (relative): 3.378646e-02 | valid loss (relative): 3.384808e-02 
Epoch 524 use: 449.27 second.

epoch 525 starting......
Epoch:  525 | train loss: 1.708711e-03 | valid loss: 1.725579e-03 
      	| train loss (relative): 3.364688e-02 | valid loss (relative): 3.397301e-02 
Epoch 525 use: 412.84 second.

epoch 526 starting......
Epoch:  526 | train loss: 1.709098e-03 | valid loss: 1.722897e-03 
      	| train loss (relative): 3.365323e-02 | valid loss (relative): 3.383010e-02 
Epoch 526 use: 414.90 second.

epoch 527 starting......
Epoch:  527 | train loss: 1.708399e-03 | valid loss: 1.720672e-03 
      	| train loss (relative): 3.364134e-02 | valid loss (relative): 3.383160e-02 
Epoch 527 use: 431.69 second.

epoch 528 starting......
Epoch:  528 | train loss: 1.714007e-03 | valid loss: 1.727141e-03 
      	| train loss (relative): 3.375544e-02 | valid loss (relative): 3.416314e-02 
Epoch 528 use: 425.92 second.

epoch 529 starting......
Epoch:  529 | train loss: 1.711072e-03 | valid loss: 1.732561e-03 
      	| train loss (relative): 3.369423e-02 | valid loss (relative): 3.404278e-02 
Epoch 529 use: 415.69 second.

epoch 530 starting......
Epoch:  530 | train loss: 1.710087e-03 | valid loss: 1.716332e-03 
      	| train loss (relative): 3.366789e-02 | valid loss (relative): 3.383082e-02 
Epoch 530 use: 425.84 second.

epoch 531 starting......
Epoch:  531 | train loss: 1.703568e-03 | valid loss: 1.728280e-03 
      	| train loss (relative): 3.354284e-02 | valid loss (relative): 3.414494e-02 
Epoch 531 use: 416.96 second.

epoch 532 starting......
Epoch:  532 | train loss: 1.711363e-03 | valid loss: 1.712173e-03 
      	| train loss (relative): 3.369639e-02 | valid loss (relative): 3.372884e-02 
Epoch 532 use: 421.91 second.

epoch 533 starting......
Epoch:  533 | train loss: 1.698185e-03 | valid loss: 1.713327e-03 
      	| train loss (relative): 3.343532e-02 | valid loss (relative): 3.355949e-02 
Epoch 533 use: 416.14 second.

epoch 534 starting......
Epoch:  534 | train loss: 1.699464e-03 | valid loss: 1.715978e-03 
      	| train loss (relative): 3.345761e-02 | valid loss (relative): 3.376752e-02 
Epoch 534 use: 423.56 second.

epoch 535 starting......
Epoch:  535 | train loss: 1.704526e-03 | valid loss: 1.723347e-03 
      	| train loss (relative): 3.355565e-02 | valid loss (relative): 3.385934e-02 
Epoch 535 use: 408.38 second.

epoch 536 starting......
Epoch:  536 | train loss: 1.700646e-03 | valid loss: 1.723739e-03 
      	| train loss (relative): 3.348216e-02 | valid loss (relative): 3.401581e-02 
Epoch 536 use: 443.04 second.

epoch 537 starting......
Epoch:  537 | train loss: 1.706001e-03 | valid loss: 1.722503e-03 
      	| train loss (relative): 3.358525e-02 | valid loss (relative): 3.403703e-02 
Epoch 537 use: 433.74 second.

epoch 538 starting......
Epoch:  538 | train loss: 1.705239e-03 | valid loss: 1.714983e-03 
      	| train loss (relative): 3.357530e-02 | valid loss (relative): 3.374639e-02 
Epoch 538 use: 441.67 second.

epoch 539 starting......
Epoch:  539 | train loss: 1.700084e-03 | valid loss: 1.703934e-03 
      	| train loss (relative): 3.346911e-02 | valid loss (relative): 3.348521e-02 
Epoch 539 use: 439.65 second.

epoch 540 starting......
Epoch:  540 | train loss: 1.691576e-03 | valid loss: 1.710504e-03 
      	| train loss (relative): 3.329556e-02 | valid loss (relative): 3.372705e-02 
Epoch 540 use: 435.59 second.

epoch 541 starting......
Epoch:  541 | train loss: 1.693712e-03 | valid loss: 1.706025e-03 
      	| train loss (relative): 3.334285e-02 | valid loss (relative): 3.364570e-02 
Epoch 541 use: 383.82 second.

epoch 542 starting......
Epoch:  542 | train loss: 1.689603e-03 | valid loss: 1.703389e-03 
      	| train loss (relative): 3.326693e-02 | valid loss (relative): 3.335102e-02 
Epoch 542 use: 364.56 second.

epoch 543 starting......
Epoch:  543 | train loss: 1.693216e-03 | valid loss: 1.706130e-03 
      	| train loss (relative): 3.333079e-02 | valid loss (relative): 3.353053e-02 
Epoch 543 use: 364.52 second.

epoch 544 starting......
Epoch:  544 | train loss: 1.694138e-03 | valid loss: 1.708683e-03 
      	| train loss (relative): 3.335140e-02 | valid loss (relative): 3.357251e-02 
Epoch 544 use: 365.48 second.

epoch 545 starting......
Epoch:  545 | train loss: 1.696290e-03 | valid loss: 1.717721e-03 
      	| train loss (relative): 3.339188e-02 | valid loss (relative): 3.391042e-02 
Epoch 545 use: 338.38 second.

epoch 546 starting......
Epoch:  546 | train loss: 1.695728e-03 | valid loss: 1.713212e-03 
      	| train loss (relative): 3.337707e-02 | valid loss (relative): 3.378752e-02 
Epoch 546 use: 364.11 second.

epoch 547 starting......
Epoch:  547 | train loss: 1.694599e-03 | valid loss: 1.710263e-03 
      	| train loss (relative): 3.336329e-02 | valid loss (relative): 3.358523e-02 
Epoch 547 use: 371.33 second.

epoch 548 starting......
Epoch:  548 | train loss: 1.689099e-03 | valid loss: 1.701917e-03 
      	| train loss (relative): 3.324565e-02 | valid loss (relative): 3.346112e-02 
Epoch 548 use: 364.84 second.

epoch 549 starting......
Epoch:  549 | train loss: 1.689002e-03 | valid loss: 1.713791e-03 
      	| train loss (relative): 3.324456e-02 | valid loss (relative): 3.383181e-02 
Epoch 549 use: 376.06 second.

epoch 550 starting......
Epoch:  550 | train loss: 1.696038e-03 | valid loss: 1.706437e-03 
      	| train loss (relative): 3.338355e-02 | valid loss (relative): 3.346246e-02 
Epoch 550 use: 364.70 second.

epoch 551 starting......
Epoch:  551 | train loss: 1.695508e-03 | valid loss: 1.705925e-03 
      	| train loss (relative): 3.336903e-02 | valid loss (relative): 3.348475e-02 
Epoch 551 use: 352.47 second.

epoch 552 starting......
Epoch:  552 | train loss: 1.687303e-03 | valid loss: 1.701826e-03 
      	| train loss (relative): 3.320822e-02 | valid loss (relative): 3.345697e-02 
Epoch 552 use: 379.00 second.

epoch 553 starting......
Epoch:  553 | train loss: 1.686759e-03 | valid loss: 1.705465e-03 
      	| train loss (relative): 3.319664e-02 | valid loss (relative): 3.353700e-02 
Epoch 553 use: 356.83 second.

epoch 554 starting......
Epoch:  554 | train loss: 1.690181e-03 | valid loss: 1.704629e-03 
      	| train loss (relative): 3.326817e-02 | valid loss (relative): 3.346723e-02 
Epoch 554 use: 356.00 second.

epoch 555 starting......
Epoch:  555 | train loss: 1.688200e-03 | valid loss: 1.710126e-03 
      	| train loss (relative): 3.323095e-02 | valid loss (relative): 3.352659e-02 
Epoch 555 use: 374.28 second.

epoch 556 starting......
Epoch:  556 | train loss: 1.688578e-03 | valid loss: 1.703034e-03 
      	| train loss (relative): 3.323507e-02 | valid loss (relative): 3.362882e-02 
Epoch 556 use: 359.97 second.

epoch 557 starting......
Epoch:  557 | train loss: 1.684175e-03 | valid loss: 1.702546e-03 
      	| train loss (relative): 3.315070e-02 | valid loss (relative): 3.345706e-02 
Epoch 557 use: 355.10 second.

epoch 558 starting......
Epoch:  558 | train loss: 1.681317e-03 | valid loss: 1.703068e-03 
      	| train loss (relative): 3.308975e-02 | valid loss (relative): 3.340923e-02 
Epoch 558 use: 362.93 second.

epoch 559 starting......
Epoch:  559 | train loss: 1.684752e-03 | valid loss: 1.718877e-03 
      	| train loss (relative): 3.316503e-02 | valid loss (relative): 3.352210e-02 
Epoch 559 use: 335.99 second.

epoch 560 starting......
Epoch:  560 | train loss: 1.693429e-03 | valid loss: 1.696705e-03 
      	| train loss (relative): 3.333092e-02 | valid loss (relative): 3.331866e-02 
Epoch 560 use: 368.40 second.

epoch 561 starting......
Epoch:  561 | train loss: 1.679510e-03 | valid loss: 1.698634e-03 
      	| train loss (relative): 3.305730e-02 | valid loss (relative): 3.311323e-02 
Epoch 561 use: 363.45 second.

epoch 562 starting......
Epoch:  562 | train loss: 1.678135e-03 | valid loss: 1.696564e-03 
      	| train loss (relative): 3.301695e-02 | valid loss (relative): 3.320887e-02 
Epoch 562 use: 368.85 second.

epoch 563 starting......
Epoch:  563 | train loss: 1.679121e-03 | valid loss: 1.703779e-03 
      	| train loss (relative): 3.303834e-02 | valid loss (relative): 3.338731e-02 
Epoch 563 use: 346.19 second.

epoch 564 starting......
Epoch:  564 | train loss: 1.676543e-03 | valid loss: 1.697384e-03 
      	| train loss (relative): 3.299190e-02 | valid loss (relative): 3.340884e-02 
Epoch 564 use: 342.56 second.

epoch 565 starting......
Epoch:  565 | train loss: 1.676235e-03 | valid loss: 1.693653e-03 
      	| train loss (relative): 3.298189e-02 | valid loss (relative): 3.322905e-02 
Epoch 565 use: 350.94 second.

epoch 566 starting......
Epoch:  566 | train loss: 1.681955e-03 | valid loss: 1.703527e-03 
      	| train loss (relative): 3.310009e-02 | valid loss (relative): 3.348868e-02 
Epoch 566 use: 347.22 second.

epoch 567 starting......
Epoch:  567 | train loss: 1.681321e-03 | valid loss: 1.712168e-03 
      	| train loss (relative): 3.308544e-02 | valid loss (relative): 3.328547e-02 
Epoch 567 use: 347.47 second.

epoch 568 starting......
Epoch:  568 | train loss: 1.679064e-03 | valid loss: 1.691268e-03 
      	| train loss (relative): 3.304405e-02 | valid loss (relative): 3.309720e-02 
Epoch 568 use: 334.60 second.

epoch 569 starting......
Epoch:  569 | train loss: 1.673728e-03 | valid loss: 1.695458e-03 
      	| train loss (relative): 3.293700e-02 | valid loss (relative): 3.294538e-02 
Epoch 569 use: 359.93 second.

epoch 570 starting......
Epoch:  570 | train loss: 1.674416e-03 | valid loss: 1.694298e-03 
      	| train loss (relative): 3.294691e-02 | valid loss (relative): 3.311230e-02 
Epoch 570 use: 367.27 second.

epoch 571 starting......
Epoch:  571 | train loss: 1.674025e-03 | valid loss: 1.695387e-03 
      	| train loss (relative): 3.293879e-02 | valid loss (relative): 3.332235e-02 
Epoch 571 use: 340.08 second.

epoch 572 starting......
Epoch:  572 | train loss: 1.672428e-03 | valid loss: 1.694511e-03 
      	| train loss (relative): 3.290957e-02 | valid loss (relative): 3.343473e-02 
Epoch 572 use: 390.83 second.

epoch 573 starting......
Epoch:  573 | train loss: 1.673897e-03 | valid loss: 1.692606e-03 
      	| train loss (relative): 3.293727e-02 | valid loss (relative): 3.335799e-02 
Epoch 573 use: 346.17 second.

epoch 574 starting......
Epoch:  574 | train loss: 1.678061e-03 | valid loss: 1.685433e-03 
      	| train loss (relative): 3.303061e-02 | valid loss (relative): 3.314083e-02 
Epoch 574 use: 371.69 second.

epoch 575 starting......
Epoch:  575 | train loss: 1.664963e-03 | valid loss: 1.681077e-03 
      	| train loss (relative): 3.276140e-02 | valid loss (relative): 3.300698e-02 
Epoch 575 use: 341.04 second.

epoch 576 starting......
Epoch:  576 | train loss: 1.662471e-03 | valid loss: 1.683241e-03 
      	| train loss (relative): 3.270619e-02 | valid loss (relative): 3.312166e-02 
Epoch 576 use: 351.87 second.

epoch 577 starting......
Epoch:  577 | train loss: 1.666223e-03 | valid loss: 1.691738e-03 
      	| train loss (relative): 3.278560e-02 | valid loss (relative): 3.329157e-02 
Epoch 577 use: 354.90 second.

epoch 578 starting......
Epoch:  578 | train loss: 1.667931e-03 | valid loss: 1.684045e-03 
      	| train loss (relative): 3.281577e-02 | valid loss (relative): 3.299245e-02 
Epoch 578 use: 355.80 second.

epoch 579 starting......
Epoch:  579 | train loss: 1.667926e-03 | valid loss: 1.689633e-03 
      	| train loss (relative): 3.280952e-02 | valid loss (relative): 3.329685e-02 
Epoch 579 use: 349.89 second.

epoch 580 starting......
Epoch:  580 | train loss: 1.666281e-03 | valid loss: 1.684500e-03 
      	| train loss (relative): 3.278436e-02 | valid loss (relative): 3.302716e-02 
Epoch 580 use: 360.45 second.

epoch 581 starting......
Epoch:  581 | train loss: 1.665218e-03 | valid loss: 1.689100e-03 
      	| train loss (relative): 3.276016e-02 | valid loss (relative): 3.335366e-02 
Epoch 581 use: 349.30 second.

epoch 582 starting......
Epoch:  582 | train loss: 1.670895e-03 | valid loss: 1.700333e-03 
      	| train loss (relative): 3.287774e-02 | valid loss (relative): 3.341516e-02 
Epoch 582 use: 364.07 second.

epoch 583 starting......
Epoch:  583 | train loss: 1.669629e-03 | valid loss: 1.683922e-03 
      	| train loss (relative): 3.285423e-02 | valid loss (relative): 3.288478e-02 
Epoch 583 use: 343.26 second.

epoch 584 starting......
Epoch:  584 | train loss: 1.661908e-03 | valid loss: 1.681494e-03 
      	| train loss (relative): 3.269665e-02 | valid loss (relative): 3.291057e-02 
Epoch 584 use: 364.73 second.

epoch 585 starting......
Epoch:  585 | train loss: 1.661015e-03 | valid loss: 1.681385e-03 
      	| train loss (relative): 3.267451e-02 | valid loss (relative): 3.288215e-02 
Epoch 585 use: 362.76 second.

epoch 586 starting......
Epoch:  586 | train loss: 1.662306e-03 | valid loss: 1.682224e-03 
      	| train loss (relative): 3.270142e-02 | valid loss (relative): 3.304095e-02 
Epoch 586 use: 352.26 second.

epoch 587 starting......
Epoch:  587 | train loss: 1.661512e-03 | valid loss: 1.682946e-03 
      	| train loss (relative): 3.268557e-02 | valid loss (relative): 3.318058e-02 
Epoch 587 use: 366.40 second.

epoch 588 starting......
Epoch:  588 | train loss: 1.662733e-03 | valid loss: 1.679649e-03 
      	| train loss (relative): 3.271161e-02 | valid loss (relative): 3.300452e-02 
Epoch 588 use: 351.33 second.

epoch 589 starting......
Epoch:  589 | train loss: 1.664460e-03 | valid loss: 1.684467e-03 
      	| train loss (relative): 3.274497e-02 | valid loss (relative): 3.301819e-02 
Epoch 589 use: 356.53 second.

epoch 590 starting......
Epoch:  590 | train loss: 1.663262e-03 | valid loss: 1.685417e-03 
      	| train loss (relative): 3.271370e-02 | valid loss (relative): 3.327145e-02 
Epoch 590 use: 366.58 second.

epoch 591 starting......
Epoch:  591 | train loss: 1.663302e-03 | valid loss: 1.686151e-03 
      	| train loss (relative): 3.271952e-02 | valid loss (relative): 3.286379e-02 
Epoch 591 use: 359.85 second.

epoch 592 starting......
Epoch:  592 | train loss: 1.661479e-03 | valid loss: 1.676842e-03 
      	| train loss (relative): 3.267968e-02 | valid loss (relative): 3.282721e-02 
Epoch 592 use: 361.56 second.

epoch 593 starting......
Epoch:  593 | train loss: 1.657027e-03 | valid loss: 1.682298e-03 
      	| train loss (relative): 3.258996e-02 | valid loss (relative): 3.302664e-02 
Epoch 593 use: 342.33 second.

epoch 594 starting......
Epoch:  594 | train loss: 1.660717e-03 | valid loss: 1.685407e-03 
      	| train loss (relative): 3.266856e-02 | valid loss (relative): 3.333298e-02 
Epoch 594 use: 374.47 second.

epoch 595 starting......
Epoch:  595 | train loss: 1.660940e-03 | valid loss: 1.678653e-03 
      	| train loss (relative): 3.267816e-02 | valid loss (relative): 3.294493e-02 
Epoch 595 use: 355.71 second.

epoch 596 starting......
Epoch:  596 | train loss: 1.654199e-03 | valid loss: 1.673242e-03 
      	| train loss (relative): 3.253725e-02 | valid loss (relative): 3.273198e-02 
Epoch 596 use: 373.93 second.

epoch 597 starting......
Epoch:  597 | train loss: 1.649089e-03 | valid loss: 1.670985e-03 
      	| train loss (relative): 3.243482e-02 | valid loss (relative): 3.273646e-02 
Epoch 597 use: 354.01 second.

epoch 598 starting......
Epoch:  598 | train loss: 1.648796e-03 | valid loss: 1.670660e-03 
      	| train loss (relative): 3.242848e-02 | valid loss (relative): 3.275162e-02 
Epoch 598 use: 358.83 second.

epoch 599 starting......
Epoch:  599 | train loss: 1.651229e-03 | valid loss: 1.668936e-03 
      	| train loss (relative): 3.247676e-02 | valid loss (relative): 3.276704e-02 
Epoch 599 use: 367.16 second.

test MSE Error: 1.661106e-03 | relative MSE Error: 3.265560e-02 
 Total time used for training: 16.93 hour.
MESLoss saved to  /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_MSELoss_Latent_64_nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_600.txt
relative MSELoss saved to  /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_reMSELoss_Latent_64_nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_600.txt
model saved to /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_64_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_600.pth
model_dict saved to /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_64_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_600_dict.pth
... Training slugflow data group 3 completed, Run finished Sun 15 Aug 05:15:10 BST 2021 ...
{'data_dir': '/rds/general/user/jy220/home/tensors/', 'mode': 'train', 'data_type': 'tensors', 'vtu_fields': 'Component1::ComponentMassFractionPhase1, phase1::Velocity', 'variational': 'False', 'optimizer': 'Adamax', 'structured': 'False', 'changing_lr': 'False', 'self_concat': '2', 'dimension': '3', 'sfc_nums': '3', 'components': '4', 'nearest_neighbouring': 'True', 'dims_latent': '64', 'activation': 'Tanh', 'tk_file': '/rds/general/user/jy220/home/slugflow_tk.pt', 'tb_file': '/rds/general/user/jy220/home/slugflow_tb.pt', 'sfc_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/sfcs.pt', 'inv_sfc_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/inv_sfcs.pt', 'coords_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/coords.pt', 'cells_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/cells.npy', 'state_load': '/rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_64_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_600_dict.pth', 'batch_size': '16', 'lr': '0.001', 'n_epoches': '150', 'seed': '17', 'visualize': 'False', 'reconstructed_path': 'None', 'reconstruct_start_index': '0', 'reconstruct_end_index': '50', 'save_path': '/rds/general/user/jy220/home/new_results/'}
[[ 2.50825269e-01 -1.67256990e-02 -1.22268730e-03]
 [ 2.38611959e-01 -5.50627777e-03 -5.06451913e-03]
 [ 2.39454874e-01 -1.59563257e-02  3.96778323e-03]
 ...
 [ 9.11361096e+00 -2.21908381e-02  2.22384902e-03]
 [ 9.11847310e+00 -2.84346931e-02 -2.34839590e-04]
 [ 9.11039661e+00 -2.51498573e-02 -8.70661044e-03]]
{'tetra': array([[      0,       1,       2,       3],
       [      4,       5,       6,       7],
       [      8,       9,      10,      11],
       ...,
       [1342744, 1342745, 1342746, 1342747],
       [1342748, 1342749, 1342750, 1342751],
       [1342752, 1342753, 1342754, 1342755]])}
vtu fields:  ['Component1::ComponentMassFractionPhase1', 'phase1::Velocity'] 
 structured  False 
 activation  Tanh() 
 self concat  2 
 sfc_nums  3 

dims_latent  64 
 components  4 
 nearest_neighbouring  True 

visualize  False 
 sample number  1706 

reading sfc nums......
[array([  41684,   41685,   41674, ..., 1235560, 1235564, 1235558]), array([954289, 954290, 954291, ..., 420685, 420687, 420688]), array([  40156,   40155,   40158, ..., 1123478, 1123475, 1123467])]
reading inverse sfc nums......
[array([   8146,    8145,    8144, ..., 1274444, 1274445, 1273166]), array([547280, 547283, 593037, ..., 933933, 933932, 864488]), array([ 66437,  66439, 122512, ..., 459416, 415130, 459418])]
tk:  tensor([0.7581, 0.0418, 0.1840, 0.1464], dtype=torch.float64) 

tb:  tensor([-0.3816, -0.3618, -0.0308,  0.1446], dtype=torch.float64) 

length of train set: 1505 

length of valid set: 100 

length of test set: 101 

torch device num: 4 

Let's use 4 GPUs!
epoch 600 starting......
Epoch:  600 | train loss: 1.879657e-03 | valid loss: 1.661147e-03 
      	| train loss (relative): 3.690575e-02 | valid loss (relative): 3.260949e-02 
Epoch 600 use: 556.49 second.

epoch 601 starting......
Epoch:  601 | train loss: 1.652246e-03 | valid loss: 1.646010e-03 
      	| train loss (relative): 3.249811e-02 | valid loss (relative): 3.237164e-02 
Epoch 601 use: 478.79 second.

epoch 602 starting......
Epoch:  602 | train loss: 1.640550e-03 | valid loss: 1.644652e-03 
      	| train loss (relative): 3.226521e-02 | valid loss (relative): 3.231145e-02 
Epoch 602 use: 459.24 second.

epoch 603 starting......
Epoch:  603 | train loss: 1.637138e-03 | valid loss: 1.642617e-03 
      	| train loss (relative): 3.219523e-02 | valid loss (relative): 3.220520e-02 
Epoch 603 use: 458.61 second.

epoch 604 starting......
Epoch:  604 | train loss: 1.633287e-03 | valid loss: 1.643069e-03 
      	| train loss (relative): 3.212008e-02 | valid loss (relative): 3.233227e-02 
Epoch 604 use: 441.89 second.

epoch 605 starting......
Epoch:  605 | train loss: 1.632027e-03 | valid loss: 1.642884e-03 
      	| train loss (relative): 3.209268e-02 | valid loss (relative): 3.229618e-02 
Epoch 605 use: 438.07 second.

epoch 606 starting......
Epoch:  606 | train loss: 1.630984e-03 | valid loss: 1.643024e-03 
      	| train loss (relative): 3.206797e-02 | valid loss (relative): 3.228430e-02 
Epoch 606 use: 439.68 second.

epoch 607 starting......
Epoch:  607 | train loss: 1.631981e-03 | valid loss: 1.645590e-03 
      	| train loss (relative): 3.209080e-02 | valid loss (relative): 3.233621e-02 
Epoch 607 use: 456.00 second.

epoch 608 starting......
Epoch:  608 | train loss: 1.632415e-03 | valid loss: 1.646237e-03 
      	| train loss (relative): 3.209836e-02 | valid loss (relative): 3.233910e-02 
Epoch 608 use: 442.84 second.

epoch 609 starting......
Epoch:  609 | train loss: 1.634093e-03 | valid loss: 1.650124e-03 
      	| train loss (relative): 3.213255e-02 | valid loss (relative): 3.243579e-02 
Epoch 609 use: 441.73 second.

epoch 610 starting......
Epoch:  610 | train loss: 1.636314e-03 | valid loss: 1.654520e-03 
      	| train loss (relative): 3.217704e-02 | valid loss (relative): 3.243314e-02 
Epoch 610 use: 432.63 second.

epoch 611 starting......
Epoch:  611 | train loss: 1.641417e-03 | valid loss: 1.663194e-03 
      	| train loss (relative): 3.227634e-02 | valid loss (relative): 3.260380e-02 
Epoch 611 use: 456.16 second.

epoch 612 starting......
Epoch:  612 | train loss: 1.640857e-03 | valid loss: 1.656900e-03 
      	| train loss (relative): 3.226400e-02 | valid loss (relative): 3.255021e-02 
Epoch 612 use: 431.22 second.

epoch 613 starting......
Epoch:  613 | train loss: 1.639637e-03 | valid loss: 1.655249e-03 
      	| train loss (relative): 3.224336e-02 | valid loss (relative): 3.241496e-02 
Epoch 613 use: 460.70 second.

epoch 614 starting......
Epoch:  614 | train loss: 1.641762e-03 | valid loss: 1.665899e-03 
      	| train loss (relative): 3.227837e-02 | valid loss (relative): 3.260060e-02 
Epoch 614 use: 456.75 second.

epoch 615 starting......
Epoch:  615 | train loss: 1.643222e-03 | valid loss: 1.671764e-03 
      	| train loss (relative): 3.231661e-02 | valid loss (relative): 3.274940e-02 
Epoch 615 use: 445.50 second.

epoch 616 starting......
Epoch:  616 | train loss: 1.644521e-03 | valid loss: 1.666127e-03 
      	| train loss (relative): 3.234211e-02 | valid loss (relative): 3.279747e-02 
Epoch 616 use: 431.89 second.

epoch 617 starting......
Epoch:  617 | train loss: 1.643438e-03 | valid loss: 1.653541e-03 
      	| train loss (relative): 3.231924e-02 | valid loss (relative): 3.239659e-02 
Epoch 617 use: 421.48 second.

epoch 618 starting......
Epoch:  618 | train loss: 1.634655e-03 | valid loss: 1.653512e-03 
      	| train loss (relative): 3.213675e-02 | valid loss (relative): 3.235806e-02 
Epoch 618 use: 429.62 second.

epoch 619 starting......
Epoch:  619 | train loss: 1.636505e-03 | valid loss: 1.670437e-03 
      	| train loss (relative): 3.217601e-02 | valid loss (relative): 3.276570e-02 
Epoch 619 use: 434.87 second.

epoch 620 starting......
Epoch:  620 | train loss: 1.638412e-03 | valid loss: 1.657247e-03 
      	| train loss (relative): 3.221712e-02 | valid loss (relative): 3.258204e-02 
Epoch 620 use: 441.38 second.

epoch 621 starting......
Epoch:  621 | train loss: 1.635777e-03 | valid loss: 1.657520e-03 
      	| train loss (relative): 3.216405e-02 | valid loss (relative): 3.239326e-02 
Epoch 621 use: 435.22 second.

epoch 622 starting......
Epoch:  622 | train loss: 1.636617e-03 | valid loss: 1.658073e-03 
      	| train loss (relative): 3.217839e-02 | valid loss (relative): 3.253810e-02 
Epoch 622 use: 428.02 second.

epoch 623 starting......
Epoch:  623 | train loss: 1.639609e-03 | valid loss: 1.662500e-03 
      	| train loss (relative): 3.223676e-02 | valid loss (relative): 3.268411e-02 
Epoch 623 use: 433.18 second.

epoch 624 starting......
Epoch:  624 | train loss: 1.641313e-03 | valid loss: 1.662697e-03 
      	| train loss (relative): 3.226813e-02 | valid loss (relative): 3.260523e-02 
Epoch 624 use: 427.14 second.

epoch 625 starting......
Epoch:  625 | train loss: 1.638278e-03 | valid loss: 1.659053e-03 
      	| train loss (relative): 3.220736e-02 | valid loss (relative): 3.255112e-02 
Epoch 625 use: 415.96 second.

epoch 626 starting......
Epoch:  626 | train loss: 1.635225e-03 | valid loss: 1.654958e-03 
      	| train loss (relative): 3.215358e-02 | valid loss (relative): 3.257176e-02 
Epoch 626 use: 416.15 second.

epoch 627 starting......
Epoch:  627 | train loss: 1.632580e-03 | valid loss: 1.653109e-03 
      	| train loss (relative): 3.209555e-02 | valid loss (relative): 3.240692e-02 
Epoch 627 use: 432.36 second.

epoch 628 starting......
Epoch:  628 | train loss: 1.633746e-03 | valid loss: 1.658628e-03 
      	| train loss (relative): 3.211801e-02 | valid loss (relative): 3.247230e-02 
Epoch 628 use: 412.23 second.

epoch 629 starting......
Epoch:  629 | train loss: 1.634268e-03 | valid loss: 1.662502e-03 
      	| train loss (relative): 3.212856e-02 | valid loss (relative): 3.277050e-02 
Epoch 629 use: 428.27 second.

epoch 630 starting......
Epoch:  630 | train loss: 1.633679e-03 | valid loss: 1.653883e-03 
      	| train loss (relative): 3.212444e-02 | valid loss (relative): 3.231692e-02 
Epoch 630 use: 497.50 second.

epoch 631 starting......
Epoch:  631 | train loss: 1.633251e-03 | valid loss: 1.671159e-03 
      	| train loss (relative): 3.210341e-02 | valid loss (relative): 3.295650e-02 
Epoch 631 use: 594.01 second.

epoch 632 starting......
Epoch:  632 | train loss: 1.638286e-03 | valid loss: 1.654956e-03 
      	| train loss (relative): 3.221356e-02 | valid loss (relative): 3.265527e-02 
Epoch 632 use: 438.95 second.

epoch 633 starting......
Epoch:  633 | train loss: 1.627485e-03 | valid loss: 1.653947e-03 
      	| train loss (relative): 3.199486e-02 | valid loss (relative): 3.237207e-02 
Epoch 633 use: 411.73 second.

epoch 634 starting......
Epoch:  634 | train loss: 1.629145e-03 | valid loss: 1.652640e-03 
      	| train loss (relative): 3.202666e-02 | valid loss (relative): 3.247460e-02 
Epoch 634 use: 410.39 second.

epoch 635 starting......
Epoch:  635 | train loss: 1.630307e-03 | valid loss: 1.677122e-03 
      	| train loss (relative): 3.205081e-02 | valid loss (relative): 3.265417e-02 
Epoch 635 use: 408.29 second.

epoch 636 starting......
Epoch:  636 | train loss: 1.635039e-03 | valid loss: 1.659543e-03 
      	| train loss (relative): 3.214626e-02 | valid loss (relative): 3.265624e-02 
Epoch 636 use: 431.55 second.

epoch 637 starting......
Epoch:  637 | train loss: 1.634125e-03 | valid loss: 1.658669e-03 
      	| train loss (relative): 3.212849e-02 | valid loss (relative): 3.265238e-02 
Epoch 637 use: 422.42 second.

epoch 638 starting......
Epoch:  638 | train loss: 1.627877e-03 | valid loss: 1.657680e-03 
      	| train loss (relative): 3.200027e-02 | valid loss (relative): 3.276927e-02 
Epoch 638 use: 416.07 second.

epoch 639 starting......
Epoch:  639 | train loss: 1.625379e-03 | valid loss: 1.651554e-03 
      	| train loss (relative): 3.195252e-02 | valid loss (relative): 3.261112e-02 
Epoch 639 use: 428.85 second.

epoch 640 starting......
Epoch:  640 | train loss: 1.620246e-03 | valid loss: 1.644506e-03 
      	| train loss (relative): 3.185170e-02 | valid loss (relative): 3.223589e-02 
Epoch 640 use: 397.62 second.

epoch 641 starting......
Epoch:  641 | train loss: 1.622021e-03 | valid loss: 1.653121e-03 
      	| train loss (relative): 3.187868e-02 | valid loss (relative): 3.232056e-02 
Epoch 641 use: 440.09 second.

epoch 642 starting......
Epoch:  642 | train loss: 1.630500e-03 | valid loss: 1.651810e-03 
      	| train loss (relative): 3.204601e-02 | valid loss (relative): 3.241464e-02 
Epoch 642 use: 421.46 second.

epoch 643 starting......
Epoch:  643 | train loss: 1.630186e-03 | valid loss: 1.654087e-03 
      	| train loss (relative): 3.204663e-02 | valid loss (relative): 3.252127e-02 
Epoch 643 use: 438.40 second.

epoch 644 starting......
Epoch:  644 | train loss: 1.621827e-03 | valid loss: 1.656642e-03 
      	| train loss (relative): 3.187902e-02 | valid loss (relative): 3.222734e-02 
Epoch 644 use: 399.57 second.

epoch 645 starting......
Epoch:  645 | train loss: 1.625036e-03 | valid loss: 1.651686e-03 
      	| train loss (relative): 3.193289e-02 | valid loss (relative): 3.237554e-02 
Epoch 645 use: 415.12 second.

epoch 646 starting......
Epoch:  646 | train loss: 1.621992e-03 | valid loss: 1.654605e-03 
      	| train loss (relative): 3.188200e-02 | valid loss (relative): 3.243588e-02 
Epoch 646 use: 399.27 second.

epoch 647 starting......
Epoch:  647 | train loss: 1.625227e-03 | valid loss: 1.648226e-03 
      	| train loss (relative): 3.194129e-02 | valid loss (relative): 3.235446e-02 
Epoch 647 use: 408.46 second.

epoch 648 starting......
Epoch:  648 | train loss: 1.624524e-03 | valid loss: 1.655145e-03 
      	| train loss (relative): 3.192769e-02 | valid loss (relative): 3.265753e-02 
Epoch 648 use: 409.25 second.

epoch 649 starting......
Epoch:  649 | train loss: 1.621193e-03 | valid loss: 1.646382e-03 
      	| train loss (relative): 3.186283e-02 | valid loss (relative): 3.231698e-02 
Epoch 649 use: 425.08 second.

epoch 650 starting......
Epoch:  650 | train loss: 1.615764e-03 | valid loss: 1.647709e-03 
      	| train loss (relative): 3.175977e-02 | valid loss (relative): 3.244514e-02 
Epoch 650 use: 407.96 second.

epoch 651 starting......
Epoch:  651 | train loss: 1.615541e-03 | valid loss: 1.650159e-03 
      	| train loss (relative): 3.175640e-02 | valid loss (relative): 3.222532e-02 
Epoch 651 use: 400.71 second.

epoch 652 starting......
Epoch:  652 | train loss: 1.620914e-03 | valid loss: 1.652264e-03 
      	| train loss (relative): 3.185437e-02 | valid loss (relative): 3.260365e-02 
Epoch 652 use: 406.52 second.

epoch 653 starting......
Epoch:  653 | train loss: 1.623755e-03 | valid loss: 1.647087e-03 
      	| train loss (relative): 3.191609e-02 | valid loss (relative): 3.219537e-02 
Epoch 653 use: 402.24 second.

epoch 654 starting......
Epoch:  654 | train loss: 1.622404e-03 | valid loss: 1.649017e-03 
      	| train loss (relative): 3.188087e-02 | valid loss (relative): 3.257889e-02 
Epoch 654 use: 399.82 second.

epoch 655 starting......
Epoch:  655 | train loss: 1.618582e-03 | valid loss: 1.653283e-03 
      	| train loss (relative): 3.181081e-02 | valid loss (relative): 3.242420e-02 
Epoch 655 use: 410.08 second.

epoch 656 starting......
Epoch:  656 | train loss: 1.619223e-03 | valid loss: 1.663470e-03 
      	| train loss (relative): 3.182626e-02 | valid loss (relative): 3.261902e-02 
Epoch 656 use: 413.70 second.

epoch 657 starting......
Epoch:  657 | train loss: 1.622147e-03 | valid loss: 1.652279e-03 
      	| train loss (relative): 3.188211e-02 | valid loss (relative): 3.253172e-02 
Epoch 657 use: 420.51 second.

epoch 658 starting......
Epoch:  658 | train loss: 1.617899e-03 | valid loss: 1.647686e-03 
      	| train loss (relative): 3.179856e-02 | valid loss (relative): 3.246439e-02 
Epoch 658 use: 399.05 second.

epoch 659 starting......
Epoch:  659 | train loss: 1.613244e-03 | valid loss: 1.646612e-03 
      	| train loss (relative): 3.170625e-02 | valid loss (relative): 3.227685e-02 
Epoch 659 use: 426.64 second.

epoch 660 starting......
Epoch:  660 | train loss: 1.614063e-03 | valid loss: 1.645306e-03 
      	| train loss (relative): 3.171784e-02 | valid loss (relative): 3.228631e-02 
Epoch 660 use: 445.31 second.

epoch 661 starting......
Epoch:  661 | train loss: 1.612878e-03 | valid loss: 1.642252e-03 
      	| train loss (relative): 3.169509e-02 | valid loss (relative): 3.210738e-02 
Epoch 661 use: 512.28 second.

epoch 662 starting......
Epoch:  662 | train loss: 1.611045e-03 | valid loss: 1.654600e-03 
      	| train loss (relative): 3.166522e-02 | valid loss (relative): 3.218514e-02 
Epoch 662 use: 422.20 second.

epoch 663 starting......
Epoch:  663 | train loss: 1.613870e-03 | valid loss: 1.639128e-03 
      	| train loss (relative): 3.171406e-02 | valid loss (relative): 3.215367e-02 
Epoch 663 use: 405.07 second.

epoch 664 starting......
Epoch:  664 | train loss: 1.610668e-03 | valid loss: 1.648545e-03 
      	| train loss (relative): 3.165259e-02 | valid loss (relative): 3.245272e-02 
Epoch 664 use: 418.08 second.

epoch 665 starting......
Epoch:  665 | train loss: 1.608314e-03 | valid loss: 1.644253e-03 
      	| train loss (relative): 3.160264e-02 | valid loss (relative): 3.233548e-02 
Epoch 665 use: 398.40 second.

epoch 666 starting......
Epoch:  666 | train loss: 1.608887e-03 | valid loss: 1.639746e-03 
      	| train loss (relative): 3.161627e-02 | valid loss (relative): 3.211934e-02 
Epoch 666 use: 392.26 second.

epoch 667 starting......
Epoch:  667 | train loss: 1.609728e-03 | valid loss: 1.636472e-03 
      	| train loss (relative): 3.163150e-02 | valid loss (relative): 3.208015e-02 
Epoch 667 use: 530.13 second.

epoch 668 starting......
Epoch:  668 | train loss: 1.605247e-03 | valid loss: 1.645159e-03 
      	| train loss (relative): 3.154195e-02 | valid loss (relative): 3.240279e-02 
Epoch 668 use: 492.37 second.

epoch 669 starting......
Epoch:  669 | train loss: 1.613775e-03 | valid loss: 1.637590e-03 
      	| train loss (relative): 3.171208e-02 | valid loss (relative): 3.206276e-02 
Epoch 669 use: 511.70 second.

epoch 670 starting......
Epoch:  670 | train loss: 1.603511e-03 | valid loss: 1.640705e-03 
      	| train loss (relative): 3.150533e-02 | valid loss (relative): 3.208233e-02 
Epoch 670 use: 517.42 second.

epoch 671 starting......
Epoch:  671 | train loss: 1.607071e-03 | valid loss: 1.645854e-03 
      	| train loss (relative): 3.157327e-02 | valid loss (relative): 3.228373e-02 
Epoch 671 use: 519.50 second.

epoch 672 starting......
Epoch:  672 | train loss: 1.611084e-03 | valid loss: 1.642808e-03 
      	| train loss (relative): 3.165274e-02 | valid loss (relative): 3.205423e-02 
Epoch 672 use: 519.81 second.

epoch 673 starting......
Epoch:  673 | train loss: 1.602281e-03 | valid loss: 1.648721e-03 
      	| train loss (relative): 3.147670e-02 | valid loss (relative): 3.226376e-02 
Epoch 673 use: 591.42 second.

epoch 674 starting......
Epoch:  674 | train loss: 1.602856e-03 | valid loss: 1.636934e-03 
      	| train loss (relative): 3.149616e-02 | valid loss (relative): 3.228537e-02 
Epoch 674 use: 494.23 second.

epoch 675 starting......
Epoch:  675 | train loss: 1.599798e-03 | valid loss: 1.638983e-03 
      	| train loss (relative): 3.143419e-02 | valid loss (relative): 3.232340e-02 
Epoch 675 use: 514.49 second.

epoch 676 starting......
Epoch:  676 | train loss: 1.601180e-03 | valid loss: 1.655445e-03 
      	| train loss (relative): 3.146220e-02 | valid loss (relative): 3.236614e-02 
Epoch 676 use: 492.75 second.

epoch 677 starting......
Epoch:  677 | train loss: 1.605216e-03 | valid loss: 1.641664e-03 
      	| train loss (relative): 3.153672e-02 | valid loss (relative): 3.231072e-02 
Epoch 677 use: 492.16 second.

epoch 678 starting......
Epoch:  678 | train loss: 1.607531e-03 | valid loss: 1.641891e-03 
      	| train loss (relative): 3.158599e-02 | valid loss (relative): 3.222613e-02 
Epoch 678 use: 490.61 second.

epoch 679 starting......
Epoch:  679 | train loss: 1.605469e-03 | valid loss: 1.654128e-03 
      	| train loss (relative): 3.154620e-02 | valid loss (relative): 3.214896e-02 
Epoch 679 use: 495.51 second.

epoch 680 starting......
Epoch:  680 | train loss: 1.610947e-03 | valid loss: 1.632945e-03 
      	| train loss (relative): 3.164709e-02 | valid loss (relative): 3.205656e-02 
Epoch 680 use: 493.65 second.

epoch 681 starting......
Epoch:  681 | train loss: 1.595538e-03 | valid loss: 1.628979e-03 
      	| train loss (relative): 3.134465e-02 | valid loss (relative): 3.190320e-02 
Epoch 681 use: 496.92 second.

epoch 682 starting......
Epoch:  682 | train loss: 1.597141e-03 | valid loss: 1.626802e-03 
      	| train loss (relative): 3.137767e-02 | valid loss (relative): 3.194889e-02 
Epoch 682 use: 477.95 second.

epoch 683 starting......
Epoch:  683 | train loss: 1.594353e-03 | valid loss: 1.626842e-03 
      	| train loss (relative): 3.132154e-02 | valid loss (relative): 3.203637e-02 
Epoch 683 use: 499.59 second.

epoch 684 starting......
Epoch:  684 | train loss: 1.595270e-03 | valid loss: 1.628806e-03 
      	| train loss (relative): 3.134250e-02 | valid loss (relative): 3.196360e-02 
Epoch 684 use: 492.04 second.

epoch 685 starting......
Epoch:  685 | train loss: 1.594521e-03 | valid loss: 1.630670e-03 
      	| train loss (relative): 3.132675e-02 | valid loss (relative): 3.213006e-02 
Epoch 685 use: 498.78 second.

epoch 686 starting......
Epoch:  686 | train loss: 1.597531e-03 | valid loss: 1.630289e-03 
      	| train loss (relative): 3.138723e-02 | valid loss (relative): 3.185453e-02 
Epoch 686 use: 498.66 second.

epoch 687 starting......
Epoch:  687 | train loss: 1.596561e-03 | valid loss: 1.634805e-03 
      	| train loss (relative): 3.137026e-02 | valid loss (relative): 3.197228e-02 
Epoch 687 use: 527.00 second.

epoch 688 starting......
Epoch:  688 | train loss: 1.596040e-03 | valid loss: 1.630218e-03 
      	| train loss (relative): 3.135584e-02 | valid loss (relative): 3.189622e-02 
Epoch 688 use: 489.33 second.

epoch 689 starting......
Epoch:  689 | train loss: 1.596847e-03 | valid loss: 1.649158e-03 
      	| train loss (relative): 3.136889e-02 | valid loss (relative): 3.212908e-02 
Epoch 689 use: 503.32 second.

epoch 690 starting......
Epoch:  690 | train loss: 1.600467e-03 | valid loss: 1.632189e-03 
      	| train loss (relative): 3.144381e-02 | valid loss (relative): 3.197973e-02 
Epoch 690 use: 459.29 second.

epoch 691 starting......
Epoch:  691 | train loss: 1.596185e-03 | valid loss: 1.631890e-03 
      	| train loss (relative): 3.135595e-02 | valid loss (relative): 3.197718e-02 
Epoch 691 use: 406.51 second.

epoch 692 starting......
Epoch:  692 | train loss: 1.595700e-03 | valid loss: 1.632117e-03 
      	| train loss (relative): 3.134709e-02 | valid loss (relative): 3.204811e-02 
Epoch 692 use: 398.17 second.

epoch 693 starting......
Epoch:  693 | train loss: 1.596266e-03 | valid loss: 1.645180e-03 
      	| train loss (relative): 3.135661e-02 | valid loss (relative): 3.249700e-02 
Epoch 693 use: 387.11 second.

epoch 694 starting......
Epoch:  694 | train loss: 1.595756e-03 | valid loss: 1.633337e-03 
      	| train loss (relative): 3.134757e-02 | valid loss (relative): 3.212725e-02 
Epoch 694 use: 416.76 second.

epoch 695 starting......
Epoch:  695 | train loss: 1.602348e-03 | valid loss: 1.634417e-03 
      	| train loss (relative): 3.147740e-02 | valid loss (relative): 3.199790e-02 
Epoch 695 use: 464.62 second.

epoch 696 starting......
Epoch:  696 | train loss: 1.592124e-03 | valid loss: 1.624103e-03 
      	| train loss (relative): 3.127063e-02 | valid loss (relative): 3.201330e-02 
Epoch 696 use: 488.74 second.

epoch 697 starting......
Epoch:  697 | train loss: 1.590234e-03 | valid loss: 1.622167e-03 
      	| train loss (relative): 3.123879e-02 | valid loss (relative): 3.188552e-02 
Epoch 697 use: 666.46 second.

epoch 698 starting......
Epoch:  698 | train loss: 1.593099e-03 | valid loss: 1.637765e-03 
      	| train loss (relative): 3.129647e-02 | valid loss (relative): 3.218043e-02 
Epoch 698 use: 747.07 second.

epoch 699 starting......
Epoch:  699 | train loss: 1.593611e-03 | valid loss: 1.630992e-03 
      	| train loss (relative): 3.130449e-02 | valid loss (relative): 3.197574e-02 
Epoch 699 use: 684.43 second.

epoch 700 starting......
Epoch:  700 | train loss: 1.590833e-03 | valid loss: 1.629461e-03 
      	| train loss (relative): 3.125183e-02 | valid loss (relative): 3.193061e-02 
Epoch 700 use: 656.07 second.

epoch 701 starting......
Epoch:  701 | train loss: 1.591590e-03 | valid loss: 1.626505e-03 
      	| train loss (relative): 3.126134e-02 | valid loss (relative): 3.196384e-02 
Epoch 701 use: 712.88 second.

epoch 702 starting......
Epoch:  702 | train loss: 1.592281e-03 | valid loss: 1.627054e-03 
      	| train loss (relative): 3.128070e-02 | valid loss (relative): 3.194146e-02 
Epoch 702 use: 655.41 second.

epoch 703 starting......
Epoch:  703 | train loss: 1.594010e-03 | valid loss: 1.634054e-03 
      	| train loss (relative): 3.131300e-02 | valid loss (relative): 3.199851e-02 
Epoch 703 use: 684.24 second.

epoch 704 starting......
Epoch:  704 | train loss: 1.592956e-03 | valid loss: 1.622300e-03 
      	| train loss (relative): 3.128842e-02 | valid loss (relative): 3.188312e-02 
Epoch 704 use: 691.18 second.

epoch 705 starting......
Epoch:  705 | train loss: 1.586430e-03 | valid loss: 1.642035e-03 
      	| train loss (relative): 3.116082e-02 | valid loss (relative): 3.191298e-02 
Epoch 705 use: 527.82 second.

epoch 706 starting......
Epoch:  706 | train loss: 1.595198e-03 | valid loss: 1.614679e-03 
      	| train loss (relative): 3.132975e-02 | valid loss (relative): 3.170238e-02 
Epoch 706 use: 483.29 second.

epoch 707 starting......
Epoch:  707 | train loss: 1.579040e-03 | valid loss: 1.615359e-03 
      	| train loss (relative): 3.101456e-02 | valid loss (relative): 3.176631e-02 
Epoch 707 use: 468.07 second.

epoch 708 starting......
Epoch:  708 | train loss: 1.578779e-03 | valid loss: 1.615599e-03 
      	| train loss (relative): 3.100957e-02 | valid loss (relative): 3.170276e-02 
Epoch 708 use: 520.46 second.

epoch 709 starting......
Epoch:  709 | train loss: 1.580676e-03 | valid loss: 1.621855e-03 
      	| train loss (relative): 3.104881e-02 | valid loss (relative): 3.191372e-02 
Epoch 709 use: 757.67 second.

epoch 710 starting......
Epoch:  710 | train loss: 1.585273e-03 | valid loss: 1.626840e-03 
      	| train loss (relative): 3.113386e-02 | valid loss (relative): 3.184467e-02 
Epoch 710 use: 931.62 second.

epoch 711 starting......
Epoch:  711 | train loss: 1.585335e-03 | valid loss: 1.621175e-03 
      	| train loss (relative): 3.113737e-02 | valid loss (relative): 3.188991e-02 
Epoch 711 use: 849.47 second.

epoch 712 starting......
Epoch:  712 | train loss: 1.588234e-03 | valid loss: 1.624225e-03 
      	| train loss (relative): 3.119937e-02 | valid loss (relative): 3.185911e-02 
Epoch 712 use: 887.71 second.

epoch 713 starting......
Epoch:  713 | train loss: 1.587572e-03 | valid loss: 1.618234e-03 
      	| train loss (relative): 3.118033e-02 | valid loss (relative): 3.175141e-02 
Epoch 713 use: 874.57 second.

epoch 714 starting......
Epoch:  714 | train loss: 1.580892e-03 | valid loss: 1.619160e-03 
      	| train loss (relative): 3.104804e-02 | valid loss (relative): 3.186146e-02 
Epoch 714 use: 879.18 second.

epoch 715 starting......
Epoch:  715 | train loss: 1.579371e-03 | valid loss: 1.612784e-03 
      	| train loss (relative): 3.101659e-02 | valid loss (relative): 3.166883e-02 
Epoch 715 use: 827.86 second.

epoch 716 starting......
Epoch:  716 | train loss: 1.582489e-03 | valid loss: 1.625705e-03 
      	| train loss (relative): 3.108100e-02 | valid loss (relative): 3.188772e-02 
Epoch 716 use: 685.68 second.

epoch 717 starting......
Epoch:  717 | train loss: 1.585687e-03 | valid loss: 1.625417e-03 
      	| train loss (relative): 3.114235e-02 | valid loss (relative): 3.177839e-02 
Epoch 717 use: 687.65 second.

epoch 718 starting......
Epoch:  718 | train loss: 1.584430e-03 | valid loss: 1.629920e-03 
      	| train loss (relative): 3.111481e-02 | valid loss (relative): 3.202149e-02 
Epoch 718 use: 683.49 second.

epoch 719 starting......
Epoch:  719 | train loss: 1.586345e-03 | valid loss: 1.619286e-03 
      	| train loss (relative): 3.115785e-02 | valid loss (relative): 3.170414e-02 
Epoch 719 use: 679.23 second.

epoch 720 starting......
Epoch:  720 | train loss: 1.579371e-03 | valid loss: 1.617590e-03 
      	| train loss (relative): 3.101627e-02 | valid loss (relative): 3.158058e-02 
Epoch 720 use: 662.19 second.

epoch 721 starting......
Epoch:  721 | train loss: 1.579197e-03 | valid loss: 1.610244e-03 
      	| train loss (relative): 3.100853e-02 | valid loss (relative): 3.164430e-02 
Epoch 721 use: 623.72 second.

epoch 722 starting......
Epoch:  722 | train loss: 1.576986e-03 | valid loss: 1.620409e-03 
      	| train loss (relative): 3.096720e-02 | valid loss (relative): 3.165400e-02 
Epoch 722 use: 636.65 second.

epoch 723 starting......
Epoch:  723 | train loss: 1.577868e-03 | valid loss: 1.618839e-03 
      	| train loss (relative): 3.098510e-02 | valid loss (relative): 3.165242e-02 
Epoch 723 use: 654.33 second.

epoch 724 starting......
Epoch:  724 | train loss: 1.579468e-03 | valid loss: 1.619669e-03 
      	| train loss (relative): 3.101604e-02 | valid loss (relative): 3.180005e-02 
Epoch 724 use: 624.69 second.

epoch 725 starting......
Epoch:  725 | train loss: 1.575840e-03 | valid loss: 1.617077e-03 
      	| train loss (relative): 3.094488e-02 | valid loss (relative): 3.179219e-02 
Epoch 725 use: 621.93 second.

epoch 726 starting......
Epoch:  726 | train loss: 1.573956e-03 | valid loss: 1.611776e-03 
      	| train loss (relative): 3.090359e-02 | valid loss (relative): 3.153614e-02 
Epoch 726 use: 644.56 second.

epoch 727 starting......
Epoch:  727 | train loss: 1.574351e-03 | valid loss: 1.616701e-03 
      	| train loss (relative): 3.091603e-02 | valid loss (relative): 3.163819e-02 
Epoch 727 use: 667.53 second.

epoch 728 starting......
Epoch:  728 | train loss: 1.578482e-03 | valid loss: 1.625285e-03 
      	| train loss (relative): 3.099964e-02 | valid loss (relative): 3.168374e-02 
Epoch 728 use: 623.98 second.

epoch 729 starting......
Epoch:  729 | train loss: 1.578844e-03 | valid loss: 1.616378e-03 
      	| train loss (relative): 3.099627e-02 | valid loss (relative): 3.182122e-02 
Epoch 729 use: 651.25 second.

epoch 730 starting......
Epoch:  730 | train loss: 1.571735e-03 | valid loss: 1.606286e-03 
      	| train loss (relative): 3.086474e-02 | valid loss (relative): 3.142200e-02 
Epoch 730 use: 647.84 second.

epoch 731 starting......
Epoch:  731 | train loss: 1.569032e-03 | valid loss: 1.613853e-03 
      	| train loss (relative): 3.081359e-02 | valid loss (relative): 3.155418e-02 
Epoch 731 use: 634.86 second.

epoch 732 starting......
Epoch:  732 | train loss: 1.570970e-03 | valid loss: 1.610825e-03 
      	| train loss (relative): 3.084658e-02 | valid loss (relative): 3.147864e-02 
Epoch 732 use: 640.80 second.

epoch 733 starting......
Epoch:  733 | train loss: 1.570890e-03 | valid loss: 1.610022e-03 
      	| train loss (relative): 3.084907e-02 | valid loss (relative): 3.151779e-02 
Epoch 733 use: 610.72 second.

epoch 734 starting......
Epoch:  734 | train loss: 1.569305e-03 | valid loss: 1.612497e-03 
      	| train loss (relative): 3.080845e-02 | valid loss (relative): 3.189238e-02 
Epoch 734 use: 661.60 second.

epoch 735 starting......
Epoch:  735 | train loss: 1.572452e-03 | valid loss: 1.610981e-03 
      	| train loss (relative): 3.087620e-02 | valid loss (relative): 3.158769e-02 
Epoch 735 use: 605.47 second.

epoch 736 starting......
Epoch:  736 | train loss: 1.573448e-03 | valid loss: 1.611919e-03 
      	| train loss (relative): 3.089784e-02 | valid loss (relative): 3.146981e-02 
Epoch 736 use: 606.96 second.

epoch 737 starting......
Epoch:  737 | train loss: 1.572079e-03 | valid loss: 1.609749e-03 
      	| train loss (relative): 3.087052e-02 | valid loss (relative): 3.139641e-02 
Epoch 737 use: 606.16 second.

epoch 738 starting......
Epoch:  738 | train loss: 1.570196e-03 | valid loss: 1.610189e-03 
      	| train loss (relative): 3.082597e-02 | valid loss (relative): 3.168062e-02 
Epoch 738 use: 626.40 second.

epoch 739 starting......
Epoch:  739 | train loss: 1.570651e-03 | valid loss: 1.615980e-03 
      	| train loss (relative): 3.083763e-02 | valid loss (relative): 3.166822e-02 
Epoch 739 use: 612.72 second.

epoch 740 starting......
Epoch:  740 | train loss: 1.574196e-03 | valid loss: 1.623150e-03 
      	| train loss (relative): 3.091193e-02 | valid loss (relative): 3.168371e-02 
Epoch 740 use: 584.90 second.

epoch 741 starting......
Epoch:  741 | train loss: 1.572313e-03 | valid loss: 1.610925e-03 
      	| train loss (relative): 3.087266e-02 | valid loss (relative): 3.162709e-02 
Epoch 741 use: 606.73 second.

epoch 742 starting......
Epoch:  742 | train loss: 1.568247e-03 | valid loss: 1.610332e-03 
      	| train loss (relative): 3.079096e-02 | valid loss (relative): 3.160930e-02 
Epoch 742 use: 648.75 second.

epoch 743 starting......
Epoch:  743 | train loss: 1.565798e-03 | valid loss: 1.608127e-03 
      	| train loss (relative): 3.074262e-02 | valid loss (relative): 3.149331e-02 
Epoch 743 use: 600.01 second.

epoch 744 starting......
Epoch:  744 | train loss: 1.569244e-03 | valid loss: 1.610891e-03 
      	| train loss (relative): 3.081156e-02 | valid loss (relative): 3.166891e-02 
Epoch 744 use: 625.84 second.

epoch 745 starting......
Epoch:  745 | train loss: 1.569742e-03 | valid loss: 1.601630e-03 
      	| train loss (relative): 3.081953e-02 | valid loss (relative): 3.147962e-02 
Epoch 745 use: 559.43 second.

epoch 746 starting......
Epoch:  746 | train loss: 1.560402e-03 | valid loss: 1.599486e-03 
      	| train loss (relative): 3.063597e-02 | valid loss (relative): 3.145777e-02 
Epoch 746 use: 627.09 second.

epoch 747 starting......
Epoch:  747 | train loss: 1.562070e-03 | valid loss: 1.603256e-03 
      	| train loss (relative): 3.066867e-02 | valid loss (relative): 3.138921e-02 
Epoch 747 use: 636.48 second.

epoch 748 starting......
Epoch:  748 | train loss: 1.562880e-03 | valid loss: 1.601302e-03 
      	| train loss (relative): 3.068187e-02 | valid loss (relative): 3.123974e-02 
Epoch 748 use: 671.10 second.

epoch 749 starting......
Epoch:  749 | train loss: 1.560901e-03 | valid loss: 1.600008e-03 
      	| train loss (relative): 3.063733e-02 | valid loss (relative): 3.144154e-02 
Epoch 749 use: 614.22 second.

test MSE Error: 1.605823e-03 | relative MSE Error: 3.156560e-02 
 Total time used for training: 21.88 hour.
MESLoss saved to  /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_MSELoss_Latent_64_nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_750.txt
relative MSELoss saved to  /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_reMSELoss_Latent_64_nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_750.txt
model saved to /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_64_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_750.pth
model_dict saved to /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_64_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_750_dict.pth
... Training slugflow data group 3 completed, Run finished Mon 16 Aug 08:07:08 BST 2021 ...
{'data_dir': '/rds/general/user/jy220/home/tensors/', 'mode': 'train', 'data_type': 'tensors', 'vtu_fields': 'Component1::ComponentMassFractionPhase1, phase1::Velocity', 'variational': 'False', 'optimizer': 'Adamax', 'structured': 'False', 'changing_lr': 'False', 'self_concat': '2', 'dimension': '3', 'sfc_nums': '3', 'components': '4', 'nearest_neighbouring': 'True', 'dims_latent': '64', 'activation': 'Tanh', 'tk_file': '/rds/general/user/jy220/home/slugflow_tk.pt', 'tb_file': '/rds/general/user/jy220/home/slugflow_tb.pt', 'sfc_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/sfcs.pt', 'inv_sfc_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/inv_sfcs.pt', 'coords_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/coords.pt', 'cells_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/cells.npy', 'state_load': '/rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_64_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_750_dict.pth', 'batch_size': '16', 'lr': '0.001', 'n_epoches': '150', 'seed': '17', 'visualize': 'False', 'reconstructed_path': 'None', 'reconstruct_start_index': '0', 'reconstruct_end_index': '50', 'save_path': '/rds/general/user/jy220/home/new_results/'}
[[ 2.50825269e-01 -1.67256990e-02 -1.22268730e-03]
 [ 2.38611959e-01 -5.50627777e-03 -5.06451913e-03]
 [ 2.39454874e-01 -1.59563257e-02  3.96778323e-03]
 ...
 [ 9.11361096e+00 -2.21908381e-02  2.22384902e-03]
 [ 9.11847310e+00 -2.84346931e-02 -2.34839590e-04]
 [ 9.11039661e+00 -2.51498573e-02 -8.70661044e-03]]
{'tetra': array([[      0,       1,       2,       3],
       [      4,       5,       6,       7],
       [      8,       9,      10,      11],
       ...,
       [1342744, 1342745, 1342746, 1342747],
       [1342748, 1342749, 1342750, 1342751],
       [1342752, 1342753, 1342754, 1342755]])}
vtu fields:  ['Component1::ComponentMassFractionPhase1', 'phase1::Velocity'] 
 structured  False 
 activation  Tanh() 
 self concat  2 
 sfc_nums  3 

dims_latent  64 
 components  4 
 nearest_neighbouring  True 

visualize  False 
 sample number  1706 

reading sfc nums......
[array([  41684,   41685,   41674, ..., 1235560, 1235564, 1235558]), array([954289, 954290, 954291, ..., 420685, 420687, 420688]), array([  40156,   40155,   40158, ..., 1123478, 1123475, 1123467])]
reading inverse sfc nums......
[array([   8146,    8145,    8144, ..., 1274444, 1274445, 1273166]), array([547280, 547283, 593037, ..., 933933, 933932, 864488]), array([ 66437,  66439, 122512, ..., 459416, 415130, 459418])]
tk:  tensor([0.7581, 0.0418, 0.1840, 0.1464], dtype=torch.float64) 

tb:  tensor([-0.3816, -0.3618, -0.0308,  0.1446], dtype=torch.float64) 

length of train set: 1505 

length of valid set: 100 

length of test set: 101 

torch device num: 4 

Let's use 4 GPUs!
epoch 750 starting......
Epoch:  750 | train loss: 1.784990e-03 | valid loss: 1.599944e-03 
      	| train loss (relative): 3.496090e-02 | valid loss (relative): 3.137692e-02 
Epoch 750 use: 517.38 second.

epoch 751 starting......
Epoch:  751 | train loss: 1.567071e-03 | valid loss: 1.581386e-03 
      	| train loss (relative): 3.077490e-02 | valid loss (relative): 3.104228e-02 
Epoch 751 use: 446.24 second.

epoch 752 starting......
Epoch:  752 | train loss: 1.555240e-03 | valid loss: 1.578627e-03 
      	| train loss (relative): 3.053968e-02 | valid loss (relative): 3.096905e-02 
Epoch 752 use: 430.56 second.

epoch 753 starting......
Epoch:  753 | train loss: 1.551493e-03 | valid loss: 1.578404e-03 
      	| train loss (relative): 3.046760e-02 | valid loss (relative): 3.098963e-02 
Epoch 753 use: 409.08 second.

epoch 754 starting......
Epoch:  754 | train loss: 1.549662e-03 | valid loss: 1.579147e-03 
      	| train loss (relative): 3.042727e-02 | valid loss (relative): 3.093999e-02 
Epoch 754 use: 406.75 second.

epoch 755 starting......
Epoch:  755 | train loss: 1.548413e-03 | valid loss: 1.578599e-03 
      	| train loss (relative): 3.040145e-02 | valid loss (relative): 3.094045e-02 
Epoch 755 use: 411.84 second.

epoch 756 starting......
Epoch:  756 | train loss: 1.548281e-03 | valid loss: 1.579849e-03 
      	| train loss (relative): 3.039625e-02 | valid loss (relative): 3.097983e-02 
Epoch 756 use: 402.62 second.

epoch 757 starting......
Epoch:  757 | train loss: 1.549202e-03 | valid loss: 1.582146e-03 
      	| train loss (relative): 3.041163e-02 | valid loss (relative): 3.106864e-02 
Epoch 757 use: 395.68 second.

epoch 758 starting......
Epoch:  758 | train loss: 1.552565e-03 | valid loss: 1.583539e-03 
      	| train loss (relative): 3.048344e-02 | valid loss (relative): 3.105491e-02 
Epoch 758 use: 406.68 second.

epoch 759 starting......
Epoch:  759 | train loss: 1.552430e-03 | valid loss: 1.582348e-03 
      	| train loss (relative): 3.047860e-02 | valid loss (relative): 3.102944e-02 
Epoch 759 use: 415.60 second.

epoch 760 starting......
Epoch:  760 | train loss: 1.553498e-03 | valid loss: 1.590640e-03 
      	| train loss (relative): 3.049818e-02 | valid loss (relative): 3.119099e-02 
Epoch 760 use: 398.96 second.

epoch 761 starting......
Epoch:  761 | train loss: 1.555528e-03 | valid loss: 1.589668e-03 
      	| train loss (relative): 3.054070e-02 | valid loss (relative): 3.112702e-02 
Epoch 761 use: 404.85 second.

epoch 762 starting......
Epoch:  762 | train loss: 1.555450e-03 | valid loss: 1.594667e-03 
      	| train loss (relative): 3.054035e-02 | valid loss (relative): 3.129493e-02 
Epoch 762 use: 414.36 second.

epoch 763 starting......
Epoch:  763 | train loss: 1.557420e-03 | valid loss: 1.596422e-03 
      	| train loss (relative): 3.057319e-02 | valid loss (relative): 3.127249e-02 
Epoch 763 use: 411.30 second.

epoch 764 starting......
Epoch:  764 | train loss: 1.558206e-03 | valid loss: 1.601091e-03 
      	| train loss (relative): 3.059372e-02 | valid loss (relative): 3.156383e-02 
Epoch 764 use: 425.95 second.

epoch 765 starting......
Epoch:  765 | train loss: 1.559899e-03 | valid loss: 1.596545e-03 
      	| train loss (relative): 3.062924e-02 | valid loss (relative): 3.124179e-02 
Epoch 765 use: 388.35 second.

epoch 766 starting......
Epoch:  766 | train loss: 1.560250e-03 | valid loss: 1.599397e-03 
      	| train loss (relative): 3.063228e-02 | valid loss (relative): 3.129282e-02 
Epoch 766 use: 412.85 second.

epoch 767 starting......
Epoch:  767 | train loss: 1.562963e-03 | valid loss: 1.594586e-03 
      	| train loss (relative): 3.068645e-02 | valid loss (relative): 3.123776e-02 
Epoch 767 use: 416.09 second.

epoch 768 starting......
Epoch:  768 | train loss: 1.557135e-03 | valid loss: 1.598902e-03 
      	| train loss (relative): 3.056958e-02 | valid loss (relative): 3.133561e-02 
Epoch 768 use: 416.95 second.

epoch 769 starting......
Epoch:  769 | train loss: 1.561220e-03 | valid loss: 1.598767e-03 
      	| train loss (relative): 3.065423e-02 | valid loss (relative): 3.145264e-02 
Epoch 769 use: 402.63 second.

epoch 770 starting......
Epoch:  770 | train loss: 1.556840e-03 | valid loss: 1.587822e-03 
      	| train loss (relative): 3.056437e-02 | valid loss (relative): 3.111502e-02 
Epoch 770 use: 408.07 second.

epoch 771 starting......
Epoch:  771 | train loss: 1.554650e-03 | valid loss: 1.596077e-03 
      	| train loss (relative): 3.052028e-02 | valid loss (relative): 3.125019e-02 
Epoch 771 use: 391.34 second.

epoch 772 starting......
Epoch:  772 | train loss: 1.562837e-03 | valid loss: 1.603684e-03 
      	| train loss (relative): 3.068074e-02 | valid loss (relative): 3.145593e-02 
Epoch 772 use: 398.31 second.

epoch 773 starting......
Epoch:  773 | train loss: 1.557991e-03 | valid loss: 1.603235e-03 
      	| train loss (relative): 3.058903e-02 | valid loss (relative): 3.135331e-02 
Epoch 773 use: 387.50 second.

epoch 774 starting......
Epoch:  774 | train loss: 1.552936e-03 | valid loss: 1.591563e-03 
      	| train loss (relative): 3.048579e-02 | valid loss (relative): 3.116433e-02 
Epoch 774 use: 401.49 second.

epoch 775 starting......
Epoch:  775 | train loss: 1.551395e-03 | valid loss: 1.592342e-03 
      	| train loss (relative): 3.045474e-02 | valid loss (relative): 3.133645e-02 
Epoch 775 use: 391.87 second.

epoch 776 starting......
Epoch:  776 | train loss: 1.551645e-03 | valid loss: 1.589575e-03 
      	| train loss (relative): 3.045837e-02 | valid loss (relative): 3.124556e-02 
Epoch 776 use: 395.69 second.

epoch 777 starting......
Epoch:  777 | train loss: 1.549900e-03 | valid loss: 1.595175e-03 
      	| train loss (relative): 3.041879e-02 | valid loss (relative): 3.128232e-02 
Epoch 777 use: 404.64 second.

epoch 778 starting......
Epoch:  778 | train loss: 1.553262e-03 | valid loss: 1.594698e-03 
      	| train loss (relative): 3.048895e-02 | valid loss (relative): 3.128724e-02 
Epoch 778 use: 392.58 second.

epoch 779 starting......
Epoch:  779 | train loss: 1.551196e-03 | valid loss: 1.596551e-03 
      	| train loss (relative): 3.045042e-02 | valid loss (relative): 3.113231e-02 
Epoch 779 use: 402.70 second.

epoch 780 starting......
Epoch:  780 | train loss: 1.552081e-03 | valid loss: 1.597983e-03 
      	| train loss (relative): 3.046417e-02 | valid loss (relative): 3.145127e-02 
Epoch 780 use: 395.35 second.

epoch 781 starting......
Epoch:  781 | train loss: 1.553310e-03 | valid loss: 1.608276e-03 
      	| train loss (relative): 3.049665e-02 | valid loss (relative): 3.174409e-02 
Epoch 781 use: 388.30 second.

epoch 782 starting......
Epoch:  782 | train loss: 1.553251e-03 | valid loss: 1.594800e-03 
      	| train loss (relative): 3.048831e-02 | valid loss (relative): 3.122615e-02 
Epoch 782 use: 382.89 second.

epoch 783 starting......
Epoch:  783 | train loss: 1.549814e-03 | valid loss: 1.597836e-03 
      	| train loss (relative): 3.042226e-02 | valid loss (relative): 3.116294e-02 
Epoch 783 use: 383.89 second.

epoch 784 starting......
Epoch:  784 | train loss: 1.549405e-03 | valid loss: 1.596358e-03 
      	| train loss (relative): 3.041368e-02 | valid loss (relative): 3.119269e-02 
Epoch 784 use: 401.65 second.

epoch 785 starting......
Epoch:  785 | train loss: 1.552146e-03 | valid loss: 1.597582e-03 
      	| train loss (relative): 3.046937e-02 | valid loss (relative): 3.112731e-02 
Epoch 785 use: 379.11 second.

epoch 786 starting......
Epoch:  786 | train loss: 1.550783e-03 | valid loss: 1.595792e-03 
      	| train loss (relative): 3.043349e-02 | valid loss (relative): 3.148546e-02 
Epoch 786 use: 375.36 second.

epoch 787 starting......
Epoch:  787 | train loss: 1.552326e-03 | valid loss: 1.602930e-03 
      	| train loss (relative): 3.047865e-02 | valid loss (relative): 3.134269e-02 
Epoch 787 use: 442.51 second.

epoch 788 starting......
Epoch:  788 | train loss: 1.549822e-03 | valid loss: 1.592889e-03 
      	| train loss (relative): 3.041832e-02 | valid loss (relative): 3.132135e-02 
Epoch 788 use: 471.82 second.

epoch 789 starting......
Epoch:  789 | train loss: 1.550605e-03 | valid loss: 1.592956e-03 
      	| train loss (relative): 3.043888e-02 | valid loss (relative): 3.129666e-02 
Epoch 789 use: 418.61 second.

epoch 790 starting......
Epoch:  790 | train loss: 1.545948e-03 | valid loss: 1.597919e-03 
      	| train loss (relative): 3.034470e-02 | valid loss (relative): 3.150090e-02 
Epoch 790 use: 382.74 second.

epoch 791 starting......
Epoch:  791 | train loss: 1.548209e-03 | valid loss: 1.595756e-03 
      	| train loss (relative): 3.038731e-02 | valid loss (relative): 3.125757e-02 
Epoch 791 use: 450.92 second.

epoch 792 starting......
Epoch:  792 | train loss: 1.550102e-03 | valid loss: 1.607471e-03 
      	| train loss (relative): 3.042385e-02 | valid loss (relative): 3.161759e-02 
Epoch 792 use: 482.34 second.

epoch 793 starting......
Epoch:  793 | train loss: 1.554327e-03 | valid loss: 1.588415e-03 
      	| train loss (relative): 3.051741e-02 | valid loss (relative): 3.121926e-02 
Epoch 793 use: 391.73 second.

epoch 794 starting......
Epoch:  794 | train loss: 1.539620e-03 | valid loss: 1.583660e-03 
      	| train loss (relative): 3.021465e-02 | valid loss (relative): 3.098996e-02 
Epoch 794 use: 386.06 second.

epoch 795 starting......
Epoch:  795 | train loss: 1.536541e-03 | valid loss: 1.580778e-03 
      	| train loss (relative): 3.015592e-02 | valid loss (relative): 3.092044e-02 
Epoch 795 use: 392.79 second.

epoch 796 starting......
Epoch:  796 | train loss: 1.535872e-03 | valid loss: 1.584115e-03 
      	| train loss (relative): 3.014090e-02 | valid loss (relative): 3.113140e-02 
Epoch 796 use: 394.99 second.

epoch 797 starting......
Epoch:  797 | train loss: 1.539112e-03 | valid loss: 1.583445e-03 
      	| train loss (relative): 3.020540e-02 | valid loss (relative): 3.111443e-02 
Epoch 797 use: 397.02 second.

epoch 798 starting......
Epoch:  798 | train loss: 1.540587e-03 | valid loss: 1.588941e-03 
      	| train loss (relative): 3.023428e-02 | valid loss (relative): 3.117645e-02 
Epoch 798 use: 386.43 second.

epoch 799 starting......
Epoch:  799 | train loss: 1.544782e-03 | valid loss: 1.595153e-03 
      	| train loss (relative): 3.032165e-02 | valid loss (relative): 3.143467e-02 
Epoch 799 use: 395.03 second.

epoch 800 starting......
Epoch:  800 | train loss: 1.544972e-03 | valid loss: 1.589732e-03 
      	| train loss (relative): 3.032116e-02 | valid loss (relative): 3.107489e-02 
Epoch 800 use: 394.11 second.

epoch 801 starting......
Epoch:  801 | train loss: 1.541097e-03 | valid loss: 1.582575e-03 
      	| train loss (relative): 3.024204e-02 | valid loss (relative): 3.114488e-02 
Epoch 801 use: 396.31 second.

epoch 802 starting......
Epoch:  802 | train loss: 1.539627e-03 | valid loss: 1.587518e-03 
      	| train loss (relative): 3.021537e-02 | valid loss (relative): 3.122775e-02 
Epoch 802 use: 384.99 second.

epoch 803 starting......
Epoch:  803 | train loss: 1.538967e-03 | valid loss: 1.590044e-03 
      	| train loss (relative): 3.020161e-02 | valid loss (relative): 3.115964e-02 
Epoch 803 use: 369.94 second.

epoch 804 starting......
Epoch:  804 | train loss: 1.542078e-03 | valid loss: 1.591639e-03 
      	| train loss (relative): 3.026653e-02 | valid loss (relative): 3.118601e-02 
Epoch 804 use: 374.40 second.

epoch 805 starting......
Epoch:  805 | train loss: 1.539547e-03 | valid loss: 1.583950e-03 
      	| train loss (relative): 3.021622e-02 | valid loss (relative): 3.118570e-02 
Epoch 805 use: 386.48 second.

epoch 806 starting......
Epoch:  806 | train loss: 1.539480e-03 | valid loss: 1.585933e-03 
      	| train loss (relative): 3.020929e-02 | valid loss (relative): 3.109000e-02 
Epoch 806 use: 392.96 second.

epoch 807 starting......
Epoch:  807 | train loss: 1.537352e-03 | valid loss: 1.588195e-03 
      	| train loss (relative): 3.016493e-02 | valid loss (relative): 3.111235e-02 
Epoch 807 use: 375.28 second.

epoch 808 starting......
Epoch:  808 | train loss: 1.540928e-03 | valid loss: 1.600211e-03 
      	| train loss (relative): 3.024330e-02 | valid loss (relative): 3.122674e-02 
Epoch 808 use: 388.34 second.

epoch 809 starting......
Epoch:  809 | train loss: 1.539925e-03 | valid loss: 1.588069e-03 
      	| train loss (relative): 3.022305e-02 | valid loss (relative): 3.097706e-02 
Epoch 809 use: 398.23 second.

epoch 810 starting......
Epoch:  810 | train loss: 1.535588e-03 | valid loss: 1.592655e-03 
      	| train loss (relative): 3.013417e-02 | valid loss (relative): 3.113013e-02 
Epoch 810 use: 395.89 second.

epoch 811 starting......
Epoch:  811 | train loss: 1.539149e-03 | valid loss: 1.587894e-03 
      	| train loss (relative): 3.020550e-02 | valid loss (relative): 3.111102e-02 
Epoch 811 use: 398.03 second.

epoch 812 starting......
Epoch:  812 | train loss: 1.539505e-03 | valid loss: 1.583511e-03 
      	| train loss (relative): 3.021035e-02 | valid loss (relative): 3.098647e-02 
Epoch 812 use: 391.14 second.

epoch 813 starting......
Epoch:  813 | train loss: 1.536058e-03 | valid loss: 1.583081e-03 
      	| train loss (relative): 3.014209e-02 | valid loss (relative): 3.101840e-02 
Epoch 813 use: 385.97 second.

epoch 814 starting......
Epoch:  814 | train loss: 1.534862e-03 | valid loss: 1.581569e-03 
      	| train loss (relative): 3.011931e-02 | valid loss (relative): 3.098423e-02 
Epoch 814 use: 379.78 second.

epoch 815 starting......
Epoch:  815 | train loss: 1.534882e-03 | valid loss: 1.584824e-03 
      	| train loss (relative): 3.012188e-02 | valid loss (relative): 3.105711e-02 
Epoch 815 use: 478.66 second.

epoch 816 starting......
Epoch:  816 | train loss: 1.539183e-03 | valid loss: 1.580811e-03 
      	| train loss (relative): 3.020248e-02 | valid loss (relative): 3.080592e-02 
Epoch 816 use: 476.98 second.

epoch 817 starting......
Epoch:  817 | train loss: 1.539713e-03 | valid loss: 1.590281e-03 
      	| train loss (relative): 3.021188e-02 | valid loss (relative): 3.117232e-02 
Epoch 817 use: 429.67 second.

epoch 818 starting......
Epoch:  818 | train loss: 1.533957e-03 | valid loss: 1.580531e-03 
      	| train loss (relative): 3.009991e-02 | valid loss (relative): 3.106043e-02 
Epoch 818 use: 401.83 second.

epoch 819 starting......
Epoch:  819 | train loss: 1.529658e-03 | valid loss: 1.577988e-03 
      	| train loss (relative): 3.001432e-02 | valid loss (relative): 3.094135e-02 
Epoch 819 use: 402.73 second.

epoch 820 starting......
Epoch:  820 | train loss: 1.528134e-03 | valid loss: 1.585272e-03 
      	| train loss (relative): 2.998536e-02 | valid loss (relative): 3.086634e-02 
Epoch 820 use: 399.15 second.

epoch 821 starting......
Epoch:  821 | train loss: 1.532236e-03 | valid loss: 1.586916e-03 
      	| train loss (relative): 3.006380e-02 | valid loss (relative): 3.114907e-02 
Epoch 821 use: 403.14 second.

epoch 822 starting......
Epoch:  822 | train loss: 1.529402e-03 | valid loss: 1.577815e-03 
      	| train loss (relative): 3.000838e-02 | valid loss (relative): 3.104663e-02 
Epoch 822 use: 395.67 second.

epoch 823 starting......
Epoch:  823 | train loss: 1.531609e-03 | valid loss: 1.583907e-03 
      	| train loss (relative): 3.005646e-02 | valid loss (relative): 3.103411e-02 
Epoch 823 use: 412.51 second.

epoch 824 starting......
Epoch:  824 | train loss: 1.533852e-03 | valid loss: 1.578395e-03 
      	| train loss (relative): 3.009493e-02 | valid loss (relative): 3.105145e-02 
Epoch 824 use: 417.12 second.

epoch 825 starting......
Epoch:  825 | train loss: 1.529360e-03 | valid loss: 1.577550e-03 
      	| train loss (relative): 3.001177e-02 | valid loss (relative): 3.090508e-02 
Epoch 825 use: 386.90 second.

epoch 826 starting......
Epoch:  826 | train loss: 1.527618e-03 | valid loss: 1.579331e-03 
      	| train loss (relative): 2.996952e-02 | valid loss (relative): 3.094833e-02 
Epoch 826 use: 403.03 second.

epoch 827 starting......
Epoch:  827 | train loss: 1.528651e-03 | valid loss: 1.584640e-03 
      	| train loss (relative): 2.999299e-02 | valid loss (relative): 3.110273e-02 
Epoch 827 use: 398.39 second.

epoch 828 starting......
Epoch:  828 | train loss: 1.532044e-03 | valid loss: 1.590076e-03 
      	| train loss (relative): 3.006215e-02 | valid loss (relative): 3.142668e-02 
Epoch 828 use: 408.48 second.

epoch 829 starting......
Epoch:  829 | train loss: 1.531062e-03 | valid loss: 1.572842e-03 
      	| train loss (relative): 3.004252e-02 | valid loss (relative): 3.086261e-02 
Epoch 829 use: 399.02 second.

epoch 830 starting......
Epoch:  830 | train loss: 1.524555e-03 | valid loss: 1.581130e-03 
      	| train loss (relative): 2.990958e-02 | valid loss (relative): 3.119904e-02 
Epoch 830 use: 406.76 second.

epoch 831 starting......
Epoch:  831 | train loss: 1.530350e-03 | valid loss: 1.574419e-03 
      	| train loss (relative): 3.003032e-02 | valid loss (relative): 3.088169e-02 
Epoch 831 use: 422.18 second.

epoch 832 starting......
Epoch:  832 | train loss: 1.525754e-03 | valid loss: 1.583114e-03 
      	| train loss (relative): 2.993627e-02 | valid loss (relative): 3.095376e-02 
Epoch 832 use: 404.00 second.

epoch 833 starting......
Epoch:  833 | train loss: 1.529107e-03 | valid loss: 1.577433e-03 
      	| train loss (relative): 2.999626e-02 | valid loss (relative): 3.103018e-02 
Epoch 833 use: 405.81 second.

epoch 834 starting......
Epoch:  834 | train loss: 1.525707e-03 | valid loss: 1.579809e-03 
      	| train loss (relative): 2.994067e-02 | valid loss (relative): 3.092231e-02 
Epoch 834 use: 389.19 second.

epoch 835 starting......
Epoch:  835 | train loss: 1.527533e-03 | valid loss: 1.584878e-03 
      	| train loss (relative): 2.996565e-02 | valid loss (relative): 3.091397e-02 
Epoch 835 use: 399.91 second.

epoch 836 starting......
Epoch:  836 | train loss: 1.530261e-03 | valid loss: 1.587107e-03 
      	| train loss (relative): 3.002251e-02 | valid loss (relative): 3.105387e-02 
Epoch 836 use: 398.80 second.

epoch 837 starting......
Epoch:  837 | train loss: 1.530768e-03 | valid loss: 1.580895e-03 
      	| train loss (relative): 3.003925e-02 | valid loss (relative): 3.087568e-02 
Epoch 837 use: 394.57 second.

epoch 838 starting......
Epoch:  838 | train loss: 1.529474e-03 | valid loss: 1.574059e-03 
      	| train loss (relative): 3.001049e-02 | valid loss (relative): 3.092995e-02 
Epoch 838 use: 390.40 second.

epoch 839 starting......
Epoch:  839 | train loss: 1.527982e-03 | valid loss: 1.575479e-03 
      	| train loss (relative): 2.997617e-02 | valid loss (relative): 3.087972e-02 
Epoch 839 use: 403.96 second.

epoch 840 starting......
Epoch:  840 | train loss: 1.526411e-03 | valid loss: 1.577299e-03 
      	| train loss (relative): 2.994862e-02 | valid loss (relative): 3.099172e-02 
Epoch 840 use: 395.52 second.

epoch 841 starting......
Epoch:  841 | train loss: 1.528375e-03 | valid loss: 1.580863e-03 
      	| train loss (relative): 2.998651e-02 | valid loss (relative): 3.073567e-02 
Epoch 841 use: 404.89 second.

epoch 842 starting......
Epoch:  842 | train loss: 1.525456e-03 | valid loss: 1.576590e-03 
      	| train loss (relative): 2.992400e-02 | valid loss (relative): 3.083281e-02 
Epoch 842 use: 388.47 second.

epoch 843 starting......
Epoch:  843 | train loss: 1.529028e-03 | valid loss: 1.584260e-03 
      	| train loss (relative): 2.999804e-02 | valid loss (relative): 3.109415e-02 
Epoch 843 use: 404.36 second.

epoch 844 starting......
Epoch:  844 | train loss: 1.528376e-03 | valid loss: 1.572469e-03 
      	| train loss (relative): 2.998743e-02 | valid loss (relative): 3.078880e-02 
Epoch 844 use: 416.33 second.

epoch 845 starting......
Epoch:  845 | train loss: 1.523809e-03 | valid loss: 1.583508e-03 
      	| train loss (relative): 2.989301e-02 | valid loss (relative): 3.117986e-02 
Epoch 845 use: 388.54 second.

epoch 846 starting......
Epoch:  846 | train loss: 1.530925e-03 | valid loss: 1.587289e-03 
      	| train loss (relative): 3.003677e-02 | valid loss (relative): 3.131479e-02 
Epoch 846 use: 399.17 second.

epoch 847 starting......
Epoch:  847 | train loss: 1.526680e-03 | valid loss: 1.574222e-03 
      	| train loss (relative): 2.995691e-02 | valid loss (relative): 3.088568e-02 
Epoch 847 use: 407.94 second.

epoch 848 starting......
Epoch:  848 | train loss: 1.518768e-03 | valid loss: 1.566198e-03 
      	| train loss (relative): 2.979248e-02 | valid loss (relative): 3.073069e-02 
Epoch 848 use: 402.22 second.

epoch 849 starting......
Epoch:  849 | train loss: 1.518887e-03 | valid loss: 1.576834e-03 
      	| train loss (relative): 2.979718e-02 | valid loss (relative): 3.083606e-02 
Epoch 849 use: 389.35 second.

epoch 850 starting......
Epoch:  850 | train loss: 1.525134e-03 | valid loss: 1.569021e-03 
      	| train loss (relative): 2.991894e-02 | valid loss (relative): 3.075173e-02 
Epoch 850 use: 408.29 second.

epoch 851 starting......
Epoch:  851 | train loss: 1.516585e-03 | valid loss: 1.567577e-03 
      	| train loss (relative): 2.975071e-02 | valid loss (relative): 3.073249e-02 
Epoch 851 use: 403.21 second.

epoch 852 starting......
Epoch:  852 | train loss: 1.518413e-03 | valid loss: 1.567708e-03 
      	| train loss (relative): 2.978398e-02 | valid loss (relative): 3.062409e-02 
Epoch 852 use: 388.79 second.

epoch 853 starting......
Epoch:  853 | train loss: 1.518534e-03 | valid loss: 1.573787e-03 
      	| train loss (relative): 2.978402e-02 | valid loss (relative): 3.097031e-02 
Epoch 853 use: 397.21 second.

epoch 854 starting......
Epoch:  854 | train loss: 1.520114e-03 | valid loss: 1.574085e-03 
      	| train loss (relative): 2.982546e-02 | valid loss (relative): 3.067913e-02 
Epoch 854 use: 419.30 second.

epoch 855 starting......
Epoch:  855 | train loss: 1.522782e-03 | valid loss: 1.571615e-03 
      	| train loss (relative): 2.987259e-02 | valid loss (relative): 3.065464e-02 
Epoch 855 use: 412.19 second.

epoch 856 starting......
Epoch:  856 | train loss: 1.522110e-03 | valid loss: 1.584796e-03 
      	| train loss (relative): 2.986006e-02 | valid loss (relative): 3.117635e-02 
Epoch 856 use: 414.39 second.

epoch 857 starting......
Epoch:  857 | train loss: 1.523794e-03 | valid loss: 1.564967e-03 
      	| train loss (relative): 2.989292e-02 | valid loss (relative): 3.053712e-02 
Epoch 857 use: 396.86 second.

epoch 858 starting......
Epoch:  858 | train loss: 1.512889e-03 | valid loss: 1.565618e-03 
      	| train loss (relative): 2.967602e-02 | valid loss (relative): 3.058989e-02 
Epoch 858 use: 402.66 second.

epoch 859 starting......
Epoch:  859 | train loss: 1.519330e-03 | valid loss: 1.570063e-03 
      	| train loss (relative): 2.980241e-02 | valid loss (relative): 3.075198e-02 
Epoch 859 use: 400.17 second.

epoch 860 starting......
Epoch:  860 | train loss: 1.515799e-03 | valid loss: 1.565993e-03 
      	| train loss (relative): 2.973312e-02 | valid loss (relative): 3.066205e-02 
Epoch 860 use: 403.13 second.

epoch 861 starting......
Epoch:  861 | train loss: 1.515294e-03 | valid loss: 1.567508e-03 
      	| train loss (relative): 2.972887e-02 | valid loss (relative): 3.075774e-02 
Epoch 861 use: 425.25 second.

epoch 862 starting......
Epoch:  862 | train loss: 1.514106e-03 | valid loss: 1.573879e-03 
      	| train loss (relative): 2.969820e-02 | valid loss (relative): 3.057378e-02 
Epoch 862 use: 411.85 second.

epoch 863 starting......
Epoch:  863 | train loss: 1.516556e-03 | valid loss: 1.567497e-03 
      	| train loss (relative): 2.975003e-02 | valid loss (relative): 3.053336e-02 
Epoch 863 use: 391.61 second.

epoch 864 starting......
Epoch:  864 | train loss: 1.516797e-03 | valid loss: 1.566323e-03 
      	| train loss (relative): 2.975280e-02 | valid loss (relative): 3.068565e-02 
Epoch 864 use: 395.75 second.

epoch 865 starting......
Epoch:  865 | train loss: 1.514870e-03 | valid loss: 1.572688e-03 
      	| train loss (relative): 2.971582e-02 | valid loss (relative): 3.062433e-02 
Epoch 865 use: 413.20 second.

epoch 866 starting......
Epoch:  866 | train loss: 1.513556e-03 | valid loss: 1.569521e-03 
      	| train loss (relative): 2.968693e-02 | valid loss (relative): 3.068672e-02 
Epoch 866 use: 395.47 second.

epoch 867 starting......
Epoch:  867 | train loss: 1.515142e-03 | valid loss: 1.571761e-03 
      	| train loss (relative): 2.971894e-02 | valid loss (relative): 3.072835e-02 
Epoch 867 use: 394.77 second.

epoch 868 starting......
Epoch:  868 | train loss: 1.516054e-03 | valid loss: 1.569512e-03 
      	| train loss (relative): 2.973753e-02 | valid loss (relative): 3.076228e-02 
Epoch 868 use: 394.68 second.

epoch 869 starting......
Epoch:  869 | train loss: 1.515647e-03 | valid loss: 1.565826e-03 
      	| train loss (relative): 2.972941e-02 | valid loss (relative): 3.073701e-02 
Epoch 869 use: 422.25 second.

epoch 870 starting......
Epoch:  870 | train loss: 1.511548e-03 | valid loss: 1.568261e-03 
      	| train loss (relative): 2.965005e-02 | valid loss (relative): 3.082211e-02 
Epoch 870 use: 395.32 second.

epoch 871 starting......
Epoch:  871 | train loss: 1.516996e-03 | valid loss: 1.571818e-03 
      	| train loss (relative): 2.975715e-02 | valid loss (relative): 3.095517e-02 
Epoch 871 use: 388.93 second.

epoch 872 starting......
Epoch:  872 | train loss: 1.515880e-03 | valid loss: 1.565605e-03 
      	| train loss (relative): 2.973781e-02 | valid loss (relative): 3.057903e-02 
Epoch 872 use: 381.47 second.

epoch 873 starting......
Epoch:  873 | train loss: 1.508054e-03 | valid loss: 1.566219e-03 
      	| train loss (relative): 2.957795e-02 | valid loss (relative): 3.071154e-02 
Epoch 873 use: 414.48 second.

epoch 874 starting......
Epoch:  874 | train loss: 1.510815e-03 | valid loss: 1.572181e-03 
      	| train loss (relative): 2.963652e-02 | valid loss (relative): 3.089045e-02 
Epoch 874 use: 404.82 second.

epoch 875 starting......
Epoch:  875 | train loss: 1.510919e-03 | valid loss: 1.565254e-03 
      	| train loss (relative): 2.963456e-02 | valid loss (relative): 3.062103e-02 
Epoch 875 use: 393.24 second.

epoch 876 starting......
Epoch:  876 | train loss: 1.510626e-03 | valid loss: 1.566793e-03 
      	| train loss (relative): 2.962839e-02 | valid loss (relative): 3.088032e-02 
Epoch 876 use: 398.27 second.

epoch 877 starting......
Epoch:  877 | train loss: 1.508671e-03 | valid loss: 1.564316e-03 
      	| train loss (relative): 2.958994e-02 | valid loss (relative): 3.065808e-02 
Epoch 877 use: 410.86 second.

epoch 878 starting......
Epoch:  878 | train loss: 1.510841e-03 | valid loss: 1.569137e-03 
      	| train loss (relative): 2.963334e-02 | valid loss (relative): 3.061799e-02 
Epoch 878 use: 409.65 second.

epoch 879 starting......
Epoch:  879 | train loss: 1.510066e-03 | valid loss: 1.560868e-03 
      	| train loss (relative): 2.961707e-02 | valid loss (relative): 3.059255e-02 
Epoch 879 use: 390.52 second.

epoch 880 starting......
Epoch:  880 | train loss: 1.507952e-03 | valid loss: 1.554893e-03 
      	| train loss (relative): 2.957877e-02 | valid loss (relative): 3.041486e-02 
Epoch 880 use: 396.51 second.

epoch 881 starting......
Epoch:  881 | train loss: 1.505586e-03 | valid loss: 1.557279e-03 
      	| train loss (relative): 2.952650e-02 | valid loss (relative): 3.054513e-02 
Epoch 881 use: 384.50 second.

epoch 882 starting......
Epoch:  882 | train loss: 1.503672e-03 | valid loss: 1.556740e-03 
      	| train loss (relative): 2.948431e-02 | valid loss (relative): 3.056516e-02 
Epoch 882 use: 394.17 second.

epoch 883 starting......
Epoch:  883 | train loss: 1.507373e-03 | valid loss: 1.562705e-03 
      	| train loss (relative): 2.956457e-02 | valid loss (relative): 3.054812e-02 
Epoch 883 use: 408.10 second.

epoch 884 starting......
Epoch:  884 | train loss: 1.507171e-03 | valid loss: 1.563366e-03 
      	| train loss (relative): 2.955638e-02 | valid loss (relative): 3.068451e-02 
Epoch 884 use: 380.23 second.

epoch 885 starting......
Epoch:  885 | train loss: 1.510497e-03 | valid loss: 1.558609e-03 
      	| train loss (relative): 2.962820e-02 | valid loss (relative): 3.049552e-02 
Epoch 885 use: 392.01 second.

epoch 886 starting......
Epoch:  886 | train loss: 1.503684e-03 | valid loss: 1.557916e-03 
      	| train loss (relative): 2.949278e-02 | valid loss (relative): 3.051167e-02 
Epoch 886 use: 440.34 second.

epoch 887 starting......
Epoch:  887 | train loss: 1.503760e-03 | valid loss: 1.553531e-03 
      	| train loss (relative): 2.949051e-02 | valid loss (relative): 3.051278e-02 
Epoch 887 use: 472.18 second.

epoch 888 starting......
Epoch:  888 | train loss: 1.503602e-03 | valid loss: 1.560031e-03 
      	| train loss (relative): 2.949011e-02 | valid loss (relative): 3.047596e-02 
Epoch 888 use: 397.69 second.

epoch 889 starting......
Epoch:  889 | train loss: 1.505577e-03 | valid loss: 1.554854e-03 
      	| train loss (relative): 2.952847e-02 | valid loss (relative): 3.042775e-02 
Epoch 889 use: 386.13 second.

epoch 890 starting......
Epoch:  890 | train loss: 1.500899e-03 | valid loss: 1.557763e-03 
      	| train loss (relative): 2.943503e-02 | valid loss (relative): 3.042679e-02 
Epoch 890 use: 407.85 second.

epoch 891 starting......
Epoch:  891 | train loss: 1.503320e-03 | valid loss: 1.557246e-03 
      	| train loss (relative): 2.947936e-02 | valid loss (relative): 3.051430e-02 
Epoch 891 use: 413.12 second.

epoch 892 starting......
Epoch:  892 | train loss: 1.503249e-03 | valid loss: 1.555811e-03 
      	| train loss (relative): 2.947585e-02 | valid loss (relative): 3.058373e-02 
Epoch 892 use: 399.97 second.

epoch 893 starting......
Epoch:  893 | train loss: 1.504946e-03 | valid loss: 1.558648e-03 
      	| train loss (relative): 2.951334e-02 | valid loss (relative): 3.059127e-02 
Epoch 893 use: 393.64 second.

epoch 894 starting......
Epoch:  894 | train loss: 1.502009e-03 | valid loss: 1.552019e-03 
      	| train loss (relative): 2.945441e-02 | valid loss (relative): 3.053404e-02 
Epoch 894 use: 410.56 second.

epoch 895 starting......
Epoch:  895 | train loss: 1.498930e-03 | valid loss: 1.554841e-03 
      	| train loss (relative): 2.939469e-02 | valid loss (relative): 3.053752e-02 
Epoch 895 use: 407.91 second.

epoch 896 starting......
Epoch:  896 | train loss: 1.504259e-03 | valid loss: 1.554016e-03 
      	| train loss (relative): 2.949786e-02 | valid loss (relative): 3.052324e-02 
Epoch 896 use: 405.16 second.

epoch 897 starting......
Epoch:  897 | train loss: 1.502280e-03 | valid loss: 1.557386e-03 
      	| train loss (relative): 2.945926e-02 | valid loss (relative): 3.056629e-02 
Epoch 897 use: 402.46 second.

epoch 898 starting......
Epoch:  898 | train loss: 1.504892e-03 | valid loss: 1.554330e-03 
      	| train loss (relative): 2.950708e-02 | valid loss (relative): 3.054825e-02 
Epoch 898 use: 405.08 second.

epoch 899 starting......
Epoch:  899 | train loss: 1.500889e-03 | valid loss: 1.563657e-03 
      	| train loss (relative): 2.943605e-02 | valid loss (relative): 3.047531e-02 
Epoch 899 use: 417.86 second.

test MSE Error: 1.489951e-03 | relative MSE Error: 2.891807e-02 
 Total time used for training: 16.85 hour.
MESLoss saved to  /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_MSELoss_Latent_64_nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_900.txt
relative MSELoss saved to  /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_reMSELoss_Latent_64_nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_900.txt
model saved to /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_64_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_900.pth
model_dict saved to /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_64_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_900_dict.pth
... Training slugflow data group 3 completed, Run finished Thu 19 Aug 08:05:27 BST 2021 ...
{'data_dir': '/rds/general/user/jy220/home/tensors/', 'mode': 'train', 'data_type': 'tensors', 'vtu_fields': 'Component1::ComponentMassFractionPhase1, phase1::Velocity', 'variational': 'False', 'optimizer': 'Adamax', 'structured': 'False', 'changing_lr': 'False', 'self_concat': '2', 'dimension': '3', 'sfc_nums': '3', 'components': '4', 'nearest_neighbouring': 'True', 'dims_latent': '64', 'activation': 'Tanh', 'tk_file': '/rds/general/user/jy220/home/slugflow_tk.pt', 'tb_file': '/rds/general/user/jy220/home/slugflow_tb.pt', 'sfc_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/sfcs.pt', 'inv_sfc_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/inv_sfcs.pt', 'coords_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/coords.pt', 'cells_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/cells.npy', 'state_load': '/rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_64_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_900_dict.pth', 'batch_size': '16', 'lr': '0.001', 'n_epoches': '150', 'seed': '17', 'visualize': 'False', 'reconstructed_path': 'None', 'reconstruct_start_index': '0', 'reconstruct_end_index': '50', 'save_path': '/rds/general/user/jy220/home/new_results/'}
[[ 2.50825269e-01 -1.67256990e-02 -1.22268730e-03]
 [ 2.38611959e-01 -5.50627777e-03 -5.06451913e-03]
 [ 2.39454874e-01 -1.59563257e-02  3.96778323e-03]
 ...
 [ 9.11361096e+00 -2.21908381e-02  2.22384902e-03]
 [ 9.11847310e+00 -2.84346931e-02 -2.34839590e-04]
 [ 9.11039661e+00 -2.51498573e-02 -8.70661044e-03]]
{'tetra': array([[      0,       1,       2,       3],
       [      4,       5,       6,       7],
       [      8,       9,      10,      11],
       ...,
       [1342744, 1342745, 1342746, 1342747],
       [1342748, 1342749, 1342750, 1342751],
       [1342752, 1342753, 1342754, 1342755]])}
vtu fields:  ['Component1::ComponentMassFractionPhase1', 'phase1::Velocity'] 
 structured  False 
 activation  Tanh() 
 self concat  2 
 sfc_nums  3 

dims_latent  64 
 components  4 
 nearest_neighbouring  True 

visualize  False 
 sample number  1706 

reading sfc nums......
[array([  41684,   41685,   41674, ..., 1235560, 1235564, 1235558]), array([954289, 954290, 954291, ..., 420685, 420687, 420688]), array([  40156,   40155,   40158, ..., 1123478, 1123475, 1123467])]
reading inverse sfc nums......
[array([   8146,    8145,    8144, ..., 1274444, 1274445, 1273166]), array([547280, 547283, 593037, ..., 933933, 933932, 864488]), array([ 66437,  66439, 122512, ..., 459416, 415130, 459418])]
tk:  tensor([0.7581, 0.0418, 0.1840, 0.1464], dtype=torch.float64) 

tb:  tensor([-0.3816, -0.3618, -0.0308,  0.1446], dtype=torch.float64) 

length of train set: 1505 

length of valid set: 100 

length of test set: 101 

torch device num: 4 

Let's use 4 GPUs!
epoch 900 starting......
Epoch:  900 | train loss: 1.682236e-03 | valid loss: 1.526563e-03 
      	| train loss (relative): 3.302638e-02 | valid loss (relative): 2.991104e-02 
Epoch 900 use: 590.17 second.

epoch 901 starting......
Epoch:  901 | train loss: 1.494913e-03 | valid loss: 1.512435e-03 
      	| train loss (relative): 2.931445e-02 | valid loss (relative): 2.961043e-02 
Epoch 901 use: 543.96 second.

epoch 902 starting......
Epoch:  902 | train loss: 1.484297e-03 | valid loss: 1.510286e-03 
      	| train loss (relative): 2.910446e-02 | valid loss (relative): 2.955971e-02 
Epoch 902 use: 505.17 second.

epoch 903 starting......
Epoch:  903 | train loss: 1.481291e-03 | valid loss: 1.510378e-03 
      	| train loss (relative): 2.903938e-02 | valid loss (relative): 2.951935e-02 
Epoch 903 use: 536.26 second.

epoch 904 starting......
Epoch:  904 | train loss: 1.480685e-03 | valid loss: 1.509981e-03 
      	| train loss (relative): 2.902753e-02 | valid loss (relative): 2.955665e-02 
Epoch 904 use: 515.65 second.

epoch 905 starting......
Epoch:  905 | train loss: 1.480848e-03 | valid loss: 1.512696e-03 
      	| train loss (relative): 2.903191e-02 | valid loss (relative): 2.962132e-02 
Epoch 905 use: 509.31 second.

epoch 906 starting......
Epoch:  906 | train loss: 1.480870e-03 | valid loss: 1.513868e-03 
      	| train loss (relative): 2.902991e-02 | valid loss (relative): 2.961665e-02 
Epoch 906 use: 503.81 second.

epoch 907 starting......
Epoch:  907 | train loss: 1.482215e-03 | valid loss: 1.517443e-03 
      	| train loss (relative): 2.905315e-02 | valid loss (relative): 2.975445e-02 
Epoch 907 use: 489.56 second.

epoch 908 starting......
Epoch:  908 | train loss: 1.484004e-03 | valid loss: 1.520097e-03 
      	| train loss (relative): 2.909142e-02 | valid loss (relative): 2.976751e-02 
Epoch 908 use: 503.85 second.

epoch 909 starting......
Epoch:  909 | train loss: 1.485981e-03 | valid loss: 1.522674e-03 
      	| train loss (relative): 2.912921e-02 | valid loss (relative): 2.977555e-02 
Epoch 909 use: 486.42 second.

epoch 910 starting......
Epoch:  910 | train loss: 1.489880e-03 | valid loss: 1.527909e-03 
      	| train loss (relative): 2.920613e-02 | valid loss (relative): 2.993840e-02 
Epoch 910 use: 512.10 second.

epoch 911 starting......
Epoch:  911 | train loss: 1.491270e-03 | valid loss: 1.527549e-03 
      	| train loss (relative): 2.923459e-02 | valid loss (relative): 2.992765e-02 
Epoch 911 use: 509.51 second.

epoch 912 starting......
Epoch:  912 | train loss: 1.487744e-03 | valid loss: 1.530554e-03 
      	| train loss (relative): 2.916459e-02 | valid loss (relative): 3.000291e-02 
Epoch 912 use: 501.37 second.

epoch 913 starting......
Epoch:  913 | train loss: 1.491148e-03 | valid loss: 1.525913e-03 
      	| train loss (relative): 2.923466e-02 | valid loss (relative): 2.986172e-02 
Epoch 913 use: 523.16 second.

epoch 914 starting......
Epoch:  914 | train loss: 1.487483e-03 | valid loss: 1.526798e-03 
      	| train loss (relative): 2.915921e-02 | valid loss (relative): 2.978478e-02 
Epoch 914 use: 523.76 second.

epoch 915 starting......
Epoch:  915 | train loss: 1.487847e-03 | valid loss: 1.530290e-03 
      	| train loss (relative): 2.916294e-02 | valid loss (relative): 3.003127e-02 
Epoch 915 use: 518.35 second.

epoch 916 starting......
Epoch:  916 | train loss: 1.490271e-03 | valid loss: 1.530323e-03 
      	| train loss (relative): 2.921188e-02 | valid loss (relative): 3.000838e-02 
Epoch 916 use: 509.12 second.

epoch 917 starting......
Epoch:  917 | train loss: 1.489469e-03 | valid loss: 1.532733e-03 
      	| train loss (relative): 2.919890e-02 | valid loss (relative): 3.008232e-02 
Epoch 917 use: 507.98 second.

epoch 918 starting......
Epoch:  918 | train loss: 1.491934e-03 | valid loss: 1.531343e-03 
      	| train loss (relative): 2.924671e-02 | valid loss (relative): 3.003142e-02 
Epoch 918 use: 504.73 second.

epoch 919 starting......
Epoch:  919 | train loss: 1.487846e-03 | valid loss: 1.534288e-03 
      	| train loss (relative): 2.916618e-02 | valid loss (relative): 3.004270e-02 
Epoch 919 use: 484.91 second.

epoch 920 starting......
Epoch:  920 | train loss: 1.487930e-03 | valid loss: 1.531134e-03 
      	| train loss (relative): 2.916643e-02 | valid loss (relative): 2.985166e-02 
Epoch 920 use: 501.99 second.

epoch 921 starting......
Epoch:  921 | train loss: 1.488755e-03 | valid loss: 1.534156e-03 
      	| train loss (relative): 2.918191e-02 | valid loss (relative): 3.004759e-02 
Epoch 921 use: 492.01 second.

epoch 922 starting......
Epoch:  922 | train loss: 1.487783e-03 | valid loss: 1.528756e-03 
      	| train loss (relative): 2.916323e-02 | valid loss (relative): 2.994770e-02 
Epoch 922 use: 485.31 second.

epoch 923 starting......
Epoch:  923 | train loss: 1.486660e-03 | valid loss: 1.530456e-03 
      	| train loss (relative): 2.913947e-02 | valid loss (relative): 2.999654e-02 
Epoch 923 use: 486.58 second.

epoch 924 starting......
Epoch:  924 | train loss: 1.488548e-03 | valid loss: 1.528595e-03 
      	| train loss (relative): 2.917777e-02 | valid loss (relative): 2.997388e-02 
Epoch 924 use: 512.93 second.

epoch 925 starting......
Epoch:  925 | train loss: 1.486636e-03 | valid loss: 1.536362e-03 
      	| train loss (relative): 2.913917e-02 | valid loss (relative): 3.025649e-02 
Epoch 925 use: 479.46 second.

epoch 926 starting......
Epoch:  926 | train loss: 1.488166e-03 | valid loss: 1.533470e-03 
      	| train loss (relative): 2.916875e-02 | valid loss (relative): 3.009328e-02 
Epoch 926 use: 500.35 second.

epoch 927 starting......
Epoch:  927 | train loss: 1.490006e-03 | valid loss: 1.532021e-03 
      	| train loss (relative): 2.920353e-02 | valid loss (relative): 2.989422e-02 
Epoch 927 use: 471.95 second.

epoch 928 starting......
Epoch:  928 | train loss: 1.486280e-03 | valid loss: 1.532346e-03 
      	| train loss (relative): 2.913480e-02 | valid loss (relative): 2.997179e-02 
Epoch 928 use: 479.99 second.

epoch 929 starting......
Epoch:  929 | train loss: 1.488769e-03 | valid loss: 1.528837e-03 
      	| train loss (relative): 2.918123e-02 | valid loss (relative): 2.997204e-02 
Epoch 929 use: 492.81 second.

epoch 930 starting......
Epoch:  930 | train loss: 1.483588e-03 | valid loss: 1.527454e-03 
      	| train loss (relative): 2.908136e-02 | valid loss (relative): 2.997233e-02 
Epoch 930 use: 499.50 second.

epoch 931 starting......
Epoch:  931 | train loss: 1.486229e-03 | valid loss: 1.530977e-03 
      	| train loss (relative): 2.913472e-02 | valid loss (relative): 3.005598e-02 
Epoch 931 use: 519.97 second.

epoch 932 starting......
Epoch:  932 | train loss: 1.484443e-03 | valid loss: 1.529519e-03 
      	| train loss (relative): 2.909662e-02 | valid loss (relative): 2.995455e-02 
Epoch 932 use: 490.55 second.

epoch 933 starting......
Epoch:  933 | train loss: 1.484663e-03 | valid loss: 1.548199e-03 
      	| train loss (relative): 2.910196e-02 | valid loss (relative): 3.002275e-02 
Epoch 933 use: 498.02 second.

epoch 934 starting......
Epoch:  934 | train loss: 1.486878e-03 | valid loss: 1.521946e-03 
      	| train loss (relative): 2.914197e-02 | valid loss (relative): 2.978946e-02 
Epoch 934 use: 491.05 second.

epoch 935 starting......
Epoch:  935 | train loss: 1.476847e-03 | valid loss: 1.522031e-03 
      	| train loss (relative): 2.894474e-02 | valid loss (relative): 2.987112e-02 
Epoch 935 use: 492.27 second.

epoch 936 starting......
Epoch:  936 | train loss: 1.476775e-03 | valid loss: 1.531882e-03 
      	| train loss (relative): 2.894598e-02 | valid loss (relative): 2.991394e-02 
Epoch 936 use: 501.19 second.

epoch 937 starting......
Epoch:  937 | train loss: 1.480406e-03 | valid loss: 1.525371e-03 
      	| train loss (relative): 2.901299e-02 | valid loss (relative): 3.003861e-02 
Epoch 937 use: 497.77 second.

epoch 938 starting......
Epoch:  938 | train loss: 1.479356e-03 | valid loss: 1.528002e-03 
      	| train loss (relative): 2.900113e-02 | valid loss (relative): 3.004492e-02 
Epoch 938 use: 485.53 second.

epoch 939 starting......
Epoch:  939 | train loss: 1.482476e-03 | valid loss: 1.534415e-03 
      	| train loss (relative): 2.905932e-02 | valid loss (relative): 3.021358e-02 
Epoch 939 use: 487.33 second.

epoch 940 starting......
Epoch:  940 | train loss: 1.484052e-03 | valid loss: 1.530910e-03 
      	| train loss (relative): 2.908965e-02 | valid loss (relative): 2.990147e-02 
Epoch 940 use: 489.00 second.

epoch 941 starting......
Epoch:  941 | train loss: 1.482178e-03 | valid loss: 1.532506e-03 
      	| train loss (relative): 2.905035e-02 | valid loss (relative): 2.988419e-02 
Epoch 941 use: 510.56 second.

epoch 942 starting......
Epoch:  942 | train loss: 1.482035e-03 | valid loss: 1.530219e-03 
      	| train loss (relative): 2.904649e-02 | valid loss (relative): 3.003821e-02 
Epoch 942 use: 484.62 second.

epoch 943 starting......
Epoch:  943 | train loss: 1.484674e-03 | valid loss: 1.530450e-03 
      	| train loss (relative): 2.909558e-02 | valid loss (relative): 2.999309e-02 
Epoch 943 use: 490.91 second.

epoch 944 starting......
Epoch:  944 | train loss: 1.482260e-03 | valid loss: 1.531837e-03 
      	| train loss (relative): 2.905277e-02 | valid loss (relative): 2.975840e-02 
Epoch 944 use: 473.36 second.

epoch 945 starting......
Epoch:  945 | train loss: 1.480461e-03 | valid loss: 1.526985e-03 
      	| train loss (relative): 2.901298e-02 | valid loss (relative): 2.997896e-02 
Epoch 945 use: 483.93 second.

epoch 946 starting......
Epoch:  946 | train loss: 1.476609e-03 | valid loss: 1.525347e-03 
      	| train loss (relative): 2.894358e-02 | valid loss (relative): 2.983095e-02 
Epoch 946 use: 484.35 second.

epoch 947 starting......
Epoch:  947 | train loss: 1.477368e-03 | valid loss: 1.528947e-03 
      	| train loss (relative): 2.895812e-02 | valid loss (relative): 2.987274e-02 
Epoch 947 use: 478.53 second.

epoch 948 starting......
Epoch:  948 | train loss: 1.477540e-03 | valid loss: 1.527100e-03 
      	| train loss (relative): 2.895854e-02 | valid loss (relative): 2.996678e-02 
Epoch 948 use: 489.00 second.

epoch 949 starting......
Epoch:  949 | train loss: 1.478581e-03 | valid loss: 1.527659e-03 
      	| train loss (relative): 2.898399e-02 | valid loss (relative): 2.979452e-02 
Epoch 949 use: 472.27 second.

epoch 950 starting......
Epoch:  950 | train loss: 1.479574e-03 | valid loss: 1.532406e-03 
      	| train loss (relative): 2.899374e-02 | valid loss (relative): 3.020625e-02 
Epoch 950 use: 486.07 second.

epoch 951 starting......
Epoch:  951 | train loss: 1.479477e-03 | valid loss: 1.529238e-03 
      	| train loss (relative): 2.899616e-02 | valid loss (relative): 2.998565e-02 
Epoch 951 use: 500.40 second.

epoch 952 starting......
Epoch:  952 | train loss: 1.478958e-03 | valid loss: 1.524641e-03 
      	| train loss (relative): 2.898525e-02 | valid loss (relative): 2.992085e-02 
Epoch 952 use: 494.28 second.

epoch 953 starting......
Epoch:  953 | train loss: 1.479955e-03 | valid loss: 1.532885e-03 
      	| train loss (relative): 2.900778e-02 | valid loss (relative): 2.994419e-02 
Epoch 953 use: 483.99 second.

epoch 954 starting......
Epoch:  954 | train loss: 1.478839e-03 | valid loss: 1.526703e-03 
      	| train loss (relative): 2.898194e-02 | valid loss (relative): 2.987777e-02 
Epoch 954 use: 486.85 second.

epoch 955 starting......
Epoch:  955 | train loss: 1.479484e-03 | valid loss: 1.530728e-03 
      	| train loss (relative): 2.899777e-02 | valid loss (relative): 3.004608e-02 
Epoch 955 use: 490.45 second.

epoch 956 starting......
Epoch:  956 | train loss: 1.481876e-03 | valid loss: 1.523840e-03 
      	| train loss (relative): 2.904061e-02 | valid loss (relative): 2.991532e-02 
Epoch 956 use: 498.20 second.

epoch 957 starting......
Epoch:  957 | train loss: 1.474749e-03 | valid loss: 1.530563e-03 
      	| train loss (relative): 2.890597e-02 | valid loss (relative): 2.988828e-02 
Epoch 957 use: 487.53 second.

epoch 958 starting......
Epoch:  958 | train loss: 1.477898e-03 | valid loss: 1.530933e-03 
      	| train loss (relative): 2.896388e-02 | valid loss (relative): 3.002334e-02 
Epoch 958 use: 480.22 second.

epoch 959 starting......
Epoch:  959 | train loss: 1.474411e-03 | valid loss: 1.521725e-03 
      	| train loss (relative): 2.889358e-02 | valid loss (relative): 2.977090e-02 
Epoch 959 use: 479.80 second.

epoch 960 starting......
Epoch:  960 | train loss: 1.471910e-03 | valid loss: 1.523076e-03 
      	| train loss (relative): 2.884439e-02 | valid loss (relative): 2.980839e-02 
Epoch 960 use: 484.80 second.

epoch 961 starting......
Epoch:  961 | train loss: 1.471992e-03 | valid loss: 1.522486e-03 
      	| train loss (relative): 2.884848e-02 | valid loss (relative): 2.976902e-02 
Epoch 961 use: 498.97 second.

epoch 962 starting......
Epoch:  962 | train loss: 1.473567e-03 | valid loss: 1.523795e-03 
      	| train loss (relative): 2.887721e-02 | valid loss (relative): 2.992527e-02 
Epoch 962 use: 478.20 second.

epoch 963 starting......
Epoch:  963 | train loss: 1.474231e-03 | valid loss: 1.522832e-03 
      	| train loss (relative): 2.888604e-02 | valid loss (relative): 2.995175e-02 
Epoch 963 use: 489.27 second.

epoch 964 starting......
Epoch:  964 | train loss: 1.475579e-03 | valid loss: 1.527414e-03 
      	| train loss (relative): 2.891692e-02 | valid loss (relative): 2.987278e-02 
Epoch 964 use: 485.81 second.

epoch 965 starting......
Epoch:  965 | train loss: 1.475148e-03 | valid loss: 1.523705e-03 
      	| train loss (relative): 2.891269e-02 | valid loss (relative): 2.983874e-02 
Epoch 965 use: 468.76 second.

epoch 966 starting......
Epoch:  966 | train loss: 1.471449e-03 | valid loss: 1.522360e-03 
      	| train loss (relative): 2.883326e-02 | valid loss (relative): 2.975921e-02 
Epoch 966 use: 468.80 second.

epoch 967 starting......
Epoch:  967 | train loss: 1.473560e-03 | valid loss: 1.526932e-03 
      	| train loss (relative): 2.887709e-02 | valid loss (relative): 2.999664e-02 
Epoch 967 use: 473.21 second.

epoch 968 starting......
Epoch:  968 | train loss: 1.476050e-03 | valid loss: 1.525861e-03 
      	| train loss (relative): 2.892749e-02 | valid loss (relative): 2.995609e-02 
Epoch 968 use: 477.40 second.

epoch 969 starting......
Epoch:  969 | train loss: 1.474585e-03 | valid loss: 1.531737e-03 
      	| train loss (relative): 2.890038e-02 | valid loss (relative): 2.998952e-02 
Epoch 969 use: 473.91 second.

epoch 970 starting......
Epoch:  970 | train loss: 1.475653e-03 | valid loss: 1.520935e-03 
      	| train loss (relative): 2.891663e-02 | valid loss (relative): 2.977917e-02 
Epoch 970 use: 473.58 second.

epoch 971 starting......
Epoch:  971 | train loss: 1.471443e-03 | valid loss: 1.532910e-03 
      	| train loss (relative): 2.883673e-02 | valid loss (relative): 3.004577e-02 
Epoch 971 use: 464.94 second.

epoch 972 starting......
Epoch:  972 | train loss: 1.471259e-03 | valid loss: 1.518902e-03 
      	| train loss (relative): 2.883204e-02 | valid loss (relative): 2.970877e-02 
Epoch 972 use: 476.25 second.

epoch 973 starting......
Epoch:  973 | train loss: 1.468925e-03 | valid loss: 1.516764e-03 
      	| train loss (relative): 2.878544e-02 | valid loss (relative): 2.967340e-02 
Epoch 973 use: 483.34 second.

epoch 974 starting......
Epoch:  974 | train loss: 1.469591e-03 | valid loss: 1.526613e-03 
      	| train loss (relative): 2.879743e-02 | valid loss (relative): 3.003643e-02 
Epoch 974 use: 475.62 second.

epoch 975 starting......
Epoch:  975 | train loss: 1.473499e-03 | valid loss: 1.523595e-03 
      	| train loss (relative): 2.887762e-02 | valid loss (relative): 2.973064e-02 
Epoch 975 use: 471.29 second.

epoch 976 starting......
Epoch:  976 | train loss: 1.468701e-03 | valid loss: 1.522343e-03 
      	| train loss (relative): 2.878046e-02 | valid loss (relative): 2.993241e-02 
Epoch 976 use: 474.68 second.

epoch 977 starting......
Epoch:  977 | train loss: 1.466915e-03 | valid loss: 1.514942e-03 
      	| train loss (relative): 2.874243e-02 | valid loss (relative): 2.968197e-02 
Epoch 977 use: 473.63 second.

epoch 978 starting......
Epoch:  978 | train loss: 1.462832e-03 | valid loss: 1.517910e-03 
      	| train loss (relative): 2.865974e-02 | valid loss (relative): 2.984877e-02 
Epoch 978 use: 466.53 second.

epoch 979 starting......
Epoch:  979 | train loss: 1.465675e-03 | valid loss: 1.517721e-03 
      	| train loss (relative): 2.871679e-02 | valid loss (relative): 2.970138e-02 
Epoch 979 use: 473.41 second.

epoch 980 starting......
Epoch:  980 | train loss: 1.467026e-03 | valid loss: 1.522363e-03 
      	| train loss (relative): 2.874588e-02 | valid loss (relative): 2.992127e-02 
Epoch 980 use: 473.72 second.

epoch 981 starting......
Epoch:  981 | train loss: 1.468530e-03 | valid loss: 1.521202e-03 
      	| train loss (relative): 2.877415e-02 | valid loss (relative): 2.977667e-02 
Epoch 981 use: 587.28 second.

epoch 982 starting......
Epoch:  982 | train loss: 1.469135e-03 | valid loss: 1.527974e-03 
      	| train loss (relative): 2.878881e-02 | valid loss (relative): 2.981360e-02 
Epoch 982 use: 498.92 second.

epoch 983 starting......
Epoch:  983 | train loss: 1.467953e-03 | valid loss: 1.522169e-03 
      	| train loss (relative): 2.876318e-02 | valid loss (relative): 2.976947e-02 
Epoch 983 use: 484.95 second.

epoch 984 starting......
Epoch:  984 | train loss: 1.464218e-03 | valid loss: 1.518408e-03 
      	| train loss (relative): 2.868975e-02 | valid loss (relative): 2.972337e-02 
Epoch 984 use: 588.38 second.

epoch 985 starting......
Epoch:  985 | train loss: 1.466384e-03 | valid loss: 1.520521e-03 
      	| train loss (relative): 2.873156e-02 | valid loss (relative): 2.974898e-02 
Epoch 985 use: 484.11 second.

epoch 986 starting......
Epoch:  986 | train loss: 1.465146e-03 | valid loss: 1.519525e-03 
      	| train loss (relative): 2.871177e-02 | valid loss (relative): 2.975670e-02 
Epoch 986 use: 489.37 second.

epoch 987 starting......
Epoch:  987 | train loss: 1.465862e-03 | valid loss: 1.523485e-03 
      	| train loss (relative): 2.871923e-02 | valid loss (relative): 2.984090e-02 
Epoch 987 use: 483.03 second.

epoch 988 starting......
Epoch:  988 | train loss: 1.468358e-03 | valid loss: 1.527532e-03 
      	| train loss (relative): 2.877327e-02 | valid loss (relative): 2.983063e-02 
Epoch 988 use: 477.37 second.

epoch 989 starting......
Epoch:  989 | train loss: 1.466967e-03 | valid loss: 1.518574e-03 
      	| train loss (relative): 2.874514e-02 | valid loss (relative): 2.978545e-02 
Epoch 989 use: 494.17 second.

epoch 990 starting......
Epoch:  990 | train loss: 1.465507e-03 | valid loss: 1.521711e-03 
      	| train loss (relative): 2.871695e-02 | valid loss (relative): 2.975803e-02 
Epoch 990 use: 478.50 second.

epoch 991 starting......
Epoch:  991 | train loss: 1.464249e-03 | valid loss: 1.515754e-03 
      	| train loss (relative): 2.869029e-02 | valid loss (relative): 2.955961e-02 
Epoch 991 use: 480.69 second.

epoch 992 starting......
Epoch:  992 | train loss: 1.459326e-03 | valid loss: 1.519204e-03 
      	| train loss (relative): 2.859218e-02 | valid loss (relative): 2.967653e-02 
Epoch 992 use: 482.73 second.

epoch 993 starting......
Epoch:  993 | train loss: 1.463235e-03 | valid loss: 1.522578e-03 
      	| train loss (relative): 2.866549e-02 | valid loss (relative): 2.977855e-02 
Epoch 993 use: 473.60 second.

epoch 994 starting......
Epoch:  994 | train loss: 1.463765e-03 | valid loss: 1.516758e-03 
      	| train loss (relative): 2.867582e-02 | valid loss (relative): 2.973351e-02 
Epoch 994 use: 485.18 second.

epoch 995 starting......
Epoch:  995 | train loss: 1.462180e-03 | valid loss: 1.525000e-03 
      	| train loss (relative): 2.864974e-02 | valid loss (relative): 2.981626e-02 
Epoch 995 use: 478.09 second.

epoch 996 starting......
Epoch:  996 | train loss: 1.463350e-03 | valid loss: 1.517451e-03 
      	| train loss (relative): 2.867526e-02 | valid loss (relative): 2.960561e-02 
Epoch 996 use: 474.76 second.

epoch 997 starting......
Epoch:  997 | train loss: 1.460645e-03 | valid loss: 1.511012e-03 
      	| train loss (relative): 2.861658e-02 | valid loss (relative): 2.954510e-02 
Epoch 997 use: 482.47 second.

epoch 998 starting......
Epoch:  998 | train loss: 1.458618e-03 | valid loss: 1.512373e-03 
      	| train loss (relative): 2.857433e-02 | valid loss (relative): 2.964638e-02 
Epoch 998 use: 466.47 second.

epoch 999 starting......
Epoch:  999 | train loss: 1.459147e-03 | valid loss: 1.511436e-03 
      	| train loss (relative): 2.859025e-02 | valid loss (relative): 2.957096e-02 
Epoch 999 use: 466.50 second.

epoch 1000 starting......
Epoch:  1000 | train loss: 1.460680e-03 | valid loss: 1.517708e-03 
      	| train loss (relative): 2.861829e-02 | valid loss (relative): 2.980963e-02 
Epoch 1000 use: 459.40 second.

epoch 1001 starting......
Epoch:  1001 | train loss: 1.460781e-03 | valid loss: 1.510906e-03 
      	| train loss (relative): 2.861956e-02 | valid loss (relative): 2.942044e-02 
Epoch 1001 use: 460.47 second.

epoch 1002 starting......
Epoch:  1002 | train loss: 1.454268e-03 | valid loss: 1.507813e-03 
      	| train loss (relative): 2.848626e-02 | valid loss (relative): 2.956505e-02 
Epoch 1002 use: 472.38 second.

epoch 1003 starting......
Epoch:  1003 | train loss: 1.456262e-03 | valid loss: 1.515207e-03 
      	| train loss (relative): 2.852947e-02 | valid loss (relative): 2.958561e-02 
Epoch 1003 use: 471.62 second.

epoch 1004 starting......
Epoch:  1004 | train loss: 1.457119e-03 | valid loss: 1.514824e-03 
      	| train loss (relative): 2.854357e-02 | valid loss (relative): 2.953221e-02 
Epoch 1004 use: 455.00 second.

epoch 1005 starting......
Epoch:  1005 | train loss: 1.456493e-03 | valid loss: 1.517174e-03 
      	| train loss (relative): 2.853254e-02 | valid loss (relative): 2.967036e-02 
Epoch 1005 use: 458.94 second.

epoch 1006 starting......
Epoch:  1006 | train loss: 1.460380e-03 | valid loss: 1.516702e-03 
      	| train loss (relative): 2.860918e-02 | valid loss (relative): 2.971184e-02 
Epoch 1006 use: 459.88 second.

epoch 1007 starting......
Epoch:  1007 | train loss: 1.458310e-03 | valid loss: 1.517425e-03 
      	| train loss (relative): 2.856617e-02 | valid loss (relative): 2.995617e-02 
Epoch 1007 use: 460.43 second.

epoch 1008 starting......
Epoch:  1008 | train loss: 1.457417e-03 | valid loss: 1.509530e-03 
      	| train loss (relative): 2.855778e-02 | valid loss (relative): 2.964703e-02 
Epoch 1008 use: 453.77 second.

epoch 1009 starting......
Epoch:  1009 | train loss: 1.457002e-03 | valid loss: 1.514903e-03 
      	| train loss (relative): 2.854430e-02 | valid loss (relative): 2.954865e-02 
Epoch 1009 use: 452.12 second.

epoch 1010 starting......
Epoch:  1010 | train loss: 1.457962e-03 | valid loss: 1.512351e-03 
      	| train loss (relative): 2.856360e-02 | valid loss (relative): 2.960070e-02 
Epoch 1010 use: 482.13 second.

epoch 1011 starting......
Epoch:  1011 | train loss: 1.455077e-03 | valid loss: 1.509052e-03 
      	| train loss (relative): 2.849926e-02 | valid loss (relative): 2.950020e-02 
Epoch 1011 use: 455.76 second.

epoch 1012 starting......
Epoch:  1012 | train loss: 1.457175e-03 | valid loss: 1.508297e-03 
      	| train loss (relative): 2.854396e-02 | valid loss (relative): 2.959150e-02 
Epoch 1012 use: 461.82 second.

epoch 1013 starting......
Epoch:  1013 | train loss: 1.451972e-03 | valid loss: 1.510536e-03 
      	| train loss (relative): 2.844248e-02 | valid loss (relative): 2.951322e-02 
Epoch 1013 use: 469.43 second.

epoch 1014 starting......
Epoch:  1014 | train loss: 1.451721e-03 | valid loss: 1.508185e-03 
      	| train loss (relative): 2.843547e-02 | valid loss (relative): 2.947113e-02 
Epoch 1014 use: 464.41 second.

epoch 1015 starting......
Epoch:  1015 | train loss: 1.453522e-03 | valid loss: 1.509144e-03 
      	| train loss (relative): 2.847702e-02 | valid loss (relative): 2.955545e-02 
Epoch 1015 use: 466.28 second.

epoch 1016 starting......
Epoch:  1016 | train loss: 1.450433e-03 | valid loss: 1.505126e-03 
      	| train loss (relative): 2.841561e-02 | valid loss (relative): 2.940612e-02 
Epoch 1016 use: 461.80 second.

epoch 1017 starting......
Epoch:  1017 | train loss: 1.449466e-03 | valid loss: 1.520472e-03 
      	| train loss (relative): 2.839708e-02 | valid loss (relative): 2.998566e-02 
Epoch 1017 use: 482.69 second.

epoch 1018 starting......
Epoch:  1018 | train loss: 1.457335e-03 | valid loss: 1.508488e-03 
      	| train loss (relative): 2.855102e-02 | valid loss (relative): 2.942975e-02 
Epoch 1018 use: 469.90 second.

epoch 1019 starting......
Epoch:  1019 | train loss: 1.450405e-03 | valid loss: 1.505076e-03 
      	| train loss (relative): 2.841278e-02 | valid loss (relative): 2.948078e-02 
Epoch 1019 use: 455.66 second.

epoch 1020 starting......
Epoch:  1020 | train loss: 1.447797e-03 | valid loss: 1.507833e-03 
      	| train loss (relative): 2.836030e-02 | valid loss (relative): 2.941026e-02 
Epoch 1020 use: 459.05 second.

epoch 1021 starting......
Epoch:  1021 | train loss: 1.449937e-03 | valid loss: 1.505877e-03 
      	| train loss (relative): 2.840051e-02 | valid loss (relative): 2.944401e-02 
Epoch 1021 use: 501.49 second.

epoch 1022 starting......
Epoch:  1022 | train loss: 1.452832e-03 | valid loss: 1.511948e-03 
      	| train loss (relative): 2.845908e-02 | valid loss (relative): 2.955522e-02 
Epoch 1022 use: 481.83 second.

epoch 1023 starting......
Epoch:  1023 | train loss: 1.451676e-03 | valid loss: 1.507700e-03 
      	| train loss (relative): 2.843571e-02 | valid loss (relative): 2.966273e-02 
Epoch 1023 use: 483.96 second.

epoch 1024 starting......
Epoch:  1024 | train loss: 1.450233e-03 | valid loss: 1.507858e-03 
      	| train loss (relative): 2.840646e-02 | valid loss (relative): 2.934864e-02 
Epoch 1024 use: 497.69 second.

epoch 1025 starting......
Epoch:  1025 | train loss: 1.449279e-03 | valid loss: 1.505553e-03 
      	| train loss (relative): 2.839010e-02 | valid loss (relative): 2.961860e-02 
Epoch 1025 use: 491.68 second.

epoch 1026 starting......
Epoch:  1026 | train loss: 1.451367e-03 | valid loss: 1.510551e-03 
      	| train loss (relative): 2.843242e-02 | valid loss (relative): 2.954037e-02 
Epoch 1026 use: 498.01 second.

epoch 1027 starting......
Epoch:  1027 | train loss: 1.451691e-03 | valid loss: 1.511681e-03 
      	| train loss (relative): 2.843612e-02 | valid loss (relative): 2.942263e-02 
Epoch 1027 use: 507.21 second.

epoch 1028 starting......
Epoch:  1028 | train loss: 1.451628e-03 | valid loss: 1.505752e-03 
      	| train loss (relative): 2.843722e-02 | valid loss (relative): 2.945898e-02 
Epoch 1028 use: 482.15 second.

epoch 1029 starting......
Epoch:  1029 | train loss: 1.452173e-03 | valid loss: 1.518689e-03 
      	| train loss (relative): 2.845145e-02 | valid loss (relative): 2.983528e-02 
Epoch 1029 use: 499.31 second.

epoch 1030 starting......
Epoch:  1030 | train loss: 1.449536e-03 | valid loss: 1.507701e-03 
      	| train loss (relative): 2.839394e-02 | valid loss (relative): 2.952387e-02 
Epoch 1030 use: 486.05 second.

epoch 1031 starting......
Epoch:  1031 | train loss: 1.450024e-03 | valid loss: 1.514411e-03 
      	| train loss (relative): 2.840513e-02 | valid loss (relative): 2.953978e-02 
Epoch 1031 use: 516.37 second.

epoch 1032 starting......
Epoch:  1032 | train loss: 1.451942e-03 | valid loss: 1.505271e-03 
      	| train loss (relative): 2.844430e-02 | valid loss (relative): 2.937224e-02 
Epoch 1032 use: 480.55 second.

epoch 1033 starting......
Epoch:  1033 | train loss: 1.448101e-03 | valid loss: 1.502382e-03 
      	| train loss (relative): 2.836338e-02 | valid loss (relative): 2.923309e-02 
Epoch 1033 use: 513.93 second.

epoch 1034 starting......
Epoch:  1034 | train loss: 1.445174e-03 | valid loss: 1.505944e-03 
      	| train loss (relative): 2.830450e-02 | valid loss (relative): 2.957344e-02 
Epoch 1034 use: 484.81 second.

epoch 1035 starting......
Epoch:  1035 | train loss: 1.447005e-03 | valid loss: 1.510243e-03 
      	| train loss (relative): 2.834451e-02 | valid loss (relative): 2.951113e-02 
Epoch 1035 use: 500.36 second.

epoch 1036 starting......
Epoch:  1036 | train loss: 1.450625e-03 | valid loss: 1.503514e-03 
      	| train loss (relative): 2.841067e-02 | valid loss (relative): 2.944235e-02 
Epoch 1036 use: 477.92 second.

epoch 1037 starting......
Epoch:  1037 | train loss: 1.447492e-03 | valid loss: 1.506995e-03 
      	| train loss (relative): 2.835348e-02 | valid loss (relative): 2.938990e-02 
Epoch 1037 use: 498.51 second.

epoch 1038 starting......
Epoch:  1038 | train loss: 1.447591e-03 | valid loss: 1.506284e-03 
      	| train loss (relative): 2.835959e-02 | valid loss (relative): 2.942370e-02 
Epoch 1038 use: 498.57 second.

epoch 1039 starting......
Epoch:  1039 | train loss: 1.448023e-03 | valid loss: 1.508276e-03 
      	| train loss (relative): 2.836465e-02 | valid loss (relative): 2.950784e-02 
Epoch 1039 use: 495.30 second.

epoch 1040 starting......
Epoch:  1040 | train loss: 1.447926e-03 | valid loss: 1.508381e-03 
      	| train loss (relative): 2.835842e-02 | valid loss (relative): 2.948465e-02 
Epoch 1040 use: 493.61 second.

epoch 1041 starting......
Epoch:  1041 | train loss: 1.448845e-03 | valid loss: 1.513799e-03 
      	| train loss (relative): 2.837678e-02 | valid loss (relative): 2.975046e-02 
Epoch 1041 use: 498.83 second.

epoch 1042 starting......
Epoch:  1042 | train loss: 1.449545e-03 | valid loss: 1.507238e-03 
      	| train loss (relative): 2.839601e-02 | valid loss (relative): 2.971455e-02 
Epoch 1042 use: 493.42 second.

epoch 1043 starting......
Epoch:  1043 | train loss: 1.444314e-03 | valid loss: 1.498997e-03 
      	| train loss (relative): 2.828902e-02 | valid loss (relative): 2.941454e-02 
Epoch 1043 use: 482.11 second.

epoch 1044 starting......
Epoch:  1044 | train loss: 1.444183e-03 | valid loss: 1.502879e-03 
      	| train loss (relative): 2.828952e-02 | valid loss (relative): 2.938848e-02 
Epoch 1044 use: 504.90 second.

epoch 1045 starting......
Epoch:  1045 | train loss: 1.446628e-03 | valid loss: 1.501092e-03 
      	| train loss (relative): 2.833135e-02 | valid loss (relative): 2.939282e-02 
Epoch 1045 use: 482.22 second.

epoch 1046 starting......
Epoch:  1046 | train loss: 1.442274e-03 | valid loss: 1.503171e-03 
      	| train loss (relative): 2.825186e-02 | valid loss (relative): 2.937489e-02 
Epoch 1046 use: 495.30 second.

epoch 1047 starting......
Epoch:  1047 | train loss: 1.443517e-03 | valid loss: 1.503196e-03 
      	| train loss (relative): 2.827273e-02 | valid loss (relative): 2.945773e-02 
Epoch 1047 use: 486.32 second.

epoch 1048 starting......
Epoch:  1048 | train loss: 1.446126e-03 | valid loss: 1.505289e-03 
      	| train loss (relative): 2.832393e-02 | valid loss (relative): 2.945474e-02 
Epoch 1048 use: 482.28 second.

epoch 1049 starting......
Epoch:  1049 | train loss: 1.445507e-03 | valid loss: 1.505541e-03 
      	| train loss (relative): 2.831164e-02 | valid loss (relative): 2.948917e-02 
Epoch 1049 use: 483.79 second.

test MSE Error: 1.506074e-03 | relative MSE Error: 2.955043e-02 
 Total time used for training: 20.36 hour.
MESLoss saved to  /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_MSELoss_Latent_64_nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_1050.txt
relative MSELoss saved to  /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_reMSELoss_Latent_64_nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_1050.txt
model saved to /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_64_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_1050.pth
model_dict saved to /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_64_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_1050_dict.pth
... Training slugflow data group 3 completed, Run finished Fri 20 Aug 22:27:17 BST 2021 ...
{'data_dir': '/rds/general/user/jy220/home/tensors/', 'mode': 'train', 'data_type': 'tensors', 'vtu_fields': 'Component1::ComponentMassFractionPhase1, phase1::Velocity', 'variational': 'False', 'optimizer': 'Adamax', 'structured': 'False', 'changing_lr': 'False', 'self_concat': '2', 'dimension': '3', 'sfc_nums': '3', 'components': '4', 'nearest_neighbouring': 'True', 'dims_latent': '64', 'activation': 'Tanh', 'tk_file': '/rds/general/user/jy220/home/slugflow_tk.pt', 'tb_file': '/rds/general/user/jy220/home/slugflow_tb.pt', 'sfc_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/sfcs.pt', 'inv_sfc_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/inv_sfcs.pt', 'coords_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/coords.pt', 'cells_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/cells.npy', 'state_load': '/rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_64_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_1050_dict.pth', 'batch_size': '16', 'lr': '0.001', 'n_epoches': '150', 'seed': '17', 'visualize': 'False', 'reconstructed_path': 'None', 'reconstruct_start_index': '0', 'reconstruct_end_index': '50', 'save_path': '/rds/general/user/jy220/home/new_results/'}
[[ 2.50825269e-01 -1.67256990e-02 -1.22268730e-03]
 [ 2.38611959e-01 -5.50627777e-03 -5.06451913e-03]
 [ 2.39454874e-01 -1.59563257e-02  3.96778323e-03]
 ...
 [ 9.11361096e+00 -2.21908381e-02  2.22384902e-03]
 [ 9.11847310e+00 -2.84346931e-02 -2.34839590e-04]
 [ 9.11039661e+00 -2.51498573e-02 -8.70661044e-03]]
{'tetra': array([[      0,       1,       2,       3],
       [      4,       5,       6,       7],
       [      8,       9,      10,      11],
       ...,
       [1342744, 1342745, 1342746, 1342747],
       [1342748, 1342749, 1342750, 1342751],
       [1342752, 1342753, 1342754, 1342755]])}
vtu fields:  ['Component1::ComponentMassFractionPhase1', 'phase1::Velocity'] 
 structured  False 
 activation  Tanh() 
 self concat  2 
 sfc_nums  3 

dims_latent  64 
 components  4 
 nearest_neighbouring  True 

visualize  False 
 sample number  1706 

reading sfc nums......
[array([  41684,   41685,   41674, ..., 1235560, 1235564, 1235558]), array([954289, 954290, 954291, ..., 420685, 420687, 420688]), array([  40156,   40155,   40158, ..., 1123478, 1123475, 1123467])]
reading inverse sfc nums......
[array([   8146,    8145,    8144, ..., 1274444, 1274445, 1273166]), array([547280, 547283, 593037, ..., 933933, 933932, 864488]), array([ 66437,  66439, 122512, ..., 459416, 415130, 459418])]
tk:  tensor([0.7581, 0.0418, 0.1840, 0.1464], dtype=torch.float64) 

tb:  tensor([-0.3816, -0.3618, -0.0308,  0.1446], dtype=torch.float64) 

length of train set: 1505 

length of valid set: 100 

length of test set: 101 

torch device num: 4 

Let's use 4 GPUs!
epoch 1050 starting......
Epoch:  1050 | train loss: 1.642853e-03 | valid loss: 1.524628e-03 
      	| train loss (relative): 3.215028e-02 | valid loss (relative): 2.988397e-02 
Epoch 1050 use: 461.50 second.

epoch 1051 starting......
Epoch:  1051 | train loss: 1.443133e-03 | valid loss: 1.506830e-03 
      	| train loss (relative): 2.827208e-02 | valid loss (relative): 2.954027e-02 
Epoch 1051 use: 405.48 second.

epoch 1052 starting......
Epoch:  1052 | train loss: 1.432930e-03 | valid loss: 1.504728e-03 
      	| train loss (relative): 2.807206e-02 | valid loss (relative): 2.955777e-02 
Epoch 1052 use: 375.66 second.

epoch 1053 starting......
Epoch:  1053 | train loss: 1.429836e-03 | valid loss: 1.501587e-03 
      	| train loss (relative): 2.800901e-02 | valid loss (relative): 2.947625e-02 
Epoch 1053 use: 389.41 second.

epoch 1054 starting......
Epoch:  1054 | train loss: 1.427544e-03 | valid loss: 1.502850e-03 
      	| train loss (relative): 2.795830e-02 | valid loss (relative): 2.948992e-02 
Epoch 1054 use: 371.37 second.

epoch 1055 starting......
Epoch:  1055 | train loss: 1.426518e-03 | valid loss: 1.505677e-03 
      	| train loss (relative): 2.793681e-02 | valid loss (relative): 2.954048e-02 
Epoch 1055 use: 376.98 second.

epoch 1056 starting......
Epoch:  1056 | train loss: 1.426189e-03 | valid loss: 1.504858e-03 
      	| train loss (relative): 2.793049e-02 | valid loss (relative): 2.955779e-02 
Epoch 1056 use: 370.74 second.

epoch 1057 starting......
Epoch:  1057 | train loss: 1.426421e-03 | valid loss: 1.505441e-03 
      	| train loss (relative): 2.793358e-02 | valid loss (relative): 2.951436e-02 
Epoch 1057 use: 361.98 second.

epoch 1058 starting......
Epoch:  1058 | train loss: 1.427270e-03 | valid loss: 1.509847e-03 
      	| train loss (relative): 2.795321e-02 | valid loss (relative): 2.965457e-02 
Epoch 1058 use: 392.15 second.

epoch 1059 starting......
Epoch:  1059 | train loss: 1.429252e-03 | valid loss: 1.512463e-03 
      	| train loss (relative): 2.799350e-02 | valid loss (relative): 2.970460e-02 
Epoch 1059 use: 398.21 second.

epoch 1060 starting......
Epoch:  1060 | train loss: 1.432133e-03 | valid loss: 1.516142e-03 
      	| train loss (relative): 2.804790e-02 | valid loss (relative): 2.971706e-02 
Epoch 1060 use: 353.80 second.

epoch 1061 starting......
Epoch:  1061 | train loss: 1.437669e-03 | valid loss: 1.519283e-03 
      	| train loss (relative): 2.815716e-02 | valid loss (relative): 2.972327e-02 
Epoch 1061 use: 373.10 second.

epoch 1062 starting......
Epoch:  1062 | train loss: 1.435448e-03 | valid loss: 1.516306e-03 
      	| train loss (relative): 2.811298e-02 | valid loss (relative): 2.974879e-02 
Epoch 1062 use: 377.96 second.

epoch 1063 starting......
Epoch:  1063 | train loss: 1.434274e-03 | valid loss: 1.518452e-03 
      	| train loss (relative): 2.808732e-02 | valid loss (relative): 2.982214e-02 
Epoch 1063 use: 352.14 second.

epoch 1064 starting......
Epoch:  1064 | train loss: 1.433136e-03 | valid loss: 1.519203e-03 
      	| train loss (relative): 2.806292e-02 | valid loss (relative): 2.974859e-02 
Epoch 1064 use: 356.97 second.

epoch 1065 starting......
Epoch:  1065 | train loss: 1.435262e-03 | valid loss: 1.519843e-03 
      	| train loss (relative): 2.810243e-02 | valid loss (relative): 2.986934e-02 
Epoch 1065 use: 360.36 second.

epoch 1066 starting......
Epoch:  1066 | train loss: 1.436974e-03 | valid loss: 1.516878e-03 
      	| train loss (relative): 2.813760e-02 | valid loss (relative): 2.978302e-02 
Epoch 1066 use: 347.54 second.

epoch 1067 starting......
Epoch:  1067 | train loss: 1.435888e-03 | valid loss: 1.522718e-03 
      	| train loss (relative): 2.812223e-02 | valid loss (relative): 2.973351e-02 
Epoch 1067 use: 371.45 second.

epoch 1068 starting......
Epoch:  1068 | train loss: 1.437588e-03 | valid loss: 1.524219e-03 
      	| train loss (relative): 2.815352e-02 | valid loss (relative): 2.970827e-02 
Epoch 1068 use: 351.12 second.

epoch 1069 starting......
Epoch:  1069 | train loss: 1.437241e-03 | valid loss: 1.523509e-03 
      	| train loss (relative): 2.814417e-02 | valid loss (relative): 2.992705e-02 
Epoch 1069 use: 375.86 second.

epoch 1070 starting......
Epoch:  1070 | train loss: 1.436753e-03 | valid loss: 1.524061e-03 
      	| train loss (relative): 2.813763e-02 | valid loss (relative): 2.982624e-02 
Epoch 1070 use: 360.70 second.

epoch 1071 starting......
Epoch:  1071 | train loss: 1.437027e-03 | valid loss: 1.524449e-03 
      	| train loss (relative): 2.813717e-02 | valid loss (relative): 3.005643e-02 
Epoch 1071 use: 363.53 second.

epoch 1072 starting......
Epoch:  1072 | train loss: 1.435417e-03 | valid loss: 1.523662e-03 
      	| train loss (relative): 2.811081e-02 | valid loss (relative): 2.991437e-02 
Epoch 1072 use: 377.77 second.

epoch 1073 starting......
Epoch:  1073 | train loss: 1.436868e-03 | valid loss: 1.525985e-03 
      	| train loss (relative): 2.814053e-02 | valid loss (relative): 2.996689e-02 
Epoch 1073 use: 362.78 second.

epoch 1074 starting......
Epoch:  1074 | train loss: 1.435624e-03 | valid loss: 1.524459e-03 
      	| train loss (relative): 2.811228e-02 | valid loss (relative): 2.985894e-02 
Epoch 1074 use: 359.86 second.

epoch 1075 starting......
Epoch:  1075 | train loss: 1.437467e-03 | valid loss: 1.535053e-03 
      	| train loss (relative): 2.814784e-02 | valid loss (relative): 3.001106e-02 
Epoch 1075 use: 373.83 second.

epoch 1076 starting......
Epoch:  1076 | train loss: 1.435452e-03 | valid loss: 1.522006e-03 
      	| train loss (relative): 2.810850e-02 | valid loss (relative): 2.985352e-02 
Epoch 1076 use: 372.82 second.

epoch 1077 starting......
Epoch:  1077 | train loss: 1.437740e-03 | valid loss: 1.531040e-03 
      	| train loss (relative): 2.815283e-02 | valid loss (relative): 3.005125e-02 
Epoch 1077 use: 376.50 second.

epoch 1078 starting......
Epoch:  1078 | train loss: 1.441888e-03 | valid loss: 1.530031e-03 
      	| train loss (relative): 2.823718e-02 | valid loss (relative): 3.008381e-02 
Epoch 1078 use: 359.87 second.

epoch 1079 starting......
Epoch:  1079 | train loss: 1.438604e-03 | valid loss: 1.527401e-03 
      	| train loss (relative): 2.817549e-02 | valid loss (relative): 3.011475e-02 
Epoch 1079 use: 371.61 second.

epoch 1080 starting......
Epoch:  1080 | train loss: 1.433355e-03 | valid loss: 1.516993e-03 
      	| train loss (relative): 2.807276e-02 | valid loss (relative): 2.979334e-02 
Epoch 1080 use: 375.47 second.

epoch 1081 starting......
Epoch:  1081 | train loss: 1.427787e-03 | valid loss: 1.519792e-03 
      	| train loss (relative): 2.795985e-02 | valid loss (relative): 2.986311e-02 
Epoch 1081 use: 346.67 second.

epoch 1082 starting......
Epoch:  1082 | train loss: 1.429710e-03 | valid loss: 1.520656e-03 
      	| train loss (relative): 2.799947e-02 | valid loss (relative): 2.978379e-02 
Epoch 1082 use: 370.89 second.

epoch 1083 starting......
Epoch:  1083 | train loss: 1.430144e-03 | valid loss: 1.519948e-03 
      	| train loss (relative): 2.800220e-02 | valid loss (relative): 2.985400e-02 
Epoch 1083 use: 345.06 second.

epoch 1084 starting......
Epoch:  1084 | train loss: 1.429072e-03 | valid loss: 1.516888e-03 
      	| train loss (relative): 2.798717e-02 | valid loss (relative): 2.974941e-02 
Epoch 1084 use: 356.29 second.

epoch 1085 starting......
Epoch:  1085 | train loss: 1.430794e-03 | valid loss: 1.519467e-03 
      	| train loss (relative): 2.801552e-02 | valid loss (relative): 2.982486e-02 
Epoch 1085 use: 375.70 second.

epoch 1086 starting......
Epoch:  1086 | train loss: 1.432269e-03 | valid loss: 1.519251e-03 
      	| train loss (relative): 2.804499e-02 | valid loss (relative): 2.979436e-02 
Epoch 1086 use: 363.07 second.

epoch 1087 starting......
Epoch:  1087 | train loss: 1.428459e-03 | valid loss: 1.522820e-03 
      	| train loss (relative): 2.796586e-02 | valid loss (relative): 2.994950e-02 
Epoch 1087 use: 376.40 second.

epoch 1088 starting......
Epoch:  1088 | train loss: 1.429274e-03 | valid loss: 1.523159e-03 
      	| train loss (relative): 2.798419e-02 | valid loss (relative): 2.986639e-02 
Epoch 1088 use: 365.14 second.

epoch 1089 starting......
Epoch:  1089 | train loss: 1.433996e-03 | valid loss: 1.525663e-03 
      	| train loss (relative): 2.807764e-02 | valid loss (relative): 2.987407e-02 
Epoch 1089 use: 356.18 second.

epoch 1090 starting......
Epoch:  1090 | train loss: 1.434357e-03 | valid loss: 1.531423e-03 
      	| train loss (relative): 2.808763e-02 | valid loss (relative): 2.986509e-02 
Epoch 1090 use: 368.53 second.

epoch 1091 starting......
Epoch:  1091 | train loss: 1.431445e-03 | valid loss: 1.518444e-03 
      	| train loss (relative): 2.802899e-02 | valid loss (relative): 2.976172e-02 
Epoch 1091 use: 378.00 second.

epoch 1092 starting......
Epoch:  1092 | train loss: 1.429427e-03 | valid loss: 1.523209e-03 
      	| train loss (relative): 2.799272e-02 | valid loss (relative): 2.986116e-02 
Epoch 1092 use: 356.14 second.

epoch 1093 starting......
Epoch:  1093 | train loss: 1.429683e-03 | valid loss: 1.523379e-03 
      	| train loss (relative): 2.799501e-02 | valid loss (relative): 2.999247e-02 
Epoch 1093 use: 361.52 second.

epoch 1094 starting......
Epoch:  1094 | train loss: 1.431160e-03 | valid loss: 1.522293e-03 
      	| train loss (relative): 2.802277e-02 | valid loss (relative): 2.988861e-02 
Epoch 1094 use: 373.72 second.

epoch 1095 starting......
Epoch:  1095 | train loss: 1.430585e-03 | valid loss: 1.525162e-03 
      	| train loss (relative): 2.801519e-02 | valid loss (relative): 2.999251e-02 
Epoch 1095 use: 352.24 second.

epoch 1096 starting......
Epoch:  1096 | train loss: 1.432035e-03 | valid loss: 1.537894e-03 
      	| train loss (relative): 2.803772e-02 | valid loss (relative): 3.041873e-02 
Epoch 1096 use: 387.00 second.

epoch 1097 starting......
Epoch:  1097 | train loss: 1.434508e-03 | valid loss: 1.523592e-03 
      	| train loss (relative): 2.809074e-02 | valid loss (relative): 2.992069e-02 
Epoch 1097 use: 378.63 second.

epoch 1098 starting......
Epoch:  1098 | train loss: 1.427836e-03 | valid loss: 1.517877e-03 
      	| train loss (relative): 2.796008e-02 | valid loss (relative): 2.980713e-02 
Epoch 1098 use: 379.22 second.

epoch 1099 starting......
Epoch:  1099 | train loss: 1.425270e-03 | valid loss: 1.519800e-03 
      	| train loss (relative): 2.790230e-02 | valid loss (relative): 2.978531e-02 
Epoch 1099 use: 378.80 second.

epoch 1100 starting......
Epoch:  1100 | train loss: 1.427461e-03 | valid loss: 1.523824e-03 
      	| train loss (relative): 2.795200e-02 | valid loss (relative): 2.996985e-02 
Epoch 1100 use: 340.16 second.

epoch 1101 starting......
Epoch:  1101 | train loss: 1.430436e-03 | valid loss: 1.531727e-03 
      	| train loss (relative): 2.801062e-02 | valid loss (relative): 2.997890e-02 
Epoch 1101 use: 370.87 second.

epoch 1102 starting......
Epoch:  1102 | train loss: 1.430826e-03 | valid loss: 1.518128e-03 
      	| train loss (relative): 2.801052e-02 | valid loss (relative): 2.972903e-02 
Epoch 1102 use: 351.61 second.

epoch 1103 starting......
Epoch:  1103 | train loss: 1.427120e-03 | valid loss: 1.515383e-03 
      	| train loss (relative): 2.794109e-02 | valid loss (relative): 2.969490e-02 
Epoch 1103 use: 368.69 second.

epoch 1104 starting......
Epoch:  1104 | train loss: 1.424002e-03 | valid loss: 1.517817e-03 
      	| train loss (relative): 2.788389e-02 | valid loss (relative): 2.978655e-02 
Epoch 1104 use: 360.72 second.

epoch 1105 starting......
Epoch:  1105 | train loss: 1.427133e-03 | valid loss: 1.515010e-03 
      	| train loss (relative): 2.794377e-02 | valid loss (relative): 2.974827e-02 
Epoch 1105 use: 357.62 second.

epoch 1106 starting......
Epoch:  1106 | train loss: 1.423765e-03 | valid loss: 1.519385e-03 
      	| train loss (relative): 2.787438e-02 | valid loss (relative): 2.983425e-02 
Epoch 1106 use: 360.35 second.

epoch 1107 starting......
Epoch:  1107 | train loss: 1.427208e-03 | valid loss: 1.527665e-03 
      	| train loss (relative): 2.794481e-02 | valid loss (relative): 2.984650e-02 
Epoch 1107 use: 368.18 second.

epoch 1108 starting......
Epoch:  1108 | train loss: 1.427596e-03 | valid loss: 1.520884e-03 
      	| train loss (relative): 2.794742e-02 | valid loss (relative): 2.967015e-02 
Epoch 1108 use: 369.85 second.

epoch 1109 starting......
Epoch:  1109 | train loss: 1.423270e-03 | valid loss: 1.517056e-03 
      	| train loss (relative): 2.786825e-02 | valid loss (relative): 2.979336e-02 
Epoch 1109 use: 351.75 second.

epoch 1110 starting......
Epoch:  1110 | train loss: 1.422446e-03 | valid loss: 1.516304e-03 
      	| train loss (relative): 2.784692e-02 | valid loss (relative): 2.983262e-02 
Epoch 1110 use: 365.51 second.

epoch 1111 starting......
Epoch:  1111 | train loss: 1.422287e-03 | valid loss: 1.531852e-03 
      	| train loss (relative): 2.784748e-02 | valid loss (relative): 3.020910e-02 
Epoch 1111 use: 333.62 second.

epoch 1112 starting......
Epoch:  1112 | train loss: 1.424137e-03 | valid loss: 1.519119e-03 
      	| train loss (relative): 2.788572e-02 | valid loss (relative): 2.989888e-02 
Epoch 1112 use: 361.12 second.

epoch 1113 starting......
Epoch:  1113 | train loss: 1.423830e-03 | valid loss: 1.525655e-03 
      	| train loss (relative): 2.788152e-02 | valid loss (relative): 2.985752e-02 
Epoch 1113 use: 346.04 second.

epoch 1114 starting......
Epoch:  1114 | train loss: 1.422944e-03 | valid loss: 1.516786e-03 
      	| train loss (relative): 2.785957e-02 | valid loss (relative): 2.978221e-02 
Epoch 1114 use: 332.49 second.

epoch 1115 starting......
Epoch:  1115 | train loss: 1.423351e-03 | valid loss: 1.521046e-03 
      	| train loss (relative): 2.786732e-02 | valid loss (relative): 2.984735e-02 
Epoch 1115 use: 354.59 second.

epoch 1116 starting......
Epoch:  1116 | train loss: 1.424076e-03 | valid loss: 1.519433e-03 
      	| train loss (relative): 2.788082e-02 | valid loss (relative): 2.985527e-02 
Epoch 1116 use: 343.62 second.

epoch 1117 starting......
Epoch:  1117 | train loss: 1.422279e-03 | valid loss: 1.521767e-03 
      	| train loss (relative): 2.784381e-02 | valid loss (relative): 3.005870e-02 
Epoch 1117 use: 362.79 second.

epoch 1118 starting......
Epoch:  1118 | train loss: 1.424458e-03 | valid loss: 1.520541e-03 
      	| train loss (relative): 2.789023e-02 | valid loss (relative): 2.987723e-02 
Epoch 1118 use: 357.05 second.

epoch 1119 starting......
Epoch:  1119 | train loss: 1.423043e-03 | valid loss: 1.517816e-03 
      	| train loss (relative): 2.786406e-02 | valid loss (relative): 2.994974e-02 
Epoch 1119 use: 379.01 second.

epoch 1120 starting......
Epoch:  1120 | train loss: 1.425626e-03 | valid loss: 1.527215e-03 
      	| train loss (relative): 2.791564e-02 | valid loss (relative): 3.007379e-02 
Epoch 1120 use: 360.03 second.

epoch 1121 starting......
Epoch:  1121 | train loss: 1.427594e-03 | valid loss: 1.522922e-03 
      	| train loss (relative): 2.795545e-02 | valid loss (relative): 2.985917e-02 
Epoch 1121 use: 359.40 second.

epoch 1122 starting......
Epoch:  1122 | train loss: 1.422873e-03 | valid loss: 1.518360e-03 
      	| train loss (relative): 2.785392e-02 | valid loss (relative): 2.988840e-02 
Epoch 1122 use: 374.71 second.

epoch 1123 starting......
Epoch:  1123 | train loss: 1.420751e-03 | valid loss: 1.517727e-03 
      	| train loss (relative): 2.781277e-02 | valid loss (relative): 2.982615e-02 
Epoch 1123 use: 361.53 second.

epoch 1124 starting......
Epoch:  1124 | train loss: 1.421523e-03 | valid loss: 1.520539e-03 
      	| train loss (relative): 2.783180e-02 | valid loss (relative): 2.982560e-02 
Epoch 1124 use: 373.99 second.

epoch 1125 starting......
Epoch:  1125 | train loss: 1.419826e-03 | valid loss: 1.513026e-03 
      	| train loss (relative): 2.779429e-02 | valid loss (relative): 2.958744e-02 
Epoch 1125 use: 360.49 second.

epoch 1126 starting......
Epoch:  1126 | train loss: 1.418628e-03 | valid loss: 1.513808e-03 
      	| train loss (relative): 2.777213e-02 | valid loss (relative): 2.974994e-02 
Epoch 1126 use: 364.10 second.

epoch 1127 starting......
Epoch:  1127 | train loss: 1.420112e-03 | valid loss: 1.519719e-03 
      	| train loss (relative): 2.780463e-02 | valid loss (relative): 2.985702e-02 
Epoch 1127 use: 380.90 second.

epoch 1128 starting......
Epoch:  1128 | train loss: 1.419273e-03 | valid loss: 1.521133e-03 
      	| train loss (relative): 2.778282e-02 | valid loss (relative): 2.990296e-02 
Epoch 1128 use: 368.01 second.

epoch 1129 starting......
Epoch:  1129 | train loss: 1.421586e-03 | valid loss: 1.523471e-03 
      	| train loss (relative): 2.783021e-02 | valid loss (relative): 3.008917e-02 
Epoch 1129 use: 376.09 second.

epoch 1130 starting......
Epoch:  1130 | train loss: 1.426080e-03 | valid loss: 1.527659e-03 
      	| train loss (relative): 2.791933e-02 | valid loss (relative): 2.993903e-02 
Epoch 1130 use: 369.24 second.

epoch 1131 starting......
Epoch:  1131 | train loss: 1.421658e-03 | valid loss: 1.517486e-03 
      	| train loss (relative): 2.782934e-02 | valid loss (relative): 2.993789e-02 
Epoch 1131 use: 375.60 second.

epoch 1132 starting......
Epoch:  1132 | train loss: 1.419933e-03 | valid loss: 1.518612e-03 
      	| train loss (relative): 2.779973e-02 | valid loss (relative): 2.982691e-02 
Epoch 1132 use: 362.27 second.

epoch 1133 starting......
Epoch:  1133 | train loss: 1.420496e-03 | valid loss: 1.518207e-03 
      	| train loss (relative): 2.780849e-02 | valid loss (relative): 2.982633e-02 
Epoch 1133 use: 364.15 second.

epoch 1134 starting......
Epoch:  1134 | train loss: 1.418053e-03 | valid loss: 1.522759e-03 
      	| train loss (relative): 2.776273e-02 | valid loss (relative): 2.997631e-02 
Epoch 1134 use: 354.23 second.

epoch 1135 starting......
Epoch:  1135 | train loss: 1.420845e-03 | valid loss: 1.517027e-03 
      	| train loss (relative): 2.781748e-02 | valid loss (relative): 2.960049e-02 
Epoch 1135 use: 386.53 second.

epoch 1136 starting......
Epoch:  1136 | train loss: 1.417039e-03 | valid loss: 1.516698e-03 
      	| train loss (relative): 2.773790e-02 | valid loss (relative): 2.969251e-02 
Epoch 1136 use: 375.52 second.

epoch 1137 starting......
Epoch:  1137 | train loss: 1.414625e-03 | valid loss: 1.510815e-03 
      	| train loss (relative): 2.769665e-02 | valid loss (relative): 2.966400e-02 
Epoch 1137 use: 365.98 second.

epoch 1138 starting......
Epoch:  1138 | train loss: 1.417421e-03 | valid loss: 1.516940e-03 
      	| train loss (relative): 2.774290e-02 | valid loss (relative): 2.980537e-02 
Epoch 1138 use: 368.63 second.

epoch 1139 starting......
Epoch:  1139 | train loss: 1.416283e-03 | valid loss: 1.510477e-03 
      	| train loss (relative): 2.772720e-02 | valid loss (relative): 2.962271e-02 
Epoch 1139 use: 365.20 second.

epoch 1140 starting......
Epoch:  1140 | train loss: 1.418004e-03 | valid loss: 1.516456e-03 
      	| train loss (relative): 2.775952e-02 | valid loss (relative): 2.976905e-02 
Epoch 1140 use: 361.12 second.

epoch 1141 starting......
Epoch:  1141 | train loss: 1.415770e-03 | valid loss: 1.518079e-03 
      	| train loss (relative): 2.771277e-02 | valid loss (relative): 2.981196e-02 
Epoch 1141 use: 357.34 second.

epoch 1142 starting......
Epoch:  1142 | train loss: 1.412706e-03 | valid loss: 1.513844e-03 
      	| train loss (relative): 2.765119e-02 | valid loss (relative): 2.977373e-02 
Epoch 1142 use: 364.72 second.

epoch 1143 starting......
Epoch:  1143 | train loss: 1.414753e-03 | valid loss: 1.514564e-03 
      	| train loss (relative): 2.769313e-02 | valid loss (relative): 2.974851e-02 
Epoch 1143 use: 375.37 second.

epoch 1144 starting......
Epoch:  1144 | train loss: 1.415311e-03 | valid loss: 1.514780e-03 
      	| train loss (relative): 2.770394e-02 | valid loss (relative): 2.978906e-02 
Epoch 1144 use: 353.54 second.

epoch 1145 starting......
Epoch:  1145 | train loss: 1.416298e-03 | valid loss: 1.513012e-03 
      	| train loss (relative): 2.772290e-02 | valid loss (relative): 2.974242e-02 
Epoch 1145 use: 348.08 second.

epoch 1146 starting......
Epoch:  1146 | train loss: 1.414475e-03 | valid loss: 1.516019e-03 
      	| train loss (relative): 2.769093e-02 | valid loss (relative): 2.966060e-02 
Epoch 1146 use: 357.96 second.

epoch 1147 starting......
Epoch:  1147 | train loss: 1.414054e-03 | valid loss: 1.512939e-03 
      	| train loss (relative): 2.767678e-02 | valid loss (relative): 2.975282e-02 
Epoch 1147 use: 358.59 second.

epoch 1148 starting......
Epoch:  1148 | train loss: 1.416590e-03 | valid loss: 1.517370e-03 
      	| train loss (relative): 2.773102e-02 | valid loss (relative): 2.969701e-02 
Epoch 1148 use: 365.92 second.

epoch 1149 starting......
Epoch:  1149 | train loss: 1.413989e-03 | valid loss: 1.512032e-03 
      	| train loss (relative): 2.767622e-02 | valid loss (relative): 2.972917e-02 
Epoch 1149 use: 345.99 second.

epoch 1150 starting......
Epoch:  1150 | train loss: 1.411346e-03 | valid loss: 1.507436e-03 
      	| train loss (relative): 2.763003e-02 | valid loss (relative): 2.964793e-02 
Epoch 1150 use: 362.53 second.

epoch 1151 starting......
Epoch:  1151 | train loss: 1.412693e-03 | valid loss: 1.513602e-03 
      	| train loss (relative): 2.765213e-02 | valid loss (relative): 2.994938e-02 
Epoch 1151 use: 379.83 second.

epoch 1152 starting......
Epoch:  1152 | train loss: 1.413561e-03 | valid loss: 1.510969e-03 
      	| train loss (relative): 2.767414e-02 | valid loss (relative): 2.962543e-02 
Epoch 1152 use: 352.13 second.

epoch 1153 starting......
Epoch:  1153 | train loss: 1.411757e-03 | valid loss: 1.510701e-03 
      	| train loss (relative): 2.763328e-02 | valid loss (relative): 2.970416e-02 
Epoch 1153 use: 380.40 second.

epoch 1154 starting......
Epoch:  1154 | train loss: 1.411532e-03 | valid loss: 1.510427e-03 
      	| train loss (relative): 2.762414e-02 | valid loss (relative): 2.976893e-02 
Epoch 1154 use: 350.13 second.

epoch 1155 starting......
Epoch:  1155 | train loss: 1.412289e-03 | valid loss: 1.515201e-03 
      	| train loss (relative): 2.764657e-02 | valid loss (relative): 2.979087e-02 
Epoch 1155 use: 354.54 second.

epoch 1156 starting......
Epoch:  1156 | train loss: 1.408419e-03 | valid loss: 1.513308e-03 
      	| train loss (relative): 2.756715e-02 | valid loss (relative): 2.986276e-02 
Epoch 1156 use: 356.63 second.

epoch 1157 starting......
Epoch:  1157 | train loss: 1.411582e-03 | valid loss: 1.505440e-03 
      	| train loss (relative): 2.762825e-02 | valid loss (relative): 2.964492e-02 
Epoch 1157 use: 370.58 second.

epoch 1158 starting......
Epoch:  1158 | train loss: 1.406874e-03 | valid loss: 1.507884e-03 
      	| train loss (relative): 2.753997e-02 | valid loss (relative): 2.971722e-02 
Epoch 1158 use: 372.17 second.

epoch 1159 starting......
Epoch:  1159 | train loss: 1.408199e-03 | valid loss: 1.509291e-03 
      	| train loss (relative): 2.756504e-02 | valid loss (relative): 2.941987e-02 
Epoch 1159 use: 378.06 second.

epoch 1160 starting......
Epoch:  1160 | train loss: 1.409458e-03 | valid loss: 1.511513e-03 
      	| train loss (relative): 2.758615e-02 | valid loss (relative): 2.973622e-02 
Epoch 1160 use: 360.22 second.

epoch 1161 starting......
Epoch:  1161 | train loss: 1.408570e-03 | valid loss: 1.510328e-03 
      	| train loss (relative): 2.756870e-02 | valid loss (relative): 2.974107e-02 
Epoch 1161 use: 374.81 second.

epoch 1162 starting......
Epoch:  1162 | train loss: 1.407628e-03 | valid loss: 1.508079e-03 
      	| train loss (relative): 2.754916e-02 | valid loss (relative): 2.954387e-02 
Epoch 1162 use: 355.94 second.

epoch 1163 starting......
Epoch:  1163 | train loss: 1.410089e-03 | valid loss: 1.511835e-03 
      	| train loss (relative): 2.759918e-02 | valid loss (relative): 2.962395e-02 
Epoch 1163 use: 370.40 second.

epoch 1164 starting......
Epoch:  1164 | train loss: 1.412025e-03 | valid loss: 1.509841e-03 
      	| train loss (relative): 2.763793e-02 | valid loss (relative): 2.965245e-02 
Epoch 1164 use: 351.30 second.

epoch 1165 starting......
Epoch:  1165 | train loss: 1.410247e-03 | valid loss: 1.506918e-03 
      	| train loss (relative): 2.759859e-02 | valid loss (relative): 2.956889e-02 
Epoch 1165 use: 378.48 second.

epoch 1166 starting......
Epoch:  1166 | train loss: 1.406809e-03 | valid loss: 1.504914e-03 
      	| train loss (relative): 2.753489e-02 | valid loss (relative): 2.947038e-02 
Epoch 1166 use: 361.36 second.

epoch 1167 starting......
Epoch:  1167 | train loss: 1.407348e-03 | valid loss: 1.506934e-03 
      	| train loss (relative): 2.754814e-02 | valid loss (relative): 2.961183e-02 
Epoch 1167 use: 360.18 second.

epoch 1168 starting......
Epoch:  1168 | train loss: 1.408462e-03 | valid loss: 1.515158e-03 
      	| train loss (relative): 2.756690e-02 | valid loss (relative): 2.978652e-02 
Epoch 1168 use: 384.66 second.

epoch 1169 starting......
Epoch:  1169 | train loss: 1.412055e-03 | valid loss: 1.510396e-03 
      	| train loss (relative): 2.764057e-02 | valid loss (relative): 2.961219e-02 
Epoch 1169 use: 363.13 second.

epoch 1170 starting......
Epoch:  1170 | train loss: 1.405139e-03 | valid loss: 1.503620e-03 
      	| train loss (relative): 2.749958e-02 | valid loss (relative): 2.950070e-02 
Epoch 1170 use: 361.05 second.

epoch 1171 starting......
Epoch:  1171 | train loss: 1.403675e-03 | valid loss: 1.506659e-03 
      	| train loss (relative): 2.747250e-02 | valid loss (relative): 2.941655e-02 
Epoch 1171 use: 369.39 second.

epoch 1172 starting......
Epoch:  1172 | train loss: 1.404829e-03 | valid loss: 1.503935e-03 
      	| train loss (relative): 2.749569e-02 | valid loss (relative): 2.951203e-02 
Epoch 1172 use: 363.75 second.

epoch 1173 starting......
Epoch:  1173 | train loss: 1.405618e-03 | valid loss: 1.503307e-03 
      	| train loss (relative): 2.750657e-02 | valid loss (relative): 2.952409e-02 
Epoch 1173 use: 356.80 second.

epoch 1174 starting......
Epoch:  1174 | train loss: 1.404565e-03 | valid loss: 1.513031e-03 
      	| train loss (relative): 2.748722e-02 | valid loss (relative): 2.993627e-02 
Epoch 1174 use: 358.42 second.

epoch 1175 starting......
Epoch:  1175 | train loss: 1.410440e-03 | valid loss: 1.511672e-03 
      	| train loss (relative): 2.761096e-02 | valid loss (relative): 2.958602e-02 
Epoch 1175 use: 358.95 second.

epoch 1176 starting......
Epoch:  1176 | train loss: 1.408789e-03 | valid loss: 1.509071e-03 
      	| train loss (relative): 2.757327e-02 | valid loss (relative): 2.973266e-02 
Epoch 1176 use: 384.35 second.

epoch 1177 starting......
Epoch:  1177 | train loss: 1.405609e-03 | valid loss: 1.504175e-03 
      	| train loss (relative): 2.751263e-02 | valid loss (relative): 2.954501e-02 
Epoch 1177 use: 359.78 second.

epoch 1178 starting......
Epoch:  1178 | train loss: 1.404563e-03 | valid loss: 1.513185e-03 
      	| train loss (relative): 2.748981e-02 | valid loss (relative): 2.970845e-02 
Epoch 1178 use: 372.47 second.

epoch 1179 starting......
Epoch:  1179 | train loss: 1.409362e-03 | valid loss: 1.505192e-03 
      	| train loss (relative): 2.758348e-02 | valid loss (relative): 2.955664e-02 
Epoch 1179 use: 358.37 second.

epoch 1180 starting......
Epoch:  1180 | train loss: 1.407155e-03 | valid loss: 1.508889e-03 
      	| train loss (relative): 2.754496e-02 | valid loss (relative): 2.965324e-02 
Epoch 1180 use: 378.28 second.

epoch 1181 starting......
Epoch:  1181 | train loss: 1.406855e-03 | valid loss: 1.512729e-03 
      	| train loss (relative): 2.753752e-02 | valid loss (relative): 2.960585e-02 
Epoch 1181 use: 379.60 second.

epoch 1182 starting......
Epoch:  1182 | train loss: 1.407528e-03 | valid loss: 1.507194e-03 
      	| train loss (relative): 2.754677e-02 | valid loss (relative): 2.960040e-02 
Epoch 1182 use: 360.44 second.

epoch 1183 starting......
Epoch:  1183 | train loss: 1.405299e-03 | valid loss: 1.508329e-03 
      	| train loss (relative): 2.750414e-02 | valid loss (relative): 2.960988e-02 
Epoch 1183 use: 352.30 second.

epoch 1184 starting......
Epoch:  1184 | train loss: 1.403842e-03 | valid loss: 1.506887e-03 
      	| train loss (relative): 2.747605e-02 | valid loss (relative): 2.964290e-02 
Epoch 1184 use: 337.17 second.

epoch 1185 starting......
Epoch:  1185 | train loss: 1.404222e-03 | valid loss: 1.508710e-03 
      	| train loss (relative): 2.748623e-02 | valid loss (relative): 2.942343e-02 
Epoch 1185 use: 381.91 second.

epoch 1186 starting......
Epoch:  1186 | train loss: 1.406268e-03 | valid loss: 1.510730e-03 
      	| train loss (relative): 2.752133e-02 | valid loss (relative): 2.964115e-02 
Epoch 1186 use: 362.12 second.

epoch 1187 starting......
Epoch:  1187 | train loss: 1.406762e-03 | valid loss: 1.503496e-03 
      	| train loss (relative): 2.752700e-02 | valid loss (relative): 2.950027e-02 
Epoch 1187 use: 358.69 second.

epoch 1188 starting......
Epoch:  1188 | train loss: 1.401822e-03 | valid loss: 1.504580e-03 
      	| train loss (relative): 2.743626e-02 | valid loss (relative): 2.943546e-02 
Epoch 1188 use: 386.64 second.

epoch 1189 starting......
Epoch:  1189 | train loss: 1.402436e-03 | valid loss: 1.503621e-03 
      	| train loss (relative): 2.744515e-02 | valid loss (relative): 2.948313e-02 
Epoch 1189 use: 355.71 second.

epoch 1190 starting......
Epoch:  1190 | train loss: 1.404138e-03 | valid loss: 1.505270e-03 
      	| train loss (relative): 2.748007e-02 | valid loss (relative): 2.950088e-02 
Epoch 1190 use: 383.32 second.

epoch 1191 starting......
Epoch:  1191 | train loss: 1.403649e-03 | valid loss: 1.504529e-03 
      	| train loss (relative): 2.747045e-02 | valid loss (relative): 2.941130e-02 
Epoch 1191 use: 387.53 second.

epoch 1192 starting......
Epoch:  1192 | train loss: 1.402136e-03 | valid loss: 1.506786e-03 
      	| train loss (relative): 2.743668e-02 | valid loss (relative): 2.962483e-02 
Epoch 1192 use: 372.48 second.

epoch 1193 starting......
Epoch:  1193 | train loss: 1.402839e-03 | valid loss: 1.507412e-03 
      	| train loss (relative): 2.745318e-02 | valid loss (relative): 2.967843e-02 
Epoch 1193 use: 380.47 second.

epoch 1194 starting......
Epoch:  1194 | train loss: 1.406722e-03 | valid loss: 1.504421e-03 
      	| train loss (relative): 2.753055e-02 | valid loss (relative): 2.946839e-02 
Epoch 1194 use: 354.14 second.

epoch 1195 starting......
Epoch:  1195 | train loss: 1.400509e-03 | valid loss: 1.507679e-03 
      	| train loss (relative): 2.741074e-02 | valid loss (relative): 2.961145e-02 
Epoch 1195 use: 356.30 second.

epoch 1196 starting......
Epoch:  1196 | train loss: 1.401395e-03 | valid loss: 1.502107e-03 
      	| train loss (relative): 2.742407e-02 | valid loss (relative): 2.944274e-02 
Epoch 1196 use: 381.95 second.

epoch 1197 starting......
Epoch:  1197 | train loss: 1.400281e-03 | valid loss: 1.502611e-03 
      	| train loss (relative): 2.740503e-02 | valid loss (relative): 2.940057e-02 
Epoch 1197 use: 363.63 second.

epoch 1198 starting......
Epoch:  1198 | train loss: 1.398710e-03 | valid loss: 1.507809e-03 
      	| train loss (relative): 2.737110e-02 | valid loss (relative): 2.956042e-02 
Epoch 1198 use: 370.22 second.

epoch 1199 starting......
Epoch:  1199 | train loss: 1.400913e-03 | valid loss: 1.503108e-03 
      	| train loss (relative): 2.741749e-02 | valid loss (relative): 2.951988e-02 
Epoch 1199 use: 367.13 second.

test MSE Error: 1.434056e-03 | relative MSE Error: 2.801754e-02 
 Total time used for training: 15.28 hour.
MESLoss saved to  /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_MSELoss_Latent_64_nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_1200.txt
relative MSELoss saved to  /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_reMSELoss_Latent_64_nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_1200.txt
model saved to /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_64_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_1200.pth
model_dict saved to /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_64_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_1200_dict.pth
... Training slugflow data group 3 completed, Run finished Sun 22 Aug 08:06:02 BST 2021 ...
{'data_dir': '/rds/general/user/jy220/home/tensors/', 'mode': 'train', 'data_type': 'tensors', 'vtu_fields': 'Component1::ComponentMassFractionPhase1, phase1::Velocity', 'variational': 'False', 'optimizer': 'Adamax', 'structured': 'False', 'changing_lr': 'False', 'self_concat': '2', 'dimension': '3', 'sfc_nums': '3', 'components': '4', 'nearest_neighbouring': 'True', 'dims_latent': '64', 'activation': 'Tanh', 'tk_file': '/rds/general/user/jy220/home/slugflow_tk.pt', 'tb_file': '/rds/general/user/jy220/home/slugflow_tb.pt', 'sfc_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/sfcs.pt', 'inv_sfc_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/inv_sfcs.pt', 'coords_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/coords.pt', 'cells_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/cells.npy', 'state_load': '/rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_64_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_1200_dict.pth', 'batch_size': '16', 'lr': '0.001', 'n_epoches': '150', 'seed': '17', 'visualize': 'False', 'reconstructed_path': 'None', 'reconstruct_start_index': '0', 'reconstruct_end_index': '50', 'save_path': '/rds/general/user/jy220/home/new_results/'}
[[ 2.50825269e-01 -1.67256990e-02 -1.22268730e-03]
 [ 2.38611959e-01 -5.50627777e-03 -5.06451913e-03]
 [ 2.39454874e-01 -1.59563257e-02  3.96778323e-03]
 ...
 [ 9.11361096e+00 -2.21908381e-02  2.22384902e-03]
 [ 9.11847310e+00 -2.84346931e-02 -2.34839590e-04]
 [ 9.11039661e+00 -2.51498573e-02 -8.70661044e-03]]
{'tetra': array([[      0,       1,       2,       3],
       [      4,       5,       6,       7],
       [      8,       9,      10,      11],
       ...,
       [1342744, 1342745, 1342746, 1342747],
       [1342748, 1342749, 1342750, 1342751],
       [1342752, 1342753, 1342754, 1342755]])}
vtu fields:  ['Component1::ComponentMassFractionPhase1', 'phase1::Velocity'] 
 structured  False 
 activation  Tanh() 
 self concat  2 
 sfc_nums  3 

dims_latent  64 
 components  4 
 nearest_neighbouring  True 

visualize  False 
 sample number  1706 

reading sfc nums......
[array([  41684,   41685,   41674, ..., 1235560, 1235564, 1235558]), array([954289, 954290, 954291, ..., 420685, 420687, 420688]), array([  40156,   40155,   40158, ..., 1123478, 1123475, 1123467])]
reading inverse sfc nums......
[array([   8146,    8145,    8144, ..., 1274444, 1274445, 1273166]), array([547280, 547283, 593037, ..., 933933, 933932, 864488]), array([ 66437,  66439, 122512, ..., 459416, 415130, 459418])]
tk:  tensor([0.7581, 0.0418, 0.1840, 0.1464], dtype=torch.float64) 

tb:  tensor([-0.3816, -0.3618, -0.0308,  0.1446], dtype=torch.float64) 

length of train set: 1505 

length of valid set: 100 

length of test set: 101 

torch device num: 4 

Let's use 4 GPUs!
epoch 1200 starting......
Epoch:  1200 | train loss: 1.563470e-03 | valid loss: 1.369380e-03 
      	| train loss (relative): 3.059898e-02 | valid loss (relative): 2.663046e-02 
Epoch 1200 use: 510.52 second.

epoch 1201 starting......
Epoch:  1201 | train loss: 1.404256e-03 | valid loss: 1.355945e-03 
      	| train loss (relative): 2.749068e-02 | valid loss (relative): 2.643692e-02 
Epoch 1201 use: 419.32 second.

epoch 1202 starting......
Epoch:  1202 | train loss: 1.396437e-03 | valid loss: 1.354268e-03 
      	| train loss (relative): 2.733451e-02 | valid loss (relative): 2.638990e-02 
Epoch 1202 use: 411.76 second.

epoch 1203 starting......
Epoch:  1203 | train loss: 1.393275e-03 | valid loss: 1.354666e-03 
      	| train loss (relative): 2.726879e-02 | valid loss (relative): 2.635334e-02 
Epoch 1203 use: 414.29 second.

epoch 1204 starting......
Epoch:  1204 | train loss: 1.392987e-03 | valid loss: 1.355575e-03 
      	| train loss (relative): 2.726056e-02 | valid loss (relative): 2.639038e-02 
Epoch 1204 use: 412.36 second.

epoch 1205 starting......
Epoch:  1205 | train loss: 1.393645e-03 | valid loss: 1.356484e-03 
      	| train loss (relative): 2.728034e-02 | valid loss (relative): 2.645653e-02 
Epoch 1205 use: 408.16 second.

epoch 1206 starting......
Epoch:  1206 | train loss: 1.393702e-03 | valid loss: 1.358601e-03 
      	| train loss (relative): 2.727815e-02 | valid loss (relative): 2.649680e-02 
Epoch 1206 use: 408.94 second.

epoch 1207 starting......
Epoch:  1207 | train loss: 1.396311e-03 | valid loss: 1.360531e-03 
      	| train loss (relative): 2.733232e-02 | valid loss (relative): 2.647522e-02 
Epoch 1207 use: 405.50 second.

epoch 1208 starting......
Epoch:  1208 | train loss: 1.395002e-03 | valid loss: 1.365780e-03 
      	| train loss (relative): 2.730544e-02 | valid loss (relative): 2.658559e-02 
Epoch 1208 use: 408.44 second.

epoch 1209 starting......
Epoch:  1209 | train loss: 1.400003e-03 | valid loss: 1.363711e-03 
      	| train loss (relative): 2.740397e-02 | valid loss (relative): 2.654468e-02 
Epoch 1209 use: 475.94 second.

epoch 1210 starting......
Epoch:  1210 | train loss: 1.397755e-03 | valid loss: 1.364092e-03 
      	| train loss (relative): 2.735380e-02 | valid loss (relative): 2.663027e-02 
Epoch 1210 use: 405.97 second.

epoch 1211 starting......
Epoch:  1211 | train loss: 1.399006e-03 | valid loss: 1.367362e-03 
      	| train loss (relative): 2.738312e-02 | valid loss (relative): 2.670462e-02 
Epoch 1211 use: 404.80 second.

epoch 1212 starting......
Epoch:  1212 | train loss: 1.401565e-03 | valid loss: 1.367518e-03 
      	| train loss (relative): 2.743378e-02 | valid loss (relative): 2.656917e-02 
Epoch 1212 use: 408.35 second.

epoch 1213 starting......
Epoch:  1213 | train loss: 1.396480e-03 | valid loss: 1.366147e-03 
      	| train loss (relative): 2.733205e-02 | valid loss (relative): 2.667467e-02 
Epoch 1213 use: 403.63 second.

epoch 1214 starting......
Epoch:  1214 | train loss: 1.398025e-03 | valid loss: 1.371456e-03 
      	| train loss (relative): 2.736143e-02 | valid loss (relative): 2.675535e-02 
Epoch 1214 use: 397.83 second.

epoch 1215 starting......
Epoch:  1215 | train loss: 1.400340e-03 | valid loss: 1.374720e-03 
      	| train loss (relative): 2.741129e-02 | valid loss (relative): 2.666850e-02 
Epoch 1215 use: 399.96 second.

epoch 1216 starting......
Epoch:  1216 | train loss: 1.401709e-03 | valid loss: 1.367838e-03 
      	| train loss (relative): 2.743191e-02 | valid loss (relative): 2.667583e-02 
Epoch 1216 use: 405.70 second.

epoch 1217 starting......
Epoch:  1217 | train loss: 1.399457e-03 | valid loss: 1.369875e-03 
      	| train loss (relative): 2.739297e-02 | valid loss (relative): 2.665855e-02 
Epoch 1217 use: 401.75 second.

epoch 1218 starting......
Epoch:  1218 | train loss: 1.400697e-03 | valid loss: 1.376122e-03 
      	| train loss (relative): 2.741627e-02 | valid loss (relative): 2.672932e-02 
Epoch 1218 use: 402.47 second.

epoch 1219 starting......
Epoch:  1219 | train loss: 1.402869e-03 | valid loss: 1.379349e-03 
      	| train loss (relative): 2.745784e-02 | valid loss (relative): 2.687281e-02 
Epoch 1219 use: 405.63 second.

epoch 1220 starting......
Epoch:  1220 | train loss: 1.402793e-03 | valid loss: 1.372677e-03 
      	| train loss (relative): 2.745641e-02 | valid loss (relative): 2.673611e-02 
Epoch 1220 use: 401.91 second.

epoch 1221 starting......
Epoch:  1221 | train loss: 1.400096e-03 | valid loss: 1.374782e-03 
      	| train loss (relative): 2.740180e-02 | valid loss (relative): 2.674055e-02 
Epoch 1221 use: 410.04 second.

epoch 1222 starting......
Epoch:  1222 | train loss: 1.402101e-03 | valid loss: 1.375494e-03 
      	| train loss (relative): 2.744156e-02 | valid loss (relative): 2.688081e-02 
Epoch 1222 use: 401.14 second.

epoch 1223 starting......
Epoch:  1223 | train loss: 1.402382e-03 | valid loss: 1.377023e-03 
      	| train loss (relative): 2.744721e-02 | valid loss (relative): 2.694922e-02 
Epoch 1223 use: 405.43 second.

epoch 1224 starting......
Epoch:  1224 | train loss: 1.400793e-03 | valid loss: 1.372315e-03 
      	| train loss (relative): 2.741488e-02 | valid loss (relative): 2.677377e-02 
Epoch 1224 use: 406.89 second.

epoch 1225 starting......
Epoch:  1225 | train loss: 1.399547e-03 | valid loss: 1.374886e-03 
      	| train loss (relative): 2.739489e-02 | valid loss (relative): 2.685803e-02 
Epoch 1225 use: 412.08 second.

epoch 1226 starting......
Epoch:  1226 | train loss: 1.401297e-03 | valid loss: 1.375910e-03 
      	| train loss (relative): 2.742296e-02 | valid loss (relative): 2.681936e-02 
Epoch 1226 use: 415.91 second.

epoch 1227 starting......
Epoch:  1227 | train loss: 1.402490e-03 | valid loss: 1.382999e-03 
      	| train loss (relative): 2.744607e-02 | valid loss (relative): 2.698499e-02 
Epoch 1227 use: 427.60 second.

epoch 1228 starting......
Epoch:  1228 | train loss: 1.404683e-03 | valid loss: 1.371560e-03 
      	| train loss (relative): 2.749580e-02 | valid loss (relative): 2.668694e-02 
Epoch 1228 use: 402.15 second.

epoch 1229 starting......
Epoch:  1229 | train loss: 1.398061e-03 | valid loss: 1.370589e-03 
      	| train loss (relative): 2.735986e-02 | valid loss (relative): 2.673545e-02 
Epoch 1229 use: 408.10 second.

epoch 1230 starting......
Epoch:  1230 | train loss: 1.399313e-03 | valid loss: 1.372597e-03 
      	| train loss (relative): 2.738843e-02 | valid loss (relative): 2.668824e-02 
Epoch 1230 use: 404.21 second.

epoch 1231 starting......
Epoch:  1231 | train loss: 1.395632e-03 | valid loss: 1.372532e-03 
      	| train loss (relative): 2.731241e-02 | valid loss (relative): 2.676678e-02 
Epoch 1231 use: 410.81 second.

epoch 1232 starting......
Epoch:  1232 | train loss: 1.395964e-03 | valid loss: 1.373499e-03 
      	| train loss (relative): 2.732072e-02 | valid loss (relative): 2.671552e-02 
Epoch 1232 use: 408.13 second.

epoch 1233 starting......
Epoch:  1233 | train loss: 1.398947e-03 | valid loss: 1.383050e-03 
      	| train loss (relative): 2.737822e-02 | valid loss (relative): 2.682626e-02 
Epoch 1233 use: 403.63 second.

epoch 1234 starting......
Epoch:  1234 | train loss: 1.401339e-03 | valid loss: 1.375745e-03 
      	| train loss (relative): 2.742703e-02 | valid loss (relative): 2.670784e-02 
Epoch 1234 use: 408.91 second.

epoch 1235 starting......
Epoch:  1235 | train loss: 1.400679e-03 | valid loss: 1.371045e-03 
      	| train loss (relative): 2.741017e-02 | valid loss (relative): 2.680027e-02 
Epoch 1235 use: 402.68 second.

epoch 1236 starting......
Epoch:  1236 | train loss: 1.394262e-03 | valid loss: 1.369674e-03 
      	| train loss (relative): 2.728510e-02 | valid loss (relative): 2.663214e-02 
Epoch 1236 use: 403.71 second.

epoch 1237 starting......
Epoch:  1237 | train loss: 1.393663e-03 | valid loss: 1.374387e-03 
      	| train loss (relative): 2.727113e-02 | valid loss (relative): 2.669352e-02 
Epoch 1237 use: 411.62 second.

epoch 1238 starting......
Epoch:  1238 | train loss: 1.395510e-03 | valid loss: 1.374808e-03 
      	| train loss (relative): 2.731216e-02 | valid loss (relative): 2.685064e-02 
Epoch 1238 use: 404.84 second.

epoch 1239 starting......
Epoch:  1239 | train loss: 1.397197e-03 | valid loss: 1.380009e-03 
      	| train loss (relative): 2.734134e-02 | valid loss (relative): 2.682527e-02 
Epoch 1239 use: 409.03 second.

epoch 1240 starting......
Epoch:  1240 | train loss: 1.401428e-03 | valid loss: 1.377448e-03 
      	| train loss (relative): 2.742585e-02 | valid loss (relative): 2.688961e-02 
Epoch 1240 use: 406.08 second.

epoch 1241 starting......
Epoch:  1241 | train loss: 1.398013e-03 | valid loss: 1.379350e-03 
      	| train loss (relative): 2.736003e-02 | valid loss (relative): 2.684195e-02 
Epoch 1241 use: 412.76 second.

epoch 1242 starting......
Epoch:  1242 | train loss: 1.397314e-03 | valid loss: 1.373041e-03 
      	| train loss (relative): 2.734312e-02 | valid loss (relative): 2.676259e-02 
Epoch 1242 use: 402.67 second.

epoch 1243 starting......
Epoch:  1243 | train loss: 1.396741e-03 | valid loss: 1.377251e-03 
      	| train loss (relative): 2.733801e-02 | valid loss (relative): 2.689812e-02 
Epoch 1243 use: 417.32 second.

epoch 1244 starting......
Epoch:  1244 | train loss: 1.398380e-03 | valid loss: 1.375086e-03 
      	| train loss (relative): 2.737229e-02 | valid loss (relative): 2.680535e-02 
Epoch 1244 use: 402.39 second.

epoch 1245 starting......
Epoch:  1245 | train loss: 1.395354e-03 | valid loss: 1.370053e-03 
      	| train loss (relative): 2.730497e-02 | valid loss (relative): 2.671389e-02 
Epoch 1245 use: 407.48 second.

epoch 1246 starting......
Epoch:  1246 | train loss: 1.392327e-03 | valid loss: 1.377599e-03 
      	| train loss (relative): 2.724439e-02 | valid loss (relative): 2.693687e-02 
Epoch 1246 use: 397.66 second.

epoch 1247 starting......
Epoch:  1247 | train loss: 1.394011e-03 | valid loss: 1.375139e-03 
      	| train loss (relative): 2.727469e-02 | valid loss (relative): 2.686875e-02 
Epoch 1247 use: 408.91 second.

epoch 1248 starting......
Epoch:  1248 | train loss: 1.393232e-03 | valid loss: 1.373256e-03 
      	| train loss (relative): 2.726738e-02 | valid loss (relative): 2.672590e-02 
Epoch 1248 use: 405.15 second.

epoch 1249 starting......
Epoch:  1249 | train loss: 1.394725e-03 | valid loss: 1.372342e-03 
      	| train loss (relative): 2.728946e-02 | valid loss (relative): 2.676834e-02 
Epoch 1249 use: 412.18 second.

epoch 1250 starting......
Epoch:  1250 | train loss: 1.392371e-03 | valid loss: 1.376896e-03 
      	| train loss (relative): 2.725418e-02 | valid loss (relative): 2.684653e-02 
Epoch 1250 use: 414.14 second.

epoch 1251 starting......
Epoch:  1251 | train loss: 1.393231e-03 | valid loss: 1.378173e-03 
      	| train loss (relative): 2.726210e-02 | valid loss (relative): 2.684519e-02 
Epoch 1251 use: 409.39 second.

epoch 1252 starting......
Epoch:  1252 | train loss: 1.394404e-03 | valid loss: 1.372352e-03 
      	| train loss (relative): 2.729101e-02 | valid loss (relative): 2.673111e-02 
Epoch 1252 use: 415.77 second.

epoch 1253 starting......
Epoch:  1253 | train loss: 1.392296e-03 | valid loss: 1.382241e-03 
      	| train loss (relative): 2.724737e-02 | valid loss (relative): 2.695932e-02 
Epoch 1253 use: 406.01 second.

epoch 1254 starting......
Epoch:  1254 | train loss: 1.395184e-03 | valid loss: 1.370617e-03 
      	| train loss (relative): 2.730102e-02 | valid loss (relative): 2.687174e-02 
Epoch 1254 use: 400.83 second.

epoch 1255 starting......
Epoch:  1255 | train loss: 1.392010e-03 | valid loss: 1.370635e-03 
      	| train loss (relative): 2.724211e-02 | valid loss (relative): 2.666050e-02 
Epoch 1255 use: 406.75 second.

epoch 1256 starting......
Epoch:  1256 | train loss: 1.390109e-03 | valid loss: 1.370507e-03 
      	| train loss (relative): 2.720034e-02 | valid loss (relative): 2.665801e-02 
Epoch 1256 use: 397.72 second.

epoch 1257 starting......
Epoch:  1257 | train loss: 1.391301e-03 | valid loss: 1.370663e-03 
      	| train loss (relative): 2.721903e-02 | valid loss (relative): 2.676291e-02 
Epoch 1257 use: 405.51 second.

epoch 1258 starting......
Epoch:  1258 | train loss: 1.397156e-03 | valid loss: 1.377727e-03 
      	| train loss (relative): 2.734106e-02 | valid loss (relative): 2.697168e-02 
Epoch 1258 use: 402.49 second.

epoch 1259 starting......
Epoch:  1259 | train loss: 1.393513e-03 | valid loss: 1.370262e-03 
      	| train loss (relative): 2.727084e-02 | valid loss (relative): 2.663006e-02 
Epoch 1259 use: 411.46 second.

epoch 1260 starting......
Epoch:  1260 | train loss: 1.389280e-03 | valid loss: 1.372366e-03 
      	| train loss (relative): 2.718706e-02 | valid loss (relative): 2.670217e-02 
Epoch 1260 use: 403.13 second.

epoch 1261 starting......
Epoch:  1261 | train loss: 1.390015e-03 | valid loss: 1.372955e-03 
      	| train loss (relative): 2.720186e-02 | valid loss (relative): 2.673630e-02 
Epoch 1261 use: 400.56 second.

epoch 1262 starting......
Epoch:  1262 | train loss: 1.393583e-03 | valid loss: 1.369443e-03 
      	| train loss (relative): 2.727328e-02 | valid loss (relative): 2.663902e-02 
Epoch 1262 use: 411.50 second.

epoch 1263 starting......
Epoch:  1263 | train loss: 1.391320e-03 | valid loss: 1.367449e-03 
      	| train loss (relative): 2.722378e-02 | valid loss (relative): 2.664047e-02 
Epoch 1263 use: 405.05 second.

epoch 1264 starting......
Epoch:  1264 | train loss: 1.390829e-03 | valid loss: 1.370171e-03 
      	| train loss (relative): 2.721466e-02 | valid loss (relative): 2.684451e-02 
Epoch 1264 use: 407.60 second.

epoch 1265 starting......
Epoch:  1265 | train loss: 1.391105e-03 | valid loss: 1.370858e-03 
      	| train loss (relative): 2.722495e-02 | valid loss (relative): 2.666632e-02 
Epoch 1265 use: 402.78 second.

epoch 1266 starting......
Epoch:  1266 | train loss: 1.391701e-03 | valid loss: 1.368868e-03 
      	| train loss (relative): 2.723461e-02 | valid loss (relative): 2.667109e-02 
Epoch 1266 use: 409.17 second.

epoch 1267 starting......
Epoch:  1267 | train loss: 1.389583e-03 | valid loss: 1.374134e-03 
      	| train loss (relative): 2.719173e-02 | valid loss (relative): 2.675126e-02 
Epoch 1267 use: 411.24 second.

epoch 1268 starting......
Epoch:  1268 | train loss: 1.389102e-03 | valid loss: 1.368656e-03 
      	| train loss (relative): 2.718111e-02 | valid loss (relative): 2.653245e-02 
Epoch 1268 use: 374.11 second.

epoch 1269 starting......
Epoch:  1269 | train loss: 1.391972e-03 | valid loss: 1.379705e-03 
      	| train loss (relative): 2.723509e-02 | valid loss (relative): 2.691155e-02 
Epoch 1269 use: 405.33 second.

epoch 1270 starting......
Epoch:  1270 | train loss: 1.391100e-03 | valid loss: 1.370362e-03 
      	| train loss (relative): 2.722057e-02 | valid loss (relative): 2.661904e-02 
Epoch 1270 use: 402.91 second.

epoch 1271 starting......
Epoch:  1271 | train loss: 1.389013e-03 | valid loss: 1.372493e-03 
      	| train loss (relative): 2.717734e-02 | valid loss (relative): 2.671480e-02 
Epoch 1271 use: 399.07 second.

epoch 1272 starting......
Epoch:  1272 | train loss: 1.390176e-03 | valid loss: 1.370786e-03 
      	| train loss (relative): 2.720374e-02 | valid loss (relative): 2.674272e-02 
Epoch 1272 use: 409.92 second.

epoch 1273 starting......
Epoch:  1273 | train loss: 1.391249e-03 | valid loss: 1.372268e-03 
      	| train loss (relative): 2.722398e-02 | valid loss (relative): 2.670921e-02 
Epoch 1273 use: 404.20 second.

epoch 1274 starting......
Epoch:  1274 | train loss: 1.390127e-03 | valid loss: 1.367222e-03 
      	| train loss (relative): 2.720497e-02 | valid loss (relative): 2.658473e-02 
Epoch 1274 use: 409.58 second.

epoch 1275 starting......
Epoch:  1275 | train loss: 1.388235e-03 | valid loss: 1.371512e-03 
      	| train loss (relative): 2.716375e-02 | valid loss (relative): 2.682997e-02 
Epoch 1275 use: 396.60 second.

epoch 1276 starting......
Epoch:  1276 | train loss: 1.387630e-03 | valid loss: 1.369157e-03 
      	| train loss (relative): 2.715508e-02 | valid loss (relative): 2.663811e-02 
Epoch 1276 use: 417.95 second.

epoch 1277 starting......
Epoch:  1277 | train loss: 1.384600e-03 | valid loss: 1.369479e-03 
      	| train loss (relative): 2.709194e-02 | valid loss (relative): 2.667163e-02 
Epoch 1277 use: 407.30 second.

epoch 1278 starting......
Epoch:  1278 | train loss: 1.386190e-03 | valid loss: 1.368867e-03 
      	| train loss (relative): 2.712660e-02 | valid loss (relative): 2.663724e-02 
Epoch 1278 use: 412.65 second.

epoch 1279 starting......
Epoch:  1279 | train loss: 1.387345e-03 | valid loss: 1.374492e-03 
      	| train loss (relative): 2.714664e-02 | valid loss (relative): 2.670277e-02 
Epoch 1279 use: 399.82 second.

epoch 1280 starting......
Epoch:  1280 | train loss: 1.387935e-03 | valid loss: 1.367853e-03 
      	| train loss (relative): 2.715680e-02 | valid loss (relative): 2.666303e-02 
Epoch 1280 use: 407.62 second.

epoch 1281 starting......
Epoch:  1281 | train loss: 1.387496e-03 | valid loss: 1.365862e-03 
      	| train loss (relative): 2.715207e-02 | valid loss (relative): 2.666657e-02 
Epoch 1281 use: 397.60 second.

epoch 1282 starting......
Epoch:  1282 | train loss: 1.385160e-03 | valid loss: 1.370229e-03 
      	| train loss (relative): 2.710435e-02 | valid loss (relative): 2.666454e-02 
Epoch 1282 use: 405.58 second.

epoch 1283 starting......
Epoch:  1283 | train loss: 1.386716e-03 | valid loss: 1.365220e-03 
      	| train loss (relative): 2.713480e-02 | valid loss (relative): 2.651880e-02 
Epoch 1283 use: 406.75 second.

epoch 1284 starting......
Epoch:  1284 | train loss: 1.383305e-03 | valid loss: 1.365050e-03 
      	| train loss (relative): 2.706654e-02 | valid loss (relative): 2.666729e-02 
Epoch 1284 use: 406.22 second.

epoch 1285 starting......
Epoch:  1285 | train loss: 1.384264e-03 | valid loss: 1.364176e-03 
      	| train loss (relative): 2.708425e-02 | valid loss (relative): 2.656141e-02 
Epoch 1285 use: 401.72 second.

epoch 1286 starting......
Epoch:  1286 | train loss: 1.382345e-03 | valid loss: 1.378882e-03 
      	| train loss (relative): 2.704687e-02 | valid loss (relative): 2.691900e-02 
Epoch 1286 use: 405.68 second.

epoch 1287 starting......
Epoch:  1287 | train loss: 1.384496e-03 | valid loss: 1.364584e-03 
      	| train loss (relative): 2.708639e-02 | valid loss (relative): 2.659464e-02 
Epoch 1287 use: 400.26 second.

epoch 1288 starting......
Epoch:  1288 | train loss: 1.382598e-03 | valid loss: 1.360636e-03 
      	| train loss (relative): 2.705056e-02 | valid loss (relative): 2.641474e-02 
Epoch 1288 use: 422.28 second.

epoch 1289 starting......
Epoch:  1289 | train loss: 1.382706e-03 | valid loss: 1.365982e-03 
      	| train loss (relative): 2.705201e-02 | valid loss (relative): 2.650625e-02 
Epoch 1289 use: 409.29 second.

epoch 1290 starting......
Epoch:  1290 | train loss: 1.383556e-03 | valid loss: 1.371407e-03 
      	| train loss (relative): 2.707086e-02 | valid loss (relative): 2.678258e-02 
Epoch 1290 use: 418.02 second.

epoch 1291 starting......
Epoch:  1291 | train loss: 1.383516e-03 | valid loss: 1.361601e-03 
      	| train loss (relative): 2.706503e-02 | valid loss (relative): 2.651944e-02 
Epoch 1291 use: 402.12 second.

epoch 1292 starting......
Epoch:  1292 | train loss: 1.380585e-03 | valid loss: 1.363604e-03 
      	| train loss (relative): 2.701057e-02 | valid loss (relative): 2.652001e-02 
Epoch 1292 use: 416.61 second.

epoch 1293 starting......
Epoch:  1293 | train loss: 1.382738e-03 | valid loss: 1.367366e-03 
      	| train loss (relative): 2.705220e-02 | valid loss (relative): 2.655900e-02 
Epoch 1293 use: 404.33 second.

epoch 1294 starting......
Epoch:  1294 | train loss: 1.383738e-03 | valid loss: 1.368240e-03 
      	| train loss (relative): 2.707066e-02 | valid loss (relative): 2.662159e-02 
Epoch 1294 use: 415.65 second.

epoch 1295 starting......
Epoch:  1295 | train loss: 1.384359e-03 | valid loss: 1.368041e-03 
      	| train loss (relative): 2.708759e-02 | valid loss (relative): 2.661906e-02 
Epoch 1295 use: 406.27 second.

epoch 1296 starting......
Epoch:  1296 | train loss: 1.382597e-03 | valid loss: 1.369375e-03 
      	| train loss (relative): 2.705127e-02 | valid loss (relative): 2.673617e-02 
Epoch 1296 use: 415.85 second.

epoch 1297 starting......
Epoch:  1297 | train loss: 1.384368e-03 | valid loss: 1.367336e-03 
      	| train loss (relative): 2.708537e-02 | valid loss (relative): 2.678955e-02 
Epoch 1297 use: 408.21 second.

epoch 1298 starting......
Epoch:  1298 | train loss: 1.382530e-03 | valid loss: 1.369408e-03 
      	| train loss (relative): 2.705332e-02 | valid loss (relative): 2.668514e-02 
Epoch 1298 use: 419.03 second.

epoch 1299 starting......
Epoch:  1299 | train loss: 1.382219e-03 | valid loss: 1.368497e-03 
      	| train loss (relative): 2.704223e-02 | valid loss (relative): 2.664978e-02 
Epoch 1299 use: 412.88 second.

epoch 1300 starting......
Epoch:  1300 | train loss: 1.386566e-03 | valid loss: 1.365993e-03 
      	| train loss (relative): 2.712556e-02 | valid loss (relative): 2.662712e-02 
Epoch 1300 use: 406.56 second.

epoch 1301 starting......
Epoch:  1301 | train loss: 1.381074e-03 | valid loss: 1.364722e-03 
      	| train loss (relative): 2.701963e-02 | valid loss (relative): 2.660220e-02 
Epoch 1301 use: 407.88 second.

epoch 1302 starting......
Epoch:  1302 | train loss: 1.379428e-03 | valid loss: 1.366193e-03 
      	| train loss (relative): 2.698684e-02 | valid loss (relative): 2.658978e-02 
Epoch 1302 use: 419.41 second.

epoch 1303 starting......
Epoch:  1303 | train loss: 1.379260e-03 | valid loss: 1.365029e-03 
      	| train loss (relative): 2.698322e-02 | valid loss (relative): 2.648651e-02 
Epoch 1303 use: 417.10 second.

epoch 1304 starting......
Epoch:  1304 | train loss: 1.383094e-03 | valid loss: 1.367826e-03 
      	| train loss (relative): 2.705747e-02 | valid loss (relative): 2.667097e-02 
Epoch 1304 use: 417.32 second.

epoch 1305 starting......
Epoch:  1305 | train loss: 1.381409e-03 | valid loss: 1.365995e-03 
      	| train loss (relative): 2.702531e-02 | valid loss (relative): 2.650500e-02 
Epoch 1305 use: 407.85 second.

epoch 1306 starting......
Epoch:  1306 | train loss: 1.380562e-03 | valid loss: 1.367379e-03 
      	| train loss (relative): 2.701098e-02 | valid loss (relative): 2.654772e-02 
Epoch 1306 use: 418.54 second.

epoch 1307 starting......
Epoch:  1307 | train loss: 1.381177e-03 | valid loss: 1.367692e-03 
      	| train loss (relative): 2.702173e-02 | valid loss (relative): 2.664181e-02 
Epoch 1307 use: 404.62 second.

epoch 1308 starting......
Epoch:  1308 | train loss: 1.380846e-03 | valid loss: 1.368208e-03 
      	| train loss (relative): 2.701419e-02 | valid loss (relative): 2.655441e-02 
Epoch 1308 use: 408.57 second.

epoch 1309 starting......
Epoch:  1309 | train loss: 1.381685e-03 | valid loss: 1.369721e-03 
      	| train loss (relative): 2.703642e-02 | valid loss (relative): 2.659615e-02 
Epoch 1309 use: 408.35 second.

epoch 1310 starting......
Epoch:  1310 | train loss: 1.380613e-03 | valid loss: 1.366951e-03 
      	| train loss (relative): 2.701018e-02 | valid loss (relative): 2.663503e-02 
Epoch 1310 use: 421.92 second.

epoch 1311 starting......
Epoch:  1311 | train loss: 1.381901e-03 | valid loss: 1.373918e-03 
      	| train loss (relative): 2.703612e-02 | valid loss (relative): 2.675394e-02 
Epoch 1311 use: 407.83 second.

epoch 1312 starting......
Epoch:  1312 | train loss: 1.382008e-03 | valid loss: 1.368250e-03 
      	| train loss (relative): 2.703764e-02 | valid loss (relative): 2.656782e-02 
Epoch 1312 use: 413.89 second.

epoch 1313 starting......
Epoch:  1313 | train loss: 1.377800e-03 | valid loss: 1.368222e-03 
      	| train loss (relative): 2.695607e-02 | valid loss (relative): 2.677457e-02 
Epoch 1313 use: 400.82 second.

epoch 1314 starting......
Epoch:  1314 | train loss: 1.379968e-03 | valid loss: 1.363843e-03 
      	| train loss (relative): 2.699869e-02 | valid loss (relative): 2.664336e-02 
Epoch 1314 use: 420.65 second.

epoch 1315 starting......
Epoch:  1315 | train loss: 1.378292e-03 | valid loss: 1.366284e-03 
      	| train loss (relative): 2.696462e-02 | valid loss (relative): 2.668240e-02 
Epoch 1315 use: 396.21 second.

epoch 1316 starting......
Epoch:  1316 | train loss: 1.379646e-03 | valid loss: 1.365270e-03 
      	| train loss (relative): 2.699036e-02 | valid loss (relative): 2.662452e-02 
Epoch 1316 use: 417.34 second.

epoch 1317 starting......
Epoch:  1317 | train loss: 1.380938e-03 | valid loss: 1.370424e-03 
      	| train loss (relative): 2.701659e-02 | valid loss (relative): 2.665220e-02 
Epoch 1317 use: 393.62 second.

epoch 1318 starting......
Epoch:  1318 | train loss: 1.380074e-03 | valid loss: 1.365520e-03 
      	| train loss (relative): 2.700043e-02 | valid loss (relative): 2.656918e-02 
Epoch 1318 use: 410.66 second.

epoch 1319 starting......
Epoch:  1319 | train loss: 1.376284e-03 | valid loss: 1.363916e-03 
      	| train loss (relative): 2.692743e-02 | valid loss (relative): 2.654742e-02 
Epoch 1319 use: 400.73 second.

epoch 1320 starting......
Epoch:  1320 | train loss: 1.379913e-03 | valid loss: 1.366846e-03 
      	| train loss (relative): 2.699667e-02 | valid loss (relative): 2.655283e-02 
Epoch 1320 use: 419.56 second.

epoch 1321 starting......
Epoch:  1321 | train loss: 1.379077e-03 | valid loss: 1.361320e-03 
      	| train loss (relative): 2.697829e-02 | valid loss (relative): 2.653660e-02 
Epoch 1321 use: 407.74 second.

epoch 1322 starting......
Epoch:  1322 | train loss: 1.378720e-03 | valid loss: 1.361690e-03 
      	| train loss (relative): 2.697358e-02 | valid loss (relative): 2.659930e-02 
Epoch 1322 use: 414.58 second.

epoch 1323 starting......
Epoch:  1323 | train loss: 1.376579e-03 | valid loss: 1.366230e-03 
      	| train loss (relative): 2.692960e-02 | valid loss (relative): 2.648429e-02 
Epoch 1323 use: 413.52 second.

epoch 1324 starting......
Epoch:  1324 | train loss: 1.378421e-03 | valid loss: 1.363460e-03 
      	| train loss (relative): 2.696612e-02 | valid loss (relative): 2.656956e-02 
Epoch 1324 use: 416.23 second.

epoch 1325 starting......
Epoch:  1325 | train loss: 1.379467e-03 | valid loss: 1.360076e-03 
      	| train loss (relative): 2.698347e-02 | valid loss (relative): 2.649985e-02 
Epoch 1325 use: 405.00 second.

epoch 1326 starting......
Epoch:  1326 | train loss: 1.372255e-03 | valid loss: 1.356342e-03 
      	| train loss (relative): 2.684795e-02 | valid loss (relative): 2.638561e-02 
Epoch 1326 use: 422.92 second.

epoch 1327 starting......
Epoch:  1327 | train loss: 1.374124e-03 | valid loss: 1.362007e-03 
      	| train loss (relative): 2.687886e-02 | valid loss (relative): 2.647625e-02 
Epoch 1327 use: 408.56 second.

epoch 1328 starting......
Epoch:  1328 | train loss: 1.374778e-03 | valid loss: 1.361071e-03 
      	| train loss (relative): 2.688907e-02 | valid loss (relative): 2.636142e-02 
Epoch 1328 use: 411.18 second.

epoch 1329 starting......
Epoch:  1329 | train loss: 1.372825e-03 | valid loss: 1.359229e-03 
      	| train loss (relative): 2.685825e-02 | valid loss (relative): 2.640166e-02 
Epoch 1329 use: 410.50 second.

epoch 1330 starting......
Epoch:  1330 | train loss: 1.372802e-03 | valid loss: 1.361634e-03 
      	| train loss (relative): 2.685184e-02 | valid loss (relative): 2.648077e-02 
Epoch 1330 use: 414.24 second.

epoch 1331 starting......
Epoch:  1331 | train loss: 1.374748e-03 | valid loss: 1.358094e-03 
      	| train loss (relative): 2.688967e-02 | valid loss (relative): 2.658015e-02 
Epoch 1331 use: 403.19 second.

epoch 1332 starting......
Epoch:  1332 | train loss: 1.368950e-03 | valid loss: 1.358928e-03 
      	| train loss (relative): 2.678020e-02 | valid loss (relative): 2.640659e-02 
Epoch 1332 use: 417.77 second.

epoch 1333 starting......
Epoch:  1333 | train loss: 1.367476e-03 | valid loss: 1.354194e-03 
      	| train loss (relative): 2.675103e-02 | valid loss (relative): 2.630165e-02 
Epoch 1333 use: 410.65 second.

epoch 1334 starting......
Epoch:  1334 | train loss: 1.368358e-03 | valid loss: 1.358161e-03 
      	| train loss (relative): 2.676412e-02 | valid loss (relative): 2.633512e-02 
Epoch 1334 use: 410.25 second.

epoch 1335 starting......
Epoch:  1335 | train loss: 1.370131e-03 | valid loss: 1.358915e-03 
      	| train loss (relative): 2.680125e-02 | valid loss (relative): 2.647012e-02 
Epoch 1335 use: 414.10 second.

epoch 1336 starting......
Epoch:  1336 | train loss: 1.371308e-03 | valid loss: 1.358277e-03 
      	| train loss (relative): 2.682520e-02 | valid loss (relative): 2.652170e-02 
Epoch 1336 use: 412.53 second.

epoch 1337 starting......
Epoch:  1337 | train loss: 1.370010e-03 | valid loss: 1.354481e-03 
      	| train loss (relative): 2.679554e-02 | valid loss (relative): 2.631316e-02 
Epoch 1337 use: 409.55 second.

epoch 1338 starting......
Epoch:  1338 | train loss: 1.367809e-03 | valid loss: 1.358691e-03 
      	| train loss (relative): 2.675628e-02 | valid loss (relative): 2.644823e-02 
Epoch 1338 use: 415.53 second.

epoch 1339 starting......
Epoch:  1339 | train loss: 1.369349e-03 | valid loss: 1.359050e-03 
      	| train loss (relative): 2.678226e-02 | valid loss (relative): 2.654422e-02 
Epoch 1339 use: 388.25 second.

epoch 1340 starting......
Epoch:  1340 | train loss: 1.372823e-03 | valid loss: 1.359676e-03 
      	| train loss (relative): 2.685173e-02 | valid loss (relative): 2.657429e-02 
Epoch 1340 use: 409.10 second.

epoch 1341 starting......
Epoch:  1341 | train loss: 1.371417e-03 | valid loss: 1.356767e-03 
      	| train loss (relative): 2.682567e-02 | valid loss (relative): 2.646161e-02 
Epoch 1341 use: 413.74 second.

epoch 1342 starting......
Epoch:  1342 | train loss: 1.367567e-03 | valid loss: 1.355771e-03 
      	| train loss (relative): 2.675240e-02 | valid loss (relative): 2.633839e-02 
Epoch 1342 use: 408.77 second.

epoch 1343 starting......
Epoch:  1343 | train loss: 1.370748e-03 | valid loss: 1.356892e-03 
      	| train loss (relative): 2.680940e-02 | valid loss (relative): 2.644997e-02 
Epoch 1343 use: 416.18 second.

epoch 1344 starting......
Epoch:  1344 | train loss: 1.368810e-03 | valid loss: 1.354116e-03 
      	| train loss (relative): 2.677382e-02 | valid loss (relative): 2.642812e-02 
Epoch 1344 use: 411.71 second.

epoch 1345 starting......
Epoch:  1345 | train loss: 1.368983e-03 | valid loss: 1.356545e-03 
      	| train loss (relative): 2.677977e-02 | valid loss (relative): 2.643382e-02 
Epoch 1345 use: 420.34 second.

epoch 1346 starting......
Epoch:  1346 | train loss: 1.370547e-03 | valid loss: 1.356886e-03 
      	| train loss (relative): 2.680748e-02 | valid loss (relative): 2.647246e-02 
Epoch 1346 use: 410.77 second.

epoch 1347 starting......
Epoch:  1347 | train loss: 1.371359e-03 | valid loss: 1.359847e-03 
      	| train loss (relative): 2.682692e-02 | valid loss (relative): 2.646933e-02 
Epoch 1347 use: 421.91 second.

epoch 1348 starting......
Epoch:  1348 | train loss: 1.371496e-03 | valid loss: 1.356893e-03 
      	| train loss (relative): 2.682746e-02 | valid loss (relative): 2.645715e-02 
Epoch 1348 use: 410.07 second.

epoch 1349 starting......
Epoch:  1349 | train loss: 1.369033e-03 | valid loss: 1.359696e-03 
      	| train loss (relative): 2.677678e-02 | valid loss (relative): 2.651492e-02 
Epoch 1349 use: 411.81 second.

test MSE Error: 1.417825e-03 | relative MSE Error: 2.781304e-02 
 Total time used for training: 17.08 hour.
MESLoss saved to  /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_MSELoss_Latent_64_nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_1350.txt
relative MSELoss saved to  /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_reMSELoss_Latent_64_nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_1350.txt
model saved to /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_64_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_1350.pth
model_dict saved to /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_64_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_1350_dict.pth
... Training slugflow data group 3 completed, Run finished Mon 23 Aug 09:32:01 BST 2021 ...
{'data_dir': '/rds/general/user/jy220/home/tensors/', 'mode': 'train', 'data_type': 'tensors', 'vtu_fields': 'Component1::ComponentMassFractionPhase1, phase1::Velocity', 'variational': 'False', 'optimizer': 'Adamax', 'structured': 'False', 'changing_lr': 'False', 'self_concat': '2', 'dimension': '3', 'sfc_nums': '3', 'components': '4', 'nearest_neighbouring': 'True', 'dims_latent': '64', 'activation': 'Tanh', 'tk_file': '/rds/general/user/jy220/home/slugflow_tk.pt', 'tb_file': '/rds/general/user/jy220/home/slugflow_tb.pt', 'sfc_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/sfcs.pt', 'inv_sfc_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/inv_sfcs.pt', 'coords_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/coords.pt', 'cells_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/cells.npy', 'state_load': '/rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_64_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_1350_dict.pth', 'batch_size': '16', 'lr': '0.001', 'n_epoches': '150', 'seed': '17', 'visualize': 'False', 'reconstructed_path': 'None', 'reconstruct_start_index': '0', 'reconstruct_end_index': '50', 'save_path': '/rds/general/user/jy220/home/new_results/'}
[[ 2.50825269e-01 -1.67256990e-02 -1.22268730e-03]
 [ 2.38611959e-01 -5.50627777e-03 -5.06451913e-03]
 [ 2.39454874e-01 -1.59563257e-02  3.96778323e-03]
 ...
 [ 9.11361096e+00 -2.21908381e-02  2.22384902e-03]
 [ 9.11847310e+00 -2.84346931e-02 -2.34839590e-04]
 [ 9.11039661e+00 -2.51498573e-02 -8.70661044e-03]]
{'tetra': array([[      0,       1,       2,       3],
       [      4,       5,       6,       7],
       [      8,       9,      10,      11],
       ...,
       [1342744, 1342745, 1342746, 1342747],
       [1342748, 1342749, 1342750, 1342751],
       [1342752, 1342753, 1342754, 1342755]])}
vtu fields:  ['Component1::ComponentMassFractionPhase1', 'phase1::Velocity'] 
 structured  False 
 activation  Tanh() 
 self concat  2 
 sfc_nums  3 

dims_latent  64 
 components  4 
 nearest_neighbouring  True 

visualize  False 
 sample number  1706 

reading sfc nums......
[array([  41684,   41685,   41674, ..., 1235560, 1235564, 1235558]), array([954289, 954290, 954291, ..., 420685, 420687, 420688]), array([  40156,   40155,   40158, ..., 1123478, 1123475, 1123467])]
reading inverse sfc nums......
[array([   8146,    8145,    8144, ..., 1274444, 1274445, 1273166]), array([547280, 547283, 593037, ..., 933933, 933932, 864488]), array([ 66437,  66439, 122512, ..., 459416, 415130, 459418])]
tk:  tensor([0.7581, 0.0418, 0.1840, 0.1464], dtype=torch.float64) 

tb:  tensor([-0.3816, -0.3618, -0.0308,  0.1446], dtype=torch.float64) 

length of train set: 1505 

length of valid set: 100 

length of test set: 101 

torch device num: 4 

Let's use 4 GPUs!
epoch 1350 starting......
Epoch:  1350 | train loss: 1.535893e-03 | valid loss: 1.370114e-03 
      	| train loss (relative): 3.001688e-02 | valid loss (relative): 2.681442e-02 
Epoch 1350 use: 474.27 second.

epoch 1351 starting......
Epoch:  1351 | train loss: 1.371266e-03 | valid loss: 1.356295e-03 
      	| train loss (relative): 2.682866e-02 | valid loss (relative): 2.650144e-02 
Epoch 1351 use: 453.07 second.

epoch 1352 starting......
Epoch:  1352 | train loss: 1.362358e-03 | valid loss: 1.353889e-03 
      	| train loss (relative): 2.664247e-02 | valid loss (relative): 2.644662e-02 
Epoch 1352 use: 408.69 second.

epoch 1353 starting......
Epoch:  1353 | train loss: 1.359594e-03 | valid loss: 1.354321e-03 
      	| train loss (relative): 2.658825e-02 | valid loss (relative): 2.643978e-02 
Epoch 1353 use: 376.08 second.

epoch 1354 starting......
Epoch:  1354 | train loss: 1.359908e-03 | valid loss: 1.356764e-03 
      	| train loss (relative): 2.659262e-02 | valid loss (relative): 2.653062e-02 
Epoch 1354 use: 389.79 second.

epoch 1355 starting......
Epoch:  1355 | train loss: 1.358306e-03 | valid loss: 1.356752e-03 
      	| train loss (relative): 2.655974e-02 | valid loss (relative): 2.651367e-02 
Epoch 1355 use: 368.07 second.

epoch 1356 starting......
Epoch:  1356 | train loss: 1.358104e-03 | valid loss: 1.358093e-03 
      	| train loss (relative): 2.655427e-02 | valid loss (relative): 2.654935e-02 
Epoch 1356 use: 376.43 second.

epoch 1357 starting......
Epoch:  1357 | train loss: 1.359472e-03 | valid loss: 1.357800e-03 
      	| train loss (relative): 2.657974e-02 | valid loss (relative): 2.653950e-02 
Epoch 1357 use: 380.08 second.

epoch 1358 starting......
Epoch:  1358 | train loss: 1.361092e-03 | valid loss: 1.363559e-03 
      	| train loss (relative): 2.661441e-02 | valid loss (relative): 2.662461e-02 
Epoch 1358 use: 376.75 second.

epoch 1359 starting......
Epoch:  1359 | train loss: 1.365201e-03 | valid loss: 1.369093e-03 
      	| train loss (relative): 2.669610e-02 | valid loss (relative): 2.666637e-02 
Epoch 1359 use: 375.34 second.

epoch 1360 starting......
Epoch:  1360 | train loss: 1.368853e-03 | valid loss: 1.370032e-03 
      	| train loss (relative): 2.676505e-02 | valid loss (relative): 2.681966e-02 
Epoch 1360 use: 373.79 second.

epoch 1361 starting......
Epoch:  1361 | train loss: 1.366312e-03 | valid loss: 1.367149e-03 
      	| train loss (relative): 2.671668e-02 | valid loss (relative): 2.668635e-02 
Epoch 1361 use: 372.46 second.

epoch 1362 starting......
Epoch:  1362 | train loss: 1.366626e-03 | valid loss: 1.367469e-03 
      	| train loss (relative): 2.672028e-02 | valid loss (relative): 2.670327e-02 
Epoch 1362 use: 364.67 second.

epoch 1363 starting......
Epoch:  1363 | train loss: 1.366967e-03 | valid loss: 1.373278e-03 
      	| train loss (relative): 2.673127e-02 | valid loss (relative): 2.680419e-02 
Epoch 1363 use: 386.04 second.

epoch 1364 starting......
Epoch:  1364 | train loss: 1.369088e-03 | valid loss: 1.373754e-03 
      	| train loss (relative): 2.677291e-02 | valid loss (relative): 2.677780e-02 
Epoch 1364 use: 369.17 second.

epoch 1365 starting......
Epoch:  1365 | train loss: 1.370328e-03 | valid loss: 1.373250e-03 
      	| train loss (relative): 2.680174e-02 | valid loss (relative): 2.682994e-02 
Epoch 1365 use: 365.36 second.

epoch 1366 starting......
Epoch:  1366 | train loss: 1.368969e-03 | valid loss: 1.374331e-03 
      	| train loss (relative): 2.676876e-02 | valid loss (relative): 2.683901e-02 
Epoch 1366 use: 373.98 second.

epoch 1367 starting......
Epoch:  1367 | train loss: 1.370376e-03 | valid loss: 1.374132e-03 
      	| train loss (relative): 2.680014e-02 | valid loss (relative): 2.689227e-02 
Epoch 1367 use: 365.13 second.

epoch 1368 starting......
Epoch:  1368 | train loss: 1.369786e-03 | valid loss: 1.374241e-03 
      	| train loss (relative): 2.678896e-02 | valid loss (relative): 2.684933e-02 
Epoch 1368 use: 368.71 second.

epoch 1369 starting......
Epoch:  1369 | train loss: 1.369091e-03 | valid loss: 1.375400e-03 
      	| train loss (relative): 2.677194e-02 | valid loss (relative): 2.686275e-02 
Epoch 1369 use: 376.10 second.

epoch 1370 starting......
Epoch:  1370 | train loss: 1.369491e-03 | valid loss: 1.379347e-03 
      	| train loss (relative): 2.678068e-02 | valid loss (relative): 2.705740e-02 
Epoch 1370 use: 360.81 second.

epoch 1371 starting......
Epoch:  1371 | train loss: 1.373066e-03 | valid loss: 1.373514e-03 
      	| train loss (relative): 2.685044e-02 | valid loss (relative): 2.681168e-02 
Epoch 1371 use: 383.18 second.

epoch 1372 starting......
Epoch:  1372 | train loss: 1.365752e-03 | valid loss: 1.368654e-03 
      	| train loss (relative): 2.670847e-02 | valid loss (relative): 2.670171e-02 
Epoch 1372 use: 379.88 second.

epoch 1373 starting......
Epoch:  1373 | train loss: 1.363673e-03 | valid loss: 1.369275e-03 
      	| train loss (relative): 2.666283e-02 | valid loss (relative): 2.672066e-02 
Epoch 1373 use: 382.11 second.

epoch 1374 starting......
Epoch:  1374 | train loss: 1.366566e-03 | valid loss: 1.375149e-03 
      	| train loss (relative): 2.672010e-02 | valid loss (relative): 2.682879e-02 
Epoch 1374 use: 370.24 second.

epoch 1375 starting......
Epoch:  1375 | train loss: 1.365435e-03 | valid loss: 1.378593e-03 
      	| train loss (relative): 2.670153e-02 | valid loss (relative): 2.682381e-02 
Epoch 1375 use: 375.91 second.

epoch 1376 starting......
Epoch:  1376 | train loss: 1.368689e-03 | valid loss: 1.376871e-03 
      	| train loss (relative): 2.676002e-02 | valid loss (relative): 2.704394e-02 
Epoch 1376 use: 366.74 second.

epoch 1377 starting......
Epoch:  1377 | train loss: 1.369943e-03 | valid loss: 1.369571e-03 
      	| train loss (relative): 2.679038e-02 | valid loss (relative): 2.673204e-02 
Epoch 1377 use: 366.91 second.

epoch 1378 starting......
Epoch:  1378 | train loss: 1.362667e-03 | valid loss: 1.371165e-03 
      	| train loss (relative): 2.664297e-02 | valid loss (relative): 2.679798e-02 
Epoch 1378 use: 380.96 second.

epoch 1379 starting......
Epoch:  1379 | train loss: 1.364791e-03 | valid loss: 1.374224e-03 
      	| train loss (relative): 2.668594e-02 | valid loss (relative): 2.682236e-02 
Epoch 1379 use: 375.75 second.

epoch 1380 starting......
Epoch:  1380 | train loss: 1.363736e-03 | valid loss: 1.369686e-03 
      	| train loss (relative): 2.666481e-02 | valid loss (relative): 2.677668e-02 
Epoch 1380 use: 378.57 second.

epoch 1381 starting......
Epoch:  1381 | train loss: 1.361450e-03 | valid loss: 1.370587e-03 
      	| train loss (relative): 2.662175e-02 | valid loss (relative): 2.678881e-02 
Epoch 1381 use: 387.37 second.

epoch 1382 starting......
Epoch:  1382 | train loss: 1.361741e-03 | valid loss: 1.382330e-03 
      	| train loss (relative): 2.662244e-02 | valid loss (relative): 2.700797e-02 
Epoch 1382 use: 368.91 second.

epoch 1383 starting......
Epoch:  1383 | train loss: 1.368331e-03 | valid loss: 1.369977e-03 
      	| train loss (relative): 2.675505e-02 | valid loss (relative): 2.668101e-02 
Epoch 1383 use: 374.97 second.

epoch 1384 starting......
Epoch:  1384 | train loss: 1.364324e-03 | valid loss: 1.373441e-03 
      	| train loss (relative): 2.667384e-02 | valid loss (relative): 2.684938e-02 
Epoch 1384 use: 372.36 second.

epoch 1385 starting......
Epoch:  1385 | train loss: 1.363392e-03 | valid loss: 1.371359e-03 
      	| train loss (relative): 2.665890e-02 | valid loss (relative): 2.678866e-02 
Epoch 1385 use: 357.86 second.

epoch 1386 starting......
Epoch:  1386 | train loss: 1.360022e-03 | valid loss: 1.371969e-03 
      	| train loss (relative): 2.659259e-02 | valid loss (relative): 2.683055e-02 
Epoch 1386 use: 374.01 second.

epoch 1387 starting......
Epoch:  1387 | train loss: 1.359164e-03 | valid loss: 1.368147e-03 
      	| train loss (relative): 2.657347e-02 | valid loss (relative): 2.668813e-02 
Epoch 1387 use: 373.86 second.

epoch 1388 starting......
Epoch:  1388 | train loss: 1.359765e-03 | valid loss: 1.369780e-03 
      	| train loss (relative): 2.658227e-02 | valid loss (relative): 2.689187e-02 
Epoch 1388 use: 368.93 second.

epoch 1389 starting......
Epoch:  1389 | train loss: 1.359718e-03 | valid loss: 1.371935e-03 
      	| train loss (relative): 2.658478e-02 | valid loss (relative): 2.675385e-02 
Epoch 1389 use: 397.63 second.

epoch 1390 starting......
Epoch:  1390 | train loss: 1.361227e-03 | valid loss: 1.371638e-03 
      	| train loss (relative): 2.661565e-02 | valid loss (relative): 2.677042e-02 
Epoch 1390 use: 374.60 second.

epoch 1391 starting......
Epoch:  1391 | train loss: 1.360471e-03 | valid loss: 1.377600e-03 
      	| train loss (relative): 2.659794e-02 | valid loss (relative): 2.694827e-02 
Epoch 1391 use: 374.30 second.

epoch 1392 starting......
Epoch:  1392 | train loss: 1.364744e-03 | valid loss: 1.373565e-03 
      	| train loss (relative): 2.668651e-02 | valid loss (relative): 2.685775e-02 
Epoch 1392 use: 372.52 second.

epoch 1393 starting......
Epoch:  1393 | train loss: 1.365639e-03 | valid loss: 1.372889e-03 
      	| train loss (relative): 2.670111e-02 | valid loss (relative): 2.683385e-02 
Epoch 1393 use: 367.97 second.

epoch 1394 starting......
Epoch:  1394 | train loss: 1.361282e-03 | valid loss: 1.370084e-03 
      	| train loss (relative): 2.661485e-02 | valid loss (relative): 2.677725e-02 
Epoch 1394 use: 369.84 second.

epoch 1395 starting......
Epoch:  1395 | train loss: 1.357868e-03 | valid loss: 1.368149e-03 
      	| train loss (relative): 2.655039e-02 | valid loss (relative): 2.664942e-02 
Epoch 1395 use: 369.79 second.

epoch 1396 starting......
Epoch:  1396 | train loss: 1.358916e-03 | valid loss: 1.371419e-03 
      	| train loss (relative): 2.656596e-02 | valid loss (relative): 2.670033e-02 
Epoch 1396 use: 363.15 second.

epoch 1397 starting......
Epoch:  1397 | train loss: 1.357399e-03 | valid loss: 1.369744e-03 
      	| train loss (relative): 2.653806e-02 | valid loss (relative): 2.678103e-02 
Epoch 1397 use: 381.34 second.

epoch 1398 starting......
Epoch:  1398 | train loss: 1.360134e-03 | valid loss: 1.372540e-03 
      	| train loss (relative): 2.659430e-02 | valid loss (relative): 2.674020e-02 
Epoch 1398 use: 363.49 second.

epoch 1399 starting......
Epoch:  1399 | train loss: 1.359490e-03 | valid loss: 1.375281e-03 
      	| train loss (relative): 2.658098e-02 | valid loss (relative): 2.688842e-02 
Epoch 1399 use: 360.68 second.

epoch 1400 starting......
Epoch:  1400 | train loss: 1.359363e-03 | valid loss: 1.369180e-03 
      	| train loss (relative): 2.657428e-02 | valid loss (relative): 2.677201e-02 
Epoch 1400 use: 384.26 second.

epoch 1401 starting......
Epoch:  1401 | train loss: 1.358359e-03 | valid loss: 1.375183e-03 
      	| train loss (relative): 2.656012e-02 | valid loss (relative): 2.691905e-02 
Epoch 1401 use: 368.17 second.

epoch 1402 starting......
Epoch:  1402 | train loss: 1.360698e-03 | valid loss: 1.376127e-03 
      	| train loss (relative): 2.660538e-02 | valid loss (relative): 2.681514e-02 
Epoch 1402 use: 373.87 second.

epoch 1403 starting......
Epoch:  1403 | train loss: 1.365350e-03 | valid loss: 1.379879e-03 
      	| train loss (relative): 2.669543e-02 | valid loss (relative): 2.692834e-02 
Epoch 1403 use: 375.16 second.

epoch 1404 starting......
Epoch:  1404 | train loss: 1.358836e-03 | valid loss: 1.375137e-03 
      	| train loss (relative): 2.657301e-02 | valid loss (relative): 2.687346e-02 
Epoch 1404 use: 371.39 second.

epoch 1405 starting......
Epoch:  1405 | train loss: 1.359974e-03 | valid loss: 1.371918e-03 
      	| train loss (relative): 2.658881e-02 | valid loss (relative): 2.695698e-02 
Epoch 1405 use: 367.92 second.

epoch 1406 starting......
Epoch:  1406 | train loss: 1.359617e-03 | valid loss: 1.375647e-03 
      	| train loss (relative): 2.657920e-02 | valid loss (relative): 2.689127e-02 
Epoch 1406 use: 375.65 second.

epoch 1407 starting......
Epoch:  1407 | train loss: 1.361760e-03 | valid loss: 1.370171e-03 
      	| train loss (relative): 2.662371e-02 | valid loss (relative): 2.673581e-02 
Epoch 1407 use: 366.89 second.

epoch 1408 starting......
Epoch:  1408 | train loss: 1.358169e-03 | valid loss: 1.371962e-03 
      	| train loss (relative): 2.655162e-02 | valid loss (relative): 2.681167e-02 
Epoch 1408 use: 375.68 second.

epoch 1409 starting......
Epoch:  1409 | train loss: 1.361057e-03 | valid loss: 1.376055e-03 
      	| train loss (relative): 2.660842e-02 | valid loss (relative): 2.690913e-02 
Epoch 1409 use: 362.93 second.

epoch 1410 starting......
Epoch:  1410 | train loss: 1.364177e-03 | valid loss: 1.374698e-03 
      	| train loss (relative): 2.667415e-02 | valid loss (relative): 2.696271e-02 
Epoch 1410 use: 372.47 second.

epoch 1411 starting......
Epoch:  1411 | train loss: 1.361679e-03 | valid loss: 1.371597e-03 
      	| train loss (relative): 2.662094e-02 | valid loss (relative): 2.681130e-02 
Epoch 1411 use: 385.23 second.

epoch 1412 starting......
Epoch:  1412 | train loss: 1.359609e-03 | valid loss: 1.374951e-03 
      	| train loss (relative): 2.658239e-02 | valid loss (relative): 2.684881e-02 
Epoch 1412 use: 364.67 second.

epoch 1413 starting......
Epoch:  1413 | train loss: 1.360514e-03 | valid loss: 1.376736e-03 
      	| train loss (relative): 2.659712e-02 | valid loss (relative): 2.689329e-02 
Epoch 1413 use: 379.84 second.

epoch 1414 starting......
Epoch:  1414 | train loss: 1.359725e-03 | valid loss: 1.376535e-03 
      	| train loss (relative): 2.658426e-02 | valid loss (relative): 2.677737e-02 
Epoch 1414 use: 365.24 second.

epoch 1415 starting......
Epoch:  1415 | train loss: 1.360101e-03 | valid loss: 1.375070e-03 
      	| train loss (relative): 2.659237e-02 | valid loss (relative): 2.685799e-02 
Epoch 1415 use: 361.92 second.

epoch 1416 starting......
Epoch:  1416 | train loss: 1.359071e-03 | valid loss: 1.374097e-03 
      	| train loss (relative): 2.657094e-02 | valid loss (relative): 2.678629e-02 
Epoch 1416 use: 382.50 second.

epoch 1417 starting......
Epoch:  1417 | train loss: 1.358800e-03 | valid loss: 1.375418e-03 
      	| train loss (relative): 2.656569e-02 | valid loss (relative): 2.695707e-02 
Epoch 1417 use: 372.40 second.

epoch 1418 starting......
Epoch:  1418 | train loss: 1.358088e-03 | valid loss: 1.371680e-03 
      	| train loss (relative): 2.655100e-02 | valid loss (relative): 2.684410e-02 
Epoch 1418 use: 369.06 second.

epoch 1419 starting......
Epoch:  1419 | train loss: 1.358434e-03 | valid loss: 1.369328e-03 
      	| train loss (relative): 2.655586e-02 | valid loss (relative): 2.674517e-02 
Epoch 1419 use: 359.86 second.

epoch 1420 starting......
Epoch:  1420 | train loss: 1.357757e-03 | valid loss: 1.371683e-03 
      	| train loss (relative): 2.654500e-02 | valid loss (relative): 2.686140e-02 
Epoch 1420 use: 373.97 second.

epoch 1421 starting......
Epoch:  1421 | train loss: 1.360492e-03 | valid loss: 1.373111e-03 
      	| train loss (relative): 2.659670e-02 | valid loss (relative): 2.692565e-02 
Epoch 1421 use: 373.68 second.

epoch 1422 starting......
Epoch:  1422 | train loss: 1.360240e-03 | valid loss: 1.375555e-03 
      	| train loss (relative): 2.659779e-02 | valid loss (relative): 2.682309e-02 
Epoch 1422 use: 370.46 second.

epoch 1423 starting......
Epoch:  1423 | train loss: 1.360950e-03 | valid loss: 1.371797e-03 
      	| train loss (relative): 2.661322e-02 | valid loss (relative): 2.679561e-02 
Epoch 1423 use: 364.75 second.

epoch 1424 starting......
Epoch:  1424 | train loss: 1.355933e-03 | valid loss: 1.371131e-03 
      	| train loss (relative): 2.650784e-02 | valid loss (relative): 2.677938e-02 
Epoch 1424 use: 366.11 second.

epoch 1425 starting......
Epoch:  1425 | train loss: 1.356276e-03 | valid loss: 1.372246e-03 
      	| train loss (relative): 2.651915e-02 | valid loss (relative): 2.670319e-02 
Epoch 1425 use: 376.16 second.

epoch 1426 starting......
Epoch:  1426 | train loss: 1.358231e-03 | valid loss: 1.370617e-03 
      	| train loss (relative): 2.655215e-02 | valid loss (relative): 2.673117e-02 
Epoch 1426 use: 360.76 second.

epoch 1427 starting......
Epoch:  1427 | train loss: 1.357277e-03 | valid loss: 1.373174e-03 
      	| train loss (relative): 2.653470e-02 | valid loss (relative): 2.690648e-02 
Epoch 1427 use: 376.02 second.

epoch 1428 starting......
Epoch:  1428 | train loss: 1.352647e-03 | valid loss: 1.366457e-03 
      	| train loss (relative): 2.644185e-02 | valid loss (relative): 2.669864e-02 
Epoch 1428 use: 365.07 second.

epoch 1429 starting......
Epoch:  1429 | train loss: 1.350836e-03 | valid loss: 1.368519e-03 
      	| train loss (relative): 2.640774e-02 | valid loss (relative): 2.679023e-02 
Epoch 1429 use: 370.84 second.

epoch 1430 starting......
Epoch:  1430 | train loss: 1.355094e-03 | valid loss: 1.369131e-03 
      	| train loss (relative): 2.649083e-02 | valid loss (relative): 2.673969e-02 
Epoch 1430 use: 385.23 second.

epoch 1431 starting......
Epoch:  1431 | train loss: 1.355209e-03 | valid loss: 1.374530e-03 
      	| train loss (relative): 2.649429e-02 | valid loss (relative): 2.690371e-02 
Epoch 1431 use: 363.60 second.

epoch 1432 starting......
Epoch:  1432 | train loss: 1.357422e-03 | valid loss: 1.372287e-03 
      	| train loss (relative): 2.653989e-02 | valid loss (relative): 2.684487e-02 
Epoch 1432 use: 363.07 second.

epoch 1433 starting......
Epoch:  1433 | train loss: 1.355631e-03 | valid loss: 1.366730e-03 
      	| train loss (relative): 2.650454e-02 | valid loss (relative): 2.666089e-02 
Epoch 1433 use: 366.80 second.

epoch 1434 starting......
Epoch:  1434 | train loss: 1.353588e-03 | valid loss: 1.367418e-03 
      	| train loss (relative): 2.646258e-02 | valid loss (relative): 2.676721e-02 
Epoch 1434 use: 365.29 second.

epoch 1435 starting......
Epoch:  1435 | train loss: 1.351862e-03 | valid loss: 1.373634e-03 
      	| train loss (relative): 2.643393e-02 | valid loss (relative): 2.661578e-02 
Epoch 1435 use: 369.57 second.

epoch 1436 starting......
Epoch:  1436 | train loss: 1.355606e-03 | valid loss: 1.370880e-03 
      	| train loss (relative): 2.650030e-02 | valid loss (relative): 2.677808e-02 
Epoch 1436 use: 370.66 second.

epoch 1437 starting......
Epoch:  1437 | train loss: 1.357625e-03 | valid loss: 1.377522e-03 
      	| train loss (relative): 2.654295e-02 | valid loss (relative): 2.681214e-02 
Epoch 1437 use: 379.64 second.

epoch 1438 starting......
Epoch:  1438 | train loss: 1.358060e-03 | valid loss: 1.374015e-03 
      	| train loss (relative): 2.655054e-02 | valid loss (relative): 2.685558e-02 
Epoch 1438 use: 359.00 second.

epoch 1439 starting......
Epoch:  1439 | train loss: 1.353775e-03 | valid loss: 1.375580e-03 
      	| train loss (relative): 2.646791e-02 | valid loss (relative): 2.683066e-02 
Epoch 1439 use: 365.76 second.

epoch 1440 starting......
Epoch:  1440 | train loss: 1.355310e-03 | valid loss: 1.373325e-03 
      	| train loss (relative): 2.649393e-02 | valid loss (relative): 2.689613e-02 
Epoch 1440 use: 372.75 second.

epoch 1441 starting......
Epoch:  1441 | train loss: 1.353608e-03 | valid loss: 1.367827e-03 
      	| train loss (relative): 2.646169e-02 | valid loss (relative): 2.680895e-02 
Epoch 1441 use: 370.37 second.

epoch 1442 starting......
Epoch:  1442 | train loss: 1.352383e-03 | valid loss: 1.371954e-03 
      	| train loss (relative): 2.643713e-02 | valid loss (relative): 2.683241e-02 
Epoch 1442 use: 362.61 second.

epoch 1443 starting......
Epoch:  1443 | train loss: 1.353647e-03 | valid loss: 1.369244e-03 
      	| train loss (relative): 2.646151e-02 | valid loss (relative): 2.672466e-02 
Epoch 1443 use: 363.00 second.

epoch 1444 starting......
Epoch:  1444 | train loss: 1.353740e-03 | valid loss: 1.369727e-03 
      	| train loss (relative): 2.646643e-02 | valid loss (relative): 2.678209e-02 
Epoch 1444 use: 363.19 second.

epoch 1445 starting......
Epoch:  1445 | train loss: 1.357426e-03 | valid loss: 1.369392e-03 
      	| train loss (relative): 2.653530e-02 | valid loss (relative): 2.665514e-02 
Epoch 1445 use: 356.61 second.

epoch 1446 starting......
Epoch:  1446 | train loss: 1.351918e-03 | valid loss: 1.371986e-03 
      	| train loss (relative): 2.642604e-02 | valid loss (relative): 2.681779e-02 
Epoch 1446 use: 359.63 second.

epoch 1447 starting......
Epoch:  1447 | train loss: 1.351336e-03 | valid loss: 1.371612e-03 
      	| train loss (relative): 2.641710e-02 | valid loss (relative): 2.677134e-02 
Epoch 1447 use: 371.05 second.

epoch 1448 starting......
Epoch:  1448 | train loss: 1.353266e-03 | valid loss: 1.369922e-03 
      	| train loss (relative): 2.645653e-02 | valid loss (relative): 2.681048e-02 
Epoch 1448 use: 364.59 second.

epoch 1449 starting......
Epoch:  1449 | train loss: 1.354291e-03 | valid loss: 1.372759e-03 
      	| train loss (relative): 2.647120e-02 | valid loss (relative): 2.679505e-02 
Epoch 1449 use: 365.10 second.

epoch 1450 starting......
Epoch:  1450 | train loss: 1.352622e-03 | valid loss: 1.368914e-03 
      	| train loss (relative): 2.643972e-02 | valid loss (relative): 2.670486e-02 
Epoch 1450 use: 378.69 second.

epoch 1451 starting......
Epoch:  1451 | train loss: 1.348625e-03 | valid loss: 1.370433e-03 
      	| train loss (relative): 2.636125e-02 | valid loss (relative): 2.675561e-02 
Epoch 1451 use: 380.30 second.

epoch 1452 starting......
Epoch:  1452 | train loss: 1.353128e-03 | valid loss: 1.364507e-03 
      	| train loss (relative): 2.645188e-02 | valid loss (relative): 2.668693e-02 
Epoch 1452 use: 368.70 second.

epoch 1453 starting......
Epoch:  1453 | train loss: 1.348593e-03 | valid loss: 1.366782e-03 
      	| train loss (relative): 2.636441e-02 | valid loss (relative): 2.662252e-02 
Epoch 1453 use: 358.46 second.

epoch 1454 starting......
Epoch:  1454 | train loss: 1.346805e-03 | valid loss: 1.361161e-03 
      	| train loss (relative): 2.632788e-02 | valid loss (relative): 2.660755e-02 
Epoch 1454 use: 370.29 second.

epoch 1455 starting......
Epoch:  1455 | train loss: 1.347653e-03 | valid loss: 1.367238e-03 
      	| train loss (relative): 2.634408e-02 | valid loss (relative): 2.666807e-02 
Epoch 1455 use: 358.16 second.

epoch 1456 starting......
Epoch:  1456 | train loss: 1.349723e-03 | valid loss: 1.365940e-03 
      	| train loss (relative): 2.638220e-02 | valid loss (relative): 2.667689e-02 
Epoch 1456 use: 357.08 second.

epoch 1457 starting......
Epoch:  1457 | train loss: 1.348726e-03 | valid loss: 1.367872e-03 
      	| train loss (relative): 2.636576e-02 | valid loss (relative): 2.674644e-02 
Epoch 1457 use: 363.73 second.

epoch 1458 starting......
Epoch:  1458 | train loss: 1.350282e-03 | valid loss: 1.363713e-03 
      	| train loss (relative): 2.639402e-02 | valid loss (relative): 2.667477e-02 
Epoch 1458 use: 374.01 second.

epoch 1459 starting......
Epoch:  1459 | train loss: 1.347367e-03 | valid loss: 1.363473e-03 
      	| train loss (relative): 2.634048e-02 | valid loss (relative): 2.655521e-02 
Epoch 1459 use: 348.39 second.

epoch 1460 starting......
Epoch:  1460 | train loss: 1.346146e-03 | valid loss: 1.368413e-03 
      	| train loss (relative): 2.631013e-02 | valid loss (relative): 2.684538e-02 
Epoch 1460 use: 357.16 second.

epoch 1461 starting......
Epoch:  1461 | train loss: 1.347428e-03 | valid loss: 1.364775e-03 
      	| train loss (relative): 2.634070e-02 | valid loss (relative): 2.662355e-02 
Epoch 1461 use: 349.28 second.

epoch 1462 starting......
Epoch:  1462 | train loss: 1.347484e-03 | valid loss: 1.372079e-03 
      	| train loss (relative): 2.634005e-02 | valid loss (relative): 2.679109e-02 
Epoch 1462 use: 355.81 second.

epoch 1463 starting......
Epoch:  1463 | train loss: 1.348226e-03 | valid loss: 1.365012e-03 
      	| train loss (relative): 2.635965e-02 | valid loss (relative): 2.661368e-02 
Epoch 1463 use: 363.15 second.

epoch 1464 starting......
Epoch:  1464 | train loss: 1.350743e-03 | valid loss: 1.368875e-03 
      	| train loss (relative): 2.640145e-02 | valid loss (relative): 2.675415e-02 
Epoch 1464 use: 357.67 second.

epoch 1465 starting......
Epoch:  1465 | train loss: 1.349960e-03 | valid loss: 1.367016e-03 
      	| train loss (relative): 2.638817e-02 | valid loss (relative): 2.664511e-02 
Epoch 1465 use: 370.13 second.

epoch 1466 starting......
Epoch:  1466 | train loss: 1.348512e-03 | valid loss: 1.366887e-03 
      	| train loss (relative): 2.635846e-02 | valid loss (relative): 2.674259e-02 
Epoch 1466 use: 368.10 second.

epoch 1467 starting......
Epoch:  1467 | train loss: 1.348863e-03 | valid loss: 1.362892e-03 
      	| train loss (relative): 2.636652e-02 | valid loss (relative): 2.661402e-02 
Epoch 1467 use: 362.13 second.

epoch 1468 starting......
Epoch:  1468 | train loss: 1.346716e-03 | valid loss: 1.364940e-03 
      	| train loss (relative): 2.632448e-02 | valid loss (relative): 2.657010e-02 
Epoch 1468 use: 396.23 second.

epoch 1469 starting......
Epoch:  1469 | train loss: 1.350913e-03 | valid loss: 1.372409e-03 
      	| train loss (relative): 2.641031e-02 | valid loss (relative): 2.679828e-02 
Epoch 1469 use: 362.28 second.

epoch 1470 starting......
Epoch:  1470 | train loss: 1.349279e-03 | valid loss: 1.363514e-03 
      	| train loss (relative): 2.637479e-02 | valid loss (relative): 2.662041e-02 
Epoch 1470 use: 349.60 second.

epoch 1471 starting......
Epoch:  1471 | train loss: 1.345733e-03 | valid loss: 1.368220e-03 
      	| train loss (relative): 2.630366e-02 | valid loss (relative): 2.678092e-02 
Epoch 1471 use: 349.37 second.

epoch 1472 starting......
Epoch:  1472 | train loss: 1.345299e-03 | valid loss: 1.360516e-03 
      	| train loss (relative): 2.629840e-02 | valid loss (relative): 2.655441e-02 
Epoch 1472 use: 349.03 second.

epoch 1473 starting......
Epoch:  1473 | train loss: 1.343055e-03 | valid loss: 1.359258e-03 
      	| train loss (relative): 2.625096e-02 | valid loss (relative): 2.654417e-02 
Epoch 1473 use: 336.90 second.

epoch 1474 starting......
Epoch:  1474 | train loss: 1.344011e-03 | valid loss: 1.364418e-03 
      	| train loss (relative): 2.626867e-02 | valid loss (relative): 2.654915e-02 
Epoch 1474 use: 366.56 second.

epoch 1475 starting......
Epoch:  1475 | train loss: 1.343941e-03 | valid loss: 1.363730e-03 
      	| train loss (relative): 2.626526e-02 | valid loss (relative): 2.661031e-02 
Epoch 1475 use: 360.08 second.

epoch 1476 starting......
Epoch:  1476 | train loss: 1.344393e-03 | valid loss: 1.364915e-03 
      	| train loss (relative): 2.627590e-02 | valid loss (relative): 2.666849e-02 
Epoch 1476 use: 363.02 second.

epoch 1477 starting......
Epoch:  1477 | train loss: 1.347493e-03 | valid loss: 1.373502e-03 
      	| train loss (relative): 2.633977e-02 | valid loss (relative): 2.672079e-02 
Epoch 1477 use: 341.79 second.

epoch 1478 starting......
Epoch:  1478 | train loss: 1.350947e-03 | valid loss: 1.367661e-03 
      	| train loss (relative): 2.640505e-02 | valid loss (relative): 2.672011e-02 
Epoch 1478 use: 371.18 second.

epoch 1479 starting......
Epoch:  1479 | train loss: 1.349257e-03 | valid loss: 1.367669e-03 
      	| train loss (relative): 2.637529e-02 | valid loss (relative): 2.667871e-02 
Epoch 1479 use: 363.29 second.

epoch 1480 starting......
Epoch:  1480 | train loss: 1.347735e-03 | valid loss: 1.367363e-03 
      	| train loss (relative): 2.634230e-02 | valid loss (relative): 2.679674e-02 
Epoch 1480 use: 372.05 second.

epoch 1481 starting......
Epoch:  1481 | train loss: 1.347008e-03 | valid loss: 1.364443e-03 
      	| train loss (relative): 2.632908e-02 | valid loss (relative): 2.673976e-02 
Epoch 1481 use: 354.13 second.

epoch 1482 starting......
Epoch:  1482 | train loss: 1.343622e-03 | valid loss: 1.364631e-03 
      	| train loss (relative): 2.626056e-02 | valid loss (relative): 2.669707e-02 
Epoch 1482 use: 352.58 second.

epoch 1483 starting......
Epoch:  1483 | train loss: 1.345838e-03 | valid loss: 1.363569e-03 
      	| train loss (relative): 2.630349e-02 | valid loss (relative): 2.666261e-02 
Epoch 1483 use: 343.02 second.

epoch 1484 starting......
Epoch:  1484 | train loss: 1.347695e-03 | valid loss: 1.368009e-03 
      	| train loss (relative): 2.634740e-02 | valid loss (relative): 2.672604e-02 
Epoch 1484 use: 350.56 second.

epoch 1485 starting......
Epoch:  1485 | train loss: 1.349171e-03 | valid loss: 1.362451e-03 
      	| train loss (relative): 2.637356e-02 | valid loss (relative): 2.658227e-02 
Epoch 1485 use: 349.60 second.

epoch 1486 starting......
Epoch:  1486 | train loss: 1.344254e-03 | valid loss: 1.361621e-03 
      	| train loss (relative): 2.627553e-02 | valid loss (relative): 2.658521e-02 
Epoch 1486 use: 355.01 second.

epoch 1487 starting......
Epoch:  1487 | train loss: 1.344849e-03 | valid loss: 1.363404e-03 
      	| train loss (relative): 2.628679e-02 | valid loss (relative): 2.651100e-02 
Epoch 1487 use: 345.89 second.

epoch 1488 starting......
Epoch:  1488 | train loss: 1.342021e-03 | valid loss: 1.363293e-03 
      	| train loss (relative): 2.622771e-02 | valid loss (relative): 2.658327e-02 
Epoch 1488 use: 358.39 second.

epoch 1489 starting......
Epoch:  1489 | train loss: 1.343467e-03 | valid loss: 1.361391e-03 
      	| train loss (relative): 2.625700e-02 | valid loss (relative): 2.665748e-02 
Epoch 1489 use: 354.80 second.

epoch 1490 starting......
Epoch:  1490 | train loss: 1.342153e-03 | valid loss: 1.365966e-03 
      	| train loss (relative): 2.623269e-02 | valid loss (relative): 2.668809e-02 
Epoch 1490 use: 366.03 second.

epoch 1491 starting......
Epoch:  1491 | train loss: 1.346465e-03 | valid loss: 1.367939e-03 
      	| train loss (relative): 2.631800e-02 | valid loss (relative): 2.673005e-02 
Epoch 1491 use: 355.32 second.

epoch 1492 starting......
Epoch:  1492 | train loss: 1.345739e-03 | valid loss: 1.365196e-03 
      	| train loss (relative): 2.630559e-02 | valid loss (relative): 2.677876e-02 
Epoch 1492 use: 369.71 second.

epoch 1493 starting......
Epoch:  1493 | train loss: 1.344548e-03 | valid loss: 1.364836e-03 
      	| train loss (relative): 2.627965e-02 | valid loss (relative): 2.669189e-02 
Epoch 1493 use: 353.91 second.

epoch 1494 starting......
Epoch:  1494 | train loss: 1.344569e-03 | valid loss: 1.359800e-03 
      	| train loss (relative): 2.627921e-02 | valid loss (relative): 2.662875e-02 
Epoch 1494 use: 363.68 second.

epoch 1495 starting......
Epoch:  1495 | train loss: 1.341426e-03 | valid loss: 1.363998e-03 
      	| train loss (relative): 2.621983e-02 | valid loss (relative): 2.652987e-02 
Epoch 1495 use: 348.04 second.

epoch 1496 starting......
Epoch:  1496 | train loss: 1.344205e-03 | valid loss: 1.364404e-03 
      	| train loss (relative): 2.627215e-02 | valid loss (relative): 2.652911e-02 
Epoch 1496 use: 365.42 second.

epoch 1497 starting......
Epoch:  1497 | train loss: 1.344541e-03 | valid loss: 1.364575e-03 
      	| train loss (relative): 2.627937e-02 | valid loss (relative): 2.662952e-02 
Epoch 1497 use: 357.02 second.

epoch 1498 starting......
Epoch:  1498 | train loss: 1.345832e-03 | valid loss: 1.365909e-03 
      	| train loss (relative): 2.630519e-02 | valid loss (relative): 2.669204e-02 
Epoch 1498 use: 370.46 second.

epoch 1499 starting......
Epoch:  1499 | train loss: 1.343152e-03 | valid loss: 1.358451e-03 
      	| train loss (relative): 2.625347e-02 | valid loss (relative): 2.650491e-02 
Epoch 1499 use: 351.55 second.

test MSE Error: 1.320304e-03 | relative MSE Error: 2.582397e-02 
 Total time used for training: 15.38 hour.
MESLoss saved to  /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_MSELoss_Latent_64_nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_1500.txt
relative MSELoss saved to  /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_reMSELoss_Latent_64_nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_1500.txt
model saved to /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_64_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_1500.pth
model_dict saved to /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_64_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_1500_dict.pth
... Training slugflow data group 3 completed, Run finished Tue 24 Aug 15:00:49 BST 2021 ...
{'data_dir': '/rds/general/user/jy220/home/tensors/', 'mode': 'train', 'data_type': 'tensors', 'vtu_fields': 'Component1::ComponentMassFractionPhase1, phase1::Velocity', 'variational': 'False', 'optimizer': 'Adamax', 'structured': 'False', 'changing_lr': 'False', 'self_concat': '2', 'dimension': '3', 'sfc_nums': '3', 'components': '4', 'nearest_neighbouring': 'True', 'dims_latent': '64', 'activation': 'Tanh', 'tk_file': '/rds/general/user/jy220/home/slugflow_tk.pt', 'tb_file': '/rds/general/user/jy220/home/slugflow_tb.pt', 'sfc_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/sfcs.pt', 'inv_sfc_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/inv_sfcs.pt', 'coords_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/coords.pt', 'cells_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/cells.npy', 'state_load': '/rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_64_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_1500_dict.pth', 'batch_size': '16', 'lr': '0.001', 'n_epoches': '1', 'seed': '17', 'visualize': 'False', 'reconstructed_path': 'None', 'reconstruct_start_index': '0', 'reconstruct_end_index': '50', 'save_path': '/rds/general/user/jy220/home/new_results/'}
[[ 2.50825269e-01 -1.67256990e-02 -1.22268730e-03]
 [ 2.38611959e-01 -5.50627777e-03 -5.06451913e-03]
 [ 2.39454874e-01 -1.59563257e-02  3.96778323e-03]
 ...
 [ 9.11361096e+00 -2.21908381e-02  2.22384902e-03]
 [ 9.11847310e+00 -2.84346931e-02 -2.34839590e-04]
 [ 9.11039661e+00 -2.51498573e-02 -8.70661044e-03]]
{'tetra': array([[      0,       1,       2,       3],
       [      4,       5,       6,       7],
       [      8,       9,      10,      11],
       ...,
       [1342744, 1342745, 1342746, 1342747],
       [1342748, 1342749, 1342750, 1342751],
       [1342752, 1342753, 1342754, 1342755]])}
vtu fields:  ['Component1::ComponentMassFractionPhase1', 'phase1::Velocity'] 
 structured  False 
 activation  Tanh() 
 self concat  2 
 sfc_nums  3 

dims_latent  64 
 components  4 
 nearest_neighbouring  True 

visualize  False 
 sample number  1706 

reading sfc nums......
[array([  41684,   41685,   41674, ..., 1235560, 1235564, 1235558]), array([954289, 954290, 954291, ..., 420685, 420687, 420688]), array([  40156,   40155,   40158, ..., 1123478, 1123475, 1123467])]
reading inverse sfc nums......
[array([   8146,    8145,    8144, ..., 1274444, 1274445, 1273166]), array([547280, 547283, 593037, ..., 933933, 933932, 864488]), array([ 66437,  66439, 122512, ..., 459416, 415130, 459418])]
tk:  tensor([0.7581, 0.0418, 0.1840, 0.1464], dtype=torch.float64) 

tb:  tensor([-0.3816, -0.3618, -0.0308,  0.1446], dtype=torch.float64) 

length of train set: 1505 

length of valid set: 100 

length of test set: 101 

torch device num: 4 

Let's use 4 GPUs!
... Training slugflow data group 3 completed, Run finished Tue 24 Aug 19:15:40 BST 2021 ...
{'data_dir': '/rds/general/user/jy220/home/tensors/', 'mode': 'train', 'data_type': 'tensors', 'vtu_fields': 'Component1::ComponentMassFractionPhase1, phase1::Velocity', 'variational': 'False', 'optimizer': 'Adamax', 'structured': 'False', 'changing_lr': 'False', 'self_concat': '2', 'dimension': '3', 'sfc_nums': '3', 'components': '4', 'nearest_neighbouring': 'True', 'dims_latent': '64', 'activation': 'Tanh', 'tk_file': '/rds/general/user/jy220/home/slugflow_tk.pt', 'tb_file': '/rds/general/user/jy220/home/slugflow_tb.pt', 'sfc_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/sfcs.pt', 'inv_sfc_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/inv_sfcs.pt', 'coords_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/coords.pt', 'cells_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/cells.npy', 'state_load': '/rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_64_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_1500_dict.pth', 'batch_size': '16', 'lr': '0.001', 'n_epoches': '1', 'seed': '17', 'visualize': 'False', 'reconstructed_path': 'None', 'reconstruct_start_index': '0', 'reconstruct_end_index': '50', 'save_path': '/rds/general/user/jy220/home/new_results/'}
[[ 2.50825269e-01 -1.67256990e-02 -1.22268730e-03]
 [ 2.38611959e-01 -5.50627777e-03 -5.06451913e-03]
 [ 2.39454874e-01 -1.59563257e-02  3.96778323e-03]
 ...
 [ 9.11361096e+00 -2.21908381e-02  2.22384902e-03]
 [ 9.11847310e+00 -2.84346931e-02 -2.34839590e-04]
 [ 9.11039661e+00 -2.51498573e-02 -8.70661044e-03]]
{'tetra': array([[      0,       1,       2,       3],
       [      4,       5,       6,       7],
       [      8,       9,      10,      11],
       ...,
       [1342744, 1342745, 1342746, 1342747],
       [1342748, 1342749, 1342750, 1342751],
       [1342752, 1342753, 1342754, 1342755]])}
vtu fields:  ['Component1::ComponentMassFractionPhase1', 'phase1::Velocity'] 
 structured  False 
 activation  Tanh() 
 self concat  2 
 sfc_nums  3 

dims_latent  64 
 components  4 
 nearest_neighbouring  True 

visualize  False 
 sample number  1706 

reading sfc nums......
[array([  41684,   41685,   41674, ..., 1235560, 1235564, 1235558]), array([954289, 954290, 954291, ..., 420685, 420687, 420688]), array([  40156,   40155,   40158, ..., 1123478, 1123475, 1123467])]
reading inverse sfc nums......
[array([   8146,    8145,    8144, ..., 1274444, 1274445, 1273166]), array([547280, 547283, 593037, ..., 933933, 933932, 864488]), array([ 66437,  66439, 122512, ..., 459416, 415130, 459418])]
tk:  tensor([0.7581, 0.0418, 0.1840, 0.1464], dtype=torch.float64) 

tb:  tensor([-0.3816, -0.3618, -0.0308,  0.1446], dtype=torch.float64) 

length of train set: 1505 

length of valid set: 100 

length of test set: 101 

torch device num: 4 

Let's use 4 GPUs!
epoch 1500 starting......
Epoch:  1500 | train loss: 1.477083e-03 | valid loss: 1.342870e-03 
      	| train loss (relative): 2.889721e-02 | valid loss (relative): 2.628724e-02 
Epoch 1500 use: 462.12 second.

test MSE Error: 1.309842e-03 | relative MSE Error: 2.558303e-02 
 Total time used for training: 0.14 hour.
MESLoss saved to  /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_MSELoss_Latent_64_nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_1501.txt
relative MSELoss saved to  /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_reMSELoss_Latent_64_nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_1501.txt
model saved to /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_64_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_1501.pth
model_dict saved to /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_64_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_1501_dict.pth
... Training slugflow data group 3 completed, Run finished Tue 24 Aug 23:28:11 BST 2021 ...
{'data_dir': '/rds/general/user/jy220/home/tensors/', 'mode': 'train', 'data_type': 'tensors', 'vtu_fields': 'Component1::ComponentMassFractionPhase1, phase1::Velocity', 'variational': 'False', 'optimizer': 'Adamax', 'structured': 'False', 'changing_lr': 'False', 'self_concat': '2', 'dimension': '3', 'sfc_nums': '3', 'components': '4', 'nearest_neighbouring': 'True', 'dims_latent': '64', 'activation': 'Tanh', 'tk_file': '/rds/general/user/jy220/home/slugflow_tk.pt', 'tb_file': '/rds/general/user/jy220/home/slugflow_tb.pt', 'sfc_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/sfcs.pt', 'inv_sfc_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/inv_sfcs.pt', 'coords_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/coords.pt', 'cells_file': '/rds/general/user/jy220/home/SFC-CAE-Ready-to-use/cells.npy', 'state_load': '/rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_64_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_1501_dict.pth', 'batch_size': '16', 'lr': '0.001', 'n_epoches': '150', 'seed': '17', 'visualize': 'False', 'reconstructed_path': 'None', 'reconstruct_start_index': '0', 'reconstruct_end_index': '50', 'save_path': '/rds/general/user/jy220/home/new_results/'}
[[ 2.50825269e-01 -1.67256990e-02 -1.22268730e-03]
 [ 2.38611959e-01 -5.50627777e-03 -5.06451913e-03]
 [ 2.39454874e-01 -1.59563257e-02  3.96778323e-03]
 ...
 [ 9.11361096e+00 -2.21908381e-02  2.22384902e-03]
 [ 9.11847310e+00 -2.84346931e-02 -2.34839590e-04]
 [ 9.11039661e+00 -2.51498573e-02 -8.70661044e-03]]
{'tetra': array([[      0,       1,       2,       3],
       [      4,       5,       6,       7],
       [      8,       9,      10,      11],
       ...,
       [1342744, 1342745, 1342746, 1342747],
       [1342748, 1342749, 1342750, 1342751],
       [1342752, 1342753, 1342754, 1342755]])}
vtu fields:  ['Component1::ComponentMassFractionPhase1', 'phase1::Velocity'] 
 structured  False 
 activation  Tanh() 
 self concat  2 
 sfc_nums  3 

dims_latent  64 
 components  4 
 nearest_neighbouring  True 

visualize  False 
 sample number  1706 

reading sfc nums......
[array([  41684,   41685,   41674, ..., 1235560, 1235564, 1235558]), array([954289, 954290, 954291, ..., 420685, 420687, 420688]), array([  40156,   40155,   40158, ..., 1123478, 1123475, 1123467])]
reading inverse sfc nums......
[array([   8146,    8145,    8144, ..., 1274444, 1274445, 1273166]), array([547280, 547283, 593037, ..., 933933, 933932, 864488]), array([ 66437,  66439, 122512, ..., 459416, 415130, 459418])]
tk:  tensor([0.7581, 0.0418, 0.1840, 0.1464], dtype=torch.float64) 

tb:  tensor([-0.3816, -0.3618, -0.0308,  0.1446], dtype=torch.float64) 

length of train set: 1505 

length of valid set: 100 

length of test set: 101 

torch device num: 4 

Let's use 4 GPUs!
epoch 1501 starting......
Epoch:  1501 | train loss: 1.336700e-03 | valid loss: 1.324308e-03 
      	| train loss (relative): 2.613042e-02 | valid loss (relative): 2.589654e-02 
Epoch 1501 use: 555.11 second.

epoch 1502 starting......
Epoch:  1502 | train loss: 1.329478e-03 | valid loss: 1.324809e-03 
      	| train loss (relative): 2.598357e-02 | valid loss (relative): 2.590819e-02 
Epoch 1502 use: 486.66 second.

epoch 1503 starting......
Epoch:  1503 | train loss: 1.326990e-03 | valid loss: 1.326157e-03 
      	| train loss (relative): 2.593659e-02 | valid loss (relative): 2.591469e-02 
Epoch 1503 use: 473.28 second.

epoch 1504 starting......
Epoch:  1504 | train loss: 1.326260e-03 | valid loss: 1.325916e-03 
      	| train loss (relative): 2.591780e-02 | valid loss (relative): 2.590139e-02 
Epoch 1504 use: 497.06 second.

epoch 1505 starting......
Epoch:  1505 | train loss: 1.326948e-03 | valid loss: 1.326530e-03 
      	| train loss (relative): 2.593338e-02 | valid loss (relative): 2.592336e-02 
Epoch 1505 use: 426.52 second.

epoch 1506 starting......
Epoch:  1506 | train loss: 1.327307e-03 | valid loss: 1.329181e-03 
      	| train loss (relative): 2.593981e-02 | valid loss (relative): 2.605443e-02 
Epoch 1506 use: 467.59 second.

epoch 1507 starting......
Epoch:  1507 | train loss: 1.330140e-03 | valid loss: 1.333712e-03 
      	| train loss (relative): 2.599758e-02 | valid loss (relative): 2.610921e-02 
Epoch 1507 use: 461.17 second.

epoch 1508 starting......
Epoch:  1508 | train loss: 1.333916e-03 | valid loss: 1.341184e-03 
      	| train loss (relative): 2.607167e-02 | valid loss (relative): 2.618273e-02 
Epoch 1508 use: 514.70 second.

epoch 1509 starting......
Epoch:  1509 | train loss: 1.339061e-03 | valid loss: 1.340825e-03 
      	| train loss (relative): 2.617406e-02 | valid loss (relative): 2.624229e-02 
Epoch 1509 use: 469.82 second.

epoch 1510 starting......
Epoch:  1510 | train loss: 1.335938e-03 | valid loss: 1.340306e-03 
      	| train loss (relative): 2.611237e-02 | valid loss (relative): 2.617013e-02 
Epoch 1510 use: 490.64 second.

epoch 1511 starting......
Epoch:  1511 | train loss: 1.335052e-03 | valid loss: 1.339452e-03 
      	| train loss (relative): 2.608916e-02 | valid loss (relative): 2.619458e-02 
Epoch 1511 use: 461.02 second.

epoch 1512 starting......
Epoch:  1512 | train loss: 1.334673e-03 | valid loss: 1.345201e-03 
      	| train loss (relative): 2.608935e-02 | valid loss (relative): 2.635290e-02 
Epoch 1512 use: 467.39 second.

epoch 1513 starting......
Epoch:  1513 | train loss: 1.337471e-03 | valid loss: 1.342816e-03 
      	| train loss (relative): 2.613825e-02 | valid loss (relative): 2.625167e-02 
Epoch 1513 use: 456.31 second.

epoch 1514 starting......
Epoch:  1514 | train loss: 1.335788e-03 | valid loss: 1.346935e-03 
      	| train loss (relative): 2.610727e-02 | valid loss (relative): 2.635741e-02 
Epoch 1514 use: 436.74 second.

epoch 1515 starting......
Epoch:  1515 | train loss: 1.338408e-03 | valid loss: 1.348236e-03 
      	| train loss (relative): 2.615917e-02 | valid loss (relative): 2.641153e-02 
Epoch 1515 use: 441.14 second.

epoch 1516 starting......
Epoch:  1516 | train loss: 1.339654e-03 | valid loss: 1.344228e-03 
      	| train loss (relative): 2.618506e-02 | valid loss (relative): 2.627318e-02 
Epoch 1516 use: 442.60 second.

epoch 1517 starting......
Epoch:  1517 | train loss: 1.335662e-03 | valid loss: 1.340733e-03 
      	| train loss (relative): 2.610450e-02 | valid loss (relative): 2.615855e-02 
Epoch 1517 use: 438.99 second.

epoch 1518 starting......
Epoch:  1518 | train loss: 1.333213e-03 | valid loss: 1.340605e-03 
      	| train loss (relative): 2.605573e-02 | valid loss (relative): 2.616538e-02 
Epoch 1518 use: 438.50 second.

epoch 1519 starting......
Epoch:  1519 | train loss: 1.334355e-03 | valid loss: 1.346911e-03 
      	| train loss (relative): 2.607918e-02 | valid loss (relative): 2.621457e-02 
Epoch 1519 use: 486.73 second.

epoch 1520 starting......
Epoch:  1520 | train loss: 1.336309e-03 | valid loss: 1.348103e-03 
      	| train loss (relative): 2.611448e-02 | valid loss (relative): 2.642692e-02 
Epoch 1520 use: 457.59 second.

epoch 1521 starting......
Epoch:  1521 | train loss: 1.336996e-03 | valid loss: 1.357585e-03 
      	| train loss (relative): 2.613192e-02 | valid loss (relative): 2.657493e-02 
Epoch 1521 use: 476.97 second.

epoch 1522 starting......
Epoch:  1522 | train loss: 1.336555e-03 | valid loss: 1.349265e-03 
      	| train loss (relative): 2.612039e-02 | valid loss (relative): 2.634151e-02 
Epoch 1522 use: 451.43 second.

epoch 1523 starting......
Epoch:  1523 | train loss: 1.336945e-03 | valid loss: 1.346900e-03 
      	| train loss (relative): 2.612807e-02 | valid loss (relative): 2.637002e-02 
Epoch 1523 use: 438.84 second.

epoch 1524 starting......
Epoch:  1524 | train loss: 1.340768e-03 | valid loss: 1.349075e-03 
      	| train loss (relative): 2.620091e-02 | valid loss (relative): 2.640773e-02 
Epoch 1524 use: 480.11 second.

epoch 1525 starting......
Epoch:  1525 | train loss: 1.338293e-03 | valid loss: 1.350110e-03 
      	| train loss (relative): 2.615681e-02 | valid loss (relative): 2.633526e-02 
Epoch 1525 use: 459.89 second.

epoch 1526 starting......
Epoch:  1526 | train loss: 1.338386e-03 | valid loss: 1.348285e-03 
      	| train loss (relative): 2.615611e-02 | valid loss (relative): 2.637242e-02 
Epoch 1526 use: 459.71 second.

epoch 1527 starting......
Epoch:  1527 | train loss: 1.336944e-03 | valid loss: 1.348316e-03 
      	| train loss (relative): 2.613200e-02 | valid loss (relative): 2.633167e-02 
Epoch 1527 use: 526.11 second.

epoch 1528 starting......
Epoch:  1528 | train loss: 1.338742e-03 | valid loss: 1.346599e-03 
      	| train loss (relative): 2.616250e-02 | valid loss (relative): 2.621968e-02 
Epoch 1528 use: 493.94 second.

epoch 1529 starting......
Epoch:  1529 | train loss: 1.333697e-03 | valid loss: 1.345397e-03 
      	| train loss (relative): 2.606578e-02 | valid loss (relative): 2.634991e-02 
Epoch 1529 use: 464.14 second.

epoch 1530 starting......
Epoch:  1530 | train loss: 1.332915e-03 | valid loss: 1.347510e-03 
      	| train loss (relative): 2.604655e-02 | valid loss (relative): 2.635661e-02 
Epoch 1530 use: 470.27 second.

epoch 1531 starting......
Epoch:  1531 | train loss: 1.332573e-03 | valid loss: 1.345635e-03 
      	| train loss (relative): 2.604318e-02 | valid loss (relative): 2.635500e-02 
Epoch 1531 use: 440.67 second.

epoch 1532 starting......
Epoch:  1532 | train loss: 1.334275e-03 | valid loss: 1.345528e-03 
      	| train loss (relative): 2.607702e-02 | valid loss (relative): 2.636579e-02 
Epoch 1532 use: 451.78 second.

epoch 1533 starting......
Epoch:  1533 | train loss: 1.330784e-03 | valid loss: 1.347610e-03 
      	| train loss (relative): 2.600863e-02 | valid loss (relative): 2.632151e-02 
Epoch 1533 use: 433.27 second.

epoch 1534 starting......
Epoch:  1534 | train loss: 1.333801e-03 | valid loss: 1.346426e-03 
      	| train loss (relative): 2.606587e-02 | valid loss (relative): 2.632335e-02 
Epoch 1534 use: 449.96 second.

epoch 1535 starting......
Epoch:  1535 | train loss: 1.329602e-03 | valid loss: 1.345487e-03 
      	| train loss (relative): 2.598447e-02 | valid loss (relative): 2.633655e-02 
Epoch 1535 use: 446.50 second.

epoch 1536 starting......
Epoch:  1536 | train loss: 1.330437e-03 | valid loss: 1.344679e-03 
      	| train loss (relative): 2.599953e-02 | valid loss (relative): 2.628038e-02 
Epoch 1536 use: 416.21 second.

epoch 1537 starting......
Epoch:  1537 | train loss: 1.332920e-03 | valid loss: 1.348202e-03 
      	| train loss (relative): 2.604766e-02 | valid loss (relative): 2.639866e-02 
Epoch 1537 use: 440.73 second.

epoch 1538 starting......
Epoch:  1538 | train loss: 1.336377e-03 | valid loss: 1.348678e-03 
      	| train loss (relative): 2.612090e-02 | valid loss (relative): 2.637340e-02 
Epoch 1538 use: 440.37 second.

epoch 1539 starting......
Epoch:  1539 | train loss: 1.330558e-03 | valid loss: 1.345029e-03 
      	| train loss (relative): 2.600040e-02 | valid loss (relative): 2.628003e-02 
Epoch 1539 use: 461.41 second.

epoch 1540 starting......
Epoch:  1540 | train loss: 1.331004e-03 | valid loss: 1.345579e-03 
      	| train loss (relative): 2.601014e-02 | valid loss (relative): 2.635551e-02 
Epoch 1540 use: 529.58 second.

epoch 1541 starting......
Epoch:  1541 | train loss: 1.331070e-03 | valid loss: 1.345944e-03 
      	| train loss (relative): 2.601125e-02 | valid loss (relative): 2.630103e-02 
Epoch 1541 use: 474.11 second.

epoch 1542 starting......
Epoch:  1542 | train loss: 1.330614e-03 | valid loss: 1.352458e-03 
      	| train loss (relative): 2.600260e-02 | valid loss (relative): 2.641331e-02 
Epoch 1542 use: 463.32 second.

epoch 1543 starting......
Epoch:  1543 | train loss: 1.336619e-03 | valid loss: 1.357198e-03 
      	| train loss (relative): 2.611955e-02 | valid loss (relative): 2.662754e-02 
Epoch 1543 use: 443.53 second.

epoch 1544 starting......
Epoch:  1544 | train loss: 1.339297e-03 | valid loss: 1.348087e-03 
      	| train loss (relative): 2.617817e-02 | valid loss (relative): 2.629409e-02 
Epoch 1544 use: 457.04 second.

epoch 1545 starting......
Epoch:  1545 | train loss: 1.329690e-03 | valid loss: 1.345299e-03 
      	| train loss (relative): 2.598693e-02 | valid loss (relative): 2.626819e-02 
Epoch 1545 use: 431.66 second.

epoch 1546 starting......
Epoch:  1546 | train loss: 1.329936e-03 | valid loss: 1.350518e-03 
      	| train loss (relative): 2.598940e-02 | valid loss (relative): 2.638353e-02 
Epoch 1546 use: 441.76 second.

epoch 1547 starting......
Epoch:  1547 | train loss: 1.331156e-03 | valid loss: 1.347592e-03 
      	| train loss (relative): 2.601533e-02 | valid loss (relative): 2.636575e-02 
Epoch 1547 use: 428.09 second.

epoch 1548 starting......
Epoch:  1548 | train loss: 1.334507e-03 | valid loss: 1.353467e-03 
      	| train loss (relative): 2.608098e-02 | valid loss (relative): 2.642271e-02 
Epoch 1548 use: 442.13 second.

epoch 1549 starting......
Epoch:  1549 | train loss: 1.334133e-03 | valid loss: 1.354099e-03 
      	| train loss (relative): 2.606895e-02 | valid loss (relative): 2.646450e-02 
Epoch 1549 use: 442.25 second.

epoch 1550 starting......
Epoch:  1550 | train loss: 1.334345e-03 | valid loss: 1.348268e-03 
      	| train loss (relative): 2.607627e-02 | valid loss (relative): 2.632157e-02 
Epoch 1550 use: 441.40 second.

epoch 1551 starting......
Epoch:  1551 | train loss: 1.331182e-03 | valid loss: 1.350297e-03 
      	| train loss (relative): 2.601330e-02 | valid loss (relative): 2.651002e-02 
Epoch 1551 use: 427.85 second.

epoch 1552 starting......
Epoch:  1552 | train loss: 1.333364e-03 | valid loss: 1.352003e-03 
      	| train loss (relative): 2.605687e-02 | valid loss (relative): 2.648118e-02 
Epoch 1552 use: 445.65 second.

epoch 1553 starting......
Epoch:  1553 | train loss: 1.331516e-03 | valid loss: 1.348836e-03 
      	| train loss (relative): 2.602008e-02 | valid loss (relative): 2.638986e-02 
Epoch 1553 use: 430.09 second.

epoch 1554 starting......
Epoch:  1554 | train loss: 1.334205e-03 | valid loss: 1.351615e-03 
      	| train loss (relative): 2.607230e-02 | valid loss (relative): 2.637513e-02 
Epoch 1554 use: 446.20 second.

epoch 1555 starting......
Epoch:  1555 | train loss: 1.335119e-03 | valid loss: 1.347796e-03 
      	| train loss (relative): 2.608902e-02 | valid loss (relative): 2.634484e-02 
Epoch 1555 use: 437.30 second.

epoch 1556 starting......
Epoch:  1556 | train loss: 1.328403e-03 | valid loss: 1.348335e-03 
      	| train loss (relative): 2.596048e-02 | valid loss (relative): 2.640602e-02 
Epoch 1556 use: 447.51 second.

epoch 1557 starting......
Epoch:  1557 | train loss: 1.327495e-03 | valid loss: 1.350260e-03 
      	| train loss (relative): 2.594198e-02 | valid loss (relative): 2.637884e-02 
Epoch 1557 use: 428.86 second.

epoch 1558 starting......
Epoch:  1558 | train loss: 1.330057e-03 | valid loss: 1.351627e-03 
      	| train loss (relative): 2.599038e-02 | valid loss (relative): 2.651865e-02 
Epoch 1558 use: 441.06 second.

epoch 1559 starting......
Epoch:  1559 | train loss: 1.331426e-03 | valid loss: 1.349603e-03 
      	| train loss (relative): 2.601974e-02 | valid loss (relative): 2.625054e-02 
Epoch 1559 use: 431.83 second.

epoch 1560 starting......
Epoch:  1560 | train loss: 1.327889e-03 | valid loss: 1.346987e-03 
      	| train loss (relative): 2.595031e-02 | valid loss (relative): 2.630378e-02 
Epoch 1560 use: 443.37 second.

epoch 1561 starting......
Epoch:  1561 | train loss: 1.325227e-03 | valid loss: 1.343981e-03 
      	| train loss (relative): 2.589327e-02 | valid loss (relative): 2.620950e-02 
Epoch 1561 use: 433.92 second.

epoch 1562 starting......
Epoch:  1562 | train loss: 1.326447e-03 | valid loss: 1.347686e-03 
      	| train loss (relative): 2.592086e-02 | valid loss (relative): 2.636970e-02 
Epoch 1562 use: 439.42 second.

epoch 1563 starting......
Epoch:  1563 | train loss: 1.327526e-03 | valid loss: 1.351814e-03 
      	| train loss (relative): 2.593774e-02 | valid loss (relative): 2.645120e-02 
Epoch 1563 use: 434.41 second.

epoch 1564 starting......
Epoch:  1564 | train loss: 1.330815e-03 | valid loss: 1.347479e-03 
      	| train loss (relative): 2.600303e-02 | valid loss (relative): 2.622432e-02 
Epoch 1564 use: 439.56 second.

epoch 1565 starting......
Epoch:  1565 | train loss: 1.327566e-03 | valid loss: 1.351188e-03 
      	| train loss (relative): 2.593967e-02 | valid loss (relative): 2.639549e-02 
Epoch 1565 use: 435.80 second.

epoch 1566 starting......
Epoch:  1566 | train loss: 1.327250e-03 | valid loss: 1.346260e-03 
      	| train loss (relative): 2.593564e-02 | valid loss (relative): 2.631648e-02 
Epoch 1566 use: 441.03 second.

epoch 1567 starting......
Epoch:  1567 | train loss: 1.327572e-03 | valid loss: 1.352601e-03 
      	| train loss (relative): 2.594134e-02 | valid loss (relative): 2.637494e-02 
Epoch 1567 use: 441.58 second.

epoch 1568 starting......
Epoch:  1568 | train loss: 1.330080e-03 | valid loss: 1.345401e-03 
      	| train loss (relative): 2.598380e-02 | valid loss (relative): 2.632860e-02 
Epoch 1568 use: 453.72 second.

epoch 1569 starting......
Epoch:  1569 | train loss: 1.325914e-03 | valid loss: 1.345426e-03 
      	| train loss (relative): 2.591328e-02 | valid loss (relative): 2.624647e-02 
Epoch 1569 use: 430.45 second.

epoch 1570 starting......
Epoch:  1570 | train loss: 1.324603e-03 | valid loss: 1.347496e-03 
      	| train loss (relative): 2.588138e-02 | valid loss (relative): 2.635688e-02 
Epoch 1570 use: 445.68 second.

epoch 1571 starting......
Epoch:  1571 | train loss: 1.326890e-03 | valid loss: 1.346668e-03 
      	| train loss (relative): 2.592593e-02 | valid loss (relative): 2.634587e-02 
Epoch 1571 use: 437.74 second.

epoch 1572 starting......
Epoch:  1572 | train loss: 1.326903e-03 | valid loss: 1.350915e-03 
      	| train loss (relative): 2.592896e-02 | valid loss (relative): 2.655153e-02 
Epoch 1572 use: 441.33 second.

epoch 1573 starting......
Epoch:  1573 | train loss: 1.327463e-03 | valid loss: 1.342990e-03 
      	| train loss (relative): 2.594105e-02 | valid loss (relative): 2.620716e-02 
Epoch 1573 use: 439.65 second.

epoch 1574 starting......
Epoch:  1574 | train loss: 1.323765e-03 | valid loss: 1.344554e-03 
      	| train loss (relative): 2.586641e-02 | valid loss (relative): 2.633957e-02 
Epoch 1574 use: 444.70 second.

epoch 1575 starting......
Epoch:  1575 | train loss: 1.324738e-03 | valid loss: 1.349908e-03 
      	| train loss (relative): 2.588888e-02 | valid loss (relative): 2.642943e-02 
Epoch 1575 use: 433.98 second.

epoch 1576 starting......
Epoch:  1576 | train loss: 1.323706e-03 | valid loss: 1.340735e-03 
      	| train loss (relative): 2.586208e-02 | valid loss (relative): 2.627889e-02 
Epoch 1576 use: 437.49 second.

epoch 1577 starting......
Epoch:  1577 | train loss: 1.321954e-03 | valid loss: 1.345409e-03 
      	| train loss (relative): 2.583171e-02 | valid loss (relative): 2.631258e-02 
Epoch 1577 use: 425.50 second.

epoch 1578 starting......
Epoch:  1578 | train loss: 1.324314e-03 | valid loss: 1.344798e-03 
      	| train loss (relative): 2.587839e-02 | valid loss (relative): 2.638238e-02 
Epoch 1578 use: 433.32 second.

epoch 1579 starting......
Epoch:  1579 | train loss: 1.324865e-03 | valid loss: 1.351452e-03 
      	| train loss (relative): 2.588820e-02 | valid loss (relative): 2.630473e-02 
Epoch 1579 use: 432.54 second.

epoch 1580 starting......
Epoch:  1580 | train loss: 1.323389e-03 | valid loss: 1.344006e-03 
      	| train loss (relative): 2.585698e-02 | valid loss (relative): 2.628278e-02 
Epoch 1580 use: 442.27 second.

epoch 1581 starting......
Epoch:  1581 | train loss: 1.322151e-03 | valid loss: 1.347051e-03 
      	| train loss (relative): 2.583596e-02 | valid loss (relative): 2.648165e-02 
Epoch 1581 use: 446.54 second.

epoch 1582 starting......
Epoch:  1582 | train loss: 1.323913e-03 | valid loss: 1.347464e-03 
      	| train loss (relative): 2.587238e-02 | valid loss (relative): 2.630117e-02 
Epoch 1582 use: 447.78 second.

epoch 1583 starting......
Epoch:  1583 | train loss: 1.326637e-03 | valid loss: 1.351941e-03 
      	| train loss (relative): 2.592114e-02 | valid loss (relative): 2.629688e-02 
Epoch 1583 use: 443.29 second.

epoch 1584 starting......
Epoch:  1584 | train loss: 1.325288e-03 | valid loss: 1.345736e-03 
      	| train loss (relative): 2.589587e-02 | valid loss (relative): 2.636419e-02 
Epoch 1584 use: 459.84 second.

epoch 1585 starting......
Epoch:  1585 | train loss: 1.323178e-03 | valid loss: 1.347265e-03 
      	| train loss (relative): 2.585376e-02 | valid loss (relative): 2.630698e-02 
Epoch 1585 use: 436.95 second.

epoch 1586 starting......
Epoch:  1586 | train loss: 1.325663e-03 | valid loss: 1.344002e-03 
      	| train loss (relative): 2.590322e-02 | valid loss (relative): 2.621391e-02 
Epoch 1586 use: 449.63 second.

epoch 1587 starting......
Epoch:  1587 | train loss: 1.325536e-03 | valid loss: 1.345937e-03 
      	| train loss (relative): 2.589897e-02 | valid loss (relative): 2.628197e-02 
Epoch 1587 use: 440.45 second.

epoch 1588 starting......
Epoch:  1588 | train loss: 1.322484e-03 | valid loss: 1.344392e-03 
      	| train loss (relative): 2.584063e-02 | valid loss (relative): 2.636021e-02 
Epoch 1588 use: 447.86 second.

epoch 1589 starting......
Epoch:  1589 | train loss: 1.321741e-03 | valid loss: 1.344288e-03 
      	| train loss (relative): 2.582614e-02 | valid loss (relative): 2.637076e-02 
Epoch 1589 use: 433.28 second.

epoch 1590 starting......
Epoch:  1590 | train loss: 1.323097e-03 | valid loss: 1.351082e-03 
      	| train loss (relative): 2.585506e-02 | valid loss (relative): 2.642601e-02 
Epoch 1590 use: 439.43 second.

epoch 1591 starting......
Epoch:  1591 | train loss: 1.326083e-03 | valid loss: 1.349389e-03 
      	| train loss (relative): 2.591032e-02 | valid loss (relative): 2.635760e-02 
Epoch 1591 use: 431.09 second.

epoch 1592 starting......
Epoch:  1592 | train loss: 1.325148e-03 | valid loss: 1.346965e-03 
      	| train loss (relative): 2.589029e-02 | valid loss (relative): 2.624286e-02 
Epoch 1592 use: 446.94 second.

epoch 1593 starting......
Epoch:  1593 | train loss: 1.324267e-03 | valid loss: 1.343029e-03 
      	| train loss (relative): 2.587152e-02 | valid loss (relative): 2.633991e-02 
Epoch 1593 use: 431.49 second.

epoch 1594 starting......
Epoch:  1594 | train loss: 1.319909e-03 | valid loss: 1.338430e-03 
      	| train loss (relative): 2.579048e-02 | valid loss (relative): 2.617905e-02 
Epoch 1594 use: 441.20 second.

epoch 1595 starting......
Epoch:  1595 | train loss: 1.321538e-03 | valid loss: 1.343154e-03 
      	| train loss (relative): 2.582224e-02 | valid loss (relative): 2.619504e-02 
Epoch 1595 use: 425.66 second.

epoch 1596 starting......
Epoch:  1596 | train loss: 1.322658e-03 | valid loss: 1.345215e-03 
      	| train loss (relative): 2.584084e-02 | valid loss (relative): 2.629733e-02 
Epoch 1596 use: 453.53 second.

epoch 1597 starting......
Epoch:  1597 | train loss: 1.323510e-03 | valid loss: 1.343179e-03 
      	| train loss (relative): 2.586397e-02 | valid loss (relative): 2.619582e-02 
Epoch 1597 use: 428.37 second.

epoch 1598 starting......
Epoch:  1598 | train loss: 1.320596e-03 | valid loss: 1.342681e-03 
      	| train loss (relative): 2.580202e-02 | valid loss (relative): 2.616968e-02 
Epoch 1598 use: 432.52 second.

epoch 1599 starting......
Epoch:  1599 | train loss: 1.319274e-03 | valid loss: 1.342295e-03 
      	| train loss (relative): 2.577695e-02 | valid loss (relative): 2.616040e-02 
Epoch 1599 use: 434.54 second.

epoch 1600 starting......
Epoch:  1600 | train loss: 1.320972e-03 | valid loss: 1.350029e-03 
      	| train loss (relative): 2.580868e-02 | valid loss (relative): 2.642020e-02 
Epoch 1600 use: 433.18 second.

epoch 1601 starting......
Epoch:  1601 | train loss: 1.324086e-03 | valid loss: 1.344810e-03 
      	| train loss (relative): 2.586896e-02 | valid loss (relative): 2.622202e-02 
Epoch 1601 use: 444.48 second.

epoch 1602 starting......
Epoch:  1602 | train loss: 1.325156e-03 | valid loss: 1.348788e-03 
      	| train loss (relative): 2.589214e-02 | valid loss (relative): 2.635897e-02 
Epoch 1602 use: 435.03 second.

epoch 1603 starting......
Epoch:  1603 | train loss: 1.324035e-03 | valid loss: 1.359112e-03 
      	| train loss (relative): 2.586742e-02 | valid loss (relative): 2.677213e-02 
Epoch 1603 use: 436.97 second.

epoch 1604 starting......
Epoch:  1604 | train loss: 1.328213e-03 | valid loss: 1.358140e-03 
      	| train loss (relative): 2.596045e-02 | valid loss (relative): 2.653918e-02 
Epoch 1604 use: 432.99 second.

epoch 1605 starting......
Epoch:  1605 | train loss: 1.324571e-03 | valid loss: 1.341505e-03 
      	| train loss (relative): 2.588048e-02 | valid loss (relative): 2.615064e-02 
Epoch 1605 use: 436.61 second.

epoch 1606 starting......
Epoch:  1606 | train loss: 1.318627e-03 | valid loss: 1.341271e-03 
      	| train loss (relative): 2.576150e-02 | valid loss (relative): 2.614669e-02 
Epoch 1606 use: 448.51 second.

epoch 1607 starting......
Epoch:  1607 | train loss: 1.320103e-03 | valid loss: 1.345971e-03 
      	| train loss (relative): 2.579154e-02 | valid loss (relative): 2.640679e-02 
Epoch 1607 use: 423.75 second.

epoch 1608 starting......
Epoch:  1608 | train loss: 1.319931e-03 | valid loss: 1.342543e-03 
      	| train loss (relative): 2.579012e-02 | valid loss (relative): 2.619641e-02 
Epoch 1608 use: 432.14 second.

epoch 1609 starting......
Epoch:  1609 | train loss: 1.319283e-03 | valid loss: 1.344790e-03 
      	| train loss (relative): 2.577596e-02 | valid loss (relative): 2.627575e-02 
Epoch 1609 use: 421.89 second.

epoch 1610 starting......
Epoch:  1610 | train loss: 1.321211e-03 | valid loss: 1.346270e-03 
      	| train loss (relative): 2.581259e-02 | valid loss (relative): 2.635644e-02 
Epoch 1610 use: 432.52 second.

epoch 1611 starting......
Epoch:  1611 | train loss: 1.321483e-03 | valid loss: 1.346322e-03 
      	| train loss (relative): 2.581955e-02 | valid loss (relative): 2.634563e-02 
Epoch 1611 use: 430.92 second.

epoch 1612 starting......
Epoch:  1612 | train loss: 1.320958e-03 | valid loss: 1.342575e-03 
      	| train loss (relative): 2.580878e-02 | valid loss (relative): 2.617484e-02 
Epoch 1612 use: 430.92 second.

epoch 1613 starting......
Epoch:  1613 | train loss: 1.316017e-03 | valid loss: 1.343408e-03 
      	| train loss (relative): 2.570935e-02 | valid loss (relative): 2.619569e-02 
Epoch 1613 use: 438.56 second.

epoch 1614 starting......
Epoch:  1614 | train loss: 1.321325e-03 | valid loss: 1.345601e-03 
      	| train loss (relative): 2.581636e-02 | valid loss (relative): 2.615475e-02 
Epoch 1614 use: 430.89 second.

epoch 1615 starting......
Epoch:  1615 | train loss: 1.319311e-03 | valid loss: 1.345598e-03 
      	| train loss (relative): 2.577381e-02 | valid loss (relative): 2.635537e-02 
Epoch 1615 use: 419.42 second.

epoch 1616 starting......
Epoch:  1616 | train loss: 1.321037e-03 | valid loss: 1.347355e-03 
      	| train loss (relative): 2.580933e-02 | valid loss (relative): 2.631159e-02 
Epoch 1616 use: 435.67 second.

epoch 1617 starting......
Epoch:  1617 | train loss: 1.321805e-03 | valid loss: 1.344273e-03 
      	| train loss (relative): 2.582548e-02 | valid loss (relative): 2.631753e-02 
Epoch 1617 use: 437.75 second.

epoch 1618 starting......
Epoch:  1618 | train loss: 1.320938e-03 | valid loss: 1.343108e-03 
      	| train loss (relative): 2.580803e-02 | valid loss (relative): 2.618355e-02 
Epoch 1618 use: 440.57 second.

epoch 1619 starting......
Epoch:  1619 | train loss: 1.318437e-03 | valid loss: 1.347265e-03 
      	| train loss (relative): 2.576119e-02 | valid loss (relative): 2.627142e-02 
Epoch 1619 use: 418.64 second.

epoch 1620 starting......
Epoch:  1620 | train loss: 1.318683e-03 | valid loss: 1.347205e-03 
      	| train loss (relative): 2.576024e-02 | valid loss (relative): 2.634291e-02 
Epoch 1620 use: 442.22 second.

epoch 1621 starting......
Epoch:  1621 | train loss: 1.321594e-03 | valid loss: 1.349445e-03 
      	| train loss (relative): 2.582044e-02 | valid loss (relative): 2.640884e-02 
Epoch 1621 use: 424.01 second.

epoch 1622 starting......
Epoch:  1622 | train loss: 1.321747e-03 | valid loss: 1.343207e-03 
      	| train loss (relative): 2.582387e-02 | valid loss (relative): 2.620964e-02 
Epoch 1622 use: 429.33 second.

epoch 1623 starting......
Epoch:  1623 | train loss: 1.318238e-03 | valid loss: 1.343480e-03 
      	| train loss (relative): 2.575531e-02 | valid loss (relative): 2.620046e-02 
Epoch 1623 use: 421.36 second.

epoch 1624 starting......
Epoch:  1624 | train loss: 1.316852e-03 | valid loss: 1.342707e-03 
      	| train loss (relative): 2.572471e-02 | valid loss (relative): 2.622756e-02 
Epoch 1624 use: 433.42 second.

epoch 1625 starting......
Epoch:  1625 | train loss: 1.318013e-03 | valid loss: 1.347180e-03 
      	| train loss (relative): 2.574533e-02 | valid loss (relative): 2.642404e-02 
Epoch 1625 use: 427.37 second.

epoch 1626 starting......
Epoch:  1626 | train loss: 1.323402e-03 | valid loss: 1.346319e-03 
      	| train loss (relative): 2.585842e-02 | valid loss (relative): 2.621429e-02 
Epoch 1626 use: 433.55 second.

epoch 1627 starting......
Epoch:  1627 | train loss: 1.317539e-03 | valid loss: 1.340028e-03 
      	| train loss (relative): 2.574090e-02 | valid loss (relative): 2.617653e-02 
Epoch 1627 use: 435.09 second.

epoch 1628 starting......
Epoch:  1628 | train loss: 1.313836e-03 | valid loss: 1.340468e-03 
      	| train loss (relative): 2.566746e-02 | valid loss (relative): 2.622872e-02 
Epoch 1628 use: 433.46 second.

epoch 1629 starting......
Epoch:  1629 | train loss: 1.316536e-03 | valid loss: 1.342693e-03 
      	| train loss (relative): 2.572084e-02 | valid loss (relative): 2.620215e-02 
Epoch 1629 use: 424.09 second.

epoch 1630 starting......
Epoch:  1630 | train loss: 1.317459e-03 | valid loss: 1.342893e-03 
      	| train loss (relative): 2.574218e-02 | valid loss (relative): 2.630867e-02 
Epoch 1630 use: 430.17 second.

epoch 1631 starting......
Epoch:  1631 | train loss: 1.318419e-03 | valid loss: 1.342013e-03 
      	| train loss (relative): 2.576004e-02 | valid loss (relative): 2.617842e-02 
Epoch 1631 use: 414.16 second.

epoch 1632 starting......
Epoch:  1632 | train loss: 1.316496e-03 | valid loss: 1.343550e-03 
      	| train loss (relative): 2.571847e-02 | valid loss (relative): 2.629630e-02 
Epoch 1632 use: 438.77 second.

epoch 1633 starting......
Epoch:  1633 | train loss: 1.315930e-03 | valid loss: 1.341754e-03 
      	| train loss (relative): 2.571067e-02 | valid loss (relative): 2.608024e-02 
Epoch 1633 use: 419.82 second.

epoch 1634 starting......
Epoch:  1634 | train loss: 1.318135e-03 | valid loss: 1.351823e-03 
      	| train loss (relative): 2.574784e-02 | valid loss (relative): 2.655470e-02 
Epoch 1634 use: 427.18 second.

epoch 1635 starting......
Epoch:  1635 | train loss: 1.319342e-03 | valid loss: 1.342684e-03 
      	| train loss (relative): 2.577870e-02 | valid loss (relative): 2.617173e-02 
Epoch 1635 use: 422.06 second.

epoch 1636 starting......
Epoch:  1636 | train loss: 1.316454e-03 | valid loss: 1.345741e-03 
      	| train loss (relative): 2.571838e-02 | valid loss (relative): 2.633834e-02 
Epoch 1636 use: 425.05 second.

epoch 1637 starting......
Epoch:  1637 | train loss: 1.317672e-03 | valid loss: 1.343344e-03 
      	| train loss (relative): 2.574153e-02 | valid loss (relative): 2.616524e-02 
Epoch 1637 use: 421.11 second.

epoch 1638 starting......
Epoch:  1638 | train loss: 1.314806e-03 | valid loss: 1.340597e-03 
      	| train loss (relative): 2.568417e-02 | valid loss (relative): 2.615489e-02 
Epoch 1638 use: 421.99 second.

epoch 1639 starting......
Epoch:  1639 | train loss: 1.315359e-03 | valid loss: 1.341781e-03 
      	| train loss (relative): 2.569967e-02 | valid loss (relative): 2.621707e-02 
Epoch 1639 use: 421.18 second.

epoch 1640 starting......
Epoch:  1640 | train loss: 1.315479e-03 | valid loss: 1.343678e-03 
      	| train loss (relative): 2.569934e-02 | valid loss (relative): 2.621126e-02 
Epoch 1640 use: 430.68 second.

epoch 1641 starting......
Epoch:  1641 | train loss: 1.317497e-03 | valid loss: 1.341902e-03 
      	| train loss (relative): 2.573833e-02 | valid loss (relative): 2.613770e-02 
Epoch 1641 use: 418.75 second.

epoch 1642 starting......
Epoch:  1642 | train loss: 1.315346e-03 | valid loss: 1.344953e-03 
      	| train loss (relative): 2.569424e-02 | valid loss (relative): 2.625700e-02 
Epoch 1642 use: 427.09 second.

epoch 1643 starting......
Epoch:  1643 | train loss: 1.316722e-03 | valid loss: 1.345789e-03 
      	| train loss (relative): 2.572079e-02 | valid loss (relative): 2.641771e-02 
Epoch 1643 use: 426.13 second.

epoch 1644 starting......
Epoch:  1644 | train loss: 1.316470e-03 | valid loss: 1.339866e-03 
      	| train loss (relative): 2.571911e-02 | valid loss (relative): 2.615533e-02 
Epoch 1644 use: 426.87 second.

epoch 1645 starting......
Epoch:  1645 | train loss: 1.315345e-03 | valid loss: 1.346212e-03 
      	| train loss (relative): 2.569682e-02 | valid loss (relative): 2.633135e-02 
Epoch 1645 use: 418.16 second.

epoch 1646 starting......
Epoch:  1646 | train loss: 1.318860e-03 | valid loss: 1.345978e-03 
      	| train loss (relative): 2.576723e-02 | valid loss (relative): 2.630155e-02 
Epoch 1646 use: 422.59 second.

epoch 1647 starting......
Epoch:  1647 | train loss: 1.313595e-03 | valid loss: 1.340201e-03 
      	| train loss (relative): 2.566156e-02 | valid loss (relative): 2.623004e-02 
Epoch 1647 use: 427.60 second.

epoch 1648 starting......
Epoch:  1648 | train loss: 1.315219e-03 | valid loss: 1.347407e-03 
      	| train loss (relative): 2.569294e-02 | valid loss (relative): 2.627624e-02 
Epoch 1648 use: 425.99 second.

epoch 1649 starting......
Epoch:  1649 | train loss: 1.317351e-03 | valid loss: 1.338786e-03 
      	| train loss (relative): 2.573807e-02 | valid loss (relative): 2.613302e-02 
Epoch 1649 use: 417.18 second.

epoch 1650 starting......
Epoch:  1650 | train loss: 1.312607e-03 | valid loss: 1.337900e-03 
      	| train loss (relative): 2.564347e-02 | valid loss (relative): 2.615435e-02 
Epoch 1650 use: 430.13 second.

test MSE Error: 1.343472e-03 | relative MSE Error: 2.625770e-02 
 Total time used for training: 18.47 hour.
MESLoss saved to  /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_MSELoss_Latent_64_nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_1651.txt
relative MSELoss saved to  /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_reMSELoss_Latent_64_nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_1651.txt
model saved to /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_64_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_1651.pth
model_dict saved to /rds/general/user/jy220/home/new_results/Variational_False_Changelr_False_Latent_64_Nearest_neighbouring_True_SFC_nums_3_startlr_0.001_n_epoches_1651_dict.pth
... Training slugflow data group 3 completed, Run finished Thu 26 Aug 05:28:56 BST 2021 ...
